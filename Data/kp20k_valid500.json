[{"id": "0", "title": "Real-Time Data Aggregation in Contention-Based Wireless Sensor Networks", "abstract": "We investigate the problem of delay constrained maximal information collection for CSMA-based wireless sensor networks. We study how to allocate the maximal allowable transmission delay at each node, such that the amount of information collected at the sink is maximized and the total delay for the data aggregation is within the given bound. We formulate the problem by using dynamic programming and propose an optimal algorithm for the optimal assignment of transmission attempts. Based on the analysis of the optimal solution, we propose a distributed greedy algorithm. It is shown to have a similar performance as the optimal one.", "keywords": ["algorithms", "design", "performance", "sensor networks", "data aggregation", "real-time traffic", "csma/ca", "delay constrained transmission"]}, {"id": "1", "title": "word sense disambiguation for event trigger word detection", "abstract": "This paper describes a method for detecting event trigger words in biomedical text based on a word sense disambiguation (WSD) approach. We first investigate the applicability of existing WSD techniques to trigger word disambiguation in the BioNLP 2009 shared task data, and find that we are able to outperform a traditional CRF-based approach for certain word-types. On the basis of this finding, we combine the WSD approach with the CRF, and obtain significant improvements over the standalone CRF, gaining particularly in recall.", "keywords": ["biomedical text", "machine learning", "information extraction"]}, {"id": "2", "title": "composing architectural aspects based on style semantics", "abstract": "The lack of architecturally-significant mechanisms for aspectual composition might artificially hinder the specification of stable and reusable design aspects. Current aspect-oriented approaches at the architecture-level tend to mimic programming language join point models while overlooking mainstream architectural concepts such as styles and their semantics. Syntax-based pointcuts are typically used to select join points based on the names of architectural elements, exposing architecture descriptions to pointcut fragility and reusability problems. This paper presents style-based composition, a new flavor of aspect composition at the architectural level based on architectural styles. We propose style-based join point models and provide a pointcut language that supports the selection of join points based on style-constrained architectural models. Stability and reusability assessments of the proposed style-based composition model were carried out through three case studies involving different styles. The interplay of style-based pointcuts and some style composition techniques is also discussed.", "keywords": ["architectural styles", "architectural aspects", "pointcut languages", "style-based composition"]}, {"id": "3", "title": "using pen-based computers across the computer science curriculum", "abstract": "This paper describes our use of pen-based electronic classrooms to enhance several computer science courses. After presenting our motivation for undertaking this work, and its relevance to the growing interest in using tablet PC's in the classroom, we present an overview of our use of this technology to engage students during class. Finally, we present the students' reaction to the approach as measured through attitude surveys and a focus group.", "keywords": ["computer science", "present", "groupware", "use", "technologies", "pen", "pen-based computing", "motivation", "survey", "tablet", "computation", "tablet pcs", "relevance", "computer science curriculum", "paper", "focus-group", "attitude", "collaborative computing", "class", "student"]}, {"id": "4", "title": "On the syntactic and functional correspondence between hybrid (or layered) normalisers and abstract machines", "abstract": "We show how to connect the syntactic and the functional correspondence for normalisers and abstract machines implementing hybrid (or layered) reduction strategies, that is, strategies that depend on subsidiary sub-strategies. Many fundamental strategies in the literature are hybrid, in particular, many full-reducing strategies, and many full-reducing and complete strategies that deliver a fully reduced result when it exists. If we follow the standard program-transformation steps the abstract machines obtained for hybrids after the syntactic correspondence cannot be refunctionalised, and the junction with the functional correspondence is severed. However, a solution is possible based on establishing the shape invariant of well-formed continuation stacks. We illustrate the problem and the solution with the derivation of substitution-based normalisers for normal order, a hybrid, full-reducing, and complete strategy of the pure lambda calculus. The machine we obtain is a substitution-based, eval/apply, open-terms version of Pierre Crgut's full-reducing Krivine machine KN.", "keywords": ["operational semantics", "program transformation", "reduction strategies", "abstract machines", "full reduction"]}, {"id": "5", "title": "A case-based method for service-oriented value chain and sustainable network design", "abstract": "The purpose of this research is to present a case-based analytic method for a service-oriented value chain and a sustainable network design considering customer, environmental and social values. Enterprises can enhance competitive advantage by providing more values to all stakeholders in the network. Our model employs a stylized database to identify successful cases of value chain application under similar company marketing conditions, illustrating potential value chains and sustainable networks as references. This work first identifies economic benefits, environmental friendliness and social contribution values based on prior studies. Next, a search engine which is developed based on the rough set theory will search and map similarities to find similar or parallel cases in the database. Finally, a visualized network mapping will be automatically generated to possible value chains. This study applies a case-based methodology to assist enterprises in developing a service-oriented value chain design. For decision makers, this can reduce survey time and inspire innovative works based on previous successful experience. Besides, successful ideas from prior cases can be reused. In addition to customer values, this methodology incorporates environment and social values that may encourage a company to build their value chain in a more comprehensive and sustainable manner. This is a pilot study which attempts to utilize computer-aided methodology to assist in service or value-related design. The pertinent existing solutions can be filtered from an array of cases to engage the advantages from both product-oriented and service-oriented companies. Finally, the visualized display of value network is formed to illustrate the results. A customized service-oriented value chains which incorporates environment and social values can be designed according to different conditions. Also, this system engages the advantages from both product-oriented and service-oriented companies to build a more comprehensive value network. Apart from this, the system can be utilized as a benchmarking tool, and it could remind the decision makers to consider potential value from a more multifaceted perspective. This is the first paper that applied a computer-aided method to design service-oriented value chains. This work also can serve as a decision support and benchmarking system because decision makers can develop different value networks according to various emphasized values. Finally, the visualized display of value network can improve the communication among stakeholders.", "keywords": ["value chain design", "sustainable network design", "case-based reasoning"]}, {"id": "6", "title": "volume subdivision based hexahedral finite element meshing of domains with interior 2-manifold boundaries", "abstract": "We present a subdivision based algorithm for multi-resolution Hexahedral meshing. The input is a bounding rectilinear domain with a set of embedded 2-manifold boundaries of arbitrary genus and topology. The algorithm first constructs a simplified Voronoi structure to partition the object into individual components that can be then meshed separately. We create a coarse hexahedral mesh for each Voronoi cell giving us an initial hexahedral scaffold. Recursive hexahedral subdivision of this hexahedral scaffold yields adaptive meshes. Splitting and Smoothing the boundary cells makes the mesh conform to the input 2-manifolds. Our choice of smoothing rules makes the resulting boundary surface of the hexahedral mesh as C 2 continuous in the limit (C 1 at extra-ordinary points), while also keeping a definite bound on the condition number of the Jacobian of the hexahedral mesh elements. By modifying the crease smoothing rules, we can also guarantee that the sharp features in the data are captured. Subdivision guarantees that we achieve a very good approximation for a given tolerance, with optimal mesh elements for each Level of Detail (LoD).", "keywords": ["hexahedral meshing", "mesh generation", "subdivision meshes", "3d meshing"]}, {"id": "7", "title": "Methods for evaluating and creating data quality", "abstract": "This paper provides a survey of two classes of methods that can be used in determining and improving the quality of individual files or groups of files. The first are edit/imputation methods for maintaining business rules and for imputing for missing data. The second are methods of data cleaning for finding duplicates within files or across files.", "keywords": ["integer programming", "set covering", "data cleaning", "approximate string comparison", "unsupervised and supervised learning"]}, {"id": "8", "title": "Nonlinear magnetostatic BEM formulation using one unknown double layer charge", "abstract": "Purpose - The purpose of this paper is to solve generic magnetostatic problems by BEM, by studying how to use a boundary integral equation (BIE) with the double layer charge as unknown derived from the scalar potential. Design/methodology/approach - Since the double layer charge produces only the potential gap without disturbing the normal magnetic flux density, the field is accurately formulated even by one BIE with one unknown. Once the double layer charge is determined, Biot-Savart's law gives easily the magnetic flux density. Findings - The BIE using double layer charge is capable of treating robustly geometrical singularities at edges and corners. It is also capable of solving the problems with extremely high magnetic permeability. Originality/value - The proposed BIE contains only the double layer charge while the conventional equations derived from the scalar potential contain the single and double layer charges as unknowns. In the multiply connected problems, the excitation potential in the material is derived from the magnetomotive force to represent the circulating fields due to multiply connected exciting currents.", "keywords": ["boundary integral equation", "double layer charge", "multiply connected problem", "nonlinear magnetostatic analysis", "scalar potential", "integral equations", "electric current"]}, {"id": "9", "title": "Mathematical modeling of electrical activity of uterine muscle cells", "abstract": "The uterine electrical activity is an efficient parameter to study the uterine contractility. In order to understand the ionic mechanisms responsible for its generation, we aimed at building a mathematical model of the uterine cell electrical activity based upon the physiological mechanisms. First, based on the voltage clamp experiments found in the literature, we focus on the principal ionic channels and their cognate currents involved in the generation of this electrical activity. Second, we provide the methodology of formulations of uterine ionic currents derived from a wide range of electrophysiological data. The model is validated step by step by comparing simulated voltage-clamp results with the experimental ones. The model reproduces successfully the generation of single spikes or trains of action potentials that fit with the experimental data. It allows analyzing ionic channels implications. Likewise, the calcium-dependent conductance influences significantly the cellular oscillatory behavior.", "keywords": ["myometrial ionic currents", "uterine excitability", "voltage clamp", "action potential", "electrophysiological model"]}, {"id": "10", "title": "Sweep synchronization as a global propagation mechanism", "abstract": "This paper presents a new generic filtering algorithm which simultaneously considers n conjunctions of constraints as well as those constraints mentioning some variables Yk Y k of the pairs X , Y k ( 1 ? k ? n ) occurring in these conjunctions. The main benefit of this new technique comes from the fact that, for adjusting the bounds of a variable X according to n conjunctions, we do not perform n sweeps in an independent way but rather synchronize them. We then specialize this technique to the non-overlapping rectangles constraint where we consider the case where several rectangles of height one have the same X coordinate for their origin as well as the same length. For this specific constraint we come up with an incremental bipartite matching algorithm which is triggered while we sweep over the time axis. We illustrate the usefulness of this new pruning method on a timetabling problem, where each task cannot be interrupted and requires the simultaneous availability of n distinct persons. In addition each person has his own periods of unavailability and can only perform one task at a time.", "keywords": ["global constraint", "filtering algorithm", "sweep", "timetabling"]}, {"id": "11", "title": "A structural approach to reversible computation", "abstract": "Reversibility is a key issue in the interface between computation and physics, and of growing importance as miniaturization progresses towards its physical limits. Most foundational work on reversible computing to date has focussed on simulations of low-level machine models. By contrast, we develop a more structural approach. We show how high-level functional programs can be mapped compositionally (i.e. in a syntax-directed fashion) into a simple kind of automata which are immediately seen to be reversible. The size of the automaton is linear in the size of the functional term. In mathematical terms, we are building a concrete model of functional computation. This construction stems directly from ideas arising in Geometry of Interaction and Linear Logic-but can be understood without any knowledge of these topics. In fact, it serves as an excellent introduction to them. At the same time, an interesting logical delineation between reversible and irreversible forms of computation emerges from our analysis.  ", "keywords": ["reversible computation", "linear combinatory algebra", "term-rewriting", "automata", "geometry of interaction"]}, {"id": "12", "title": "UBL: The DNA of next generation e-Business", "abstract": "This paper introduces into the evolution of Electronic Data Interchange (EDI) and the Universal Business Language (UBL). an OASIS standard to encode and customize business documents. It shows its peculiarities and also sets it into a broader picture showing where UBL is positioned in relationship to business processes and standards like BPEL and BPMN.", "keywords": ["universal business language ", "electronic data interchange ", "e-business"]}, {"id": "13", "title": "K-GENI testbed deployment and federated meta operations experiment over GENI and KREONET", "abstract": "The classical Internet has confronted many drawbacks in terms of network security, scalability, and performance, although it has strongly influenced the development and evolution of diverse network technologies, applications, and services. Therefore, new innovative research on the Future Internet has been performed to resolve the inherent weaknesses of the traditional Internet, which, in turn, requires new at-scale network testbeds and research infrastructure for large-scale experiments. In this context, K-GENI has been developed as an international programmable Future Internet testbed in the GENI spiral-2 program, and it has been operational between the USA (GENI) and Korea (KREONET) since 2010. The K-GENI testbed and the related collaborative efforts will be introduced with two major topics in this paper: (1) the design and deployment of the K-GENI testbed and (2) the federated meta operations between the K-GENI and GENI testbeds. Regarding the second topic in particular, we will describe how meta operations are federated across K-GENI between GMOC (GENI Meta Operations Center) and DvNOC (Distributed virtual Network Operations Center on KREONET/K-GENI), which is the first trial of an international experiment on the federated network operations over GENI.", "keywords": ["geni", "k-geni", "kreonet", "federation", "dvnoc"]}, {"id": "14", "title": "Conceptions and modeling for transmitted information evaluation by ANN", "abstract": "In this paper, the main measure, an amount of information, of the information theory is analyzed and corrected. The three conceptions of the theory on the microstate, dissipation pathways, and self-organization levels with a tight connection to the statistical physics are discussed. The concepts of restricted information were introduced as well as the proof of uniqueness of the entropy function, when the probabilities are rational numbers, is presented. The artificial neural network (ANN) model for mapping the evaluation of transmitted information has been designed and experimentally approbated in the biological area.", "keywords": ["information theory", "entropy", "amount of information", "artificial neural networks"]}, {"id": "15", "title": "FMESP: Framework for the modeling and evaluation of software processes", "abstract": "Nowadays, organizations face with a very high competitiveness and for this reason they have to continuously improve their processes. Two key aspects to be considered in the software processes management in order to promote their improvement are their effective modeling and evaluation. The integrated management of these key aspects is not a trivial task, the huge number and diversity of elements to take into account makes it complex the management of software processes. To ease and effectively support this management, in this paper we propose FMESP: a framework for the integrated management of the modeling and measurement of software processes. FMESP incorporates the conceptual and technological elements necessary to ease the integrated management of the definition and evaluation of software processes. From the measurement perspective of the framework and in order to provide the support for the software process measurement at model level a set of representative measures have been defined and validated.", "keywords": ["software process modeling", "software measurement", "conceptual framework", "software engineering environment"]}, {"id": "16", "title": "ANTICIPATED FUNCTION SYNCHRONIZATION WITH UNKNOWN PARAMETERS OF DISCRETE-TIME CHAOTIC SYSTEMS", "abstract": "In this paper, firstly, the control problem for the chaos synchronization of discrete-time chaotic (hyperchaotic) systems with unknown parameters are considered. Next, back-stepping control law is derived to make the error signals between drive 2D discrete-time chaotic system and response 2D discrete-time chaotic system with two uncertain parameters asymptotically synchronized. Finally, the approach is extended to the synchronization problem for 3D discrete-time chaotic system with two unknown parameters. Numerical simulations are presented to show the effectiveness of the proposed chaos synchronization scheme.", "keywords": ["anticipated function synchronization", "backstepping design", "fold maps", "henon maps"]}, {"id": "17", "title": "Methods of assessing spinal radiographs in scoliosis are functions of its geometry", "abstract": "The most important feature of scoliosis is the lateral curvature of the spine. It can be treated either conservatively or by surgery; however, treatment choice depends mainly on curve progression which is determined by frequent curve assessment. This is a review of methods of curve measurement and proof of the relationship between them.", "keywords": ["scoliosis", "curve progression", "curve measurement"]}, {"id": "18", "title": "Feedback control of natural convection", "abstract": "An applicable method is developed for the identification and feedback control of natural convection. The Boussinesq equation is reduced to a small set of ordinary differential equations by means of the KarhunenLove Galerkin procedure [Int. J. Heat Mass Transfer 39 (1996) 3311]. Based on this low-dimensional dynamic model, a feedback control synthesis is constructed by first performing an extended Kalman filter estimate of the velocity and temperature fields to treat the measurement errors and then developing the optimal feedback law by means of the linear quadratic regulator theory. The present method allows for the practical implementation of modern control concepts to many flow systems including natural convection.", "keywords": ["karhunenlove galerkin procedure", "feedback control", "natural convection"]}, {"id": "19", "title": "Demand assigned capacity management (DACM) in IP over optical (IPO) networks", "abstract": "The demand assigned capacity management (DACM) problem in IP over optical (IPO) network aims at devising efficient bandwidth replenishment schedules from the optical domain conditioned upon traffic evolution processes in the IP domain. A replenishment schedule specifies the location, sizing, and sequencing of link capacity expansions to support the growth of Internet traffic demand in the IP network subject to economic considerations. A major distinction in the approach presented in this paper is the focus of attention on the economics of \"excess bandwidth\" in the IP domain, which can be viewed as an inventory system that is endowed with fixed and variable costs and depletes with increase in IP traffic demand requiring replenishment from the optical domain. We, develop mathematical models to address the DACM problem in IPO networks based on a class of inventory management replenishment methods. We apply the technique to IPO networks that implement capacity adaptive routing in the IP domain and networks without capacity adaptive routing. We analyze the performance characteristics under both scenarios, in terms of minimizing cumulative replenishment cost over an interval of time. For the non-capacity adaptive routing scenario, we consider a shortest path approach in the IP domain, specifically OSPF. For the capacity adaptive scenario, we use an online constraint-based routing scheme. This study. represents an application of integrated traffic engineering which concerns collaborative decision making targeted towards network performance improvement that takes into consideration traffic demands, control capabilities, and network assets at different levels in the network hierarchy.", "keywords": ["ason", "bandwidth replenishment", "capacity management", "demand assigned capacity management", "gmpls", "integrated traffic engineering", "inventory management", "ip over optical networks", "ipo", "mpls", "network performance optimization", "traffic engineering"]}, {"id": "20", "title": "Parallel algorithm for finding modules of large-scale coherent fault trees", "abstract": "We propose a new parallel algorithm to find all modules of a large fault tree. An experiment is used to compare the linear time algorithm and parallel algorithm. The result shows that our method is efficient in handling large-scale fault trees.", "keywords": ["modularization", "parallel algorithm", "fault tree", "directed acyclic graph"]}, {"id": "21", "title": "Finding relevant clustering directions in high-dimensional data using Particle Swarm Optimization", "abstract": "A method based on Particle Swarm Optimization (PSO) is proposed and described for finding subspaces that carry meaningful information about the presence of groups in high-dimensional data sets. The advantage of using PSO is that not only the variables that are responsible for the main data structure are identified but also other subspaces corresponding to local optima. The characteristics of the method are shown on two simulated data sets and on a real matrix coming from the analysis of genomic microarrays. In all cases, PSO allowed to explore different subspaces and to discover meaningful structures in the analyzed data. ", "keywords": ["variable selection", "clustering", "particle swarm optimization ", "swarm intelligence"]}, {"id": "22", "title": "Neural network learning of optimal Kalman prediction and control", "abstract": "Although there are many neural network (NN) algorithms for prediction and for control, and although methods for optimal estimation (including filtering and prediction) and for optimal control in linear systems were provided by Kalman in 1960 (with nonlinear extensions since then), there has been, to my knowledge, no NN algorithm that learns either Kalman prediction or Kalman control (apart from the special case of stationary control). Here we show how optimal Kalman prediction and control (KPC), as well as system identification, can be learned and executed by a recurrent neural network composed of linear-response nodes, using as input only a stream of noisy measurement data. The requirements of KPC appear to impose significant constraints on the allowed NN circuitry and signal flows. The NN architecture implied by these constraints bears certain resemblances to the local-circuit architecture of mammalian cerebral cortex. We discuss these resemblances, as well as caveats that limit our current ability to draw inferences for biological function. It has been suggested that the local cortical circuit (LCC) architecture may perform core functions (as yet unknown) that underlie sensory, motor, and other cortical processing. It is reasonable to conjecture that such functions may include prediction, the estimation or inference of missing or noisy sensory data, and the goal-driven generation of control signals. The resemblances found between the KPC NN architecture and that of the LCC are consistent with this conjecture.", "keywords": ["kalman filter", "kalman control", "recurrent neural network", "local cortical circuit"]}, {"id": "23", "title": "Learning point-to-point movements on an elastic limb using dynamic movement primitives", "abstract": "Expansion of the DMP approach for gravitation compensation in elastic robots. Grid-based mixture approach based on bilinear interpolation of learned trajectories. Model-free gravitation compensation in directed limb movements.", "keywords": ["passive compliance", "compliant robotics", "movement primitives", "reinforcement learning", "robot arm", "directed limb movement"]}, {"id": "24", "title": "Audio dual watermarking scheme for copyright protection and content authentication", "abstract": "We propose a new multipurpose audio watermarking scheme in which two watermarks are used. For intellectual property protection, audio clip is divided into frames and robust watermark is embedded. At the same time, the feature of each frame is extracted, and it is quantized as semi-fragile watermark. Then, the frame is cut into sections and the semi-fragile watermark bits are embedded into these sections. For content authentication, the semi-fragile watermark extracted from each frame is compared with the watermark generated from the same frame to judge whether the watermarked audio is tampered, and locate the tampered position. Experimental results show that our scheme is inaudibility. The two watermark schemes are all robust to common signal processing operations such as additive noise, resampling, re-quantization and low-pass filtering, and the semi-fragile watermark scheme can achieve tampered detection and location.", "keywords": ["multipurpose audio watermarking", "robust watermark", "copyright protection", "semi-fragile watermark", "content authentication"]}, {"id": "25", "title": "A comparison of path planning strategies for autonomous exploration and mapping of unknown environments", "abstract": "To date, a large number of algorithms to solve the problem of autonomous exploration and mapping has been presented. However, few efforts have been made to compare these techniques. In this paper, an extensive study of the most important methods for autonomous exploration and mapping of unknown environments is presented. Furthermore, a representative subset of these techniques has been chosen to be analysed. This subset contains methods that differ in the level of multi-robot coordination and in the grade of integration with the simultaneous localization and mapping (SLAM) algorithm. These exploration techniques were tested in simulation and compared using different criteria as exploration time or map quality. The results of this analysis are shown in this paper. The weaknesses and strengths of each strategy have been stated and the most appropriate algorithm for each application has been determined.", "keywords": ["autonomous exploration", "mapping of unknown environments", "path planning for multiple mobile robot systems"]}, {"id": "26", "title": "Improving reliable multicast using active parity encoding services", "abstract": "We propose and evaluate novel reliable multicast protocols that combine active repair service (a.k.a. local recovery) and parity encoding (a.k.a. forward error correction or FEC) techniques. We show that, compared to other repair service protocols, our protocols require less buffer inside the network, maintain the low bandwidth requirements of previously proposed repair service/FEC combination protocols, and reduce the amount of FEC processing at repair servers, moving more of this processing to the end-hosts. We also examine repair service/FEC combination protocols in an environment where loss rates differ across domains within the network. We find that repair services are more effective than FEC at reducing bandwidth utilization in such environments. Furthermore, we show that adding FEC to a repair services protocol not only reduces buffer requirements at repair servers, but also reduces bandwidth utilization in domains with high loss, or in domains with large populations of receivers.", "keywords": ["reliable multicast", "forward error correction", "repair services", "active services", "performance analysis"]}, {"id": "27", "title": "A comparative study on concept drift detectors", "abstract": "We evaluated eight different concept drift detectors. A 2k factorial design was used to indicate the best parameters for each method. Tests compared accuracy, evaluation time, false alarm and miss detection rates. A Mahalanobis distance is proposed as a metric to compare drift methods. DDM was the method that presented the best average results in all tested datasets.", "keywords": ["data streams", "time-changing data", "concept drift detectors", "comparison"]}, {"id": "28", "title": "automatic photo pop-up", "abstract": "This paper presents a fully automatic method for creating a 3D model from a single photograph. The model is made up of several texture-mapped planar billboards and has the complexity of a typical children's pop-up book illustration. Our main insight is that instead of attempting to recover precise geometry, we statistically model geometric classes defined by their orientations in the scene. Our algorithm labels regions of the input image into coarse categories: \"ground\", \"sky\", and \"vertical\". These labels are then used to \"cut and fold\" the image into a pop-up model using a set of simple assumptions. Because of the inherent ambiguity of the problem and the statistical nature of the approach, the algorithm is not expected to work on every image. However. it performs surprisingly well for a wide range of scenes taken from a typical person's photo album.", "keywords": ["single-view reconstruction", "image-based rendering", "machine learning", "image segmentation"]}, {"id": "29", "title": "H and H2 stabilisers via static output feedback based on coordinate transformations with free variables", "abstract": "This article designs H and H2 stabilisers, respectively, for linear time-invariant systems via static output feedback (SOF). A state coordinate transformation of controlled system generates a dummy system with lower dimension, which cannot be directly influenced by the SOF stabiliser. Then the H (H2) stabiliser via SOF may be obtained by solving proper linear matrix inequality (LMI). This LMI is feasible only if the dummy system has a state feedback stabiliser with the same H (H2) index. Meanwhile, a free matrix variable in coordinate transformation can act as the state feedback gain matrix. Hence after the design of dummy system, the SOF stabiliser can be determined if certain LMI is feasible. This method does not concern any conservative reduction or enlargement of matrix inequalities. Numerical examples show the validity of the proposed algorithms.", "keywords": ["h2", "static output feedback", "coordinates transformation", "lmi", "optimal control", "h control"]}, {"id": "30", "title": "Observation of linear systems with unknown inputs via high-order sliding-modes", "abstract": "A high- order sliding- mode observer is designed for linear time invariant systems with single output and unknown bounded single input. It provides for the global observation of the state and the output under sufficient and necessary conditions of strong observability or strong detectability. The observation is finite- time- convergent and exact in the strong observability case. The accuracy of the proposed observation and identification schemes is estimated via the sampling step or magnitude of deterministic noises. The results are extended to the multi- input multi- output case.", "keywords": ["high order sliding modes", "observation", "identification"]}, {"id": "31", "title": "A Mathematical Model of Penile Vascular Dysfunction and Its Application to a New Diagnostic Technique", "abstract": "A noninvasive diagnostic device was developed to assess the vascular origin and severity of penile dysfunction. It was designed and studied using both a mathematical model of penile hemodynamics and preliminary experiments on healthy young volunteers. The device is based on the application of an external pressure (or vacuum) perturbation to the penis following the induction of erection. The rate of volume change while the penis returns to its natural condition is measured using a noninvasive system that includes a volume measurement mechanism that has very low friction, thereby not affecting the measured system. The rate of volume change (net flow) is obtained and analyzed. Simulations using a mathematical model show that the device is capable of differentiating between arterial insufficiency and venous leak and indicate the severity of each. In preliminary measurements on young healthy volunteers, the feasibility of the measurement has been demonstrated. More studies are required to confirm the diagnostic value of the measurements", "keywords": ["erectile dysfunction", "arterial insufficiency", "venous leak", "veno-occlusive mechanism", "mathematical model", "hemodynamics"]}, {"id": "32", "title": "The spectra of irreducible matrices over completed idempotent semifields", "abstract": "Motivated by some spectral results in the characterization of concept lattices we investigate the spectra of reducible matrices over complete idempotent semifields in the framework of naturally-ordered semirings, or dioids. We find non-null eigenvectors for every non-null element in the semifield and conclude that the notion of spectrum has to be refined to encompass that of the incomplete semifield case so as to include only those eigenvalues with eigenvectors that have finite coordinates. Considering special sets of eigenvectors brings out finite complete lattices in the picture and we contend that such structure may be more important than standard eigenspaces for matrices over completed idempotent semifields.", "keywords": ["matrix spectra", "dioids", "complete idempotent semifields", "complete idempotent semimodules", "spectral order lattices"]}, {"id": "33", "title": "interactive technologies for health special interest group", "abstract": "Health and how to support it with interactive computer systems, networks, and devices is a global and, for many countries, an explicit national priority. Significant interest in issues related to interactive systems for health has been demonstrated repeatedly within SIGCHI. A community focused on health started in 2010, fostering collaboration and dissemination of research findings as well as bridging with practitioners. As part of this community's on-going efforts, we will hold a special interest group session during ACM CHI 2011 to discuss, prioritize, and promote some of these most pressing issues facing the community.", "keywords": ["fitness", "assistive technologies", "medicine", "telecare", "wellness", "health", "nutrition", "health informatics"]}, {"id": "34", "title": "Cyclic sequence alignments: Approximate versus optimal techniques", "abstract": "The problem of cyclic sequence alignment is considered. Most existing optimal methods for comparing cyclic sequences are very time consuming. For applications where these alignments are intensively used, optimal methods axe seldom a feasible choice. The alternative to an exact and costly solution is to use a close-to-optimal but cheaper approach. In previous works, we have presented three suboptimal techniques inspired on the quadratic-time suboptimal algorithm proposed by Bunke and Buhler. Do these approximate approaches come sufficiently close to the optimal solution, with a considerable reduction in computing time? Is it thus worthwhile investigating these approximate methods? This paper shows that approximate techniques are good alternatives to optimal methods.", "keywords": ["cyclic sequences", "cyclic string matching", "structural pattern analysis"]}, {"id": "35", "title": "Direct AuAu bonding technology for high performance GaAs/AlGaAs quantum cascade lasers", "abstract": "In this paper we investigate chip bonding technology of GaAs/AlGaAs quantum cascade lasers (QCLs). Its results have strong influence on final performance of devices and are essential for achieving room temperature operation. Various solders were investigated and compared in terms of their thermal resistance and induced stress. The spatially resolved photoluminescence technique has been applied for a device thermal analysis. The soldering quality was also investigated by means of a scanning acoustic microscopy. The particular attention has been paid to AuAu die bonding, which seems to be a promising alternative to the choice between hard and soft solder bonding of GaAs/AlGaAs QCLs operating from cryogenic temperatures up to room temperatures. A good quality direct AuAu bonding was achieved for bonding parameters comparable with the ones typical for AuSn eutectic bonding process. High performance room temperature operation of GaAs/AlGaAs QCLs has been achieved with the state-of-the-art parameters.", "keywords": ["gaas/algaas quantum cascade laser", "mounting technology", "die-bonding", "packaging", "scanning accustic microscopy"]}, {"id": "36", "title": "Harnessing Cloud Technologies for a Virtualized Distributed Computing Infrastructure", "abstract": "The InterGrid system aims to provide an execution environment for running applications on top of interconnected infrastructures. The system uses virtual machines as building blocks to construct execution environments that span multiple computing sites. Such environments can be extended to operate on cloud infrastructures, such as Amazon EC2. This article provides an abstract view of the proposed architecture and its implementation; experiments show the scalability of an InterGrid-managed infrastructure and how the system can benefit from using the cloud.", "keywords": ["amazon ec2", "cloud computing", "grid computing", "distributed systems", "scheduling", "resource management", "virtualization", "intergrid gateway"]}, {"id": "37", "title": "A method of MPEG2-TS test stream generation for digital TV software", "abstract": "Input of the digital TV software is a transport stream (TS) in MPEG (Moving Picture Expert Group)-2 format, a standard specification for moving picture compression. We propose a method to thoroughly generate MPEG-2 TS test data, namely, a test stream based on the black-box test concept for digital TV software. We also introduce a tool to automate the test stream generation known as Auto-TEst data generator from Protocol standard (ATEP). This empirical study of the application of an ATEP-derived test stream to an actual digital TV software settop box should benefit digital TV software developers as well as other testers.", "keywords": ["mpeg2-ts test stream", "digital tv software test", "black-box test"]}, {"id": "38", "title": "UNIFIED REPRESENTATION OF ZIPF DISTRIBUTIONS", "abstract": "Certain discrete probability distributions, used independently from each other in linguistics and other sciences, can be considered as special cases of the distribution based on the Lerch zeta function. We will list the probability functions for some of the most important cases. Moments and estimators are derived for the general Lerch distribution.", "keywords": ["lerch zeta function", "zipf distributions", "estimators", "nonlinear equation systems"]}, {"id": "39", "title": "Physical quantum algorithms", "abstract": "I review the differences between classical and quantum systems, emphasizing the connection between no-hidden variable theorems and superior computational power of quantum computers. Using quantum lattice gas automata as examples, I describe possibilities for efficient simulation of quantum and classical systems with a quantum computer. I conclude with a list of research directions. ", "keywords": ["quantum simulation", "quantum lattice gas automata"]}, {"id": "40", "title": "A slack-diversifying nonlinear fluctuation smoothing rule for job dispatching in a wafer fabrication factory", "abstract": "This study proposes a slack-diversifying nonlinear fluctuation smoothing rule to reduce the average cycle time in a wafer fabrication factory. The slack-diversifying nonlinear fluctuation smoothing rule is derived from the one-factor tailored nonlinear fluctuation smoothing rule for cycle time variation (1f-TNFSVCT) by dynamically maximizing the standard deviation of the slack, which has been shown to improve scheduling performance in several previous studies. The effectiveness of the proposed rule has been validated via using it with a simulated data set. Based on the findings in this research we also derived several directions that can be exploited in the future.", "keywords": ["wafer fabrication", "dispatching rule", "slack", "diversify", "fluctuation smoothing"]}, {"id": "41", "title": "Linear and compact floating node voltage-controlled variable resistor circuit", "abstract": "In this letter, my proposals for a Floating node voltage controlled Variable Resistor circuit (FVR) are based upon its advantages as linear and compact. The performance of the proposed circuit was confirmed by PSpice simulation. The simulation results are reported in this letter.", "keywords": ["analog integrated circuits", "floating node", "voltage controlled variable resistor circuit"]}, {"id": "42", "title": "The management information systems (MIS) job market late 1970s-late 1990s", "abstract": "The rapidly changing information technology (IT) environment continues to pose a challenging dilemma for both management information systems (MIS) managers and MIS educators at all levels, especially the collegiate level. This research examines the content of MIS-related job advertisements over a 20-year period: late 1970s-late 1990s. It is the continuation of a study initially published in The Journal of Computer Information Systems (6) and includes the data that represents the late 1990s timeframe. Results trace the rise and fall in demand for certain IT skills and knowledge and identify the growing strength or stability of others. The study clearly exposes the great diversity in the MIS job market. This diversity is the root cause of the dilemma confronting MIS managers and MIS educators as they try to recruit workers from or prepare students for the changing IT environment.", "keywords": ["mis job market", "mis job market diversity", "mis skills", "mis knowledge"]}, {"id": "43", "title": "Development of a ceramic bolus for the permanent electronic identification of sheep, goat and cattle", "abstract": "Retention rate and digestive and performance effects of ceramic boluses (6620 mm, 65 g) enclosing passive transponders (32.53.8 mm) were studied in three experiments. Reading distances of transponders inside and outside the boluses (n=10) did not vary. In the first experiment, a total of 2452 boluses were applied to 74 lambs and 808 ewes, 16 young and 67 adult goats, 1138 calves and 349 cows. Plastic balling guns were used to insert the boluses and their effects were evaluated during 3 years or until slaughter. Time needed for application and recommended live-weights (LW) depended on animal category (sheep, 24 s and >25 kg; goats, 26 s and >20 kg; cattle, 19240 s and >30 kg). Application in calves was possible during the first week of life. Retention rates were 100, 98.8 and 99.7% in sheep, goats and cattle, respectively. The location of boluses in the reticulum was checked with hand-held readers and verified by X-ray in a sample (n=4) of each animal category or directly in cannulated cows (n=3). Transceivers were interfaced with electronic scales for automatic weight recording. Dynamic reading efficiency was 100% in race-ways with a frame antenna (9452 cm). Health and performances were not modified by boluses. An average of 93% of boluses were found in the reticulum at slaughter. Recovery rates and times varied according to animal category (lambs, 100% and 5 s; ewes and goats, 100% and 8 s; fattened calves, 91.3% and 12 s; dairy cows, 72% and 14 s). In the second experiment, two groups of adult ewes (control, n=5; bolus, n=5) were housed in individual pens and fed forage ad libitum. Mean forage intake and nutrient digestibility were not varied by the ceramic boluses. In the third experiment 45 fattening male lambs (20 kg LW) and 20 replacement ewe-lambs (30 kg LW) were used. Fattening lambs were divided into two groups and assigned to the treatments (control, n=25; bolus, n=20) until slaughter (25 kg LW). In spite of the difficulties observed in the force-feeding of boluses in eight lambs (40%), average daily gain and reticulum-rumen mucosa were not altered. Ewe-lambs were also assigned to the treatments in two groups (control, n=10; bolus, n=10) and monitored until first lambing or 1 year old. The weight, body condition score and reproductive performance were not affected by boluses. In conclusion, the use of the ceramic bolus is recommended as a safe and tamper-proof method for electronic identification of ruminants once the animals have reached a weight where successful administration is possible. Moreover, boluses proved to be useful for dynamic reading and automatic weight recording on farm conditions.", "keywords": ["animal identification", "ceramic bolus", "reading range"]}, {"id": "44", "title": "OWL-Eu: Adding customised datatypes into OWL", "abstract": "Although OWL is rather expressive, it has a very serious limitation on datatypes; i.e., it does not support customised datatypes. It has been pointed out that many potential users will not adopt OWL unless this limitation is overcome, and the W3C Semantic Web Best Practices and Development Working Group has set up a task force to address this issue. This paper makes the following two contributions: (i) it provides a brief summary of OWL-related datatype formalisms, and (ii) it provides a decidable extension of OWL DL, called OWL-Eu, that supports customised datatypes. A detailed proof of the decidability of OWL-Eu is presented.", "keywords": ["ontologies", "semantic web", "description logics", "customised datatypes", "unary datatype groups"]}, {"id": "45", "title": "Turning a community into a market: A practice perspective on information technology use in boundary spanning", "abstract": "This paper examines how information technology (IT) transforrns relations across fields of practice within organizations. Drawing on Bourdieu's practice theory, we argue that the production of any practice involves varying degrees of embodiment (i.e., relying on personal relationships) and objectification (i.e., relying on the exchange of objects). We subsequently characterize boundary-spanning practices according to their relative degrees of embodiment and objectification. We distinguish between \"market-like\" boundary-spanning practices, which rely primarily on an objectified mode of practice production, from \"community-like\" practices, which involve mostly the embodied mode of practice production. IT is then conceptualized as a medium for sharing objects in the production of practices. As such, IT use allows for the sharing of objects without relying on embodied relationships. We use data from an in-depth ethnographic case study to investigate how IT was used to transform community-like boundary-spanning practices within an organization into market-like ones. Moreover, we demonstrate how, as IT was used to support the exchange and combination of depersonalized objects, other aspects of the practice (such as the roles of intermediaries and the nature of meetings) also changed. The related changes in these diverse aspects of a boundary-spanning practice supported the trend toward greater objectification. IT use also increased visibility of the terms associated with object exchange. This increased visibility exposed the inequity of the exchange and encouraged the disadvantaged party to renegotiate the relationship.", "keywords": ["boundary objects", "boundary spanners", "boundary spanning", "communities of practice", "coordination mechanisms", "information technology use", "practice theory", "qualitative methods"]}, {"id": "46", "title": "Integration of Different Risk Assessment Tools to Improve Stratification of Patients with Coronary Artery Disease", "abstract": "Cardiovascular disease (CVD) causes unaffordable social and health costs that tend to increase as the European population ages. In this context, clinical guidelines recommend the use of risk scores to predict the risk of a cardiovascular disease event. Some useful tools have been developed to predict the risk of occurrence of a cardiovascular disease event (e.g. hospitalization or death). However, these tools present some drawbacks. These problems are addressed through two methodologies: (i) combination of risk assessment tools: fusion of nave Bayes classifiers complemented with a genetic optimization algorithm and (ii) personalization of risk assessment: subtractive clustering applied to a reduced-dimensional space to create groups of patients. Validation was performed based on two ACS-NSTEMI patient data sets. This work improved the performance in relation to current risk assessment tools, achieving maximum values of sensitivity, specificity, and geometric mean of, respectively, 79.8, 83.8, and 80.9%. Additionally, it assured clinical interpretability, ability to incorporate of new risk factors, higher capability to deal with missing risk factors and avoiding the selection of a standard CVD risk assessment tool to be applied in the clinical practice.", "keywords": ["information and knowledge management", "management of cardiovascular diseases", "decision-support systems"]}, {"id": "47", "title": "Segmenting ideal morphologies of sewer pipe defects on CCTV images for automated diagnosis", "abstract": "Several literatures presented automated systems for detecting or classifying sewer pipe defects based on morphological features of pipe defects. In those automated systems, however, the morphologies of the darker center or some uncertain objects on CCTV images are also segmented and become noises while morphology-based pipe defect segmentation is implemented. In this paper, the morphology-based pipe defect segmentation is proposed and discussed to be an improved approach for automated diagnosis of pipe defects on CCTV images. The segmentation of pipe defect morphologies is first to implement an opening operation for gray-level CCTV images to distinguish pipe defects. Then, Otsu's technique is used to segment pipe defects by determining the optimal thresholds for gray-level CCTV images of opening operation. Based on the segmentation results of CCTV images, the ideal morphologies of four typical pipe defects are defined. If the segmented CCTV images match the definition of those ideal morphologies, the pipe defects on those CCTV images can be successfully identified by a radial basis network (RBN) based diagnostic system. As for the rest CCTV images failing to match the ideal morphologies, the failure causes was discussed so to suggest a regulation for imaging conditions, such as camera pose and light source, in order to obtain CCTV images for successful segmentation.  ", "keywords": ["cctv", "image processing", "morphologies of pipe defects", "diagnostic system"]}, {"id": "48", "title": "A new fuzzy rule-based classification system for word sense disambiguation", "abstract": "Word sense disambiguation (WSD) can be thought of as the most challenging task in the process of machine translation. Various supervised and unsupervised learning methods have already been proposed for this purpose. In this paper, we propose a new efficient fuzzy classification system in order to be applied for WSD. In order to optimize the generalization accuracy, we use rule-weight as a simple mechanism to tune the classifier and propose a new learning method to iteratively adjust the weight of fuzzy rules. Through computer simulations on TWA data as a standard corpus, the proposed scheme shows a uniformly good behavior and achieves results which are comparable or better than other classification systems, proposed in the past.", "keywords": ["word sense disambiguation", "machine translation", "fuzzy systems", "classification", "rule-weight", "generalization accuracy"]}, {"id": "49", "title": "On the hierarchy of conservation laws in a cellular automaton", "abstract": "Conservation laws in cellular automata (CA) are studied as an abstraction of the conservation laws observed in nature. In addition to the usual real-valued conservation laws we also consider more general group-valued and semigroup-valued conservation laws. The (algebraic) conservation laws in a CA form a hierarchy, based on the range of the interactions they take into account. The conservation laws with smaller interaction ranges are the homomorphic images of those with larger interaction ranges, and for each specific range there is a most general law that incorporates all those with that range. For one-dimensional CA, such a most general conservation law haseven in the semigroup-valued casean effectively constructible finite presentation, while for higher-dimensional CA such effective construction exists only in the group-valued case. It is even undecidable whether a given two-dimensional CA conserves a given semigroup-valued energy assignment. Although the local properties of this hierarchy are tractable in the one-dimensional case, its global properties turn out to be undecidable. In particular, we prove that it is undecidable whether this hierarchy is trivial or unbounded. We point out some interconnections between the structure of this hierarchy and the dynamical properties of the CA. In particular, we show that positively expansive CA do not have non-trivial real-valued conservation laws.", "keywords": ["cellular automata", "conservation laws", "energy", "reversibility", "undecidability", "dynamical systems", "chaos"]}, {"id": "50", "title": "Positive periodic solutions for the neutral ratio-dependent predatorprey model", "abstract": "By using a continuation theorem based on coincidence degree theory, we obtain some new sufficient conditions for the existence of positive periodic solutions for the neutral ratio-dependent predatorprey model with Holling type II functional response.", "keywords": ["predatorprey model", "ratio-dependent", "periodic solution", "neutral", "coincidence degree"]}, {"id": "51", "title": "Impact of dual-k spacer on analog performance of underlap FinFET", "abstract": "Multigate structures have better short channel control than conventional bulk devices due to increased gate electrostatic control. FinFET is a promising candidate among multigate structures due to its ease of manufacturability. The RF performance of FinFET is affected by gate controlled parameters such as transconductance, output conductance and total gate capacitance. In this paper we have used dual-k spacers in underlap FinFETs to improve the gate electrostatic integrity. The inner high-k spacer helps in better screening out the gate sidewall fringing fields, thereby, increasing transconductance and reducing output conductance with increase in total gate capacitance. At 16nm gate lengths, we have observed that, the intrinsic gain of dual-k spacer based FinFET can be increased by more than 100% (>6dB) without affecting cutoff frequency and maximum oscillation frequency, as compared to conventional single spacer based FinFET. Improvement in cutoff frequency by 11% and maximum oscillation frequency by 5% can be achieved, when the gate lengths are scaled down to 12nm, in addition to 2.75 times (8.8dB) increase in intrinsic gain.", "keywords": ["short channel effect ", "dual-k spacer", "figures of merit ", "electrostatic integrity ", "intrinsic gain", "cutoff frequency"]}, {"id": "52", "title": "3D depth estimation for visual inspection using in wavelet transform modulus maxima", "abstract": "A vision based approach for calculating accurate 3D models of the objects is presented. Generally industrial visual inspection systems capable of accurate 3D depth estimation rely on extra hardware tools like laser scanners or light pattern projectors. These tools improve the accuracy of depth estimation but also make the vision system costly and cumbersome. In the proposed algorithm, depth and dimensional accuracy of the produced 3D depth model depends on the existing reference model instead of the information from extra hardware tools. The proposed algorithm is a simple and cost effective software based approach to achieve accurate 3D depth estimation with minimal hardware involvement. The matching process uses the well-known coarse to fine strategy, involving the calculation of matching points at the coarsest level with consequent refinement up to the finest level. Vector coefficients of the wavelet transform-modulus are used as matching features, where wavelet transform-modulus maxima defines the shift invariant high-level features with phase pointing to the normal of the feature surface. The technique addresses the estimation of optimal corresponding points and the corresponding 2D disparity maps leading to the creation of accurate depth perception model.  ", "keywords": ["wavelet transform modulus", "coarse to fine", "disparity", "3d depth"]}, {"id": "53", "title": "Adaptive FEC-based packet loss resilience scheme for supporting voice communication over ad hoc wireless networks", "abstract": "Providing real-time voice support over multihop ad hoc wireless networks (AWNS) is a challenging task. The standard retransmission-based strategies proposed in the literature are poorly matched to voice applications because of timeliness and large overheads involved in transmitting small-sized voice packets. To make a voice application feasible over AWNS, the perceived voice quality must be improved while not significantly increasing the packet overhead. We suggest packet-level media-dependent adaptive forward error correction (FEC) at the application layer in tandem with multipath transport for improving the voice quality. Since adaptive FEC masks packet losses in the network, at the medium access control (MAC) layer, we avoid retransmissions (hence, no acknowledgments) in order to reduce the control overhead and end-to-end delay. Further, we exploit the combined strengths of layered coding and multiple description (MD) coding for supporting error-resilient voice communication in AWNS. We propose an efficient packetization scheme in which the important substream of the voice stream is protected adaptively with FEC depending on the loss rate present in the network and is transmitted over two maximally node-disjoint paths. The less important substream of the voice stream is encoded into two descriptions, which are then transmitted over two maximally node-disjoint paths. The performance of our scheme (packet-level media-dependent adaptive FEC scheme) is evaluated in terms of two parameters: residual packet loss rate (RPLR, packet loss rate after FEC recovery) and average burst length (ABL, average length of consecutive packet losses after FEC recovery) of voice data after FEC recovery. The sets of equations leading to the analytical formulation of both RPLR and ABL are first given for a renewal error process. The values of both these parameters depend on FEC-Offset (r, the distance between original voice frame and piggybacked redundant voice frame) and loss rate present in the network. Then, these parameters are computed for a Gilbert-Elliott (GE) two-state Markov error model and compared with experimental data. Our scheme adaptively selects the FEC-Offset (it chooses r that minimizes RPLR and ABL as much as possible) based on the loss rate feedback obtained from the destination. The proposed scheme achieves significant gains in terms of reduced frame loss rate (FLR), reduced control overhead, and minimum end-to-end delay and almost doubles the perceived voice quality compared to the existing approaches.", "keywords": ["ad hoc networks", "voice frame", "layered coding", "multiple description coding", "forward error correction", "packetization scheme", "voice quality", "multipath transport", "multimedia"]}, {"id": "54", "title": "Introduction of a data schema to support a design repository", "abstract": "This paper presents the data schema required to capture fundamental elements of design information in a heterogeneous repository supporting design reuse. Design information captured by the repository can be divided into seven main categories of artifact-, function-, failure-, physical-, performance-, sensory- and media-related information types. Each of the seven types of design information is described in detail. The repository schema is specific to a relational database system driving the implemented design repository; however, the types of design information recorded are applicable to any implementation of a design repository. The aim of this paper is to fully describe the data schema such that it could be recreated or specialized for industrial or research applications. The result is a complete description of fundamental design knowledge to support design reuse and a data schema specification. The data schema has been vetted with the implemented design repository that contains design information for over 100 consumer electro-mechanical products.", "keywords": ["design repository schema", "conceptual design"]}, {"id": "55", "title": "Distributivity in lattices of fuzzy subgroups", "abstract": "The main goal of this paper is to study the finite groups whose lattices of fuzzy subgroups are distributive. We obtain a characterization of these groups which is similar to a well-known result of group theory.  ", "keywords": ["fuzzy subgroup lattices", "subgroup lattices", "distributivity", "finite cyclic groups", "equivalence relations"]}, {"id": "56", "title": "on delegation and workflow execution models", "abstract": "Workflow systems have long been of interest to computer science researchers due to their practical relevance. Supporting delegation mechanisms in workflow systems is receiving increasing research interest. In this paper, we conduct a comprehensive study of user delegation operations in computerized workflow systems. In a workflow system, the semantics of a delegation operation are largely based on three factors: the underlying workflow execution model, task type and delegation type. We describe three different workflow execution models and examine the effect of various delegation operations in each workflow execution model. We then extend our workflow execution models to examine the effect of various delegation operations in different role-based workflow execution models.", "keywords": ["delegation", "workflow management systems"]}, {"id": "57", "title": "On-line anomaly detection and resilience in classifier ensembles", "abstract": "Detection of anomalies is a broad field of study, which is applied in different areas such as data monitoring, navigation, and pattern recognition. In this paper we propose two measures to detect anomalous behaviors in an ensemble of classifiers by monitoring their decisions; one based on Mahalanobis distance and another based on information theory. These approaches are useful when an ensemble of classifiers is used and a decision is made by ordinary classifier fusion methods, while each classifier is devoted to monitor part of the environment. Upon detection of anomalous classifiers we propose a strategy that attempts to minimize adverse effects of faulty classifiers by excluding them from the ensemble. We applied this method to an artificial dataset and sensor-based human activity datasets, with different sensor configurations and two types of noise (additive and rotational on inertial sensors). We compared our method with two other well-known approaches, generalized likelihood ratio (GLR) and One-Class Support Vector Machine (OCSVM), which detect anomalies at data/feature level. We found that our method is comparable with GLR and OCSVM. The advantages of our method compared to them is that it avoids monitoring raw data or features and only takes into account the decisions that are made by their classifiers, therefore it is independent of sensor modality and nature of anomaly. On the other hand, we found that OCSVM is very sensitive to the chosen parameters and furthermore in different types of anomalies it may react differently. In this paper we discuss the application domains which benefit from our method.", "keywords": ["anomaly detection", "classifier ensemble", "decision fusion", "human activity recognition"]}, {"id": "58", "title": "Scheduling unrelated parallel machines to minimize total weighted tardiness", "abstract": "This article considers the problem of scheduling a given set of independent jobs on unrelated parallel machines to minimize the total weighted tardiness. The problem is known to be NP-hard in the strong sense. Efficient lower and upper bounds are developed. The lower bound is based on the solution of an assignment problem, while the upper bound is obtained by a two-phase heuristic. A branch-and-bound algorithm that incorporates various dominance rules is presented. Computational experiments are conducted to demonstrate the performance of the proposed algorithm. Scope and purpose Parallel machine scheduling models are important from both the theoretical and practical points of view. From the theoretical point of view, they generalize the single machine scheduling models. From the practical point of view, they are important because the occurrence of a bank of machines in parallel is common in industries. In this article, the unrelated parallel machine total weighted tardiness scheduling problem is examined. The tardiness criterion has many applications in real world. This problem is difficult to solve. A branch-and-bound algorithm that incorporates various dominance rules along with efficient lower and upper bounds is proposed to find an optimal solution.", "keywords": ["scheduling", "parallel machines", "branch-and-bound", "tardiness"]}, {"id": "59", "title": "Lock-free deques and doubly linked lists", "abstract": "We present a practical lock-free shared data structure that efficiently implements the operations of a concurrent deque as well as a general doubly linked list. The implementation supports parallelism for disjoint accesses and uses atomic primitives which are available in modern computer systems. Previously known lock-free algorithms of doubly linked lists are either based on non-available atomic synchronization primitives, only implement a subset of the functionality, or are not designed for disjoint accesses. Our algorithm only requires single-word compare-and-swap atomic primitives, supports fully dynamic list sizes, and allows traversal also through deleted nodes and thus avoids unnecessary operation retries. We have performed an empirical study of our new algorithm on two different multiprocessor platforms. Results of the experiments performed under high contention show that the performance of our implementation scales linearly with increasing number of processors. Considering deque implementations and systems with low concurrency, the algorithm by Michael shows the best performance. However, as our algorithm is designed for disjoint accesses, it performs significantly better on systems with high concurrency and non-uniform memory architecture.", "keywords": ["deque", "doubly linked list", "non-blocking", "lock-free", "shared data structure", "multi-thread", "concurrent"]}, {"id": "60", "title": "An optimal deployable bandwidth aggregation system", "abstract": "The explosive increase in data demand coupled with the rapid deployment of various wireless access technologies have led to the increase of number of multi-homed or multi-interface enabled devices. Fully exploiting these interfaces has motivated researchers to propose numerous solutions that aggregate their available bandwidths to increase overall throughput and satisfy the end-users growing data demand. These solutions, however, do not utilize their interfaces to the maximum without network support, and more importantly, have faced a steep deployment barrier. In this paper, we propose an optimal deployable bandwidth aggregation system (DBAS) for multi-interface enabled devices. We present the DBAS architecture that does not introduce any intermediate hardware, modify current operating systems, modify socket implementations, nor require changes to current applications or legacy servers. The DBAS architecture is designed to automatically estimate the characteristics of applications and dynamically schedule various connections and/or packets to different interfaces. We also formulate our optimal scheduler as a mixed integer programming problem yielding an efficient solution. We evaluate DBAS via implementation on the Windows OS and further verify our results with simulations on NS2. Our evaluation shows that, with current Internet characteristics, DBAS reaches the throughput upper bound with no modifications to legacy servers. It also highlights the significant enhancements in the response time introduced by DBAS, which directly enhances the user experience.", "keywords": ["bandwidth aggregation", "multiple network interfaces", "throughput", "optimization", "multihoming"]}, {"id": "61", "title": "Dynamic bandwidth allocation in heterogeneous WDM EPONs", "abstract": "Several dynamic bandwidth allocation algorithms have been introduced to schedule upstream wavelength channels in wavelength division multiplexing Ethernet passive optical networks (WDM EPONs), but mostly for homogenous WDM EPON networks with the same distance between each optical network unit (ONU) and the optical line terminal. For WDM EPON with heterogeneous round trip times (RTTs), we propose two algorithms for ONU scheduling, called nearest first and early allocation (EA); and a wavelength assignment algorithm, called best fit (BF). Both ONU scheduling algorithms take RTT dissimilarities into account, thus minimizing packet delay and packet drop ratio at ONUs. Additionally, EA can relive the common drawback of offline scheduling, i.e., channel idle time. On the other hand, the BF wavelength assignment is proposed that assigns the best wavelength to each ONU in order to improve network performances in terms of packet queuing delay and packet drop ratio at ONU sides.", "keywords": ["heterogeneous wdm eopn", "dynamic bandwidth allocation", "nearest first", "early allocation", "wavelength assignment"]}, {"id": "62", "title": "A layered framework to study collaboration as a form of knowledge sharing and diffusion", "abstract": "Collaboration is presented as a form of knowledge sharing and hence of knowledge diffusion. A layered framework for collaboration studies is proposed. The notions of relative and absolute proper essential node (PEN) centrality are introduced as indicators of a node's importance for diffusion of knowledge through collaboration.", "keywords": ["collaboration", "diffusion", "layered systems", "networks", "centrality indicators"]}, {"id": "63", "title": "Classes of tree languages determined by classes of monoids", "abstract": "In this paper finite state recognizers are considered as unary tree recognizers with unary operational symbols. We introduce translation recognizers of a tree recognizer, which are finite state recognizers whose operations are the elementary translations of the underlying algebra of the considered tree recognizer. In terms of translation recognizers we give general conditions under which a class of recognizable tree languages with a given property can be determined by a class of monoids determining the class of string languages having the same property.", "keywords": ["tree automata", "tree languages", "syntactic monoids"]}, {"id": "64", "title": "On a transfer theorem for the P not equal NP conjecture", "abstract": "A model of computation is defined over the algebraic numbers and over number fields. This model is non-uniform, and the cost of operations depends on the height of the operands and on the degree of the extension of the rational defined by those operands. A transfer theorem for the P not equal NP Conjecture is proved, namely: P not equal NP in this model over the real algebraic numbers if and only if P not equal NP in the classical setting. ", "keywords": ["np-completeness", "computability over a ring", "height", "transfer theorem"]}, {"id": "65", "title": "The design and implementation of a future Internet live TV system over 4G networks", "abstract": "The emerging 4G (fourth generation) networks featuring wider coverage, higher transmission bandwidth and easier deployment have a desirable potential to serve ubiquitous and pervasive multimedia applications in creating new user-centric communication services. However, the practical implementation of 4G network to demonstrate such potential, especially for delivering real-time and high-quality video services, is scarce. This paper therefore provides the design and implementation of a future Internet live TV system over 4G networks to achieve cost-effectiveness, instead of using expensive satellite news gathering (SNG) vehicle and costly satellite transmissions in traditional TV stations. To effectively provide live TV services, we apply not only the hybrid duplex modes but also the port-based VLAN on the deployed networks for maximizing bandwidth, minimizing signal interference, and guaranteeing QoS of differentiated services. Performance metrics are applied to demonstrate that the proposed solution is cost-effective and is feasible for live TV services in future Internet.", "keywords": ["future internet", "4g networks", "live tv system", "satellite news gathering  service", "vlan"]}, {"id": "66", "title": "Numerical study of a stochastic particle method for homogeneous gas-phase reactions", "abstract": "In this paper, we study a stochastic particle system that describes homogeneous gasphase reactions of a number of chemical species. First, we introduce the system as a Markov jump process and discuss how relevant physical quantities are represented in terms of appropriate random variables. Then, we show how various deterministic equations, used in the literature, are derived from the stochastic system in the limit when the number of particles goes to infinity. Finally, we apply the corresponding stochastic algorithm to a toy problem, a simple formal reaction mechanism, and a real combustion problem. This problem is given by the isothermal combustion of a homogeneous mixture of heptane and air modelled by a detailed reaction mechanism with 107 chemical species and 808 reversible reactions. Heptane as described in this chemical mechanism serves as model-fuel for different types of internal combustion engines. In particular, we study the order of convergence with respect to the number of simulation particles, and illustrate the limitations of the method.  ", "keywords": ["stochastic particle method", "combustion", "convergence", "efficiency"]}, {"id": "67", "title": "A Nitsche stabilized finite element method for frictional sliding on embedded interfaces. Part II: Intersecting interfaces", "abstract": "Developed a weighted Nitsche stabilized method for embedded interfaces with junctions. Provided an explicit expression for the method parameter for lower order elements in the presence of junctions. Examples highlight the method capabilities in modeling grain-boundary sliding behavior.", "keywords": ["frictional contact", "grain-boundary sliding", "junctions", "nitsche", "polycrystalline", "x-fem"]}, {"id": "68", "title": "A class of aggregation functions encompassing two-dimensional OWA operators", "abstract": "In this paper we prove that, under suitable conditions, Atanassovs K? operators, which act on intervals, provide the same numerical results as OWA operators of dimension two. On one hand, this allows us to recover OWA operators from K? operators. On the other hand, by analyzing the properties of Atanassovs operators, we can generalize them. In this way, we introduce a class of aggregation functions  the generalized Atanassov operators  that, in particular, include two-dimensional OWA operators. We investigate under which conditions these generalized Atanassov operators satisfy some properties usually required for aggregation functions, such as bisymmetry, strictness, monotonicity, etc. We also show that if we apply these aggregation functions to interval-valued fuzzy sets, we obtain an ordered family of fuzzy sets.", "keywords": ["OWA operators", "Interval-valued fuzzy sets operators", "Generalized K", "operators", "Dispersion"]}, {"id": "69", "title": "Data Structures on Event Graphs", "abstract": "We investigate the behavior of data structures when the input and operations are generated by an event graph. This model is inspired by Markov chains. We are given a fixed graph G, whose nodes are annotated with operations of the type insert, delete, and query. The algorithm responds to the requests as it encounters them during a (random or adversarial) walk in G. We study the limit behavior of such a walk and give an efficient algorithm for recognizing which structures can be generated. We also give a near-optimal algorithm for successor searching if the event graph is a cycle and the walk is adversarial. For a random walk, the algorithm becomes optimal.", "keywords": ["successor searching", "markov chain", "low entropy", "data structure"]}, {"id": "70", "title": "Radar detection algorithm for GARCH clutter model", "abstract": "We propose a GARCH model to represent the clutter in radar applications. We fit this model to real sea clutter data and we show that it represents adequately the statistics of the data. Then, we develop a detection test based on this model. Using synthetic and real radar data, we evaluate its performance and we show that the proposed detector offers higher probability of detection for a specified value of probability of false alarm than tests based on Gaussian and Weibull models, especially for low signal to clutter ratios.", "keywords": ["radar", "detection", "non-gaussian clutter", "garch processes"]}, {"id": "71", "title": "Particle-based non-photorealistic volume visualization", "abstract": "Non-photorealistic techniques are usually applied to produce stylistic renderings. In visualization, these techniques are often able to simplify data, producing clearer images than traditional visualization methods. We investigate the use of particle systems for visualizing volume datasets using non-photorealistic techniques. In our VolumeFlies framework, user-selectable rules affect particles to produce a variety of illustrative styles in a unified way. The techniques presented do not require the generation of explicit intermediary surfaces.", "keywords": ["visualization", "non-photorealistic rendering", "volume rendering", "particle systems"]}, {"id": "72", "title": "A framework for sequential multiblock component methods", "abstract": "Multiblock or multiset methods are starting to be used in chemistry and biology to study complex data sets. In chemometrics, sequential multiblock methods are popular; that is, methods that calculate one component at a time and use deflation for finding the next component. In this paper a framework is provided for sequential multiblock methods, including hierarchical PCA (HPCA; two versions), consensus PCA (CPCA; two versions) and generalized PCA (GPCA). Properties of the methods are derived and characteristics of the methods are discussed. All this is illustrated with a real five-block example from chromatography. The only methods with clear optimization criteria are GPCA and one version of CPCA. Of these, GPCA is shown to give inferior results compared with CPCA. ", "keywords": ["multiblock methods", "hierarchical pca", "consensus pca", "generalized pca", "multiway methods", "stationary phases", "reversed phase liquid chromatography"]}, {"id": "73", "title": "Of the rich and the poor and other curious minds: on open access and \"development\"", "abstract": "Purpose - The paper seeks to reconsider open access and its relation to issues of \"development\" by highlighting the ties the open access movement has with the hegemonic discourse of development and to question some of the assumptions about science and scientific communication upon which the open access debates are based. The paper also aims to bring out the conflict arising from the convergence of the hegemonic discourses of science and development with the contemporary discourse of openness. Design/methodology/approach - The paper takes the form of a critical reading of a range of published work on open access and the so-called \"developing world\" as well as of various open access declarations. The argument is supported by insights from post-development studies. Findings - Open access is presented as an issue of moral concern beyond the narrow scope of scholarly communication. Claims are made based on hegemonic discourses that are positioned as a priori and universal. The construction of open access as an issue of unquestionable moral necessity also impedes the problematisation of its own heritage. Originality/value - This paper is intended to open up the view for open access's less obvious alliances and conflicting discursive ties and thus to initiate a politisation, which is necessary in order to further the debate in a more fruitful way.", "keywords": ["developing countries", "sciences", "communication technologies", "journal publishers"]}, {"id": "74", "title": "geometric surrogate-based optimisation for permutation-based problems", "abstract": "In continuous optimisation, surrogate models (SMs) are used when tackling real-world problems whose candidate solutions are expensive to evaluate. In previous work, we showed that a type of SMs - radial basis function networks (RBFNs) - can be rigorously generalised to encompass combinatorial spaces based in principle on any arbitrarily complex underlying solution representation by extending their natural geometric interpretation from continuous to general metric spaces. This direct approach to representations does not require a vector encoding of solutions, and allows us to use SMs with the most natural representation for the problem at hand. In this work, we apply this framework to combinatorial problems using the permutation representation and report experimental results on the quadratic assignment problem.", "keywords": ["radial-basis functions", "representations", "surrogate model optimization"]}, {"id": "75", "title": "Computational approach to ensure the stability of the favorable ATP binding site in E. coli Hfq", "abstract": "Bacterial Hfq is a highly conserved thermostable protein of about 10kDa. The Hfq protein was discovered in 1968 as an E. coli host factor that was essential for replication of the bacteriophage Q?. It is now clear that Hfq has many important physiological roles. In E. coli, Hfq mutants show a multiple stress response related phenotypes. Hfq is now known to regulate the translation of two major stress transcription factors RpoS and RpoE in Enterobacteria and mediates its plieotrophic effects through several mechanisms. It interacts with regulatory sRNA and facilitates their antisense interaction with their targets. It also acts independently to modulate mRNA decay and in addition acts as a repressor of mRNA translation. Recent paper from Arluison et al. [9] provided the first evidence indicating that Hfq is an ATP-binding protein. They determined a plausible ATP-binding site in Hfq and tested Hfq's ATP-binding affinity and stoichiometry. Experimental data suggest that the ATP-binding by the HfqRNA complex results in its significant destabilization of the protein and the result also proves important role of Tyr25 that flanks the cleft and stabilizes the adenine portion of ATP, possibly via aromatic stacking. In our study, the ATP molecule was docked into the predicted binding cleft using GOLD docking software. The binding nature of ATP and its effect on HfqRNA complex was studied using molecular dynamics simulations. Importance of Tyr25 residue was monitored and revealed using mutational study on the modeled systems. Our data and the corresponding results point to one of Hfq functional structural consequences due to ATP binding and Tyr25Ala mutation.", "keywords": ["host factor protein-hfq", "atp", "oligoribonucleotide", "rna", "post-transcriptional regulation", "molecular dynamics simulation", "mutation", "aromatic stacking", "destabilization"]}, {"id": "76", "title": "The SDL pattern approach  a reuse-driven SDL design methodology", "abstract": "There are several SDL methodologies that offer full system life-cycle support. Only few of them consider software reuse, not to mention high-level reuse of architecture and design. However, software reuse is a proven software engineering paradigm leading to high quality and reduced development effort. Experience made it apparent that  beyond the more traditional reuse of code  especially high-level reuse of architecture and design (as in the case of design patterns or frameworks) has the potential of achieving more systematic and widespread reuse. This paper presents the SDL pattern approach, a design methodology for distributed systems which integrates SDL-based system development with the pattern paradigm. It supports reuse of design knowledge modeled as SDL patterns and concentrates on the design phase of SDL-based system development. In order to get full life-cycle support, the pattern-based design process can be integrated within existing SDL methodologies.", "keywords": ["sdl", "design methodology", "software reuse", "patterns", "distributed systems", "process model"]}, {"id": "77", "title": "Database repairing using updates", "abstract": "Repairing a database means bringing the database in accordance with a given set of integrity constraints by applying some minimal change. If a database can be repaired in more than one way, then the consistent answer to a query is defined as the intersection of the query answers on all repaired versions of the database. Earlier approaches have confined the repair work to deletions and insertions of entire tuples. We propose a theoretical framework that also covers updates as a repair primitive. Update-based repairing is interesting in that it allows rectifying an error within a tuple without deleting the tuple, thereby preserving consistent values in the tuple. Another novel idea is the construct of nucleus: a single database that yields consistent answers to a class of queries, without the need for query rewriting. We show the construction of nuclei for full dependencies and conjunctive queries. Consistent query answering and constructing nuclei is generally intractable under update-based repairing. Nevertheless, we also show some tractable cases of practical interest.", "keywords": ["theory", "consistent query answering", "database repairing"]}, {"id": "78", "title": "Simulation of face/hairstyle swapping in photographs with skin texture synthesis", "abstract": "The modern trend of diversification and personalization has encouraged people to boldly express their differentiation and uniqueness in many aspects, and one of the noticeable evidences is the wide variety of hairstyles that we could observe today. Given the needs for hairstyle customization, approaches or systems, ranging from 2D to 3D, or from automatic to manual, have been proposed or developed to digitally facilitate the choice of hairstyles. However, nearly all existing approaches suffer from providing realistic hairstyle synthesis results. By assuming the inputs to be 2D photos, the vividness of a hairstyle re-synthesis result relies heavily on the removal of the original hairstyle, because the co-existence of the original hairstyle and the newly re-synthesized hairstyle may lead to serious artifact on human perception. We resolve this issue by extending the active shape model to more precisely extract the entire facial contour, which can then be used to trim away the hair from the input photo. After hair removal, the facial skin of the revealed forehead needs to be recovered. Since the skin texture is non-stationary and there is little information left, the traditional texture synthesis and image inpainting approaches do not fit to solve this problem. Our proposed method yields a more desired facial skin patch by first interpolating a base skin patch, and followed by a non-stationary texture synthesis. In this paper, we also would like to reduce the user assistance during such a process as much as possible. We have devised a new and friendly facial contour and hairstyle adjusting mechanism that make it extremely easy to manipulate and fit a desired hairstyle onto a face. In addition, our system is also equipped with the functionality of extracting the hairstyle from a given photo, which makes our work more complete. Moreover, by extracting the face from the input photo, our system allows users to exchange faces as well. In the end of this paper, our re-synthesized results are shown, comparisons are made, and user studies are conducted as well to further demonstrate the usefulness of our system.", "keywords": ["active shape model", "skin texture synthesis", "hairstyle extraction"]}, {"id": "79", "title": "Ubiquitous provision of context-aware web services", "abstract": "Providing context-aware Web services refers to an adaptive process of delivering contextually matched Web services to meet service requesters needs at the moment. This article presents an ontology-based context model that enables formal description and acquisition of contextual information pertaining to both service requesters and services. The context model is supported by context query and phased acquisition techniques. Re also report two context-aware Web services built on top of our context model to demonstrate how the model can be used to facilitate Web services discovery and Web content adaptation. Implementation details of the context elicitation system and the evaluation results of context-aware services provision are also reported.", "keywords": ["context-aware", "owl-s", "portable devices", "service oriented architecture", "ubiquitous", "web services"]}, {"id": "80", "title": "Extracting Representative Information to Enhance Flexible Data Queries", "abstract": "Extracting representative information is of great interest in data queries and web applications nowadays, where approximate match between attribute values/records is an important issue in the extraction process. This paper proposes an approach to extracting representative tuples from data classes under an extended possibility-based data model, and to introducing a measure (namely, relation compactness) based upon information entropy to reflect the degree that a relation is compact in light of information redundancy. Theoretical analysis and data experiments show that the approach has desirable properties that: 1) the set of representative tuples has high degrees of compactness (less redundancy) and coverage (rich content); 2) it provides a way to obtain data query outcomes of different sizes in a flexible manner according to user preference; and 3) the approach is also meaningful and applicable to web search applications.", "keywords": ["flexible data queries", "information equivalence", "relation compactness", "representativeness", "web search"]}, {"id": "81", "title": "points-to analysis using bdds", "abstract": "This paper reports on a new approach to solving a subset-based points-to analysis for Java using Binary Decision Diagrams (BDDs). In the model checking community, BDDs have been shown very effective for representing large sets and solving very large verification problems. Our work shows that BDDs can also be very effective for developing a points-to analysis that is simple to implement and that scales well, in both space and time, to large programs.The paper first introduces BDDs and operations on BDDs using some simple points-to examples. Then, a complete subset-based points-to algorithm is presented, expressed completely using BDDs and BDD operations. This algorithm is then refined by finding appropriate variable orderings and by making the algorithm propagate sets incrementally, in order to arrive at a very efficient algorithm. Experimental results are given to justify the choice of variable ordering, to demonstrate the improvement due to incrementalization, and to compare the performance of the BDD-based solver to an efficient hand-coded graph-based solver. Finally, based on the results of the BDD-based solver, a variety of BDD-based queries are presented, including the points-to query.", "keywords": ["point", "order", "communities", "examples", "efficiency", "analysis", "performance", "points-to analysis", "timing", "paper", "binary decision diagrams", "incremental", "program", "variability", "model checking", "experimentation", "space", "verification", "operability", "queries", "demonstrate", "graph", "algorithm", "effect", "query", "completeness"]}, {"id": "82", "title": "Fuzzy Q-Learning Admission Control for WCDMA/WLAN Heterogeneous Networks with Multimedia Traffic", "abstract": "In this paper, admission control by a fuzzy Q-learning technique is proposed for WCDMA/WLAN heterogeneous networks with multimedia traffic. The fuzzy Q-learning admission control (FQAC) system is composed of a neural-fuzzy inference system (NFIS) admissibility estimator, an NFIS dwelling estimator, and a decision maker. The NFIS admissibility estimator takes essential system measures into account to judge how each reachable subnetwork can support the admission request's required QoS and then output admissibility costs. The NFIS dwelling estimator considers the Doppler shift and the power strength of the requested user to assess his/her dwell time duration in each reachable subnetwork and then output dwelling costs. Also, in order to minimize the expected maximal cost of the user's admission request, a minimax theorem is applied in the decision maker to determine the most suitable subnetwork for the user request or to reject. Simulation results show that FQAC can always maintain the system QoS requirement up to a traffic intensity of 1.1 because it can appropriately admit or reject the users' admission requests. Also, the FQAC can achieve lower blocking probabilities than conventional JSAC proposed in [20] and can significantly reduce the handoff rate by 15-20 percent.", "keywords": ["fuzzy q-learning", "admission control", "handoff", "heterogeneous network"]}, {"id": "83", "title": "A numerical study of three-dimensional liquid sloshing in tanks", "abstract": "A numerical model NEWTANK (Numerical Wave TANK) has been developed to study three-dimensional (3-D) non-linear liquid sloshing with broken free surfaces. The numerical model solves the spatially averaged NavierStokes equations, which are constructed on a non-inertial reference frame having arbitrary six degree-of-freedom (DOF) of motions, for two-phase flows. The large-eddy-simulation (LES) approach is adopted to model the turbulence effect by using the Smagorinsky sub-grid scale (SGS) closure model. The two-step projection method is employed in the numerical solutions, aided by the Bi-CGSTAB technique to solve the pressure Poisson equation for the filtered pressure field. The second-order accurate volume-of-fluid (VOF) method is used to track the distorted and broken free surface. Laboratory experiments are conducted for both 2-D and 3-D non-linear liquid sloshing in a rectangular tank. A linear analytical solution of 3-D liquid sloshing under the coupled surge and sway excitation is also developed in this study. The numerical model is first validated against the available analytical solution and experimental data for 2-D liquid sloshing of both inviscid and viscous fluids. The validation is further extended to 3-D liquid sloshing. The numerical results match with the analytical solution when the excitation amplitude is small. When the excitation amplitude is large where sloshing becomes highly non-linear, large discrepancies are developed between the numerical results and the analytical solutions, the former of which, however, agree well with the experimental data. Finally, as a demonstration, a violent liquid sloshing with broken free surfaces under six DOF excitations is simulated and discussed.", "keywords": ["liquid sloshing", "three-dimensional numerical model", "navierstokes equations", "non-inertial reference frame", "vof method", "broken free surface", "analytical solution"]}, {"id": "84", "title": "Spike-based VLSI modeling of the ILD system in the echolocating bat", "abstract": "The azimuthal localization of objects by echolocating bats is based on the difference of echo intensity received at the two ears, known as the interaural level difference (ILD). Mimicking the neural circuitry in the bat associated with the computation of ILD, we have constructed a spike-based VLSI model that can produce responses similar to those seen in the lateral superior olive (LSO) and some parts of the inferior colliculus (IC). We further explore some of the interesting computational consequences of the dynamics of both synapses and cellular mechanisms.", "keywords": ["echolocation", "spike-based", "vlsi", "azimuthal localization", "hardware model", "masking"]}, {"id": "85", "title": "Having expectations of information systems benefits that match received benefits: does it really matter", "abstract": "A study was conducted to examine the effect of implementing a new system on its users, specifically, the relationship between pre-implementation expectations and their perceived benefits based on post-implementation experience. Disconfirmation theory was used as the theoretical basis; this predicts that unrealistically high expectations will result in lower levels of perceived benefit than those associated with realistic expectations (i.e. where expectations match experience). Support was found for this prediction, refuting the predictions of dissonance theory. In addition to examining expectations of system use generally, six expectation categories were examined to identify the critical categories where managers should keep expectations from becoming unrealistically high. Significant relationships were found for three expectation categories: system usefulness, ease of use, and information quality. The results indicate that creating and maintaining realistic expectations of future system benefits really does matter.", "keywords": ["information systems success", "end-user satisfaction", "user expectations", "disconfirmation theory"]}, {"id": "86", "title": "Online updating belief rule based system for pipeline leak detection under expert intervention", "abstract": "A belief rule base inference methodology using the evidential reasoning approach (RIMER) has been developed recently, where a new belief rule base (BRB) is proposed to extend traditional IF-THEN rules and can capture more complicated causal relationships using different types of information with uncertainties, but these models are trained off-line and it is very expensive to train and re-train them. As such, recursive algorithms have been developed to update the BRB systems online and their calculation speed is very high, which is very important, particularly for the systems that have a high level of real-time requirement. The optimization models and recursive algorithms have been used for pipeline leak detection. However, because the proposed algorithms are both locally optimal and there may exist some noise in the real engineering systems, the trained or updated BRB may violate some certain running patterns that the pipeline leak should follow. These patterns can be determined by human experts according to some basic physical principles and the historical information. Therefore, this paper describes under expert intervention, how the recursive algorithm update the BRB system so that the updated BRB cannot only be used for pipeline leak detection but also satisfy the given patterns. Pipeline operations under different conditions are modeled by a BRB using expert knowledge, which is then updated and fine tuned using the proposed recursive algorithm and pipeline operating data, and validated by testing data. All training and testing data are collected from a real pipeline. The study demonstrates that under expert intervention, the BRB expert system is flexible, can be automatically tuned to represent complicated expert systems, and may be applied widely in engineering. It is also demonstrated that compared with other methods such as fuzzy neural networks (FNNs), the RIMER has a special characteristic of allowing direct intervention of human experts in deciding the internal structure and the parameters of a BRB expert system.", "keywords": ["belief rule base", "expert system", "evidential reasoning", "recursive algorithm", "leak detection"]}, {"id": "87", "title": "Negative concord with polyadic quantifiers", "abstract": "In this paper we develop a syntax-semantics of negative concord in Romanian within a constraint-based lexicalist framework. We show that n-words in Romanian are best treated as negative quantifiers which may combine by resumption to form polyadic negative quantifiers. Optionality of resumption explains the existence of simple sentential negation readings alongside double negation readings. We solve the well-known problem of defining general semantic composition rules for translations of natural language expressions in a logical language with polyadic quantifiers by integrating our higher-order logical object language in Lexical Resource Semantics (LRS), whose constraint-based composition mechanisms directly support a systematic syntax-semantics for negative concord with polyadic quantification in Head-driven Phrase Structure Grammar (HPSG).", "keywords": ["negative concord", "romanian", "polyadic quantifiers", "head-driven phrase structure grammar", "lexical resource semantics"]}, {"id": "88", "title": "Psychological reactance to online recommendation services", "abstract": "Adoption of online recommendation services can improve the quality of decision making or it can pose threats to free choice. When people perceive that their freedom is reduced or threatened by others, they are likely to experience a psychological reactance where they attempt to restore the freedom. We performed an experimental study to determine whether users expectation of personalization increased their intention to use recommendation services, because their perception of expected threat to freedom caused by the recommendations reduced their intention to participate. An analysis based on subjects responses after using a hypothetical shopping website confirmed the two-sided nature of personalized recommendations, suggesting that the approach and avoidance strategies in persuasive communications can be effectively applied to personalized recommendation services on the web. Theoretical and practical implications are discussed.", "keywords": ["threat to freedom", "psychological reactance", "recommendation", "personalization", "online shopping"]}, {"id": "89", "title": "Enhanced ab initio protein folding simulations in PoissonBoltzmann molecular dynamics with self-guiding forces", "abstract": "We have investigated the sampling efficiency in molecular dynamics with the PB implicit solvent when self-guiding forces are added. Compared with a high-temperature dynamics simulation, the use of self-guiding forces in room-temperature dynamics is found to be rather efficient as measured by potential energy fluctuation, gyration radius fluctuation, backbone RMSD fluctuation, number of unique clusters, and distribution of low RMSD structures over simulation time. Based on the enhanced sampling method, we have performed ab initio folding simulations of two small proteins,1 and villin headpiece. The preliminary data for the folding simulations is presented. It is found that1 folding proceeds by initiation of the turn and the helix. The hydrophobic collapse seems to be lagging behind or at most concurrent with the formation of the helix. The hairpin stability is weaker than the helix in our simulations. Its role in the early folding events seems to be less important than the more stable helix. In contrast, villin headpiece folding proceeds first by hydrophobic collapse. The formation of helices is later than the collapse phase, different from the1 folding.", "keywords": ["poissonboltzmann", "molecular dynamics", "self-guiding forces", "protein folding", "bba1", "villin headpiece"]}, {"id": "90", "title": "An implicit Galerkin finite element RungeKutta algorithm for shock-structure investigations", "abstract": "This paper introduces an implicit high-order Galerkin finite element RungeKutta algorithm for efficient computational investigations of shock structures. The algorithm induces no spatial-discretization artificial diffusion, relies on cubic and higher-degree elements for an accurate resolution of the steep shock gradients, uses an implicit time integration for swift convergence to steady states, and employs original Neumann-type outlet boundary conditions in the form of generalized RankineHugoniot conditions on normal stress and balance of heat flux and deviatoric-stress work per unit time. The formulation automatically calculates the spatial extent of the shock and employs the single non-dimensional (0,1) computational domain for the determination of any shock structure. Since it is implicit, the algorithm rapidly generates steady shock structures, in at most 150 time steps for any upstream Mach number considered in this study. The finite element discretization is shown to be asymptotically convergent under progressive grid refinements, in respect of both the H0 H 0 and H1 H 1 error norms, with an H0 H 0 accuracy order as high as 6 and reduction of the discretization error to the round-off-error threshold of 110?9 with just 420 computational cells and 5th-degree elements. For upstream Mach numbers in the range 1.05?M?10.0, the computational results satisfy the RankineHugoniot conditions and reflect independently published NavierStokes results.", "keywords": ["normal shocks", "shock-wave structures", "computational stability", "finite elements", "implicit rungekutta"]}, {"id": "91", "title": "A contemporary view of organizational safety: variability and interactions of organizational processes", "abstract": "Studies of qualitative assessment of organizational processes (e.g., safety audits and performance indicators) and their incorporation into risk models have been based on a normative view that decomposes organizations into separate processes that are likely to fail and lead to accidents. This paper discusses a control theoretic framework of organizational safety that views accidents as a result of performance variability of human behaviors and organizational processes whose complex interactions and coincidences lead to adverse events. Safety-related tasks managed by organizational processes are examined from the perspective of complexity and coupling. This allows safety analysts to look deeper into the complex interactions of organizational processes and how these may remain hidden or migrate toward unsafe boundaries. A taxonomy of variability of organizational processes is proposed and challenges in managing adaptability are discussed. The proposed framework can be used for studying interactions between organizational processes, changes of priorities over time, delays in effects, reinforcing influences, and long-term changes of processes. These dynamic organizational interactions are visualized with the use of system dynamics. The framework can provide a new basis for modeling organizational factors in risk analysis, analyzing accidents and designing safety reporting systems.", "keywords": ["organizational safety", "systems theory", "variability", "complexity and coupling", "safety management", "system dynamics"]}, {"id": "92", "title": "Robust image-based visual servoing using invariant visual information", "abstract": "Image-based visual servoing from spherical projection. Decoupled control using invariant features. A near-linear behavior is obtained thanks to the proposed features. The sensitivity to image noise is taken into account.", "keywords": ["robust visual servoing", "spherical projection"]}, {"id": "93", "title": "a distributed numerical/simulative algorithm for the analysis of large continuous time markov chains", "abstract": "A distributed algorithm is introduced for the analysis of large continuous time Markov chains (CTMCs) by combining in some sense numerical solution techniques and simulation. CTMCs are described as a set of processes communicating via message passing. The state of a process is described by a probability distribution over a set of reachable states rather than by a single state. Simulation is used to determine event times and messages types to be exchanged, whereas transitions are realized by vector matrix products as in iterative numerical analysis techniques. In this way, the state space explosion of numerical analysis is avoided, but it is still possible to determine more detailed results than with simulation. Parallelization of the algorithm is realized applying a conservative synchronization scheme, which exploits the possibility of precomputing event times as already proposed for parallel simulation of CTMCs. In contrast to a pure simulation approach, the amount of computation is increased, whereas the amount of communication keeps constant. Hence it is possible to achieve even on a workstation cluster a significant speedup.", "keywords": ["communication", "speedup", "distributed algorithms", "simulation", "parallel simulation", "synchronization", "event", "analysis", "computation", "probability", "timing", "combinational", "product", "vectorization", "transit", "space", "exploit", "parallel", "message", "types", "process", "message-passing", "algorithm", "distributed", "continuation", "iter", "scheme", "cluster"]}, {"id": "94", "title": "Properties of prediction sorting", "abstract": "One of the major sources of unwanted variation in an industrial process is the raw material quality. However, if the raw materials are sorted into more homogeneous groups before production, each group can be treated differently. In this way the raw materials can be better utilized and the stability of the end product may be improved. Prediction sorting is a methodology for doing this. The procedure is founded on the fuzzy c-means algorithm where the distance in the objective function is based on the predicted end product quality. Usually empirical models such as linear regression are used for predicting the end product quality. By using simulations and bootstrapping, this paper investigates how the uncertainties connected with empirical models affect the optimization of the splitting and the corresponding process variables. The results indicate that the practical consequences of uncertainties in regression coefficients are small. ", "keywords": ["raw material variability", "sorting", "robustness", "fuzzy clustering"]}, {"id": "95", "title": "Interstitial fluid glucose time-lag correction for real-time continuous glucose monitoring", "abstract": "Time lag between subcutaneous interstitial fluid and plasma glucose decreases the accuracy of real-time continuous glucose monitors. However, inverse filters can be designed to correct time lag and attenuate noise enabling the bloodglucose profile to be reconstructed in real time from continuous measurements of the interstitial-fluid glucose. We designed and tested a Wiener filter using a set of 20 sensor-glucose tracings (?30h each) with a 1-min sample interval. Delays of 102min (meanSD) were introduced into each signal with additive Gaussian white noise (SNR=40dB). Performance of the filter was compared to conventional causal and non-causal seventh-order finite-impulse response (FIR) filters. Time lags introduced an error of 5.32.7%. The error increased in the presence of noise (to 5.72.6%) and attempts to remove the noise with conventional low-pass filtering increased the error still further (to 7.03.5%). In contrast, the Wiener filter decreased the error attributed to time delay by ?50% in the presence of noise (from 5.7% to 2.601.26%) and by ?75% in the absence of noise (5.3% to 1.31%). Introducing time-lag correction without increasing sensitivity to noise can increase CGM accuracy.", "keywords": ["continuous glucose monitoring", "wiener filter", "time-lag", "interstitial fluid"]}, {"id": "96", "title": "Fast data access and energy-efficient protocol for wireless data broadcast", "abstract": "In wireless mobile computing environments, broadcasting is an effective and scalable technique to disseminate information to a massive number of clients, wherein the energy usage and responsiveness are considered major concerns. Existing air indexing schemes for data broadcast have focused on energy efficiency (reducing tuning time) only. On the other hand, existing broadcast scheduling schemes have aimed at reducing access latency through nonflat data broadcast to improve responsiveness only. Not much work has addressed the energy efficiency and responsiveness issues concurrently. In this paper, we propose a fast data access scheme concurrently supporting energy saving protocol that constructs the broadcast channels according to the access frequency of each type of message in order to improve energy efficiency in mobile devices (MDs). The pinwheel scheduling algorithm (PSA) presented in this paper is used to organize all types of messages in the broadcast channel in the most symmetrical distribution in order to reduce both the tuning and access time. The performance of the proposed mechanism is analyzed, and the improvement over existing methods is demonstrated numerically. The results show that the proposed mechanism is capable of improving both the tuning and access time due to the presence of skewness in the access distribution among the disseminated messages. ", "keywords": ["mobile computing", "wireless broadcast system", "energy saving", "access time", "tuning time"]}, {"id": "97", "title": "ONOFF retinal ganglion cells temporally encode OFF/ON sequence", "abstract": "While the functions of ON and OFF retinal ganglion cells have been intensively investigated, that of ONOFF cells has not. In the present study, the temporal properties of spike trains emitted from ON-OFF cells in response to randomly flickering or multiphase ramp stimuli were examined in the Japanese quail. The results indicate that the firing of ON-spikes was influenced by the recent firing of OFF-spikes, and vice versa. As a result of this interaction, OFF/ON sequence of light intensity change was encoded with a spike pair with an interval of 20ms, indicating that temporal coding is utilized in the vertebrate visual system as early as the retina. Thus, the present results suggest that retinal neuronal circuits may detect specific sequential features of stimuli.", "keywords": ["onoff retinal ganglion cells", "spike train", "stimulus sequence", "temporal coding", "retina", "optic nerve", "quail"]}, {"id": "98", "title": "Noise reduction for magnetic resonance images via adaptive multiscale products thresholding", "abstract": "Edge-preserving denoising is of great interest in medical image processing. This paper presents a wavelet-based multiscale products thresholding scheme for noise suppression of magnetic resonance images. A Canny edge detector-like dyadic wavelet transform is employed. This results in the significant features in images evolving with high magnitude across wavelet scales, while noise decays rapidly. To exploit the wavelet interscale dependencies we multiply the adjacent wavelet subbands to enhance edge structures while weakening noise. In the multiscale products, edges can be effectively distinguished from noise. Thereafter, an adaptive threshold is calculated and imposed on the products, instead of on the wavelet coefficients, to identify important features. Experiments show that the proposed scheme better suppresses noise and preserves edges than other wavelet-thresholding denoising methods.", "keywords": ["denoising", "magnetic resonance image", "multiscale products", "thresholding", "wavelet transform"]}, {"id": "99", "title": "Integration of semi-fuzzy SVDD and CC-Rule method for supplier selection", "abstract": "A model based on semi-fuzzy support vector domain description (semi-fuzzy SVDD) is put forward to address multi-classification problem involved in supplier selection. By preprocessing using semi-fuzzy kernel clustering algorithm, original samples are divided into two subsets: deterministic samples and fuzzy samples. Only the fuzzy samples, rather than all original ones, require expert judgment to decide their categories and are selected as training samples to accomplish SVDD specification. Therefore, the samples preprocessing method can not only decrease experts working strength, but also achieve less computational consumption and better performance of the classifier. Nevertheless, in order to accomplish practical decision making, another condition has to be met: good explanations to the decision. A rule extraction method based on cooperative coevolution algorithm (CCEA), is introduced to achieve the target. To validate the proposed methodology, samples from real world were employed for experiments, with results compared with conventional multi-classification support vector machine approaches and other artificial intelligence techniques. Moreover, in terms of rule extraction, experiments on key parameters, different methods including decompositional and pedagogical ones etc. were also conducted.", "keywords": ["supplier selection", "semi-fuzzy kernel clustering algorithm", "support vector domain description", "cooperative coevolution algorithm"]}, {"id": "100", "title": "A distributed disk layer for mass storage at DESY", "abstract": "For the mass storage system at DESY, a disk layer is under development. Decoupling the client request queue and access to the mass storage system by means of migration, staging and prefetching it shall provide full utilization of robot and drive resources. By managing distributed disk resources in the heterogeneous computing environment of DESY, optimized data access shall be given. ", "keywords": ["mass storage", "disk layer", "client-server architecture", "network file system"]}, {"id": "101", "title": "On the learning machine for three dimensional mapping", "abstract": "In this paper, we investigate the neural network with three-dimensional parameters for applications like 3D image processing, interpretation of 3D transformations, and 3D object motion. A 3D vector represents a point in the 3D space, and an object might be represented with a set of these points. Thus, it is desirable to have a 3D vector-valued neural network, which deals with three signals as one cluster. In such a neural network, 3D signals are flowing through a network and are the unit of learning. This article also deals with a related 3D back-propagation (3D-BP) learning algorithm, which is an extension of conventional back-propagation algorithm in the single dimension. 3D-BP has an inherent ability to learn and generalize the 3D motion. The computational experiments presented in this paper evaluate the performance of considered learning machine in generalization of 3D transformations and 3D pattern recognition.", "keywords": ["3d back-propagation algorithm", "3d real-valued vector", "orthogonal matrix", "3d face"]}, {"id": "102", "title": "A unified approach to the analysis of Horton-Strahler parameters of binary tree structures", "abstract": "The Horton-Strahler number naturally arose from problems in various fields, e.g., geology, molecular biology and computer science. Consequently, detailed investigations of related parameters for different classes of binary tree structures are of interest. This paper shows one possibility of how to perform a mathematical analysis for parameters related to the Horton-Strahler number in a unified way such that only a single analysis is needed to obtain results for many different classes of trees. The method is explained by the examples of the expected Horton-Strahler number and the related rth moments, the average number of critical nodes, and the expected distance between critical nodes. ", "keywords": ["average-case analysis", "combinatorial structures", "horton-strahler numbers", "analytic combinatorics"]}, {"id": "103", "title": "Visitors of two types of museums: A segmentation study", "abstract": "Market segmentation comprises a wide range of measurement tools that are useful for the sake of supporting marketing and promotional policies also in the sector of cultural economics. This paper aims to contribute to the literature on segmenting cultural visitors by using the Bagged Clustering method, as an alternative and effective strategy to conduct cluster analysis when binary variables are used. The technique is a combination of hierarchical and partitioning methods and presents several advantages with respect to more standard techniques, such as k-means and LVQ. For this purpose, two ad hoc surveys were conducted between June and September 2011 in the two principal museums of the two provinces of the Trentino-South Tyrol region (Bolzano and Trento), Northern Italy: the South Tyrol Museum of Archaeology in Bolzano (TZI), hosting the permanent exhibition of the Iceman tzi, and the Museum of Modern and Contemporaneous Art of Trento and Rovereto (MART). The segmentation analysis was conducted separately for the two kinds of museums in order to find similarities and differences in behaviour patterns and characteristics of visitors. The analysis identified three and two cluster segments respectively for the MART and TZI visitors, where two TZI clusters presented similar characteristics to two out of three MART groups. Conclusions highlight marketing and managerial implications for a better direction of the museums.", "keywords": ["bagged clustering", "logit models", "museum", "segmentation", "motivation"]}, {"id": "104", "title": "Grid Resource Availability Prediction-Based Scheduling and Task Replication", "abstract": "The frequent and volatile unavailability of volunteer-based Grid computing resources challenges Grid schedulers to make effective job placements. The manner in which host resources become unavailable will have different effects on different jobs, depending on their runtime and their ability to be checkpointed or replicated. A multi-state availability model can help improve scheduling performance by capturing the various ways a resource may be available or unavailable to the Grid. This paper uses a multi-state model and analyzes a machine availability trace in terms of that model. Several prediction techniques then forecast resource transitions into the model's states. We analyze the accuracy of our predictors, which outperform existing approaches. We also propose and study several classes of schedulers that utilize the predictions, and a method for combining scheduling factors. We characterize the inherent tradeoff between job makespan and the number of evictions due to failure, and demonstrate how our schedulers can navigate this tradeoff under various scenarios. Lastly, we propose job replication techniques, which our schedulers utilize to replicate those jobs that are most likely to fail. Our replication strategies outperform others, as measured by improved makespan and fewer redundant operations. In particular, we define a new metric for replication efficiency, and demonstrate that our multi-state availability predictor can provide information that allows our schedulers to be more efficient than others that blindly replicate all jobs or some static percentage of jobs.", "keywords": ["multi-state", "prediction", "availability", "characterization", "scheduling", "replication"]}, {"id": "105", "title": "On the number of optimal identifying codes in a twin-free graph", "abstract": "Let G be a simple, undirected graph with vertex set V . For v?V v ? V and r?1 r ? 1 , we denote by BG,r(v) B G , r ( v ) the ball of radius r and centre v . A set C?V C ? V is said to be an r-identifying code  in G if the sets BG,r(v)?C B G , r ( v ) ? C , v?V v ? V , are all nonempty and distinct. A graph G which admits an r-identifying code is called r-twin-free  or r-identifiable , and in this case the smallest size of an r-identifying code in G is denoted by ? r I D ( G ) . We study the number of different optimal r-identifying codes C , i.e., such that | C | = ? r I D ( G ) , that a graph G can admit, and try to construct graphs having many such codes.", "keywords": ["graph theory", "twin-free graphs", "identifiable graphs", "identifying codes"]}, {"id": "106", "title": "On the impact of triangle shapes for boundary layer problems using high-order finite element discretization", "abstract": "The impact of triangle shapes, including angle sizes and aspect ratios, on accuracy and stiffness is investigated for simulations of highly anisotropic problems. The results indicate that for high-order discretizations, large angles do not have an adverse impact on solution accuracy. However, a correct aspect ratio is critical for accuracy for both linear and high-order discretizations. Large angles are also found to be not problematic for the conditioning of the linear systems arising from the discretizations. Further, when choosing preconditioning strategies, coupling strengths among elements rather than element angle sizes should be taken into account. With an appropriate preconditioner, solutions on meshes with and without large angles can be achieved within a comparable time.", "keywords": ["triangle shape", "large angle", "aspect ratio", "high-order finite element", "anisotropic problems", "ilu-factorization"]}, {"id": "107", "title": "Tree-walking automata cannot be determinized", "abstract": "Tree-walking automata are a natural sequential model for recognizing languages of finite trees. Such automata walk around the tree and may decide in the end to accept it. It is shown that deterministic tree-walking automata are weaker than nondeterministic tree-walking automata.  ", "keywords": ["tree-walking automata", "deterministic tree-walking automata"]}, {"id": "108", "title": "ADAPTABLE MULTI-AGENT SYSTEMS: THE CASE OF THE GAIA METHODOLOGY", "abstract": "Changes and adaptations are always necessary after the deployment of a multi-agent system (MAS), as well as of any other type of software systems. Some of these changes may be simply perfective and have local impact only. However, adaptive changes to meet new situations in the operational environment of the MAS may impact globally on the overall design. More specifically, those changes usually affect the organizational structure of the MAS. In this paper we analyze the issue of design change/adaptation in a MAS organization, and the specific problem of how to properly model/design a MAS so as to make it ready for adaptation. Special attention is paid to the Gaia methodology, whose suitability in dealing with adaptable MAS organizations is also discussed with the help of an illustrative application example.", "keywords": ["agent-oriented methodologies", "adaptive/adaptable organizations", "design for changes", "gaia methodology"]}, {"id": "109", "title": "On the multicriteria allocation problem", "abstract": "We consider multicriteria allocation problems with linear sum objectives. Despite the fact that the single objective allocation problem is easily solvable, we show that already in the bicriteria case the problem becomes intractable, is NP-hard and has a non-connected efficient set in general. Using the equivalence to appropriately defined multiple criteria multiple-choice knapsack problems, an algorithm is suggested that uses partial dominance conditions to save computational time. Different types of enumeration schemes are discussed, for example, with respect to the number of necessary filtering operations and with regard to possible parallelizations of the procedure.", "keywords": ["multicriteria optimization", "combinatorial optimization", "location-allocation problem"]}, {"id": "110", "title": "Optimization and correction of the tool path of the five-axis milling machine - Part 2: Rotations and setup", "abstract": "We present two new algorithms (which supplement Algorithms 1, 2 and 3 presented in part 1) to optimize the tool path of the five-axis numerically controlled milling machine. Algorithm 4 optimizes a set of feasible rotations. Algorithm 5 presents a least-square optimization with regard to a setup of the machine  ", "keywords": ["nc-programming", "cad/cam", "optimization", "milling machines"]}, {"id": "111", "title": "teaching about the risks of electronic voting technology", "abstract": "In these interesting times computer scientists are increasingly called upon to help concerned citizens understand the risks involved in the current generation of electronic voting machines. These risks and the concurrent escalation of legal challenges to the election system in the United States have shaken the confidence of many Americans that a fair and accurate election is even possible. As computer science educators we have an opportunity to add breadth and depth to our curriculum by using these issues to show how existing concepts can be applied to new problems, and how new problems extend our field. In this paper we identify some of the main problems with e-voting machines and vote-counting technology and suggest ways that discussions of the risks and the attendant societal and ethical issues might be incorporated into the computer science curriculum.", "keywords": ["electronic voting"]}, {"id": "112", "title": "Effectiveness of bibliographic searches performed by paediatric residents and interns assisted by librarians. A randomised controlled trial", "abstract": "Background:? Considerable barriers still prevent paediatricians from successfully using information retrieval technology.", "keywords": ["decision support", "evidence based library and information practice", "evidence based practice", "evidence-based medicine", "health science", "health services research", "information seeking behaviour", "librarians", "library and information science", "reflective practice"]}, {"id": "113", "title": "A genetic programming model for bankruptcy prediction: Empirical evidence from Iran", "abstract": "Prediction of corporate bankruptcy is a phenomenon of increasing interest to investors/creditors, borrowing firms, and governments alike. Timely identification of firms impending failure is indeed desirable. By this time, several methods have been used for predicting bankruptcy but some of them suffer from underlying shortcomings. In recent years, Genetic Programming (GP) has reached great attention in academic and empirical fields for efficient solving high complex problems. GP is a technique for programming computers by means of natural selection. It is a variant of the genetic algorithm, which is based on the concept of adaptive survival in natural organisms. In this study, we investigated application of GP for bankruptcy prediction modeling. GP was applied to classify 144 bankrupt and non-bankrupt Iranian firms listed in Tehran stock exchange (TSE). Then a multiple discriminant analysis (MDA) was used to benchmarking GP model. Genetic model achieved 94% and 90% accuracy rates in training and holdout samples, respectively; while MDA model achieved only 77% and 73% accuracy rates in training and holdout samples, respectively. McNemar test showed that GP approach outperforms MDA to the problem of corporate bankruptcy prediction.", "keywords": ["bankruptcy prediction", "financial ratios", "genetic programming", "multiple discriminant analysis", "iranian companies"]}, {"id": "114", "title": "A queueing model for general group screening policies and dynamic item arrivals", "abstract": "Classification of items as good or bad can often be achieved more economically by examining the items in groups rather than individually. If the result of a group test is good, all items within it can be classified as good, whereas one or more items are bad in the opposite case. Whether it is necessary to identify the bad items or not, and if so, how, is described by the screening policy. In the course of time, a spectrum of group screening models has been studied, each including some policy. However, the majority ignores that items may arrive at random time epochs at the testing center in real life situations. This dynamic aspect leads to two decision variables: the minimum and maximum group size. In this paper, we analyze a discrete-time batch-service queueing model with a general dependency between the service time of a batch and the number of items within it. We deduce several important quantities, by which the decision variables can be optimized. In addition, we highlight that every possible screening policy can, in principle, be studied, by defining the dependency between the service time of a batch and the number of items within it appropriately.", "keywords": ["queueing", "group screening policies", "dynamic item arrivals"]}, {"id": "115", "title": "efficient character-level taint tracking for java", "abstract": "Over 80% of web services are vulnerable to attack, and much of the danger arises from command injection vulnerabilities. We present an efficient character-level taint tracking system for Java web applications and argue that it can be used to defend against command injection vulnerabilities. Our approach involves modification only to Java library classes and the implementation of the Java servlets framework, so it requires only a one-time modification to the server without any subsequent modifications to a web application's bytecode or access to the web application's source code. This makes it easy to deploy our technique and easy to secure legacy web software. Our preliminary experiments with the JForum web application suggest that character-level taint tracking adds 0-15% runtime overhead.", "keywords": ["information flow", "java", "dynamic taint tracking", "web applications"]}, {"id": "116", "title": "How Accurate Can Block Matches Be in Stereo Vision", "abstract": "This article explores the subpixel accuracy attainable for the disparity computed from a rectified stereo pair of images with small baseline. In this framework we consider translations as the local deformation model between patches in the images. A mathematical study first shows how discrete block-matching can be performed with arbitrary precision under Shannon-Whittaker conditions. This study leads to the specification of a block-matching algorithm which is able to refine disparities with subpixel accuracy. Moreover, a formula for the variance of the disparity error caused by the noise is introduced and proved. Several simulated and real experiments show a decent agreement between this theoretical error variance and the observed root mean squared error in stereo pairs with good signal-to-noise ratio and low baseline. A practical consequence is that under realistic sampling and noise conditions in optical imaging, the disparity map in stereo-rectified images can be computed for the majority of pixels (but only for those pixels with meaningful matches) with a 1/20 pixel precision.", "keywords": ["block-matching", "subpixel accuracy", "noise error estimate"]}, {"id": "117", "title": "The Cognitive Internet of Things: A Unified Perspective", "abstract": "In this article, we present a unified perspective on the cognitive internet of things (CIoT). It is noted that within the CIoT design we observe the convergence of energy harvesting, cognitive spectrum access and mobile cloud computing technologies. We unify these distinct technologies into a CIoT architecture which provides a flexible, dynamic, scalable and robust network design road-map for large scale IoT deployment. Since the prime objective of the CIoT network is to ensure connectivity between things, we identify key metrics which characterize the network design space. We revisit the definition of cognition in the context of IoT networks and argue that both the energy efficiency and the spectrum efficiency are key design constraints. To this end, we define a new performance metric called the overall link success probability which encapsulates these constraints. The overall link success probability is characterized by both the self-sustainablitiy of the link through energy harvesting and the availability of spectrum for transmissions. With the help of a reference scenario, we demonstrate that well-known tools from stochastic geometry can be employed to investigate both the node and the network level performance. In particular, the reference scenario considers a large scale deployment of a CIoT network empowered by solar energy harvesting deployed along with the centralized CIoT device coordinators. It is assumed that CIoT network is underlaid with a cellular network, i.e., CIoT nodes share spectrum with mobile users subject to a certain co-existence constraint. Considering the dynamics of both energy harvesting and spectrum sharing, the overall link success probability is then quantified. It is shown that both the self-sustainability of the link, and the availability of transmission opportunites, are coupled through a common parameter, i.e., the node level transmit power. Furthermore, provided the co-existence constraint is satisfied, the link level success in the presence of both the inter-network and intra-network interference is an increasing function of the transmit power. We demonstrate that the overall link level success probability can be maximized by employing a certain optimal transmit power. Characterization of such an optimal operational point is presented. Finally, we highlight some of the future directions which can benefit from the analytical framework developed in this paper.", "keywords": ["internet-of-things", "cognitive radios", "solar energy harvesting", "stochastic cloud cover", "shared spectrum", "underlay", "interference"]}, {"id": "118", "title": "Lower bounds for modular counting by circuits with modular gates", "abstract": "We prove that constant depth circuits, with one layer of MODm gates at the inputs, followed by a fixed number of layers of MODp gates, where p is prime, require exponential size to compute the MODq function, if q is a prime that divides neither p nor m.", "keywords": ["circuit complexity", "modular counting"]}, {"id": "119", "title": "On the physical implementation of logical transformations: Generalized L-machines", "abstract": "Any account of computation in a physical system, whether an artificial computing device or a natural system considered from a computational point of view, invokes some notion of the relationship between the abstract-logical and concrete-physical aspects of computation. In a recent paper, James Ladyman explored this relationship using a \"hybrid physical-logical entity\" - the L-machine - and the general account of computation that it supports [J. Ladyman, What does it mean to say that a physical system implements a computation?, Theoretical Computer Science 410 (2009) 376-383]. The underlying L-machine of Ladyman's analysis is, however, classical and highly idealized, and cannot capture essential aspects of computation in important classes of physical systems (e.g. emerging nanocomputing devices) where logical states do not have faithful physical representations and where noise and quantum effects prevail. In this work we generalize the L-machine to allow for generally unfaithful and noisy implementations of classical logical transformations in quantum mechanical systems. We provide a formal definition and physical-information-theoretic characterization of generalized quantum L-machines (QLMs), identify important classes of QLMs, and introduce new efficacy measures that quantify the faithfulness and fidelity with which logical transformations are implemented by these machines. Two fundamental issues emphasized by Ladyman - realism about computation and the connection between logical and physical irreversibility - are reconsidered within the more comprehensive account of computation that follows from our generalization of the L-machine.  ", "keywords": ["implementation", "l-machine", "landauer's principle", "physical information theory", "physics of computation", "realism about computation", "representation"]}, {"id": "120", "title": "Tangibles for learning: a representational analysis of physical manipulation", "abstract": "Manipulativesphysical learning materials such as cubes or tilesare prevalent in educational settings across cultures and have generated substantial research into how actions with physical objects may support childrens learning. The ability to integrate digital technology into physical objectsso-called digital manipulativeshas generated excitement over the potential to create new educational materials. However, without a clear understanding of how actions with physical materials lead to learning, it is difficult to evaluate or inform designs in this area. This paper is intended to contribute to the development of effective tangible technologies for childrens learning by summarising key debates about the representational advantages of manipulatives under two key headings: offloading cognitionwhere manipulatives may help children by freeing up valuable cognitive resources during problem solving, and conceptual metaphorswhere perceptual information or actions with objects have a structural correspondence with more symbolic concepts. The review also indicates possible limitations of physical objectsmost importantly that their symbolic significance is only granted by the context in which they are used. These arguments are then discussed in light of tangible designs drawing upon the authors current research into tangibles and young childrens understanding of number.", "keywords": ["tangible technologies", "physical manipulatives", "mathematics learning", "educational technology", "virtual manipulatives"]}, {"id": "121", "title": "Identifying Spurious Interactions and Predicting Missing Interactions in the Protein-Protein Interaction Networks via a Generative Network Model", "abstract": "With the rapid development of high-throughput experiment techniques for protein-protein interaction (PPI) detection, a large amount of PPI network data are becoming available. However, the data produced by these techniques have high levels of spurious and missing interactions. This study assigns a new reliably indication for each protein pairs via the new generative network model (RIGNM) where the scale-free property of the PPI network is considered to reliably identify both spurious and missing interactions in the observed high-throughput PPI network. The experimental results show that the RIGNM is more effective and interpretable than the compared methods, which demonstrate that this approach has the potential to better describe the PPI networks and drive new discoveries.", "keywords": ["protein-protein interaction network", "generative network model", "ppi data denoising"]}, {"id": "122", "title": "Similarities between powersets of terms", "abstract": "Generalisation of the foundational basis for many-valued logic programming builds upon generalised terms in the form of powersets of terms. A categorical approach involving set and term functors as monads allows for a study of monad compositions that provide variable substitutions and compositions thereof. In this paper, substitutions and unifiers appear as constructs in Kleisli categories related to particular composed powerset term monads. Specifically, we show that a frequently used similarity-based approach to fuzzy unification is compatible with the categorical approach, and can be adequately extended in this setting; also some examples are included in order to illuminate the definitions.  ", "keywords": ["similarities", "fuzzy unification", "category theory and unification", "generalised terms"]}, {"id": "123", "title": "Feedback vertex sets on restricted bipartite graphs", "abstract": "A feedback vertex set (FVS) in a graph is a subset of vertices whose complement induces a forest. Finding a minimum FVS is N P-complete on bipartite graphs, but tractable on convex bipartite graphs and on chordal bipartite graphs. A bipartite graph is called tree convex, if a tree is defined on one part of the vertices, such that for every vertex in the other part, its neighborhood induces a subtree. When the tree is a path, a triad or a star, the bipartite graph is called convex bipartite, triad convex bipartite or star convex bipartite, respectively. We show that: (I) FVS is tractable on triad convex bipartite graphs; (2) FVS is N P-complete on star convex bipartite graphs and on tree convex bipartite graphs where the maximum degree of vertices on the tree is at most three.  ", "keywords": ["feedback vertex set", "tree convex bipartite", "polynomial time", "n p-complete"]}, {"id": "124", "title": "The extended Delaunany tessellation", "abstract": "The extended Delaunay tessellation (EDT) is presented in this paper as the unique partition of a node set into polyhedral regions defined by nodes lying on the nearby Voronoi spheres. Until recently, all the FEM mesh generators were limited to the generation of tetrahedral or hexahedral elements (or triangular and quadrangular in 2D problems). The reason for this limitation was the lack of any acceptable shape function to be used in other kind of geometrical elements. Nowadays, there are several acceptable shape functions for a very large class of polyhedra. These new shape functions, together with the EDT, gives an optimal combination and a powerful tool to solve a large variety of physical problems by numerical methods. The domain partition into polyhedra presented here does not introduce any new node nor change any node position. This makes this Process suitable for Lagrangian problems and meshless methods in which only the connectivity information is used and there is no need for any expensive smoothing process.", "keywords": ["mesh generation", "delaunayl/voronoi tessellations"]}, {"id": "125", "title": "interlaced euler scheme for stiff systems of stochastic differential equations", "abstract": "In deterministic as well as stochastic models, stiff systems, i.e., systems with vastly different time scales where the fast scales are stable, are very common. It is well known that the implicit Euler method is well suited for stiff deterministic equations (modeled by ODEs) while the explicit Euler is not. In particular, once the fast transients are over, the implicit Euler allows for the choice of time steps comparable to the slowest time scales of the system. In stochastic systems (modeled by SDEs) the picture is more complex. While the implicit Euler has better stability properties over the explicit Euler, it underestimates the stationary variance. In general, one may not expect any method to work successfully by taking time steps of the order of the slowest time scale. We explore the idea of interlacing large implicit Euler steps with a sequence of small explicit Euler steps. In particular, we present our study of a linear test system of SDEs and demonstrate that such interlacing could effectively deal with stiffness. We also discuss the uniform convergence of mean and variance.", "keywords": ["explicit euler method", "stochastic differential equations", "implicit euler method", "uniform convergence", "stiffness"]}, {"id": "126", "title": "Inverse problem for wave propagation in a perturbed layered half-space", "abstract": "This paper is concerned with the inverse medium scattering problem in a perturbed, layered, half-space, which is a problem related to the seismologial investigation of inclusions inside the earth's crust. A wave penetrable object is located in a layer where the refraction index is different from the other part of the half-space. Wave propagation in such a layered half-space is different from that in a homogeneous half-space. In a layered half-space, a scattered wave consists of a free wave and a guided wave. In many cases, only the free-wave far-field or only the guided-wave far-field can be measured. We establish mathematical formulas for relations between the object, the incident wave and the scattered wave. In the ideal condition where exact data are given, we prove the uniqueness of the inverse problem. A numerical example is presented for the reconstruction of a penetrable object from simulated noise data.  ", "keywords": ["inverse problems"]}, {"id": "127", "title": "an explorative analysis of user evaluation studies in information visualisation", "abstract": "This paper presents an analysis of user studies from a review of papers describing new visualisation applications and uses these to highlight various issues related to the evaluation of visualisations. We first consider some of the reasons why the process of evaluating visualisations is so difficult. We then dissect the problem by discussing the importance of recognising the nature of experimental design, datasets and participants as well as the statistical analysis of results. We propose explorative evaluation as a method of discovering new things about visualisation techniques, which may give us a better understanding of the mechanisms of visualisations. Finally we give some practical guidance on how to do evaluation correctly.", "keywords": ["explorative evaluation", "case study", "evaluation", "information visualisation"]}, {"id": "128", "title": "An investigation the factors affecting MIS student burnout in technical-vocational college", "abstract": "Management information system (MIS) students are one of the most important information system (IS) employee sources. However, the determinants of student's burnout for MIS major students have received little attentions, despite their importance as indicator in predicting professional burnout and their working intention after their graduation and becoming IS professionals. This study explores the antecedents of student burnout for MIS major at technical-vocational college. Self-efficacy, social support, and sex-role were considered as antecedents to MIS student burnout. A questionnaire method by self-administered technique was used in this study. Multiple regression analysis was used to analyze the hypotheses. Statistical results displayed that MIS students with social support, self-efficacy and femininity have predictive power over student burnout. MIS students with social support and masculinity also have predictive power over self-efficacy.", "keywords": ["technical-vocational education", "student burnout", "self-efficacy", "social support", "sex-role"]}, {"id": "129", "title": "BEM calculation of the complex thermal impedance of microelectronic devices", "abstract": "This paper presents a numerical method for modelling the dynamic thermal behaviour of microelectronic structures in the frequency domain. A boundary element method (BEM) based on a Green's function solution is proposed for solving the 3D heat equation in phasor notation. The method is capable of calculating the AC temperature and heat flux distributions and complex thermal impedance for packages composed of an arbitrary number of bar-shaped components. Various types of boundary conditions, including thermal contact resistance and convective cooling, can be taken into account. A simple benchmark case is investigated and a good convergence towards the analytical solution is obtained. Simulation results for a thin plate under convective cooling are compared with a theoretical model and an excellent agreement is observed. In a second example a more complicated three-layer structure is investigated. The BEM is used to analyse the thermal behaviour if delamination of the package occurs, and a physical explanation for the results is given.", "keywords": ["thermal impedance", "microelectronics", "boundary element method", "green's function", "nyquist plot", "heat transfer", "phasor notation"]}, {"id": "130", "title": "Directional texture transfer for video", "abstract": "Texture transfer is a method that copies the texture of a reference image to a target image. This technique has an advantage in that various styles can be expressed according to the reference image, in a single framework. However, in this technique, it is not easy to control the effect of each style. In addition, when this technique is extended to processing video images, maintaining temporal coherence is very difficult. In this paper, we propose an algorithm that transfers the texture of a reference image to a target video while retaining the directionality of the target video. The algorithm maintains the temporal coherency of the transferred texture, and controls the style of the texture transfer.", "keywords": ["texture transfer", "temporal coherence", "example-based rendering", "video processing"]}, {"id": "131", "title": "Wind parameters extraction from aircraft trajectories", "abstract": "High altitude wind parameters can be extracted from recorded aircraft positions. This method gives the best results when aircraft are stabilized (no turn, no climb, no descend) This method can extract wind dynamics, e.g. how wind parameters change over time. Our method performs well thanks to a mix of automatic wind extraction and direct manipulation technique.", "keywords": ["wind extraction", "least squares approximation", "air traffic control", "data exploration", "visual analytics"]}, {"id": "132", "title": "A three-dimensional model of the human transglutaminase 1: insights into the understanding of lamellar ichthyosis", "abstract": "The stratum corneum, the outer layer of the epidermis, serves as a protective barrier to isolate the skin from the external environment. Keratinocyte transglutaminase 1 (TGase 1) catalyzes amide crosslinking between glutamine and lysine residues on precursor proteins forming the impermeable layers of the epidermal cell envelopes (CE), the highly insoluble membranous structures of the stratum corneum. Patients with the autosomal recessive skin disorder lamellar ichthyosis (LI) appear to have deficient cross-linking of the cell envelope due to mutations identified in TGase 1, linking this enzyme to LI. In the absence of a crystal structure, molecular modeling was used to generate the structure of TGase 1. We have mapped the known mutations of TGase 1 from our survey obtained from a search of PubMed and successfully predicted the impact of these mutations on LI. Furthermore, we have identified Ca(2+) binding sites and propose that Ca(2+) induces a cis to trans isomerization in residues near the active site as part of the enzyme transamidation activation. Docking experiments suggest that substrate binding subsequently induces the reverse cis to trans isomerization, which may be a significant part of the catalytic process. These results give an interpretation at the molecular level of previously reported mutations and lead to further insights into the structural model of TGase 1, providing a new basis for understanding LI.", "keywords": ["keratinocyte transglutaminase 1", "lamellar ichthyosis", "mutations", "metal ions", "isomerization", "molecular modeling"]}, {"id": "133", "title": "Understanding continuance usage of mobile sites", "abstract": "Purpose - The purpose of this research is to draw on both perspectives of technological perceptions and flow experience to examine continuance usage of mobile sites. Design/methodology/approach - Based on the valid responses collected from a. survey questionnaire, structural equation modeling technology was employed to examine the research model. Findings - The results indicated that both perspectives of technological perceptions and flow experience have effects on satisfaction, which in turn affects continuance usage. Technological perceptions include system quality and information quality, whereas flow experience includes perceived enjoyment and attention focus. Among them, perceived enjoyment has the largest effect on satisfaction. Research limitations/implications - This research is conducted in China, where mobile internet is still in its early stage. Thus, the results need to be generalized to other countries that had developed mobile internet. Originality/value - Previous research has focused on the effects of instrumental beliefs such as perceived usefulness on mobile user continuance. However, user behavior may be also affected by intrinsic motivations such as flow. This research tries to fill the gap.", "keywords": ["information quality", "system quality", "mobile sites"]}, {"id": "134", "title": "Interconnection analysis for standard cell layouts", "abstract": "We present an accurate model and procedures for predicting the common physical design characteristics of standard cell layouts (i.e., the interconnection length and the chip area). The predicted results are obtained from analysis of the net list only, that is, no prior knowledge of the functionality of the design is used, Random and optimized placements, global routing, and detailed routing are each abstracted by procedural models that capture the important features of these processes, and closed-form expressions that define these procedural models are presented. We have verified both the global characteristics (total interconnection length and layout area) and the detailed characteristics (wire length and feedthrough distributions) of the model, On the designs in our test suite, the estimates are very close to the actual layouts.", "keywords": ["global route modeling", "interconnection length estimation", "layout area estimation", "placement modeling", "standard cell layout"]}, {"id": "135", "title": "MISEP - Linear and nonlinear ICA based on mutual information", "abstract": "Linear Independent Components Analysis (ICA) has become an important signal processing and data analysis technique, the typical application being blind source separation in a wide range of signals, such as biomedical, acoustical and astrophysical ones. Nonlinear ICA is less developed, but has the potential to become at least as powerful. This paper presents MISEP, an ICA technique for linear and nonlinear mixtures, which is based on the minimization of the mutual information of the estimated components. MISEP is a generalization of the popular INFOMAX technique, which is extended in two ways: (1) to deal with nonlinear mixtures, and (2) to be able to adapt to the actual statistical distributions of the sources, by dynamically estimating the nonlinearities to be used at the outputs. The resulting MISEP method optimizes a network with a specialized architecture, with a single objective function: the output entropy. The paper also briefly discusses the issue of nonlinear source separation. Examples of linear and nonlinear source separation performed by MISEP are presented.", "keywords": ["ica", "blind source separation", "nonlinear ica", "mutual information"]}, {"id": "136", "title": "Beyond state-of-the-art topology as normative ground for decison-making systems", "abstract": "Reviews experiments in design and urbanism, intervening in the development of transdisciplinary systems theory for decision-making organizations. Presents beyond state-of-the-art phenomena, of a morphological and topological type (out of architecture), and advocates harnessing such creativity power to problem solving in informatics.", "keywords": ["architecture", "cybernetics", "perception"]}, {"id": "137", "title": "Fuzzy-hybrid modelling of an Ackerman steered electric vehicle", "abstract": "Physical system modelling with known parameters together with 2-D or high order look-up tables (obtained from experimental data), have been the preferred method for simulating electric vehicles. The non-linear phenomena which are present at the vehicle tyre patch and ground interface have resulted in it quantitative understanding of this phenomena. However, nowadays, there is it requirement for a deeper understanding of the vehicle sub-models which previously used look-up tables. In this paper the hybrid modelling methodology used for electric vehicle systems offers a two-stage advantage: firstly, the vehicle model retains a comprehensive analytical formulation and secondly, the 'fuzzy' element offers, in addition to the quantitative results, a qualitative understanding of specific vehicle sub-models. In the literature several hybrid topologies are reported, sequential, auxiliary, and embedded. In this paper, the hybrid model topology selected is auxiliary and within the same hybrid model, the first paradigm used is the vehicle dynamics together With the actuator/gearbox system. The second paradigm is the non-linear fuzzy tyre model for each wheel. In particular, conventional physical system dynamic modelling has been combined with the fuzzy logic type-II or type-III methodology. The resulting hybrid-fuzzy tyre models were estimated for a-priori number of rules from experimental data. The physical system modelling required the available vehicle parameters such as the overall mass, wheel radius and chassis dimensions. The suggested synergetic fusion of the two methods, (hybrid-fuzzy), allowed the vehicle planar trajectories to be obtained prior to the hardware development of the entire vehicle. The strength of this methodology is that it requires localised system experimental data rather than global system data. The disadvantage in obtaining global experimental data is the requirement for comprehensive testing of it vehicle prototype which is both time consuming process and requires extensive resources. In this paper the authors have proposed the use of existing experimental rigs which are available from the leading automotive manufacturers. Hence, for the 'hybrid' modelling, localised data sets were used. In particular, wheel-tyre experimental data were obtained from the University Tyre Rig experimental facilities. Tyre forces acting on the tyre patch are mainly responsible for the overall electric vehicle motion. In addition, tyre measurement rigs are a well known method for obtaining localised data thus allowing the effective simulation of more detailed mathematical models. These include, firstly, physical system modelling (conventional vehicle dynamics), secondly, fuzzy type II or III modelling (for the tyre characteristics), and thirdly, electric drive modelling within the context of electric vehicles. The proposed hybrid model synthesis has resulted in simulation results which are similar to piece-wise 'look-up' table solutions. In addition, the strength of the 'hybrid' synthesis is that the analyst has a set of rules which clearly show the reasoning behind the complex development of the vehicle tyre forces. This is due to the inherent transparency of the type II and type III methodologies. Finally, the authors discussed the reasons for selecting a type-III framework. The paper concludes with a plethora of simulation results.  ", "keywords": ["hybrid model synthesis-", "type-ii and type-iii fuzzy systems", "parameter estimation", "electric drives"]}, {"id": "138", "title": "On the assessment and evaluation of voice hoarseness", "abstract": "This article presents a non-invasive speech processing method for the assessment and evaluation of voice hoarseness. A technique based on time-scale analysis of the voice signal is used to decompose the signal into a suitable number of high-frequency details and extract the high-frequency bands of the signal. A discriminating measure, which measures the roll-off in power in the high-frequency bands of the signal, with respect to the decomposition index, is developed. The measure reflects the presence and degree of severity of hoarseness in the analyzed voice signals. The discriminating measure is supported by frequency-domain and time-series analyses of the high-frequency bands of normal and hoarse voice signals to provide a visual aid to the clinician or therapist. A database of sustained long vowels of normal and hoarse voices is created and used to assess the presence and degree of severity of hoarseness. The results obtained by the proposed method are compared to results obtained by perturbation analysis.", "keywords": ["voice hoarseness", "speech pathology", "speech analysis", "time-scale and time-series analysis"]}, {"id": "139", "title": "probabilistic models of computer systems", "abstract": "We develop a method based on diffusion approximations in order to compute, under some general conditions, the queue length distribution for a queue in a network. Applications to computer networks and to time-sharing systems are presented.", "keywords": ["sharing", "network", "order", "applications", "method", "systems", "diffuse", "computer network", "computation", "general", "timing", "model", "distributed"]}, {"id": "140", "title": "Integrating synchronization with priority into a Kronecker representation", "abstract": "The compositional representation of a Markov chain using Kronecker algebra, according to a compositional model representation as a superposed generalized stochastic Petri net or a stochastic automata network, has been studied for a while. In this paper we describe a Kronecker expression and associated data structures, that allows to handle nets with synchronization over activities of different levels of priority. New algorithms for these structures are provided to perform an iterative solution method of Jacobi or GaussSeidel type. These algorithms are implemented in the APNN Toolbox. We use this implementation in combination with GreatSPN and exercise an example that illustrates characteristics of the presented algorithms.", "keywords": ["stochastic petri nets", "performance evaluation tools", "numerical algorithms"]}, {"id": "141", "title": "Non-monotone trust region methods for nonlinear equality constrained optimization without a penalty function", "abstract": "We propose and analyze a class of penalty-function-free nonmonotone trust-region methods for nonlinear equality constrained optimization problems. The algorithmic framework yields global convergence without using a merit function and allows nonmonotonicity independently for both, the constraint violation and the value of the Lagrangian function. Similar to the Byrd-Omojokun class of algorithms, each step is composed of a quasi-normal and a tangential step. Both steps are required to satisfy a decrease condition for their respective trust-region subproblems. The proposed mechanism for accepting steps combines nonmonotone decrease conditions on the constraint violation and/or the Lagrangian function, which leads to a flexibility and acceptance behavior comparable to filter-based methods. We establish the global convergence of the method. Furthermore, transition to quadratic local convergence is proved. Numerical tests are presented that confirm the robustness and efficiency of the approach.", "keywords": ["nonmonotone trust-region methods", "sequential quadratic programming", "penalty function", "global convergence", "equality constraints", "local convergence", "large-scale optimization"]}, {"id": "142", "title": "Numerical computation of the helical Chandrasekhar-Kendall modes", "abstract": "A new formulation is presented for numerically computing the helical Chandrasekhar-Kendall modes in an axisymmetric torus. It explicitly imposes del . B = 0 and yields a standard matrix eigenvalue problem, which can then be solved by standard matrix eigenvalue techniques. Numerical implementation and computational results are shown for an axisymmetric torus typical of reversed field pinch and spherical tokamak.  ", "keywords": ["magnetic relaxation", "taylor state", "chandrasekhar-kendall modes", "eigenvalue", "spherical tokamak", "reversed field pinch", "helicity injection"]}, {"id": "143", "title": "Fuzzy adaptive neural network approach to path loss prediction in urban areas at GSM-900 band", "abstract": "This paper presents the results of the Adaptive-Network Based Fuzzy Inference System (ANFIS) for the prediction of path loss in a specific urban environment. A new algorithm based ANFIS for tuning the path loss model is introduced in this work. The performance of the path loss model which is obtained from proposed algorithm is compared to the Bertoni-Walfisch model, which is one of the best studied for propagation analysis involving buildings. This comparison is based on the mean square error between predicted and measured values. According to the indicated error criterion, the errors related to the predictions that are obtained from the algorithm are less than the errors that are obtained from the Bertoni-Walfisch Model. In this study, propagation measurements were carried out in the 900 MHz band in the city of Istanbul, Turkey.", "keywords": ["anfis", "propagation measurements", "path loss", "urban environment"]}, {"id": "144", "title": "Optimization of microwave devices combining topology gradient and genetic algorithm", "abstract": "Topology optimization can be seen as optimizing a distribution of small topological elements within a domain with respect to given specifications. A numerical topology gradient (TG) algorithm is applied in the context of electromagnetism for optimizing microwave devices, computing the sensitivity on adding or removing small metallic elements. This method leads to an optimum topology with very little initial information in acceptable time consumption. The method is applied to the design of a microstrip component in which the topology gradient is directly used as a direction of descent. However, in some ill-behavior problems, topology gradient is not sufficient to converge to the global optimum. In the latter case, the basic TG is coupled with a genetic algorithm (G.A) to make a more suitable algorithm for solving local optima problems. ", "keywords": ["shape optimization", "topology gradient", "genetic algorithm"]}, {"id": "145", "title": "A layout algorithm for undirected compound graphs", "abstract": "We present an algorithm for the layout of undirected compound graphs, relaxing restrictions of previously known algorithms in regards to topology and geometry. The algorithm is based on the traditional force-directed layout scheme with extensions to handle multi-level nesting, edges between nodes of arbitrary nesting levels, varying node sizes, and other possible application-specific constraints. Experimental results show that the execution time and quality of the produced drawings with respect to commonly accepted layout criteria are quite satisfactory. The algorithm has also been successfully implemented as part of a pathway integration and analysis toolkit named PATIKA, for drawing complicated biological pathways with compartmental constraints and arbitrary nesting relations to represent molecular complexes and various types of pathway abstractions.  ", "keywords": ["information visualization", "graph drawing", "force-directed graph layout", "compound graphs", "bioinformatics"]}, {"id": "146", "title": "A knowledge-based scheduling system for Emergency Departments", "abstract": "A knowledge-based reactive scheduling system is proposed to answer the requirements of Emergency Departments (EDs). The algorithm includes detailed patient priority, arrival time, flow time and doctor load. The main aim is to determine the patients who have higher priorities initially, and then minimize their waiting times. To achieve this aim, physicians and the other related workers can use an interactive system. In this study, we evaluated the existing system by comparing the proposed system. Also, reactive scheduling cases were evaluated for some items such as decreasing the number of doctors, changing durations and entering of an urgent patient to the system. All experiments were performed with proposed algorithm and right shift rescheduling approach.", "keywords": ["knowledge-based system", "reactive scheduling", "emergency department", "health care system", "patient priorities"]}, {"id": "147", "title": "Specifying and proving properties of timed I/O automata using Tempo", "abstract": "Timed I/O automata (TIOA) is a mathematical framework for modeling and verification of distributed systems that involve discrete and continuous dynamics. TIOA can be used for example, to model a real-time software component controlling a physical process. The TIOA model is sufficiently general to subsume other models in use for timed systems. The Tempo Toolset, currently under development, is aimed at supporting system development based on TIOA specifications. The Tempo Toolset is an extension of the IOA toolkit, which provides a specification simulator, a code generator, and both model checking and theorem proving support for analyzing specifications. This paper focuses on the modeling of timed systems and their properties with TIOA and on the use of TAME4TIOA, the TAME (Timed Automata Modeling Environment) based theorem proving support provided in Tempo, for proving system properties, including timing properties. Several examples are provided by way of illustration.", "keywords": ["system development frameworks", "modeling environments", "tool suites", "automata models", "timed automata", "hybrid systems", "formal methods", "specification", "verification", "theorem proving"]}, {"id": "148", "title": "Distributed virtual backbone construction in sensor networks with asymmetric links", "abstract": "In this paper, we study the problem of distributed virtual backbone construction in sensor networks, where the coverage area of nodes are disks with different radii. This problem is modeled by the construction of a minimum connected dominating set (MCDS) in geometric k-disk graphs. We derive the size relationship of any maximal independent set (MIS) and MCDS in geometric k-disk graphs, and apply it to analyze the performances of two distributed connected dominating set (CDS) algorithms we propose in this paper. These algorithms have bounded performance ratio and low communication overhead. To the best of our knowledge, the results reported in this paper represent the state-of-the-art. ", "keywords": ["sensor networks with asymmetric transmission links", "connected dominating set", "geometric k-disk graphs", "maximal independent set"]}, {"id": "149", "title": "A Real-Time Java Chip-Multiprocessor", "abstract": "Chip-multiprocessors are an emerging trend for embedded systems. In this article, we introduce a real-time Java multiprocessor called JopCMP. It is a symmetric shared-memory multiprocessor, and consists of up to eight Java Optimized Processor (JOP) cores, an arbitration control device, and a shared memory. All components are interconnected via a system on chip bus. The arbiter synchronizes the access of multiple CPUs to the shared main memory. In this article, three different arbitration policies are presented, evaluated, and compared with respect to their real-time and average-case performance: a fixed priority, a fair-based, and a time-sliced arbiter. Tasks running on different CPUs of a chip-multiprocessor (CMP) influence each others' execution times when accessing a shared memory. Therefore, the system needs an arbiter that is able to limit the worst-case execution time of a task running on a CPU, even though tasks executing simultaneously on other CPUs access the main memory. Our research shows that timing analysis is in fact possible for homogeneous multiprocessor systems with a shared memory. The timing analysis of tasks, executing on the CMP using time-sliced memory arbitration, leads to viable worst-case execution time bounds. The time-sliced arbiter divides the memory access time into equal time slots, one time slot for each CPU. This memory arbitration scheme allows for a calculation of upper bounds of Java application worst-case execution times, depending on the number of CPUs, the time slot size, and the memory access time. Examples of worst-case execution time calculation are presented, and the analyzed results of a real-world application task are compared to measured execution time results. Finally, we evaluate the tradeoffs when using a time-predictable solution compared to using average-case optimized chip-multiprocessors, applying three different benchmarks. These experiments are carried out by executing the programs on the CMP prototype.", "keywords": ["design", "experimentation", "measurement", "performance", "real-time system", "multiprocessor", "java processor", "shared memory", "worst-case execution time"]}, {"id": "150", "title": "A new finite element to represent prismatic joint constraints in mechanisms", "abstract": "Among existing kinematic analysis methods of mechanisms, the techniques based on finite elements represent a generally applicable alternative which enable a wide variety of problems to be solved, including linear (velocities, accelerations, jerk, ) and non-linear ones (position). To modelize a mechanism via these techniques, the link element may be used to introduce a distance constraint between two points. The stiffness matrix assembly of these link elements enables stiffness matrix construction from the model, from which the kinematic behaviour of the mechanism may be extracted. Normally kinematic link conditions introduced directly into the system stiffness matrix are used to introduce point to line constraints like those originated by prismatic joints. A new finite element is presented in this paper, defined by its stiffness or geometric matrix, capable of alternatively modelizing the constraints imposed by the prismatic joint. This new element offers numerous advantages against the procedure based on anterior link conditions, particularly in the case of non-linear problems.", "keywords": ["finite elements", "mechanism kinematics", "linear problems", "prismatic joint", "velocity", "acceleration"]}, {"id": "151", "title": "Image thresholding based on the EM algorithm and the generalized Gaussian distribution", "abstract": "In this paper, a novel parametric and global image histogram thresholding method is presented. It is based on the estimation of the statistical parameters of object and background classes by the expectationmaximization (EM) algorithm, under the assumption that these two classes follow a generalized Gaussian (GG) distribution. The adoption of such a statistical model as an alternative to the more common Gaussian model is motivated by its attractive capability to approximate a broad variety of statistical behaviors with a small number of parameters. Since the quality of the solution provided by the iterative EM algorithm is strongly affected by initial conditions (which, if inappropriately set, may lead to unreliable estimation), a robust initialization strategy based on genetic algorithms (GAs) is proposed. Experimental results obtained on simulated and real images confirm the effectiveness of the proposed method.", "keywords": ["image thresholding", "expectationmaximization algorithm", "generalized gaussian distribution", "genetic algorithms"]}, {"id": "152", "title": "Fast fractal image coding based on LMSE analysis and subblock feature", "abstract": "In this paper, we propose a fast fractal image coding based on LMSE (least mean square error) analysis and subblock feature. The proposed method focuses on efficient search of contrast scaling, position of its matched domain block, and isometric transform for a range block. The contrast scaling and the domain block position are searched using a cost function that comes from the LMSE analysis of the range block and its fractal-approximated block. The isometric transform is searched using 2 x 2 blocks formed with the averages of subblocks of range block and domain block. Experimental results show that the encoding time of a conventional fractal image coding with our search method is 25.6-39.7 times faster than that with full search method at the same bit rate while giving PSNR decrement of 0.2-0.7 dB with negligible deterioration in subjective quality. It is also shown that the encoding time of a conventional fractal image coding with our search method is 3.4-4.2 times faster than Jacquin's fractal image coding and is superior by maximum 0.8 dB in PSNR. It also yields reconstructed images of better quality.", "keywords": ["image coding", "fractal image coding", "lmse", "contractive mapping"]}, {"id": "153", "title": "Finite element solution to passive scalar transport behind line sources under neutral and unstable stratification", "abstract": "This study employed a direct numerical simulation (DNS) technique to contrast the plume behaviours and mixing of passive scalar emitted from line sources (aligned with the spanwise direction) in neutrally and unstably stratified open-channel flows. The DNS model was developed using the Galerkin finite element method (FEM) employing trilinear brick elements with equal-order interpolating polynomials that solved the momentum and continuity equations, together with conservation of energy and mass equations in incompressible flow. The second-order accurate fractional-step method was used to handle the implicit velocity-pressure Coupling in incompressible flow. It also segregated the solution to the advection and diffusion terms, which were then integrated in time, respectively, by the explicit third-order accurate Runge-Kutta method and the implicit second-order accurate Crank-Nicolson method. The buoyancy term under unstable stratification was integrated in time explicitly by the first-order accurate Euler method. The DNS FEM model calculated the scalar-plume development and the mean plume path. In particular, it calculated the plume meandering in the wall-normal direction under unstable stratification that agreed well with the laboratory and field measurements. as well as previous modelling results available in literature. ", "keywords": ["fluid turbulence", "direct numerical simulation ", "finite element method ", "open-channel flow", "passive scalar plume"]}, {"id": "154", "title": "The effect of radius/height ratio on truss optimization", "abstract": "The optimal topology of a Michell's truss is being considered as a benchmark problem. It has been observed that this optimal topology is only applicable up to a particular ratio of distance between the loading point to the line joining the supports and the span of the supports. Once the ratio exceeds this critical ratio, the optimum topology of the Michell's truss changes. It has been observed from the studies that it is possible to demarcate the region of two different types of optimum topologies by a linear relation. Extending this problem to a 3-D, similar type of observation of different optimum topologies has been observed above and below the critical ratio of height to radius ratio. This critical ratio, similar to a 2-D case, follows almost a linear relation.", "keywords": ["genetic algorithms", "optimization", "2-d truss", "3-d truss", "structure", "topology"]}, {"id": "155", "title": "exploratory evaluations of a computer game supporting cognitive behavioural therapy for adolescents", "abstract": "The need to provide effective mental health treatments for adolescents has been described as a global public health challenge [27]. In this paper we discuss the exploratory evaluations of the first adolescent intervention to fully integrate a computer game implementing Cognitive Behavioural Therapy. Three distinct studies are presented: a detailed evaluation in which therapists independent of the design team used the game with 6 adolescents experiencing clinical anxiety disorders; a study in which a member of the design team used the game with 15 adolescents; and finally a study assessing the acceptability of the game and intervention with 216 practicing therapists. Findings are presented within the context of a framework for the design and evaluation of complex health interventions. The paper provides an in-depth insight into the use of therapeutic games to support adolescent interventions and provides stronger evidence than previously available for both their effectiveness and acceptability to stakeholders.", "keywords": ["computer games", "evaluations", "cognitive behavioural therapy", "complex health interventions", "adolescent mental health"]}, {"id": "156", "title": "Efficient high order waveguide mode solvers based on boundary integral equations", "abstract": "For optical waveguides with high index contrast and sharp corners, high order full-vectorial mode solvers are difficult to develop, due to the field singularities at the corners. A recently developed method (the so-called BIE-NtD method) based on boundary integral equations (BIEs) and Neumann-to-Dirichlet (NtD) maps achieves high order of accuracy for dielectric waveguides. In this paper, we develop two new BIE mode solvers, including an improved version of the BIE-NtD method and a new BIE-DtN method based on Dirichlet-to-Neumann (DtN) maps. For homogeneous domains with sharp corners, we propose better BIEs to compute the DtN and NtD maps, and new kernel-splitting techniques to discretize hypersingular operators. Numerical results indicate that the new methods are more efficient and more accurate, and work very well for metallic waveguides and waveguides with extended mode profiles.", "keywords": ["optical waveguides", "boundary integral equations", "dirichlet-to-neumann map", "neumann-to-dirichlet map", "mode solvers", "hypersingular integral operators"]}, {"id": "157", "title": "Caches for Multimedia Workloads: Power and Energy Tradeoffs", "abstract": "One of the significant workloads in current generation desktop processors and mobile devices is multimedia processing. Large on-chip caches are common in modern processors, but large caches will result in increased power consumption and increased access delays. Regular data access patterns in streaming multimedia applications and video processing applications can provide high hit-rates, but due to issues associated with access time, power and energy, caches cannot be made very large. Characterizing and optimizing the memory system is conducive for designing power and performance efficient multimedia application processors. Performance tradeoffs for multimedia applications have been studied in the past, however, power and energy tradeoffs for caches for multimedia processing have not been adequately studied in the past. In this paper, we characterize multimedia applications for I-cache and D-cache power and energy using a multilevel cache hierarchy. Both dynamic and static power increase with increasing cache sizes, however, the increase in dynamic power is small. The increase in static power is significant, and becomes increasingly relevant for smaller feature sizes. There is significant static power dissipation, similar to 45%, in L1 & L2 caches at 70 ram technology sizes, emphasizing the fact that future multimedia systems must be designed by taking leakage power reduction techniques into account. The energy consumption of on-chip L2 caches is seen to be very sensitive to cache size variations. Sizes larger than 16 k for I-caches and 32 k for D-caches will not be efficient choices to maintain power and performance balance. Since multimedia applications spend significant amounts of time in integer operations, to improve the performance, we propose implementing low power full adders and hybrid multipliers in the data path, which results in 9% to 21% savings in the overall power consumption.", "keywords": ["cache", "leakage power", "low power", "multimedia workload characterization"]}, {"id": "158", "title": "Methodology for modeling and analysis of supply networks", "abstract": "The analysis and modeling of business processes are the basis on which management methodologies, simulation models and information systems are developed. The goal of this paper is to point out the possibility of establishing relationships between processes in supply networks and functioning of the whole system. In this integrated system, all relevant factors for supply network management, both at the global level and at the single process level, could be observed. The idea is to form a process library of the supply network, which would contain process description, inputs, outputs, and the way the process is realized. Every record in the library presents the single instance of that process. The relationships of one process with another depend on process structure and the way of its realization. Every instance of a process represents its realization. The assembly of mutual compatible instances of all processes represents one realization of the supply network. The key problem, triggering the process realization, is solved by specific production expert system. Process realization is very similar to a real system, because the environment influence, uncertainty, and available resources are taken into consideration. As the output, the aggregate of relevant parameters for the evaluation of model functioning are derived. This concept presents the basis of virtual framework for supply network simulation.", "keywords": ["supply network", "modeling", "simulation model", "analysis"]}, {"id": "159", "title": "quantifying information leaks in software", "abstract": "Leakage of confidential information represents a serious security risk. Despite a number of novel, theoretical advances, it has been unclear if and how quantitative approaches to measuring leakage of confidential information could be applied to substantial, real-world programs. This is mostly due to the high complexity of computing precise leakage quantities. In this paper, we introduce a technique which makes it possible to decide if a program conforms to a quantitative policy which scales to large state-spaces with the help of bounded model checking. Our technique is applied to a number of officially reported information leak vulnerabilities in the Linux Kernel. Additionally, we also analysed authentication routines in the Secure Remote Password suite and of a Internet Message Support Protocol implementation. Our technique shows when there is unacceptable leakage; the same technique is also used to verify, for the first time, that the applied software patches indeed plug the information leaks. This is the first demonstration of quantitative information flow addressing security concerns of real-world industrial programs.", "keywords": ["quantitative information flow", "linux kernel", "information leakage"]}, {"id": "160", "title": "Four-dimensional radiotherapeutic dose calculation using biomechanical respiratory motion description", "abstract": "Organ motion due to patient breathing introduces a technical challenge for dosimetry and lung tumor treatment by hadron therapy. Accurate dose distribution estimation requires patient-specific information on tumor position, size, and shape as well as information regarding the material density and stopping power of the media along the beam path. A new 4D dosimetry method was developed, which can be coupled to any motion estimation method. As an illustration, the new method was implemented and tested with a biomechanical model and clinical data.", "keywords": ["particle therapy", "moving organs", "dosimetry", "4d-ct"]}, {"id": "161", "title": "Fast dynamic organization without short-term synaptic plasticity: A new view on Hebb's dynamical assemblies", "abstract": "Hebb postulated cell assemblies as the basic computational elements for understanding cortical processing. He defined them as temporary associations of neurons that organize fast and flexibly into functional units, using correlation-based short-term synaptic plasticity. Based on the properties of spiking neurons, we implement dynamical assemblies that organize completely without synaptic plasticity. Instead, we find varying effective connection strengths that reflect the organizational process. We propose that this dynamic reorganization capabilities ocurring on a fast temporal scale may be a central element of cortical processing.", "keywords": ["hebb", "spiking neurons", "pools", "dynamical assemblies", "grouping", "correlations", "coherence", "oscillatory activity"]}, {"id": "162", "title": "State space analysis of Petri nets with relation-algebraic methods", "abstract": "A large variety of systems can be modelled by Petri nets. Their formal semantics are based on linear algebra which in particular allows the Calculation of a Petri net's state space. Since state space explosion is still a serious problem, efficiently calculating, representing, and analysing the state space is mandatory. We propose a formal semantics of Petri nets based on executable relation-algebraic specifications. Thereupon, we suggest how to calculate the markings reachable from a given one simultaneously. We provide an efficient representation of reachability graphs and show in a correct-by-construction approach how to efficiently analyse their properties. Therewith we cover two aspects: modelling and model checking systems by means of one and the same logic-based approach. On a practical side, we explore the power and limits of relation-algebraic concepts for concurrent system analysis.  ", "keywords": ["relation algebra", "petri nets", "reachability graph", "state space analysis", "systems analysis"]}, {"id": "163", "title": "Longitudinal Image Registration With Temporally-Dependent Image Similarity Measure", "abstract": "Longitudinal imaging studies are frequently used to investigate temporal changes in brain morphology and often require spatial correspondence between images achieved through image registration. Beside morphological changes, image intensity may also change over time, for example when studying brain maturation. However, such intensity changes are not accounted for in image similarity measures for standard image registration methods. Hence, 1) local similarity measures, 2) methods estimating intensity transformations between images, and 3) metamorphosis approaches have been developed to either achieve robustness with respect to intensity changes or to simultaneously capture spatial and intensity changes. For these methods, longitudinal intensity changes are not explicitly modeled and images are treated as independent static samples. Here, we propose a model-based image similarity measure for longitudinal image registration that estimates a temporal model of intensity change using all available images simultaneously.", "keywords": ["deformable registration", "longitudinal registration", "magnetic resonance imaging ", "nonuniform appearance change"]}, {"id": "164", "title": "Simultaneous optimization of phase balancing and reconfiguration in distribution networks using BFNM algorithm", "abstract": "Rephasing strategy is one of the main methods used for phase balancing and neutral current reduction in electrical distribution networks and the reconfiguration technique is an effective method for network loss reduction. In this paper, a new method for the simultaneous implementation of reconfiguration and phase balancing strategies is presented as a combinational strategy. In order to solve the proposed optimization problem, Nelder Mead algorithm combined with a bacterial foraging algorithm (BFNM) is used based on a fuzzy multi-objective function. The proposed method allows for the simultaneous execution of reconfiguration and phase balancing while minimizing the interruption cost of rephasing in addition to eliminating network unbalancing and reducing neutral current and network losses. To demonstrate the efficiency of the BFNM algorithm, its performance is compared with bacterial foraging (BF), particle swarm optimization (PSO), genetic and immune algorithms (GA and IA). The proposed method is applied to the IEEE 123-bus test network for evaluation. The simulation results confirm the efficiency of the method in reducing the system costs and network phase balancing.", "keywords": ["phase balancing", "rephasing strategy", "reconfiguration technique", "bfnm algorithm", "distribution networks"]}, {"id": "165", "title": "Vowel onset point detection for noisy speech using spectral energy at formant frequencies", "abstract": "In this paper, we propose a method for robust detection of the vowel onset points (VOPs) from noisy speech. The proposed VOP detection method exploits the spectral energy at formant frequencies of the speech segments present in glottal closure region. In this work, formants are extracted by using group delay function, and glottal closure instants are extracted by using zero frequency filter based method. Performance of the proposed VOP detection method is compared with the existing method, which uses the combination of evidence from excitation source, spectral peaks energy and modulation spectrum. Speech data from TIMIT database and noise samples from NOISEX database are used for analyzing the performance of the VOP detection methods. Significant improvement in the performance of VOP detection is observed by using proposed method compared to existing method.", "keywords": ["vowel onset point ", "formant frequencies", "glottal closure region", "excitation source", "spectral peaks", "modulation spectrum"]}, {"id": "166", "title": "Permutable fuzzy consequence and interior operators and their connection with fuzzy relations", "abstract": "Fuzzy operators are an essential tool in many fields and the operation of composition is often needed. In general, composition is not a commutative operation. However, it is very useful to have operators for which the order of composition does not affect the result. In this paper, we analyze when permutability appears. That is, when the order of application of the operators does not change the outcome. We characterize permutability in the case of the composition of fuzzy consequence operators and the dual case of fuzzy interior operators. We prove that for these cases, permutability is completely connected to the preservation of the operator type. We also study the particular case of fuzzy operators induced by fuzzy relations through Zadehs compositional rule and the inf-composition. For this cases, we connect permutability of the fuzzy relations (using the sup-? composition) with permutability of the induced operators. Special attention is paid to the cases of operators induced by fuzzy preorders and similarities. Finally, we use these results to relate the operator induced by the transitive closure of the composition of two reflexive fuzzy relations with the closure of the operator this composition induces.", "keywords": ["permutability", "fuzzy consequence operator", "fuzzy closure operator", "fuzzy interior operator", "fuzzy preorder", "indistinguishability relation"]}, {"id": "167", "title": "Text mining, names and security", "abstract": "A Process Query System, a new approach to representing and querying multiple hypotheses, is proposed for cross-document co-reference and linking based on existing entity extraction, co-reference and database name-matching technologies. A crucial component of linking entities across documents is the ability to recognize when different name strings are potential references to the same entity. Given the extraordinary range of variation international names can take when rendered in the Roman alphabet, this is a daunting task. The extension of name variant matching to free text will add important text mining functionality for intelligence and security informatics' toolkits.", "keywords": ["co-reference", "multiple hypothesis tracking", "name matching", "natural language processors"]}, {"id": "168", "title": "Carbenic vs. ionic mechanistic pathway in reaction of cyclohexanone with bromoform", "abstract": "The extensive computation study was done to elucidate the mechanism of formation dibromoepoxide from cyclohexanone and bromoform. In this reaction, the formation of dihaloepoxide 2 is postulated as a key step that determines the distribution and stereochemistry of products. Two mechanistic paths of reaction were investigated: the addition of dibromocarbene to carbonyl group of ketone, and the addition of tribromomethyl carbanion to the same (C=O) group. The mechanisms for the addition reactions of dibromocarbenes and tribromomethyl carbanions with cyclohexanone have been investigated using ab initio HF/6-311++G** and MP2/6-311+G* level of theory. Solvent effects on these reactions have been explored by calculations which included a continuum polarizable conductor model (CPCM) for the solvent (H2O). The calculations showed that both mechanisms are possible and are exothermic, but have markedly different activation energies.", "keywords": ["ab initio calculations", "dibromocarbene", "dihaloepoxides", "reaction mechanisms", "tribromomethyl carbanion"]}, {"id": "169", "title": "query operations for moving objects database systems", "abstract": "Geographical Information Systems were originally intended to deal with snapshots representing a single state of some reality but there are more and more applications requiring the representation and querying of time-varying information. This work addresses the representation of moving objects on GIS. The continuous nature of movement raises problems for representation in information systems due to the limited capacity of storage systems and the inherently discrete nature of measurement instruments. The stored information has therefore to be partial and does not allow an exact inference of the real-world object's behavior. To cope with this, query operations must take uncertainty into consideration in their semantics in order to give accurate answers to the users. The paper proposes a set of operations to be included in a GIS or a spatial database to make it able to answer queries on the spatio-temporal behavior of moving objects. The operations have been selected according to the requirements of real applications and their semantics with respect to uncertainty is specified. A collection of examples from a case study is included to illustrate the expressiveness of the proposed operations.", "keywords": ["moving objects", "movement operations", "spatio-temporal databases", "spatio-temporal uncertainty"]}, {"id": "170", "title": "ON THE RELATIONSHIP BETWEEN THE TRACEABILITY PROPERTIES OF REED-SOLOMON CODES", "abstract": "Fingerprinting codes are used to prevent dishonest users (traitors) from redistributing digital contents. In this context, codes with the traceability (TA) property and codes with the identifiable parent property (IPP) allow the unambiguous identification of traitors. The existence conditions for IPP codes are less strict than those for TA codes. In contrast, IPP codes do not have an efficient decoding algorithm in the general case. Other codes that have been widely studied but possess weaker identification capabilities are separating codes. It is a well-known result that a TA code is an IPP code, and an IPP code is a separating code. The converse is in general false. However, it has been conjectured that for Reed-Solomon codes all three properties are equivalent. In this paper we investigate this equivalence, providing a positive answer when the number of traitors divides the size of the ground field.", "keywords": ["fingerprinting and traitor tracing", "identifiable parent property", "separating codes", "mds codes", "reed-solomon codes"]}, {"id": "171", "title": "occlusion handling based on sub-blobbing in automated video surveillance system", "abstract": "Object tracking with occlusion handling is a challenging problem in automated video surveillance. In particular, occlusion handling and tracking have been often considered as separate modules. This paper proposes a tracking method in the context of video surveillance, where occlusions are automatically detected and handled to solve ambiguities. Hence, the tracking process can continue to track the different moving objects correctly. The proposed approach is based on sub-blobbing, that is, blobs representing moving objects are segmented into sections whenever occlusions occur. These sub-blobs are then treated as blobs with the occluded ones ignored. By doing so, the tracking of objects has become more accurate and less sensitive to occlusions. We have also used a feature-based framework for identifying the tracked objects, where several flexible attributes were involved. Experiments on several videos have clearly demonstrated the success of the proposed method.", "keywords": ["occlusion handling", "features", "video surveillance", "object tracking"]}, {"id": "172", "title": "towards the formalization of interaction semantics", "abstract": "With the advent of Web 2.0 and the emergence of improved technologies to enhance UI, the importance of user experience and intuitiveness of Web interfaces led to the growth and success of Interaction Design. Web designers often turn to pre-defined and well-founded design patterns and user interaction paradigms to build novel and more effective Web interfaces. The rational behind Interaction Design patterns is based on user behavior and Web navigation studies. The \"semantics\" of user interaction is therefore a rich and interesting area that is worth exploring in association with traditional Semantic Web approaches. In this paper, we present our first attempts of an ontological formalization of interaction patterns and its implications. To prove our concept, we illustrate the mapping approach we employed to put in relation that interaction formalization with data-specific ontologies, to create Web interfaces to browse and navigate that specialized kind of information; the aforementioned ontologies and mapping rules are the basis of the internal operation of a Semantic Web application framework called STAR:chart, leveraged to build the Service-Finder portal; finally, we present our evaluation results.", "keywords": ["web interfaces", "semantics", "semantic web", "interaction semantics"]}, {"id": "173", "title": "College students academic motivation, media engagement and fear of missing out", "abstract": "Possible links between FoMO, social media engagement, and three motivational constructs were examined. A new scale was designed to measure the extent to which students used social media tools in the classroom. The links between social media engagement and motivational factors were mediated by FoMO.", "keywords": ["fear of missing out", "social media engagement", "self-determination theory", "academic motivation", "higher education"]}, {"id": "174", "title": "A hybrid collaborative filtering method for multiple-interests and multiple-content recommendation in E-Commerce", "abstract": "Recommender systems apply knowledge discovery techniques to the problem of making personalized recommendations for products or services during a live interaction. These systems, especially collaborative filtering based on user, are achieving widespread success on the Web. The tremendous growth in the amount of available information and the kinds of commodity to Web sites in recent years poses some key challenges for recommender systems. One of these challenges is ability of recommender systems to be adaptive to environment where users have many completely different interests or items have completely different content (We called it as Multiple interests and Multiple-content problem). Unfortunately, the traditional collaborative filtering systems can not make accurate recommendation for the two cases because the predicted item for active user is not consist with the common interests of his neighbor users. To address this issue we have explored a hybrid collaborative filtering method, collaborative filtering based on item and user techniques, by combining collaborative filtering based on item and collaborative filtering based on user together. Collaborative filtering based on item and user analyze the user-item matrix to identify similarity of target item to other items, generate similar items of target item, and determine neighbor users of active user for target item according to similarity of other users to active user based on similar items of target item. In this paper we firstly analyze limitation of collaborative filtering based on user and collaborative filtering based on item algorithms respectively and emphatically make explain why collaborative filtering based on user is not adaptive to Multiple-interests and Multiplecontent recommendation. Based on analysis, we present collaborative filtering based on item and user for Multiple-interests and Multiple-content recommendation. Finally, we experimentally evaluate the results and compare them with collaborative filtering based on user and collaborative filtering based on item, respectively. The experiments suggest that collaborative filtering based on item and user provide better recommendation quality than collaborative filtering based on user and collaborative filtering based on item dramatically. ", "keywords": ["collaborative filtering", "recommender systems", "personalization", "e-commerce"]}, {"id": "175", "title": "new approaches to covering and packing problems", "abstract": "Covering and packing integer programs model a large family of combinatorial optimization problems. The current-best approximation algorithms for these are an instance of the basic probabilistic method: showing that a certain randomized approach produces a good approximation with positive probability. This approach seems inherently sequential; by employing the method of alteration we present the first RNC and NC approximation algorithms that match the best sequential guarantees. Extending our approach, we get the first RNC and NC approximation algorithms for certain multi-criteria versions of these problems. We also present the first NC algorithms for two packing and covering problems that are not subsumed by the above result: finding large independent sets in graphs, and rounding fractional Group Steiner solutions on trees.", "keywords": ["approximation algorithms", "method", "families", "approximation", "matching", "trees", "graph", "probability", "version", "group", "combinatorial optimization", "model", "algorithm", "randomization", "posit"]}, {"id": "176", "title": "Novel phenotype issues raised in cross-national epidemiological research on drug dependence", "abstract": "Stage-transition models based on the American Diagnostic and Statistical Manual (DSM) generally are applied in epidemiology and genetics research on drug dependence syndromes associated with cannabis, cocaine, and other internationally regulated drugs (IRDs). Difficulties with DSM stage-transition models have surfaced during cross-national research intended to provide a truly global perspective, such as the work of the World Mental Health Surveys Consortium. Alternative simpler dependence-related phenotypes are possible, including population-level count process models for steps early and before coalescence of clinical features into a coherent syndrome (e.g., zero-inflated Poisson [ZIP] regression). Selected findings are reviewed, based on ZIP modeling of alcohol, tobacco, and IRD count processes, with an illustration that may stimulate new research on genetic susceptibility traits. The annual National Surveys on Drug Use and Health (NSDUH) can be readily modified for this purpose, along the lines of a truly anonymous research approach that can help make NSDUH-type cross-national epidemiological surveys more useful in the context of subsequent genomewide association (GWAS) research and post-GWAS investigations with a truly global health perspective.", "keywords": ["alcohol", "tobacco", "dependence", "epidemiology", "phenotype"]}, {"id": "177", "title": "TCAD study on gate-all-around cylindrical (GAAC) transistor for CMOS scaling to the end of the roadmap", "abstract": "In this paper, we report TCAD study on gate-all-around cylindrical (GAAC) transistor for sub-10-nm scaling. The GAAC transistor device physics, TCAD simulation, and proposed fabrication procedure have been discussed. Among all other novel fin field effect transistor (FinFET) devices, the gate-all-around cylindrical device can be particularly used for reducing the problems of conventional multi-gate FinFET, improving device performance, and scaling-down capabilities. With gate-all-around cylindrical architecture, the transistor is controlled essentially by infinite number of gates surrounding the entire cylinder-shaped channel. Electrical integrity within the channel is improved by reducing the leakage current due to the non-symmetrical field accumulation such as the corner effect. Our proposed fabrication procedure for making devices having the gate-all-around cylindrical (GAAC) device architecture is also discussed.", "keywords": ["gate-all-around cylindrical  transistor", "device physics", "tcad simulation", "fabrication procedure"]}, {"id": "178", "title": "a rule engine to process acceleration data on small sensor nodes", "abstract": "In this paper, we propose a compact rule processing engine to process acceleration data on a small sensor device. Our proposed engine enables us to develop applications using acceleration data on the small device with a quite simple and short description. We describe the outline of both our proposed rule engine and an implementation on our developed sensor device called the Mo-Co-Mi chip.", "keywords": ["ubiquitous computing", "sensor node"]}, {"id": "179", "title": "mug1 - an incremental compiler-compiler", "abstract": "MUG1 is a compiler generating system developed and implemented at the Technical University of Munich. The structure of the system and the concepts used in the compiler description are presented. Special emphasis is laid on the use of MUG1 as a tool for the incremental design of programming languages and the construction of their compilers in parallel.", "keywords": ["concept", "tool", "compilation", "use", "structure", "systems", "parallel", "design", "programming language", "incremental"]}, {"id": "180", "title": "Parallel and distributed local search in COMET", "abstract": "The availability of commodity multiprocessors and high-speed networks of workstations offer significant opportunities for addressing the increasing computational requirements of optimization applications. To leverage these potential benefits, it is important, however, to make parallel and distributed processing easily accessible to a wide audience of optimization programmers. This paper addresses this challenge by proposing parallel and distributed programming abstractions that keep the distance from sequential local search algorithms as small as possible. The abstractions, including parallel loops, interruptions, thread pools, and shared objects, are compositional and cleanly separate the optimization program and the parallel instructions. They have been evaluated experimentally on a variety of applications, including warehouse location and coloring, for which they provide significant speedups.", "keywords": ["combinatorial optimization", "local search", "constraint programming", "parallel", "distributed", "language"]}, {"id": "181", "title": "Causal Relationship from Exposure to Chemicals in Oil Refining and Chemical Industries and Malignant Melanoma", "abstract": "Malignant melanoma has been thought to be related mainly to exposure to the sun or radiation. A review of the scientific literature reveals many significant correlations between benzene and benzene-containing solvents in the workplace and the occurrence of malignant melanoma, particularly in sites that have never been exposed to sunlight. A comparison of positive correlations between such exposure and malignant melanoma by independent investigators and negative findings by investigators with industry affiliations reveals that this difference, at least in part, may account for the discrepant findings. Based on independent studies, it is reasonable to conclude that malignant melanoma is causally related to employment-related chemical exposures in the petroleum refining industry", "keywords": ["benzene", "malignant melanoma", "industry studies", "independent investigators", "petroleum industry", "chemical carcinogenesis", "cutaneous malignancies"]}, {"id": "182", "title": "Control of a wind turbine cluster based on squirrel cage induction generators connected to a single VSC power converter", "abstract": "A control procedure for wind farms connected to a unique converter is presented. The control method is based on vector control, providing high performance. The system operates in the maximum efficiency area due to the use of a MPPT. A power reduction method used in case of an electrical contingency is described. The proposed wind farm layout claims to improve the efficiency and the reliability.", "keywords": ["wind power generation", "high voltage direct current ", "variable frequency wind farm", "offshore wind power", "wind turbine cluster"]}, {"id": "183", "title": "Semantic inference of user's reputation and expertise to improve collaborative recommendations", "abstract": "Collaborative recommender systems select potentially interesting items for each user based on the preferences of like-minded individuals. Particularly, e-commerce has become a major domain in these research field due to its business interest, since identifying the products the users may like or find useful can boost consumption. During the last years, a great number of works in the literature have focused in the improvement of these tools. Expertise, trust and reputation models are incorporated in collaborative recommender systems to increase their accuracy and reliability. However, current approaches require extra data from the users that is not often available. In this paper, we present two contributions that apply a semantic approach to improve recommendation results transparently to the users. On the one hand, we automatically build implicit trust networks in order to incorporate trust and reputation in the selection of the set of like-minded users that will drive the recommendation. On the other hand, we propose a measure of practical expertise by exploiting the data available in any e-commerce recommender system - the consumption histories of the users.  ", "keywords": ["personalized e-commerce", "semantic reasoning", "collaborative filtering", "trust", "reputation", "expertise"]}, {"id": "184", "title": "Two-tier image annotation model based on a multi-label classifier and fuzzy-knowledge representation scheme", "abstract": "Multi-label classification and knowledge-based approach to image annotation. The definition of the fuzzy knowledge representation scheme based on FPN. Novel data-driven algorithms for automatic acquisition of fuzzy knowledge. Novel inference based algorithms for annotation refinement and scene recognition. A comparison of inference-based scene classification with an ordinary approach.", "keywords": ["image annotation", "knowledge representation", "inference algorithms", "fuzzy petri net", "multi-label image classification"]}, {"id": "185", "title": "Betting system for formative code review in educational competitions", "abstract": "Grading systems based on competition ranking usually limit the grade distribution. We propose a methodology based on a betting system to relax the ranking restrictions. Betting assesses the skill to critically analyze source code. A case study in a video game development course validates our proposal.", "keywords": ["assessment", "gamification", "competition", "software development", "code review"]}, {"id": "186", "title": "COMBINATORIAL CONSTRUCTION OF LOCALLY TESTABLE CODES", "abstract": "An error correcting code is said to be locally testable if there is a test that checks whether a given string is a codeword, or rather far from the code, by reading only a constant number of symbols of the string. While the best known construction of locally testable codes (LTCs) by Ben-Sasson and Sudan [SIAM J. Comput., 38 (2008), pp. 551-607] and Dinur [J. ACM, 54 (2007), article 12] achieves very efficient parameters, it relies heavily on algebraic tools and on probabilistically checkable proof (PCP) machinery. In this work we present a new and arguably simpler construction of LTCs that is purely combinatorial, does not rely on PCP machinery, and matches the parameters of the best known construction. However, unlike the latter construction, our construction is not entirely explicit.", "keywords": ["locally testable codes ", "probabilistically checkable proofs ", "pcps of proximity "]}, {"id": "187", "title": "Deterministic and probabilistic multi-modal analysis of slope stability", "abstract": "Traditional slope stability analysis involves predicting the location of the critical slip surface for a given slope and computing a safety factor at that location. However, for some slopes with complicated stratigraphy several distinct critical slip surfaces can exist. Furthermore, the global minimum safety factor in some cases can be less important than potential failure zones when rehabilitating or reinforcing a slope. Existing search techniques used in slope stability analysis cannot find all areas of concern, but instead converge exclusively on the critical slip surface. This paper therefore proposes the use of a holistic multi modal optimisation technique which is able to locate and converge to multiple failure modes simultaneously. The search technique has been demonstrated on a number of benchmark examples using both deterministic and probabilistic analysis to find all possible failure mechanisms, and their respective factors of safety and reliability indices. The results from both the deterministic and probabilistic models show that the search technique is effective in locating the known critical slip surface while also establishing the locations of any other distinct critical slip surfaces within the slope. The approach is of particular relevance for investigating the stability of large slopes with complicated stratigraphy, as these slopes are likely to contain multiple failure mechanisms.", "keywords": ["multi-modal failure", "probabilistic analysis", "deterministic analysis", "slope stability", "multi-modal optimisation"]}, {"id": "188", "title": "Epithelial Tight Junctions in Intestinal Inflammation", "abstract": "The epithelium in inflamed intestinal segments of patients with Crohn's disease is characterized by a reduction of tight junction strands, strand breaks, and alterations of tight junction protein content and composition. In ulcerative colitis, epithelial leaks appear early due to micro-erosions resulting from upregulated epithelial apoptosis and in addition to a prominent increase of claudin-2. Th1-cytokine effects by interferon-? in combination with TNF? are important for epithelial damage in Crohn's disease, while interleukin-13 (IL-13) is the key effector cytokine in ulcerative colitis stimulating apoptosis and upregulation of claudin-2 expression. Focal lesions caused by apoptotic epithelial cells contribute to barrier disturbance in IBD by their own conductivity and by confluence toward apoptotic foci or erosions. Another type of intestinal barrier defect can arise from ?-hemolysin harboring E. coli strains among the physiological flora, which can gain pathologic relevance in combination with proinflammatory cytokines under inflammatory conditions. On the other hand, intestinal barrier impairment can also result from transcellular antigen translocation via an initial endocytotic uptake into early endosomes, and this is intensified by proinflammatory cytokines as interferon-? and may thus play a relevant role in the onset of IBD. Taken together, barrier defects contribute to diarrhea by a leak flux mechanism (e.g., in IBD) and can cause mucosal inflammation by luminal antigen uptake. Immune regulation of epithelial functions by cytokines may cause barrier dysfunction not only by tight junction impairments but also by apoptotic leaks, transcytotic mechanisms, and mucosal gross lesions.", "keywords": ["apoptosis", "barrier function", "claudins", "crohn's disease", "inflammatory bowel disease", "interleukin-13", "tight junction", "tumor necrosis factor-alpha", "ulcerative colitis"]}, {"id": "189", "title": "Fusion of perceptual cues for robust tracking of head pose and position", "abstract": "The paradigm of perceptual fusion provides robust solutions to computer vision problems. By combining the outputs of multiple vision modules, the assumptions and constraints of each module are factored out to result in a more robust system overall. The integration of different modules can be regarded as a form of data fusion. To this end, we propose a framework for fusing different information sources through estimation of covariance from observations. The framework is demonstrated in a face and 3D pose tracking system that fuses similarity-to-prototypes measures and skin colour to track head pose and face position. The use of data fusion through covariance introduces constraints that allow the tracker to robustly estimate head pose and track face position simultaneously.  ", "keywords": ["data fusion", "pose estimation", "similarity representation", "face recognition"]}, {"id": "190", "title": "Encoding multiple orientations in a recurrent network", "abstract": "Models containing recurrent connections amongst the cells within a population can account for a range of empirical data on orientation selectivity in striate cortex. However, existing recurrent models are unable to veridically encode more than one orientation at a time. Underlying this inability is an inherent limitation in the variety of activity profiles that can be stably maintained. We propose a new recurrent model that can form a broader range of stable population activity patterns. We demonstrate that these patterns preserve information about multiple orientations present in the population inputs. This preservation has significant computational consequences when information encoded in several populations must be integrated to perform behavioral tasks, such as visual discrimination.", "keywords": ["population codes", "orientation selectivity", "recurrent network models"]}, {"id": "191", "title": "An automated design and assembly of interference-free modular fixture setup", "abstract": "This paper describes an automated modular fixture design system developed using a CAD-based methodology and implemented on a 3-D CAD/CAM software package. The developed automated fixture design (AFD) system automates the fixturing points determination and is integrated on top of the previously developed interactive and semi-automated fixture design systems. The determination of fixturing points is implemented in compliance with the fixturing principles that are formulated as heuristics rules to generate candidate list of points and then select the exact points from the list. Apart from determining the fixturing points automatically, the system is capable of producing cutting tool collision-free fixture design using its machining interference detection sub-module. The machining interference detection is accomplished through the use of cutter swept solid based on cutter swept volume approach. Therefore, using the developed AFD, an interference-free fixture design and assembly can be achieved in the possible shortest design lead-time.", "keywords": ["modular fixture", "automated fixture design", "machining interference", "cutter swept volume approach"]}, {"id": "192", "title": "Pseudorandomness and average-case complexity via uniform reductions", "abstract": "Impagliazzo and Wigderson (1998) gave the first construction of pseudorandom generators from a uniform complexity assumption on EXP (namely EXP not equal BPP). Unlike results in the nonuniform setting, their result does not provide a continuous trade-off between worst-case hardness and pseudorandomness, nor does it explicitly establish an average-case hardness result. In this paper: We obtain an optimal worst-case to average-case connection for EXP: if EXP not subset of BPTIME(t(n)), then EXP has problems that cannot be solved on a fraction 1/2 + 1/t'(n) of the inputs by BPTIME(t'(n)) algorithms, for t' = t(Omega(1)). We exhibit a PSPACE-complete self-correctible and downward self-reducible problem. This slightly simplifies and strengthens the proof of Impagliazzo and Wigderson, which used a #P-complete problem with these properties. We argue that the results of lmpagliazzo and Wigderson, and the ones in this paper, cannot be proved via \"black-box\" uniform reductions.", "keywords": ["pseudorandomness", "average-case complexity", "derandomization", "instance checkers"]}, {"id": "193", "title": "Galectins in acute and chronic inflammation", "abstract": "Galectins are animal lectins that bind to ?-galactosides, such as lactose and N-acetyllactosamine, in free form or contained in glycoproteins or glycolipids. They are located intracellularly or extracellularly. In the latter they exhibit bivalent or multivalent interactions with glycans on cell surfaces and induce various cellular responses, including production of cytokines and other inflammatory mediators, cell adhesion, migration, and apoptosis. Furthermore, they can form lattices with membrane glycoprotein receptors and modulate receptor properties. Intracellular galectins can participate in signaling pathways and alter biological responses, including apoptosis, cell differentiation, and cell motility. Current evidence indicates that galectins play important roles in acute and chronic inflammatory responses, as well as other diverse pathological processes. Galectin involvement in some processes in vivo has been discovered, or confirmed, through studies of genetically engineered mouse strains, each deficient in a given galectin. Current evidence also suggests that galectins may be therapeutic targets or employed as therapeutic agents for these inflammatory responses.", "keywords": ["galectins", "inflammation", "allergic inflammation", "autoimmune disease", "atherosclerosis"]}, {"id": "194", "title": "Hierarchical analysis of power distribution networks", "abstract": "Careful design and verification of the power distribution network of a chip are of critical importance to ensure its reliable performance. With the increasing number of transistors on a chip, the size of the power network has grown so large as to make the verification task very challenging. The available computational power and memory resources impose limitations on the size of networks that can be analyzed using currently known techniques. Many of today's designs have power networks that are too large to be analyzed in the traditional way as flat networks. In this paper, we propose a hierarchical analysis technique to overcome the aforesaid capacity limitation. We present a new technique for analyzing a power grid using macromodels that are created for a set of partitions of the grid. Efficient numerical techniques for the computation and sparsification of the port admittance matrices of the macromodels are presented. A novel sparsification technique using a 0-1 integer linear programming formulation is proposed to achieve superior sparsification for a specified error. The run-time and memory efficiency of the proposed method are illustrated on industrial designs. It is shown that even for a 60 million-node power grid, our approach allows for an efficient analysis, whereas previous approaches have been unable to handle power grids of such size.", "keywords": ["circuit simulation", "ir drop", "matrix sparsification", "partitioning", "power distribution networks", "power grid", "signal integrity"]}, {"id": "195", "title": "On the interference of ultra wide band systems on point to point links and fixed wireless access systems", "abstract": "Ultra Wide Bandwidth (UWB) spread-spectrum techniques will play a key role in short range wireless connectivity supporting high bit rates availability and low power consumption. UWB can be used in the design of wireless local and personal area networks providing advanced integrated multimedia services to nomadic users within hot-spot areas. Thus the assessment of the possible interference caused by UWB devices on already existing narrowband and wideband systems is fundamental to ensure nonconflicting coexistence and, therefore, to guarantee acceptance of UWB technology worldwide. In this paper, we study the coexistence issues between an indoor UWB-based system (hot-spot) and outdoor point to point (PP) links and Fixed Wireless Access (FWA) systems operating in the 3.5 - 5.0 GHz frequency range. We consider a realistic UWB master/slave system architecture and we show through computer simulation, that in all practical cases UWB system can coexist with PP and FWA without causing any dangerous interference.", "keywords": ["4g communication systems", "spread spectrum", "ultra wide band"]}, {"id": "196", "title": "Scheduling Multimedia Services in a Low-Power MAC for Wireless and Mobile ATM Networks", "abstract": "This paper describes the design and analysis of the scheduling algorithm for energy conserving medium access control (EC-MAC), which is a low-power medium access control (MAC) protocol for wireless and mobile ATM networks. We evaluate the scheduling algorithms that have been proposed for traditional ATM networks. Based on the structure of EC-MAC and the characteristics of wireless channel, we propose a new algorithm that can deal with the burst errors and the location-dependent errors. Most scheduling algorithms proposed for either wired or wireless networks were analyzed with homogeneous traffic or multimedia services with simplified traffic models. We analyze our scheduling algorithm with more realistic multimedia traffic models based on H. 263 video traces and self-similar data traffic. One of the key goals of the scheduling algorithms is simplicity and fast implementation. Unlike the time-stamped based algorithms, our algorithm does not need to sort the virtual time, and thus, the complexity of the algorithm is reduced significantly.", "keywords": ["low-power operation", "multiple access methods", "queuing and scheduling algorithms", "wireless and mobile atm", "wireless multimedia communications"]}, {"id": "197", "title": "Solving capacitated arc routing problems using a transformation to the CVRP", "abstract": "A well-known transformation by Pearn, Assad and Golden reduces a capacitated arc routing problem (CARP) into an equivalent capacitated vehicle routing problem (CVRP). However, that transformation is regarded as unpractical, since an original instance with r  required edges is turned into a CVRP over a complete graph with 3r+1 3 r + 1 vertices. We propose a similar transformation that reduces this graph to 2r+1 2 r + 1 vertices, with the additional restriction that a previously known set of r pairwise disconnected edges must belong to every solution. Using a recent branch-and-cut-and-price algorithm for the CVRP, we observed that it yields an effective way of attacking the CARP, being significantly better than the exact methods created specifically for that problem. Computational experiments obtained improved lower bounds for almost all open instances from the literature. Several such instances could be solved to optimality. Scope and purpose The scope of this paper is transforming arc routing problems into node routing problems. The paper shows that this approach can be effective and, in particular, that the original instances may generate node routing instances that behave as if the size is not increased. This result is obtained by slightly modifying the well-known transformation by Pearn, Assad and Golden from capacitated arc routing problem (CARP) to the capacitated vehicle routing problem (CVRP), that is regarded as unpractical. The paper provides a computational experience using a recent branch-and-cut-and-price algorithm for the CVRP. The results are significantly better than the exact methods created specifically for that problem, improving lower bounds for almost all open instances from the literature. Several such instances could be solved to optimality.", "keywords": ["arc routing", "mixed-integer programming"]}, {"id": "198", "title": "Combining two pheromone structures for solving the car sequencing problem with Ant Colony Optimization", "abstract": "The car sequencing problem involves scheduling cars along an assembly line while satisfying capacity constraints. In this paper, we describe an Ant Colony Optimization (ACO) algorithm for solving this problem, and we introduce two different pheromone structures for this algorithm: the first pheromone structure aims at learning for good sequences of cars, whereas the second pheromone structure aims at learning for critical cars. We experimentally compare these two pheromone structures, that have complementary performances, and show that their combination allows ants to solve very quickly most instances.", "keywords": ["ant colony optimization", "car sequencing problem", "multiple pheromone structures"]}, {"id": "199", "title": "Calibrating information users' views on relevance: A social representations approach", "abstract": "The purpose of this study is to investigate how information users view the concept of relevance and make their judgement(s) on relevant information through the framework of social representations theory. More specifically, this study attempts to address the questions of what users view as the constituent concepts of relevance, what are core and peripheral concepts of relevance, and how these concepts are structured by applying a structural analysis approach of social representations theory. We employ a free word association method for data collection. Two hundred and forty four information users of public and academic libraries responded to questionnaires on their relevance judgement criteria. Collected data were content analysed and assessed using weighted frequency, similarity measure, and core/periphery measurements to identify key elements of relevance and to differentiate core and periphery elements of relevance. Results show that four out of 26 emerged elements (concepts) are core and 22 are periphery elements of the concept of relevance. The findings of this study provide a quantitative measure of weighing various elements of relevance and the internal structure of the concept of relevance from users' perspectives providing enhancements for search algorithms with quantitative metadata support.", "keywords": ["relevance", "relevance criteria", "social representations", "structural analysis", "core-periphery analysis"]}, {"id": "200", "title": "Constructing fault-tolerant communication trees in hypercubes", "abstract": "A communication tree is a binomial tree embedded in a hypercube, whose communication direction is from its leaves to its root. If a problem to be solved is first divided into independent subproblems, then each subproblem can be solved by one of the hypercube processors, and all the subresults can be merged into the final results through tree communication. This paper uses two random search techniques, the genetic algorithm (GA) and simulated annealing (SA), to construct fault-tolerant communication trees with the minimum data transmission time. Experimental evaluation shows that, with reasonably low search time, the proposed GA and SA approaches are able to find more desirable communication trees (i.e., trees with less data transmission time) than the minimal cost approach can. A distributed approach which applies parallel search to communication subtrees in disjoint subcubes is also provided to reduce the search time of the proposed approaches.", "keywords": ["fault-tolerant communication trees", "hypercubes", "genetic algorithms", "simulated annealing", "data transmission time", "search time", "maximal fault-free subcubes"]}, {"id": "201", "title": "The firekeepers: aging considered as a resource", "abstract": "Technology can improve the quality of life for elderly persons by supporting and facilitating the unique leadership roles that elderly play in groups, communities, and other organizations. Elderly people are often organizational firekeepers. They maintain community memory, pass on organizational practices, and ensure social continuity. This paper reports studies of several essential community roles played by elderly community membersincluding the role of volunteer community webmasterand describes two positive design projects that investigated how technology can support new kinds of social endeavors and contributions to society by elderly citizens. Finally, the paper speculates on the utility of intergenerational teams in strengthening societys workforce.", "keywords": ["aging", "elderly", "positive design", "non-profit community-based groups", "intergenerational teams"]}, {"id": "202", "title": "Limited error based event localizing temporal decomposition and its application to variable-rate speech coding", "abstract": "This paper proposes a novel algorithm for temporal decomposition (TD) of speech, called limited error based event localizing temporal decomposition (LEBEL-TD), and its application to variable-rate speech coding. In previous work with TD, TD analysis was usually performed on each speech segment of about 200300ms or more, making it impractical for online applications. In this present work, the event localization is determined based on a limited error criterion and a local optimization strategy, which results in an average algorithmic delay of 65ms. Simulation results show that an average log spectral distortion of about 1.5dB can be achievable at an event rate of 20events/s. Also, LEBEL-TD uses neither the computationally costly singular value decomposition routine nor the event refinement process, thus reducing significantly the computational cost of TD. Further, a method for variable-rate speech coding an average rate of around 1.8kbps based on STRAIGHT (Speech Transformation and Representation using Adaptive Interpolation of weiGHTed spectrum), which is a high-quality speech analysissynthesis framework, using LEBEL-TD is also realized. Subjective test results indicate that the performance of the proposed speech coding method is comparable to that of the 4.8kbps FS-1016 CELP coder.", "keywords": ["temporal decomposition", "event vector", "event function", "straight", "speech coding", "line spectral frequency"]}, {"id": "203", "title": "ONLINE AND OFFLINE SOCIAL TIES OF SOCIAL NETWORK WEBSITE USERS: AN EXPLORATORY STUDY IN ELEVEN SOCIETIES", "abstract": "This study presents results of a survey about social network website (SNW) usage that was administered to university students in China, Egypt, France, Israel, India, Korea, Macao, Sweden, Thailand, Turkey, and the United States. The offline and online social ties of SNW users were examined by nationality, levels of individualism-collectivism (I-C), gender, SNW usage, age, and access location. Contrary to existing literature, we found no differences in the number of offline friends between individualist and collectivist nations. Similarly, there was not a difference in the number of online social ties between individualist and collectivist nations. However, members of collectivist nations had significantly more online social ties never met in person. Heavy SNW users in individualist nations maintained significantly higher numbers of offline social ties; however, heavy SNW users in collectivist nations did not have higher numbers of offline social ties. Related implications and recommendations are provided.", "keywords": ["social ties", "online social ties", "individualism", "collectivism", "social networking websites"]}, {"id": "204", "title": "Pair-wise path key establishment in wireless sensor networks", "abstract": "When sensor networks deployed in unattended and hostile environments, for securing communication between sensors, secret keys must be established between them. Many key establishment schemes have been proposed for large scale sensor networks. In these schemes, each sensor shares a secret key with its neighbors via preinstalled keys. But it may occur that two end nodes which do not share a key with each other could use a secure path to share a secret key between them. However during the transmission of the secret key, the secret key will be revealed to each node along the secure path. Several researchers proposed a multi-path key establishment to prevent a few compromised sensors from knowing the secret key, but it is vulnerable to stop forwarding or Byzantine attacks. To counter these attacks, we propose a hop by hop authentication scheme for path key establishment to prevent Byzantine attacks. Compared to conventional protocols, our proposed scheme can mitigate the impact of malicious nodes from doing a Byzantine attack and sensor nodes can identify the malicious nodes. In addition, our scheme can save energy since it can detect and filter false data not beyond two hops.  ", "keywords": ["byzantine attacks", "path key establishment", "security", "wireless sensor networks"]}, {"id": "205", "title": "Affect-aware behaviour modelling and control inside an intelligent environment", "abstract": "The evidence suggests that human actions are supported by emotional elements that complement logic inference in our decision-making processes. In this paper an exploratory study is presented providing initial evidence of the positive effects of emotional information on the ability of intelligent agents to create better models of user actions inside smart-homes. Preliminary results suggest that an agent incorporating valence-based emotional data into its input array can model user behaviour in a more accurate way than agents using no emotion-based data or raw data based on physiological changes.", "keywords": ["emotion detection", "ambient intelligence", "artificial neural networks", "fuzzy controllers"]}, {"id": "206", "title": "A new DSmT combination rule in open frame of discernment and its application", "abstract": "A new combination rule based on Dezert-Smarandache theory (DSmT) is proposed to deal with the conflict evidence resulting from the non-exhaustivity of the discernment frame. A two-dimensional measure factor in Dempster-Shafer theory (DST) is extended to DSmT to judge the conflict degree between evidence. The original DSmT combination rule or new DSmT combination rule can be selected for fusion according to this degree. Finally, some examples in simultaneous fault diagnosis of motor rotor are given to illustrate the effectiveness of the proposed combination rule.", "keywords": ["dsmt rule of combination", "open frame of discernment", "evidence conflict", "simultaneous faults diagnosis", "generalized basic probability assignment"]}, {"id": "207", "title": "trading off computation for error in providing immersive voice communications for mobile gaming", "abstract": "The interactive experiences of players in networked games can be enhanced with the provision of an Immersive Voice Communication Service. Game players are immersed in their voice communication experience as they exchange live voice streams which are rendered in real-time with directional and distance cues corresponding to the users' positions in the virtual game world. In particular, we propose a Mobile Immersive Communication Environment (MICE) which targets mobile game players using platforms such as Sony PSP and Nintendo DS. A computation reduction scheme was proposed in our previous work for the scalable delivery of MICE from a central server. On the basis of that computation reduction scheme, this paper identifies what factors, and to what extent, affect the unacceptable voice rendering error incurred when providing MICE. In the first experimental scenario, we investigate the level of unacceptable voice rendering error incurred in MICE for different avatar densities or avatar population sizes, with a fixed level of processing limit. In the second experimental scenario, we studied the level of unacceptable voice rendering error incurred in MICE for different processing resource limits, with a fixed avatar population size or avatar density. Our findings provide important insights into the planning and dimensioning of processing resources for the support of MICE, with due considerations to the impact on the unacceptable voice rendering error incurred.", "keywords": ["computation cost reduction", "immersive voice communications", "voice over ip ", "mobile gaming"]}, {"id": "208", "title": "a scalable heuristic for evacuation planning in large road network", "abstract": "Evacuation planning is of critical importance for civil authorities to prepare for natural disasters, but efficient evacuation planning in large city is computationally challenging due to the large number of evacuees and the huge size of transportation networks. One recently proposed algorithm Capacity Constrained Route Planner (CCRP) can give sub-optimal solution with good accuracy in less time and use less memory compared to previous approaches. However, it still can not scale to large networks. In this paper, we analyze the overhead of CCRP and come to a new heuristic CCRP++ that scalable to large network. Our algorithm can reuse search results in previous iterations and avoid the repetitive global shortest path expansion in CCRP. We conducted extensive experiments with real world road networks and different evacuation parameter settings. The result shows it can gives great speed-up without loosing the optimality.", "keywords": ["evacuation planning", "ccrp", "shortest path"]}, {"id": "209", "title": "Battery energy storage system for frequency support in microgrids and with enhanced control features for uninterruptible supply of local loads", "abstract": "A battery energy storage system to support the frequency in autonomous microgrids. Original frequency controller to better damp the frequency oscillations. The frequency controller covers the main two control levels, namely primary and secondary. Enhanced control functions to ensure uninterruptible power supply to local sensitive loads. Simulations and experimental results validate the proposed control solution.", "keywords": ["battery energy storage system", "microgrid", "frequency control", "single-phase inverter"]}, {"id": "210", "title": "Extended Average Magnitude Difference Function Based Pitch Detection", "abstract": "This paper presents a new extended average magnitude difference function for noise robust pitch detection. Average magnitude difference function based algorithms are suitable for real time operations, but suffer from incorrect pitch detection in noisy conditions. The proposed new extended average magnitude difference function involves in sufficient number of averaging for all lag values compared to the original average magnitude difference function, and thereby eliminates the falling tendency of the average magnitude difference function without emphasizing pitch harmonics at higher lags, which is a severe limitation of other existing improvements of the average magnitude difference function. A noise robust post processing that explores the contribution of each frequency channel is also presented. Experimental results on Keele pitch database in different noise level, both with white and color noise, shows the superiority of the proposed extended average magnitude difference function based pitch detection method over other methods based on average magnitude difference function.", "keywords": ["pitch detection", "amdf", "eamdf", "noise robust"]}, {"id": "211", "title": "multi-core demands multi-interfaces", "abstract": "The challenge for the microarchitect has always been (with very few notable domain-specific exceptions) how to translate the continually increasing processing power provided by Moore's Law into increased performance, or more recently into similar performance at lower cost in energy. The mechanisms in the past (almost entirely) kept the interface intact and used the increase in transistor count to improve the performance of the microarchitecture of the uniprocessor. When that became too hard, we went to larger and larger on-chip caches. Both are consistent with the notion that \"abstractions are good.\" At some point, we got overwhelmed with too many transistors; predictably, multi-core was born. As the transistor count continues to skyrocket, we are faced with two questions: what should be on the chip, and how should the software interface to it. If we expect to continue to take advantage of what process technology is providing, I think we need to do several things, starting with rethinking the notion of abstraction and providing multiple interfaces for the programmer.", "keywords": ["multicore", "software interface", "design", "performance"]}, {"id": "212", "title": "Decisions, decisions, decisions: transfer and specificity of decision-making skill between sports", "abstract": "The concept of transfer of learning holds that previous practice or experience in one task or domain will enable successful performance in another related task or domain. In contrast, specificity of learning holds that previous practice or experience in one task or domain does not transfer to other related tasks or domains. The aim of the current study is to examine whether decision-making skill transfers between sports that share similar elements, or whether it is specific to a sport. Participants (n=205) completed a video-based temporal occlusion decision-making test in which they were required to decide on which action to execute across a series of 4 versus 4 soccer game situations. A sport engagement questionnaire was used to identify 106 soccer players, 43 other invasion sport players and 58 other sport players. Positive transfer of decision-making skill occurred between soccer and other invasion sports, which are related and have similar elements, but not from volleyball, supporting the concept of transfer of learning.", "keywords": ["cognitive processes", "knowledge", "skill acquisition", "perceptualcognitive skill"]}, {"id": "213", "title": "an approach of creative application evolution on cloud computing platform", "abstract": "Cloud computing is a paradigm that focuses on sharing data and computing resources over a scalable network of nodes, so it is becoming a preferred environment for those applications with large scalability, dynamic collaboration and elastic resource requirements. Creative computing is an emerging research field in these applications, which can be considered as the study of computer science and related technologies and how they are applied to support creativity, take part in creative processes, and solve creativity related problems. However, it is a very hard work to develop such applications from the very beginning under new environment, while it is a big waste for legacy systems under existing environment. Now software evolution plays an important role. In this paper, we introduced creative computing firstly, including definition, properties and requirements. Then the advantages of cloud computing platform for supporting creative computing were analysed. Next, a private cloud as experimental environment was built. Finally, the process of creative application evolution was illustrated. Our work is about research and application of software evolution methodology, also is an exploratory try to do creative computing research under cloud environment.", "keywords": ["creative application", "creative computing", "software evolution", "cloud computing"]}, {"id": "214", "title": "A sensitivity-based approach to analyzing signal delay uncertainty of coupled interconnects", "abstract": "Performance optimization is a critical step in the design of integrated circuits. Rapid advances in very large scale integration (VLSI) technology have enabled shrinking feature sizes, wire widths, and wire spacings, making the effects of coupling capacitance more apparent. As signals switch faster, noise due to coupling between neighboring wires becomes more pronounced. Changing the relative signal arrival times (RSATs) alters the victim line delay due to the varying coupling noise on the victim line. The authors propose a sensitivity-based method to analyze delay uncertainties of coupled interconnects due to uncertain signal arrival times at its inputs. Compared to existing methods of analyzing delay uncertainties of coupled interconnects, the simulation results show that the proposed method strikes a good balance between model accuracy and complexity compared to the existing approaches.", "keywords": ["coupled interconnects", "delay changes", "sensitivity", "signal arrival times", "statistical timing"]}, {"id": "215", "title": "Visual estimation of pointed targets for robot guidance via fusion of face pose and hand orientation", "abstract": "Problem formulation: given a number of possible pointed targets, compute the target that the user points to. Estimate head pose by visually tracking the off-plane rotations of the face. Recognize two different hand pointing gestures (point left and point right). Model the problem using the DempsterShafer theory of evidence. Use Demspsters rule of combination to fuse information and derive the pointed target.", "keywords": ["humanrobot interaction", "computer vision", "gesture recognition", "pointing gestures", "head pose estimation"]}, {"id": "216", "title": "RTN distribution comparison for bulk, FDSOI and FinFETs devices", "abstract": "In this paper we investigate the sensitivity of RTN noise spectra to statistical variability alone and in combination with variability in the traps properties, such as trap level and trap activation energy. By means of 3D statistical simulation, we demonstrate the latter to be mostly responsible for noise density spectra dispersion, due to its large impact on the RTN characteristic time. As a result FinFETs devices are shown to be slightly more sensitive to RTN than FDSOI devices. In comparison bulk MOSFETs are strongly disadvantaged by the statistical variability associated with high channel doping.", "keywords": ["random telegraph noise", "simulation", "finfet", "statistical variability", "reliability"]}, {"id": "217", "title": "Learning similarity matching in multimedia content-based retrieval", "abstract": "Many multimedia content-based retrieval systems allow query formulation with user setting of relative importance of features (e.g., color, texture, shape, etc) to mimic the user's perception of similarity. However, the systems do not modify their similarity matching functions, which are defined during the system development. In this paper, we present a neural network-based learning algorithm for adapting similarity matching function toward the user's query preference based on his/her relevance feedback. The relevance feedback is given as ranking errors (misranks) between the retrieved and desired lists of multimedia objects. The algorithm is demonstrated for facial image retrieval using the NIST Mugshot Identification Database with encouraging results.", "keywords": ["content-based retrieval", "image retrieval", "multimedia databases", "learning", "ranking", "similarity matching", "relevance feedback"]}, {"id": "218", "title": "Self-organization of decentralized swarm agents based on modified particle swarm algorithm", "abstract": "In this paper, an attempt has been made by incorporating some special features in the conventional particle swarm optimization (PSO) technique for decentralized swarm agents. The modified particle swarm algorithm ( MPSA) for the self-organization of decentralized swarm agents is proposed and studied. In the MPSA, the update rule of the best agent in swarm is based on a proportional control concept and the objective value of each agent is evaluated on-line. In this scheme, each agent self-organizes to flock to the best agent in swarm and migrate to a moving target while avoiding collision between the agent and the nearest obstacle/agent. To analyze the dynamics of the MPSA, stability analysis is carried out on the basis of the eigenvalue analysis for the time-varying discrete system. Moreover, a guideline about how to tune the MPSA's parameters is proposed. The simulation results have shown that the proposed scheme effectively constructs a self-organized swarm system in the capability of flocking and migration.", "keywords": ["decentralized swarm systems", "particle swarm optimization", "self-organization"]}, {"id": "219", "title": "Computing large deformation metric mappings via geodesic flows of diffeomorphisms", "abstract": "This paper examine the Euler-Lagrange equations for the solution of the large deformation diffeomorphic metric mapping problem studied in Dupuis et al. (1998) and Trouve (1995) in which two images 10, 1, are given and connected via the diffeomorphic change of coordinates I-0 o phi(-1) = I, where p = 01 is the end point at t = 1 of curve phi(t), t is an element of [0, 1] satisfying (phi)over dot(t) = v(t)(phi(t)), t is an element of [0, 1] with phi(0) = id. The variational problem takes the form [GRAPHICS] where parallel tov(t)parallel to(V) is an appropriate Sobolev norm on the velocity field v(t)(.), and the second term enforces matching of the images with parallel to.parallel to(L2) representing the squared-error norm. In this paper we derive the Euler-Lagrange equations characterizing the minimizing vector fields vt(,) t is an element of [0, 1] assuming sufficient smoothness of the norm to guarantee existence of solutions in the space of diffeomorphisms. We describe the implementation of the Euler equations using semi-lagrangian method of computing particle flows and show the solutions for various examples. As well, we compute the metric distance on several anatomical configurations as measured by integral(0)(1) parallel tov(t)parallel to(v)dt on the geodesic shortest paths.", "keywords": ["computational anatomy", "euler-lagrange equation", "variational optimization", "deformable template", "metrics"]}, {"id": "220", "title": "ARVO-CL: The OpenCL version of the ARVO package - An efficient tool for computing the accessible surface area and the excluded volume of proteins via analytical equations", "abstract": "Introduction of Graphical Processing Units (GPUs) and computing using GPUs in recent years opened possibilities for simple parallelization of programs. In this update, we present the modernized version of program ARVO [J. Busa, J. Dzurina, E. Hayryan, S. Hayryan, C-K. Hu, J. Plavka, I. Pokorny, J. Skivanek, M.-C. Wu, Comput. Phys. Comm. 165 (2005) 59]. The whole package has been rewritten in the C language and parallelized using OpenCL. Some new tricks have been added to the algorithm in order to save memory much needed for efficient usage of graphical cards. A new tool called 'input_structure' was added for conversion of pdb files into files suitable for work with the C and OpenCL version of ARVO. New version program summary Program title: ARVO-CL Catalog identifier: ADUL_v2_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/ADUL_v2_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 11834 No. of bytes in distributed program, including test data, etc.: 182528 Distribution format: tar.gz Programming language: C. OpenCL. Computer: PC Pentium; SPP'2000. Operating system: All OpenCL capable systems. Has the code been vectorized or parallelized?: Parallelized using GPUs. A serial version (non GPU) is also included in the package. Classification: 3. External routines: cl.hpp (http://www.khronos.org/registry/cl/api/1.1/cl.hpp) Catalog identifier of previous version: ADUL_v1_0 Journal reference of previous version: Comput. Phys. Comm. 165(2005)59 Does the new version supercede the previous version?: Yes", "keywords": ["arvo", "proteins", "solvent accessible area", "excluded volume", "stereographic projection", "opencl package"]}, {"id": "221", "title": "Three-Dimensional Near-Field MIMO Array Imaging Using Range Migration Techniques", "abstract": "This paper presents a 3-D near-field imaging algorithm that is formulated for 2-D wideband multiple-input-multiple-output (MIMO) imaging array topology. The proposed MIMO range migration technique performs the image reconstruction procedure in the frequency-wavenumber domain. The algorithm is able to completely compensate the curvature of the wavefront in the near-field through a specifically defined interpolation process and provides extremely high computational efficiency by the application of the fast Fourier transform. The implementation aspects of the algorithm and the sampling criteria of a MIMO aperture are discussed. The image reconstruction performance and computational efficiency of the algorithm are demonstrated both with numerical simulations and measurements using 2-D MIMO arrays. Real-time 3-D near-field imaging can be achieved with a real-aperture array by applying the proposed MIMO range migration techniques.", "keywords": ["multiple-input multiple-output ", "near-field imaging", "range migration", "sparse array"]}, {"id": "222", "title": "Imprint lithography for flexible transparent plastic substrates", "abstract": "A novel imprinting process has been developed for the use of resist pattern transfer on flexible transparent plastic substrates. The polymer resist was first spin-coated on the mold, which was treated with a release agent. After softbaking, the resist layer was attached to a plastic substrate coated with an adhesive. The patterns were completely transferred to the substrate after removing the mold. Using this method, we were able to obtain the desired patterns on the plastic substrate without heating the substrate, which could deform the substrate.", "keywords": ["imprint lithography", "flexible plastic substrate", "pmma"]}, {"id": "223", "title": "Efficient Routing of Subspace Skyline Queries over Highly Distributed Data", "abstract": "Data generation increases at highly dynamic rates, making its storage, processing, and update costs at one central location excessive. The P2P paradigm emerges as a powerful model for organizing and searching large data repositories distributed over independent sources. Advanced query operators, such as skyline queries, are necessary in order to help users handle the huge amount of available data. A skyline query retrieves the set of nondominated data points in a multidimensional data set. Skyline query processing in P2P networks poses inherent challenges and demands nontraditional techniques, due to the distribution of content and the lack of global knowledge. Relying on a superpeer architecture, we propose a threshold-based algorithm, called SKYPEER and its variants, for efficient computation of skyline points in arbitrary subspaces, while reducing both computational time and volume of transmitted data. Furthermore, we address the problem of routing skyline queries over the superpeer network and we propose an efficient routing mechanism, namely SKYPEER(+), which further improves the performance by reducing the number of contacted superpeers. Finally, we provide an extensive experimental evaluation showing that our approach performs efficiently and provides a viable solution when a large degree of distribution is required.", "keywords": ["skyline queries", "peer-to-peer systems", "routing indexes"]}, {"id": "224", "title": "Parallel processing in regional climatology: The parallel version of the \"Karlsruhe Atmospheric Mesoscale Model\" (KAMM)", "abstract": "Simultaneously to improvements of computer performance and of availability of memory not only the resolution of meteorological models of atmospheric currents has been refined but also the accuracy of the necessary physical approximations has been improved more and more. Now full elastic models are developed which describe also sound waves, although sound processes are not supposed to be relevant for atmospheric flow phenomena. But the full set of the elastic Navier-Stokes equations has a quite simple structure in comparison to sound proved systems like \"anelastically\" approximated models, so that the corresponding numerical models can be implemented on parallel computer systems without too much efforts. This has been considered by the redesign of the \"Karlsruhe Atmospheric Mesoscale Model\" (KAMM) for parallel processing. The new full elastic version of this model is written in FORTRAN-90. The necessary communication operations are gathered into few functions of a communication library, which is designed for different computer architectures, for massive parallel systems, for parallel vector computers requiring long vectors, but also for mono processors.  ", "keywords": ["navier-stokes equation", "elastic model", "regional climatology", "communication library", "massive parallel systems", "vector computers", "benchmark"]}, {"id": "225", "title": "X-ray scattering processes and chemometrics for differentiating complex samples using conventional EDXRF equipment", "abstract": "Mild variations in organic matrices, which are investigated in this work, are caused by alterations in X-ray Raman scattering. The multivariate approaches, principal component analysis (PCA) and hierarchical cluster analysis (HCA), are applied to visualize these effects. Conventional energy-dispersive X-ray fluorescence equipment is used, where organic compounds produce intense scattering of the X-ray source. X-ray Raman processes, before obtained only for solid samples using synchrotron radiation, are indirectly visualized here through PCA scores and HCA cluster analysis, since they alter the Compton and Rayleigh scattering. As a result, their influences can be seen in known sample characteristics, as those associated with gender and melanin in dog hairs, and the differentiation in coconut varieties. Chemometrics has shown that, despite their complexity, natural samples can be easily classified.  ", "keywords": ["principal component analysis ", "hierarchical cluster analysis ", "natural sample differentiation", "x-ray raman scatter spectrometry ", "complex organic mixtures"]}, {"id": "226", "title": "A Deterministic Time Algorithm for the Reeb Graph", "abstract": "We present a deterministic algorithm to compute the Reeb graph of a PL real-valued function on a simplicial complex in time, where is the size of the 2-skeleton. The problem can be solved using dynamic graph connectivity. We obtain the running time by using offline graph connectivity which assumes that the deletion time of every arc inserted is known at the time of insertion. The algorithm is implemented and experimental results are given. In addition, we reduce the offline graph connectivity problem to computing the Reeb graph.", "keywords": ["algorithms", "reeb graph", "pl topology", "graph connectivity"]}, {"id": "227", "title": "advances in pcb routing", "abstract": "Due to rapid increases in printed circuit board (PCB) complexity and lack of research progresses in PCB routing algorithms over the years, routing has become a bottleneck in overall circuit board design time. Today, a high-end PCB typically takes significant tedious manual efforts to complete the wiring and this problem will only get worse for future generations of PCBs. In this talk, we present some of our recent research results on this problem.", "keywords": ["routing", "pcb", "algorithms"]}, {"id": "228", "title": "A Test of Two Models of Value Creation in Virtual Communities", "abstract": "Does a firm get any extra value from investing resources in sponsoring its own virtual community above and beyond the value that could be created for the firm, indirectly, via customer-initiated communities? If so, what explains the extra value derived from a firm-sponsored virtual community and how might this understanding inform managers about appropriate strategies for leveraging virtual communities as part of a value-creating strategy for the firm? We test two models of virtual community to help shed light on the answers to these questions. We hypothesize that in customer-initiated virtual communities, three attributes of member-generated information (MGI) drive value, while in firm-sponsored virtual communities, a sponsoring firm's efforts, as well as MGI, drive value. Drawing on information search and processing theories, and developing new measures of three attributes of MGI (consensus, consistency, and distinctiveness), we surveyed 465 consumers across numerous communities. We find that value can emerge via both models, but that in a firm-sponsored model, a sponsor's efforts are more powerful than MGI and have a positive, direct effect on the trust-building process. Our results suggest a continuum of value creation whereby firms extract greater value as they migrate toward the firm-sponsored model.", "keywords": ["attribution theory", "co-creation", "online communities", "online trust", "user-generated content", "virtual communities"]}, {"id": "229", "title": "Performance optimization and modeling of blocked sparse kernels", "abstract": "We present a method for automatically selecting optimal implementations of sparse matrix-vector operations. Our software \"AcCELS\" (Accelerated Compress-storage Elements for Linear Solvers) involves a setup phase that probes machine characteristics, and a run-time phase where stored characteristics are combined with a measure of the actual sparse matrix to find the optimal kernel implementation. We present a performance model that is shown to be accurate over a large range of matrices.", "keywords": ["optimization", "sparse", "matrix-vector product", "blocking", "self-adaptivity"]}, {"id": "230", "title": "Parameter estimation of two-level nonlinear mixed effects models using first order conditional linearization and the EM algorithm", "abstract": "Multi-level nonlinear mixed effects (ML-NLME) models have received a great deal of attention in recent years because of the flexibility they offer in handling the repeated-measures data arising from various disciplines. In this study, we propose both maximum likelihood and restricted maximum likelihood estimations of ML-NLME models with two-level random effects, using first order conditional expansion (FOCE) and the expectationmaximization (EM) algorithm. The FOCEEM algorithm was compared with the most popular Lindstrom and Bates (LB) method in terms of computational and statistical properties. Basal area growth series data measured from Chinese fir (Cunninghamia lanceolata) experimental stands and simulated data were used for evaluation. The FOCEEM and LB algorithms given the same parameter estimates and fit statistics for models that converged by both. However, FOCEEM converged for all the models, while LB did not, especially for the models in which two-level random effects are simultaneously considered in several base parameters to account for between-group variation. We recommend the use of FOCEEM in ML-NLME models, particularly when convergence is a concern in model selection.", "keywords": ["cunninghamia lanceolata", "expectationmaximization algorithm", "first order conditional expansion", "lindstrom and bates algorithm", "simulated data", "two-level nonlinear mixed effects models"]}, {"id": "231", "title": "Multi-scale modelling of sandwich structures using the Arlequin method Part I: Linear modelling", "abstract": "The paper presents an Arlequin based multi-scale method for studying problems related to the mechanical behaviour of sandwich composite structures. Towards this end, different models are mixed and glued to each other. Several coupling operators are tested in order to assess the usefulness of the proposed approach. A new coupling operator is proposed and tested on the different glued Arlequin zones. A freeclamped sandwich beam with soft core undergoing a concentrated effort on the free edge is used as a typical example (benchmark) in the validation procedure. Numerical simulations were conducted as the preliminary evaluation of the various coupling operators and the discrepancies between local and global models in the gluing zone have been addressed with sufficient care.", "keywords": ["arlequin", "multi-scale", "sandwich", "local effects", "finite element"]}, {"id": "232", "title": "DETERMINATES OF EIS ACCEPTANCE", "abstract": "The large number of organizations developing executive information systems (EISs) highlights the importance of understanding why executives use these systems. This survey investigated how ease of use, the number of features, and support staff characteristics are related to EIS acceptance. Acceptance was measured by the percentage of the targeted users who incorporate the EIS into their daily routine. High usage was not associated with ease of use, a large number of features, or the staff being physically close to the users. However, rapid development time was positively correlated with acceptance. Higher numbers of available features were associated with larger support staffs and larger user groups. The number of users was positively correlated with both staff size and EIS age. Existing EISs place a stronger emphasis on reporting internal rather than external data.", "keywords": ["executive information systems", "information systems features", "information systems support"]}, {"id": "233", "title": "SIFT-Based Non-blind Watermarking Robust to Non-linear Geometrical Distortions", "abstract": "This paper presents a non-blind watermarking technique that is robust to non-linear geometric distortion attacks. This is one of the most challenging problems for copyright protection of digital content because it is difficult to estimate the distortion parameters for the embedded blocks. In our proposed scheme, the location of the blocks are recorded by the translation parameters from multiple Scale Invariant Feature Transform (SIFT) feature points. This method is based on two assumptions: SIFT features are robust to non-linear geometric distortion and even such non-linear distortion can be regarded as \"linear\" distortion in local regions. We conducted experiments using 149,800 images (7 standard images and 100 images downloaded from Flickr, 10 different messages, 10 different embedding block patterns, and 14 attacks). The results show that the watermark detection performance is drastically improved, while the baseline method can achieve only chance level accuracy.", "keywords": ["watermarking", "scale invariant feature transform ", "non-linear geometric distortion attacks"]}, {"id": "234", "title": "zoom navigation exploring large information and application spaces", "abstract": "We present the concept of ZOOM NAVIGATION, a new interaction paradigm to cope with visualization and navigation problems as found in large information and application spaces. It is based on the pluggable zoom , an object-oriented component derived from the variable zoom fisheye algorithm.Working with a limited screen space we apply a Degree-of-interest (DOI) function to guide the level of detail used in presenting information. Furthermore we determine the user's information and navigation needs by analysing the interaction history. This leads to the definition of the aspect-of-interest (AOI) function. The AOI is evaluated in order to choose one of the several information aspects , under which an item can be studied. This allows us to change navigational affordance and thereby enhance navigation.In this paper we describe the ideas behind the pluggable zoom and the definition of DOI and AOI functions. The application of these functions is demonstrated within two case studies, the ZOOM ILLUSTRATOR and the ZOOM NAVIGATOR. We discuss our experience with these implemented systems.", "keywords": ["zooming interfaces", "zoom navigation", "screen layout", "information navigation", "fisheye display", "human-computer interfaces", "detail + context technique"]}, {"id": "235", "title": "FITS: A Finite-Time Reputation System for Cooperation in Wireless Ad Hoc Networks", "abstract": "A wireless ad hoc network does not have an infrastructure, and thus, needs the cooperation of nodes in forwarding other nodes' packets. Reputation system is an effective approach to give nodes incentives to cooperate in packet forwarding. However, existing reputation systems either lack rigorous analysis, or have analysis in unrealistic models. In this paper, we propose FITS, the first reputation system that has rigorous analysis and guaranteed incentive compatibility in a practical model. FITS has two schemes: the first scheme is very simple, but needs a Perceived Probability Assumption (PPA); the second scheme uses more sophisticated techniques to remove the need for PPA. We show that both of these two FITS schemes have a subgame perfect Nash equilibrium in which the packet forwarding probability of every node is one. Experimental results verify that FITS provides strong incentives for nodes to cooperate.", "keywords": ["keywords ad hoc networks", "incentive compatibility", "routing", "packet forwarding"]}, {"id": "236", "title": "Developing a verbal protocol method for collecting and analysing reports of workers thoughts during manual handling tasks", "abstract": "Concurrent and retrospective verbal protocol methods were used to collect thoughts from 18 participants during a manual handling task involving the repeated transfer of loads between locations at two tables. The effectiveness of qualitative and quantitative methods of analysing the reported information was tested in the study. A simple taxonomy was developed to investigate the content of the reports (including reports on postures and loads) and determine how the participants approached the task (whether they made plans, described actions or evaluated their completion of the task). References to posture were obtained in the verbal protocol reports, indicating that the participants had some awareness of their postures during parts of the task. There were similarities in the content of the concurrent and retrospective reports, but there were differences in the amount of detail between the methods and differences in the way the reports were constructed. There could be some scope for developing the quantitative analysis of the frequencies of references to classes of information, though this can only be recommended for concurrent reports on tasks of short duration. The analyses of qualitative data gave a deeper insight into the reports, such as identifying factors that can be important when planning to handle a load, or illustrating how participants can change their focus of attention periodically throughout the task. The relative strengths of the concurrent and retrospective methods are described, along with ideas for improving the quality of information collected in future studies. A number of potential problems with the interpretation of the reported information are explained.", "keywords": ["self reports", "verbal protocol methods", "manual handling tasks"]}, {"id": "237", "title": "Fast and flexible instruction selection with on-demand tree-parsing automata", "abstract": "Tree parsing as supported by code generator generators like BEG, burg, iburg, lburg and ml-burg is a popular instruction selection method. There are two existing approaches for implementing tree parsing: dynamic programming, and tree-parsing automata; each approach has its advantages and disadvantages. We propose a new implementation approach that combines the advantages of both existing approaches: we start out with dynamic programming at compile time, but at every step we generate a state for a tree-parsing automaton, which is used the next time a tree matching the state is found, turning the instruction selector into a fast tree-parsing automaton. We have implemented this approach in the Gforth code generator. The implementation required little effort and reduced the startup time of Gforth by up to a factor of 2.5.", "keywords": ["algorithms", "performance", "instruction selection", "tree parsing", "dynamic programming", "automaton", "lazy"]}, {"id": "238", "title": "challenges on preserving scientific data with data grids", "abstract": "The emerging context of e-Science imposes new scenarios and requirements for digital preservation. In particular, the data must be reliably stored, for which redundancy is a key strategy. But managing redundancy must take into account the potential failure of component. Considering that correlated failures can affect multiple components and potentially cause a complete loss of data, we propose an innovative solution to manage redundancy strategies in heterogeneous environments such as data grids. This solution comprises a simulator that can be used to evaluate redundancy strategies according to preservation requirements and supports the process to design the best architecture to be deployed, which can latter be used as an observer of the deployed system, supporting its monitoring and management.", "keywords": ["e-science", "simulation", "digital libraries", "data grid", "digital preservation"]}, {"id": "239", "title": "automated test order generation for software component integration testing", "abstract": "The order in which software components are tested can have a significant impact on the number of stubs required during component integration testing. This paper presents an efficient approach that applies heuristics based on a given software component test dependency graph to automatically generate a test order that requires a (near) minimal number of test stubs. Thus, the approach reduces testing effort and cost. The paper describes the proposed approach, analyses its complexity and illustrates its use. Comparison with three well known graph-based approaches, for a real-world software application, shows that only the classic Le Traon et al.s approach and ours give an optimal number of stubs. However, experiments on randomly simulated dependency models with 100 to 10,000 components show that our approach has a significant performance advantage with a reduction in the average running time of 96.01%.", "keywords": ["heuristic algorithms", "software testing", "component integration", "directed feedback vertex-set problem"]}, {"id": "240", "title": "Inhomogeneous and self-organized temperature in Schelling-Ising model", "abstract": "The Schelling model of 1971 is a complicated version of a square-lattice Ising model at zero temperature, to explain urban segregation, based on the neighbor preferences of the residents, without external reasons. Various versions between Ising and Schelling models give about the same results. Inhomogeneous \"temperatures\" T do not change the results much, while a feedback between segregation and T leads to a self-organization of an average T.", "keywords": ["urban segregation", "feedback", "phase transition", "randomness"]}, {"id": "241", "title": "An exploratory study of architectural effects on requirements decisions", "abstract": "The question of the \"manner in which an existing software architecture affects requirements decision-making\" is considered important in the research community; however, to our knowledge, this issue has not been scientifically explored. We do not know, for example, the characteristics of such architectural effects. This paper describes an exploratory study on this question. Specific types of architectural effects on requirements decisions are identified, as are different aspects of the architecture together with the extent of their effects. This paper gives quantitative measures and qualitative interpretation of the findings. The understanding gained from this study has several implications in the areas of: project planning and risk management, requirements engineering (RE) and software architecture (SA) technology, architecture evolution, tighter integration of RE and SA processes, and middleware in architectures. Furthermore, we describe several new hypotheses that have emerged from this study, that provide grounds for future empirical work. This study involved six RE teams (of university students), whose task was to elicit new requirements for upgrading a pre-existing banking software infrastructure. The data collected was based on a new meta-model for requirements decisions, which is a bi-product of this study.  ", "keywords": ["software architecture", "requirements engineering", "empirical study", "software quality", "process improvement", "quantitative and qualitative research", "architecture and requirements technology"]}, {"id": "242", "title": "A direction of arrival estimation method for spatial optical beam-forming network", "abstract": "Spatial optical beam-forming network (OBFN) is a superior structure than traditional ones in bandwidth adaptability, system complexity, and so forth. Compared with conventional beam-forming network, the output signal model of OBFN is different, making the previous direction of arrival (DOA) estimation methods improper for this structure. At present, DOA estimation method for this structure has not been sufficiently explored and there is no efficient algorithm. In this paper, the observation model of the network is established first, and then a new DOA estimation method is proposed. The new method makes use of the amplitude distribution of the fiber array to achieve direction finding. Sufficient numerical simulations are carried out to demonstrate the feasibility and efficiency of the proposed algorithm.", "keywords": ["spatial optical beam-forming network", "doa estimation", "fiber array", "fourier transform"]}, {"id": "243", "title": "On central algorithms of approximation under fuzzy information", "abstract": "We consider the problem of approximation of an operator by information described by n real characteristics in the case when this information is fuzzy. We develop the well-known idea of an optimal error method of approximation for this case. It is a method whose error is the infimum of the errors of all methods for a given problem characterized by fuzzy numbers in this case. We generalize the concept of central algorithms, which are always optimal error algorithms and in the crisp case are useful both in practice and in theory. In order to do this we define the centre of an L-fuzzy subset of a normed space. The introduced concepts allow us to describe optimal methods of approximation for linear problems using balanced fuzzy information.  ", "keywords": ["l-fuzzy number", "fuzzy information", "central algorithm of approximation"]}, {"id": "244", "title": "Prediction of currents and sea surface elevation in the Gulf of California from tidal to seasonal scales", "abstract": "A web tool that provides currents and/or sea surface elevation in the Gulf of California is presented. The above variables are reconstructed from harmonic constants obtained from harmonic analyses of time series produced by a 3D baroclinic numerical model of the Gulf. The numerical model was forced (1) at the Gulf's mouth by the tides and the hydrographic variability of the Pacific Ocean (at semiannual and annual frequencies), and (2) at the Gulf's surface by winds, heat and fresh water fluxes (also at the semiannual and annual frequencies). The response to these forcings results in motions with time scales limited to semidiurnal and diurnal, fortnightly and monthly (due to nonlinear interactions of the tidal components), and semiannual and annual frequencies (due to the nontidal forcing).", "keywords": ["prediction", "sea level", "currents", "gulf of california", "tides", "hamsom"]}, {"id": "245", "title": "Surpassing the fractional derivative: Concept of the memory-dependent derivative", "abstract": "Enlightened by the Caputo type of fractional derivative, here we bring forth a concept of memory-dependent derivative, which is simply defined in an integral form of a common derivative with a kernel function on a slipping interval. In case the time delay tends to zero it tends to the common derivative. High order derivatives also accord with the first order one. Comparatively, the form of kernel function for the fractional type is fixed, yet that of the memory-dependent type can be chosen freely according to the necessity of applications. So this kind of definition is better than the fractional one for reflecting the memory effect (instantaneous change rate depends on the past state). Its definition is more intuitionistic for understanding the physical meaning and the corresponding memory-dependent differential equation has more expressive force.", "keywords": ["memory-dependent derivative ", "memory-dependent differential equation ", "fractional differential equation ", "caputo derivative", "time delay"]}, {"id": "246", "title": "Deriving robust counterparts of nonlinear uncertain inequalities", "abstract": "In this paper we provide a systematic way to construct the robust counterpart of a nonlinear uncertain inequality that is concave in the uncertain parameters. We use convex analysis (support functions, conjugate functions, Fenchel duality) and conic duality in order to convert the robust counterpart into an explicit and computationally tractable set of constraints. It turns out that to do so one has to calculate the support function of the uncertainty set and the concave conjugate of the nonlinear constraint function. Conveniently, these two computations are completely independent. This approach has several advantages. First, it provides an easy structured way to construct the robust counterpart both for linear and nonlinear inequalities. Second, it shows that for new classes of uncertainty regions and for new classes of nonlinear optimization problems tractable counterparts can be derived. We also study some cases where the inequality is nonconcave in the uncertain parameters.", "keywords": ["fenchel duality", "robust counterpart", "nonlinear inequality", "robust optimization", "support functions", ""]}, {"id": "247", "title": "On the weight distribution of terminated convolutional codes", "abstract": "In this correspondence, the low-weight terms of the weight distribution of the block code obtained by terminating a convolutional code after x information blocks are expressed as a function of x. It is shown that this function is linear in x for codes with noncatastrophic encoders, but quadratic in x for codes with catastrophic encoders, These results are useful to explain the poor performance of convolutional codes with a catastrophic encoder at low-to-medium signal-to-noise ratios.", "keywords": ["block codes", "convolutional codes", "soft-decision decoding", "viterbi decoding", "weight distribution"]}, {"id": "248", "title": "Common data model for natural language processing based on two existing standard information models: CDA+GrAF", "abstract": "An increasing need for collaboration and resources sharing in the Natural Language Processing (NLP) research and development community motivates efforts to create and share a common data model and a common terminology for all information annotated and extracted from clinical text. We have combined two existing standards: the HL7 Clinical Document Architecture (CDA), and the ISO Graph Annotation Format (GrAF; in development), to develop such a data model entitled CDA+GrAF. We experimented with several methods to combine these existing standards, and eventually selected a method wrapping separate CDA and GrAF parts in a common standoff annotation (i.e., separate from the annotated text) XML document. Two use cases, clinical document sections, and the 2010 i2b2/VA NLP Challenge (i.e., problems, tests, and treatments, with their assertions and relations), were used to create examples of such standoff annotation documents, and were successfully validated with the XML schemata provided with both standards. We developed a tool to automatically translate annotation documents from the 2010 i2b2/VA NLP Challenge format to GrAF, and automatically generated 50 annotation documents using this tool, all successfully validated. Finally, we adapted the XSL stylesheet provided with HL7 CDA to allow viewing annotation XML documents in a web browser, and plan to adapt existing tools for translating annotation documents between CDA+GrAF and the UIMA and GATE frameworks. This common data model may ease directly comparing NLP tools and applications, combining their output, transforming and translating annotations between different NLP applications, and eventually plug-and-play of different modules in NLP applications.", "keywords": ["natural language processing ", "medical informatics ", "data model", "information model", "hl7 clinical document architecture", "iso graph annotation format"]}, {"id": "249", "title": "Labelings and encoders with the uniform bit error property with applications to serially concatenated trellis codes", "abstract": "The well-known uniform error property for signal constellations and codes is extended to encompass information bits. We introduce a class of binary labelings for signal constellations, called bit geometrically uniform (BGU) labelings, for which the uniform bit error property holds, i.e., the bit error probability does not depend on the transmitted signal. Strong connections between the symmetries of constellations and binary Hamming spaces are involved. For block-coded modulation (BCM) and trellis-coded modulation (TCM) Euclidean-space codes, BGU encoders are introduced and studied. The properties of BGU encoders prove quite useful for the analysis and design of codes aimed at minimizing the bit, rather than symbol, error probability. Applications to the analysis and the design of serially concatenated trellis codes are presented, together with a case study which realizes a spectral efficiency of 2 b/s/Hz.", "keywords": ["coding", "concatenated codes", "constellations", "encoding", "labeling", "trellis codes", "uniform error property"]}, {"id": "250", "title": "Stochastic analysis of packet-pair probing for network bandwidth estimation", "abstract": "In this paper, we perform a stochastic analysis of the packet-pair technique, which is a widely used method for estimating the network bandwidth in an end-to-end manner. There has been no explicit delay model of the packet-pair technique primarily because the stochastic behavior of a packet pair has not been fully understood. Our analysis is based on a novel insight that the transient analysis of the G/D/1 system can accurately describe the behavior of a packet pair, providing an explicit stochastic model. We first investigate a single-hop case and derive an analytical relationship between the input and the output probing gaps of a packet pair. Using this single-hop model, we provide a multi-hop model under an assumption of a single tight link. Our model shows the following two important features of the packet-pair technique: (i) The difference between the proposed model and the previous fluid model becomes significant when the input probing gap is around the characteristic value. (ii) The available bandwidth of any link after the tight link is not observable. We verify our model via ns-2 simulations and empirical results. We give a discussion on recent packet-pair models in relation to the proposed model and show that most of them can be regarded as special cases of the proposed model.", "keywords": ["packet-pair technique", "bandwidth estimation", "m/d/1 queue", "transient analysis"]}, {"id": "251", "title": "Development of a teleoperation system for agricultural vehicles", "abstract": "A teleoperation system for a hydro-static transmission (HST) drive crawler-type robotic vehicle is described in this paper. The system was developed to satisfy the needs of various farm operations and teleoperation in unknown agricultural fields. The controller has a layered architecture and supports two degrees of cooperation between the operator and robot, direct and supervisory control. The vehicle can travel autonomously by using an RTK-GPS and a fiber-optic gyroscope during supervisory control, and the operator interface also provides a field navigator based on Google Map technology. The vehicle's position and heading direction was capable of 1Hz update using precise satellite image maps. The results of field tests using direct control showed that it is difficult for the operator to control the movement of the vehicle along the target lines. On the other hand, the vehicle could travel in a straight line with a maximum lateral error of 0.3m by using supervisory control.", "keywords": ["agriculture", "computer vision", "autonomous mobile robot", "communication", "vehicle"]}, {"id": "252", "title": "A general audio classifier based on human perception motivated model", "abstract": "The audio channel conveys rich clues for content-based multimedia indexing. Interesting audio analysis includes, besides widely known speech recognition and speaker identification problems, speech/music segmentation, speaker gender detection, special effect recognition such as gun shots or car pursuit, and so on. All these problems can be considered as an audio classification problem which needs to generate a label from low audio signal analysis. While most audio analysis techniques in the literature are problem specific, we propose in this paper a general framework for audio classification. The proposed technique uses a perceptually motivated model of the human perception of audio classes in the sense that it makes a judicious use of certain psychophysical results and relies on a neural network for classification. In order to assess the effectiveness of the proposed approach, large experiments on several audio classification problems have been carried out, including speech/music discrimination in Radio/TV programs, gender recognition on a subset of the switchboard database, highlights detection in sports videos, and musical genre recognition. The classification accuracies of the proposed technique are comparable to those obtained by problem specific techniques while offering the basis of a general approach for audio classification.", "keywords": ["audio classification", "gender identification", "music genre recognition", "highlights detection", "perceptually motivated features", "content-based audio indexing", "piecewise gaussian modelling"]}, {"id": "253", "title": "Stability routing with constrained path length for improved routability in dynamic MANETs", "abstract": "Quality of service (QoS) routing is known to be an NP-hard problem in case of two or more additive constraints, and several exact algorithms and heuristics have been proposed to address this issue. In this paper, we consider a particular two-constrained quality of service routing problem maximizing path stability with a limited path length in the quest of improving routability in dynamic multi-hop mobile wireless ad hoc networks. First, we propose a novel exact algorithm to solve the optimal weight-constrained path problem. We instantiate our algorithm to solve the most stable path not exceeding a certain number of hops, in polynomial time. This algorithm is then applied to the practical case of proactive routing in dynamic multi-hop wireless ad hoc networks. In these networks, an adequate compromise between route stability and its length in hops is essential for appropriately mitigating the impact of the network dynamics on the validity of established routes. Secondly, we set up a common framework for the comparison between three families of proactive routing: the shortest path-based routing, the most stable path-based routing and our proposed most stable constrained path routing. We show then through extensive simulations that routing based on our proposed algorithm selects appropriate stable paths yielding a very high routability with an average path length just above that of the shortest paths.", "keywords": ["manets", "constrained-based routing", "quality of service routing", "stability constraint", "np-hard problems", "polynomial algorithms"]}, {"id": "254", "title": "SAR complex image data compression based on quadtree and zerotree Coding in Discrete Wavelet Transform Domain: A Comparative Study", "abstract": "SAR complex image data compression based on wavelet-quadtree is proposed. QC-DWT has achieved the-state-of-the-art performance. QC-DWT has achieved higher performance compared with wavelet-zerotree.", "keywords": ["quadtree", "sar complex image data compression", "zerotree"]}, {"id": "255", "title": "On the power of tree-walking automata", "abstract": "Tree-walking automata (TWAs) recently received new attention in the fields of formal languages and databases. To achieve a better understanding of their expressiveness, we characterize them in terms of transitive closure logic formulas in normal form. It is conjectured by Engelfriet and Hoogeboom that TWAs cannot define all regular tree languages, or equivalently, all of monadic second-order logic. We prove this conjecture for a restricted, but powerful, class of TWAs. In particular, we show that 1-bounded TWAs, that is TWAs that are only allowed to traverse every edge of the input tree at most once in every direction, cannot define all regular languages. We then extend this result to a class of TWAs that can simulate first-order logic (FO) and is capable of expressing properties not definable in FO extended with regular path expressions; the latter logic being a valid abstraction of current query languages for XML and semistructured data.", "keywords": ["tree-walking automata", "regular tree languages", "logic", "formal languages."]}, {"id": "256", "title": "authenticity by tagging and typing", "abstract": "We propose a type and effect system for  authentication  protocols built upon a tagging scheme that formalizes the intended semantics of ciphertexts. The main result is that the validation of each component in isolation is provably sound and  fully compositional : if all the protocol participants are independently validated, then the protocol as a whole guarantees authentication in the presence of Dolev-Yao intruders. The highly compositional nature of the analysis makes it suitable for multi-protocol systems, where different protocols might be executed concurrently.", "keywords": ["authentication", "static analysis", "process calculi"]}, {"id": "257", "title": "The conformational behavior, geometry and energy parameters of Menshutkin-like reaction of O-isopropylidene-protected glycofuranoid mesylates in view of DFT calculations", "abstract": "The reaction of three mesylates of furanoderivatives in pyridine is presented at the DFT. All the structures were fully optimized in the gas phase, in chloroform and water. The calculations revealed the barrier height increasing order as follows: 1>2>3. MPW1K/6-31+G** level activation barriers are higher than those from B3LYP/6-31+G**. The furanoid ring conformations are close to E0 or 0E.", "keywords": ["furanoid ring", "conformation", "dft calculations", "nbo", "quaternary ammonium salts"]}, {"id": "258", "title": "Translating update operations from relational to object-oriented databases", "abstract": "In migrating a legacy relational database system to the object-oriented (OO) platform, when database migration completes, application modules are to be migrated, where embedded relational database operations are mapped into their OO correspondents. In this paper we study mapping relational update operations to their OO equivalents, which include UPDATE1, INSERT and DELETE operations. Relational update operation translation from relational to OO faces the touchy problem of transformation from a value-based relationship model to a reference-based model and maintaining the relational integrity constraints. Moreover, with a relational database where inheritance is expressed as attribute value subset relationship, changing of some attribute values may lead to the change of the position of an object in the class inheritance hierarchy, which we call object migration. Considering all these aspects, algorithms are given mapping relational UPDATE, INSERT and DELETE operations to their OO correspondents. Our work emphasize in examining the differences in the representation of the source schema's semantics resulting from the translation process, as well as differences in the inherent semantics of the two models.", "keywords": ["relational model", "object-oriented model", "query translation", "update translation"]}, {"id": "259", "title": "Secure threshold multi authority attribute based encryption without a central authority", "abstract": "An attribute based encryption scheme (ABE) is a cryptographic primitive in which every user is identified by a set of attributes, and some function of these attributes is used to determine the ability to decrypt each ciphertext. Chase proposed the first multi authority ABE scheme which requires a fully trusted central authority who has the ability to decrypt each ciphertext in the system. This central authority would endanger the whole system if it is corrupted. This paper provides a threshold multi authority fuzzy identity based encryption (MA-FIBE) scheme without a central authority for the first time. An encrypter can encrypt a message such that a user could only decrypt if he has at least d(k) of the given attributes about the message for at least t + 1, t <= n/2 honest authorities of all the n attribute authorities in the proposed scheme. This paper considers a stronger adversary model in the sense that the corrupted authorities are allowed to distribute incorrect secret keys to the users. The security proof is based on the secrecy of the underlying distributed key generation protocol and joint zero secret sharing protocol and the standard decisional bilinear Diffie-Hellman assumption. The proposed MA-FIBE could be extended to the threshold multi authority attribute based encryption (MA-ABE) scheme, and both key policy based and ciphertext policy based MA-ABE schemes without a central authority are presented in this paper. Moreover, several other extensions, such as a proactive large universe MA-ABE scheme, are also provided in this paper.  ", "keywords": ["threshold multi authority abe", "without a central authority"]}, {"id": "260", "title": "Segmentation According to Natural Examples: Learning Static Segmentation from Motion Segmentation", "abstract": "The Segmentation According to Natural Examples (SANE) algorithm learns to segment objects in static images from video training data. SANE uses background subtraction to find the segmentation of moving objects in videos. This provides object segmentation information for each video frame. The collection of frames and segmentations forms a training set that SANE uses to learn the image and shape properties of the observed motion boundaries. When presented with new static images, the trained model infers segmentations similar to the observed motion segmentations. SANE is a general method for learning environment-specific segmentation models. Because it can automatically generate training data from video, it can adapt to a new environment and new objects with relative ease, an advantage over untrained segmentation methods or those that require human-labeled training data. By using the local shape information in the training data, it outperforms a trained local boundary detector. Its performance is competitive with a trained top-down segmentation algorithm that uses global shape. The shape information it learns from one class of objects can assist the segmentation of other classes.", "keywords": ["segmentation", "machine learning", "motion", "computer vision", "markov random field"]}, {"id": "261", "title": "New delay-dependent conditions on robust stability and stabilisation for discrete-time systems with time-delay", "abstract": "This article is concerned with new robust stability conditions and robust stabilisation method for a discrete-time system with time-delay and time-varying structured uncertainties that come into state and input matrices. An improved approach to obtain new robust stability conditions is proposed. Our approach employs a generalised Lyapunov functional combined with the parameterised model transformation method and the generalised free weighting matrix method. These generalisations lead to generalised robust stability conditions that are given in terms of linear matrix inequalities. Moreover, based on new robust stability conditions, a robust stabilisation method for uncertain discrete-time systems with time-delay is given. Numerical examples compare our robust stability conditions with some existing conditions to show the effectiveness of our approach and also illustrate the improvement of our robust stabilisation method.", "keywords": ["robust stability", "time-delay systems", "discrete-time systems", "uncertain systems", "delay-dependent conditions", "linear matrix inequality"]}, {"id": "262", "title": "Link testA statistical method for finding prostate cancer biomarkers", "abstract": "We present a new method, link-test, to select prostate cancer biomarkers from SELDI mass spectrometry and microarray data sets. Biomarkers selected by link-test are supported by data sets from both mRNA and protein levels, and therefore results in improved robustness. Link-test determines the level of significance of the association between a microarray marker and a specific mass spectrum marker by constructing background mass spectra distributions estimated by all human protein sequences in the SWISS-PROT database. The data set consist of both microarray and mass spectrometry data from prostate cancer patients and healthy controls. A list of statistically justified prostate cancer biomarkers is reported by link-test. Cross-validation results show high prediction accuracy using the identified biomarker panel. We also employ a text-mining approach with OMIM database to validate the cancer biomarkers. The study with link-test represents one of the first cross-platform studies of cancer biomarkers.", "keywords": ["microarray", "mass spectrometry", "biomarker", "prostate cancer", "text mining"]}, {"id": "263", "title": "a low leakage 9t sram cell for ultra-low power operation", "abstract": "This paper presents the design and evaluation of a new SRAM cell made of nine transistors (9T). The proposed 9T cell utilizes a scheme with separate read and write wordlines; it is shown that the 9T cell achieves improvements in power dissipation, performance and stability compared with previous designs (that require 10T and 8T) for low-power operation. The 9T scheme is amenable to small feature sizes as encountered in the deep sub-micron/nano ranges of CMOS technology.", "keywords": ["static noise margin", "sram cell", "nanotechnology", "leakage power", "low power"]}, {"id": "264", "title": "Bacteria Hunt Evaluating multi-paradigm BCI interaction", "abstract": "The multimodal, multi-paradigm brain-computer interfacing (BCI) game Bacteria Hunt was used to evaluate two aspects of BCI interaction in a gaming context. One goal was to examine the effect of feedback on the ability of the user to manipulate his mental state of relaxation. This was done by having one condition in which the subject played the game with real feedback, and another with sham feedback. The feedback did not seem to affect the game experience (such as sense of control and tension) or the objective indicators of relaxation, alpha activity and heart rate. The results are discussed with regard to clinical neurofeedback studies. The second goal was to look into possible interactions between the two BCI paradigms used in the game: steady-state visually-evoked potentials (SSVEP) as an indicator of concentration, and alpha activity as a measure of relaxation. SSVEP stimulation activates the cortex and can thus block the alpha rhythm. Despite this effect, subjects were able to keep their alpha power up, in compliance with the instructed relaxation task. In addition to the main goals, a new SSVEP detection algorithm was developed and evaluated.", "keywords": ["brain-computer interfacing", "multimodal interaction", "steady-state visually-evoked potentials", "concentration", "neurofeedback relaxation", "game"]}, {"id": "265", "title": "Metamodels and emergent behaviour in models of conflict", "abstract": "In this paper, we develop a simplified mathematical model (a metamodel) of a simulation model of conflict, based on ideas drawn from the analysis of more general physical systems, such as found in fluid dynamics modelling. We show that there is evidence from the analysis of historical conflicts to support the kind of emergent behaviour implied by this approach. We then apply this approach to the development of a metamodel of a particular complexity based simulation model of conflict (ISAAC), developed for the US Marine Corps. The approach we have illustrated here is very generic, and is applicable to any simulation model which has complex interactions similar to those found in fluid dynamic modelling, or in simulating the emergent behaviour of large numbers of simple systems which interact with each other locally.", "keywords": ["conflict", "emergent behaviour", "metamodel", "simulation", "complexity"]}, {"id": "266", "title": "Topological Implications of Selfish Neighbor Selection in Unstructured Peer-to-Peer Networks", "abstract": "Current peer-to-peer (P2P) systems often suffer from a large fraction of freeriders not contributing any resources to the network. Various mechanisms have been designed to overcome this problem. However, the selfish behavior of peers has aspects which go beyond resource sharing. This paper studies the effects on the topology of a P2P network if peers selfishly select the peers to connect to. In our model, a peer exploits locality properties in order to minimize the latency (or response times) of its lookup operations. At the same time, the peer aims at not having to maintain links to too many other peers in the system. By giving tight bounds on the price of anarchy, we show that the resulting topologies can be much worse than if peers collaborated. Moreover, the network may never stabilize, even in the absence of churn. Finally, we establish the complexity of Nash equilibria in our game theoretic model of P2P networks. Specifically, we prove that it is NP-hard to decide whether our game has a Nash equilibrium and can stabilize.", "keywords": ["game theory", "peer-to-peer", "price of anarchy", "np-hardness", "metric spaces"]}, {"id": "267", "title": "a readable tcp in the prolac protocol language", "abstract": "Prolac is a new statically-typed, object-oriented language for network protocol implementation. It is designed for readability, extensibility, and real-world implementation; most previous protocol languages, in contrast, have been based on hard-to-implement theoretical models and have focused on verification. We present a working Prolac TCP implementation directly derived from 4.4BSD. Our implementation is modular---protocol processing is logically divided into minimally-interacting pieces; readable---Prolac encourages top-down structure and naming intermediate computations; and extensible---subclassing cleanly separates protocol extensions like delayed acknowledgements and slow start. The Prolac compiler uses simple global analysis to remove expensive language features like dynamic dispatch, resulting in end-to-end performance comparable to an unmodified Linux 2.0 TCP.", "keywords": ["network protocol", "structure", "analysis", "dynamic", "language", "performance", "implementation", "verification", "readability", "process", "modular", "compilation", "extensibility", "model", "feature", "global", "object oriented language"]}, {"id": "268", "title": "A WEARABLE DOCUMENT READER FOR THE VISUALLY IMPAIRED: DEWARPING AND SEGMENTATION", "abstract": "While reading devices for the visually impaired have been available for many years, they are often expensive and difficult to use. The image processing required to enable the reading task is a composition of several important sub-tasks, such as image capture, image stabilization, image enhancement and page-curl dewarping region segmentation, regions grouping, and word recognition In this paper we deal with some of these sub-tasks in an effort to prototype a device (Tyflos-reader) that will read a document for a person with a visual impairment and respond to voice commands for control. Initial experimental results on a set of textbook and newspaper pages are also presented.", "keywords": ["assistive devices", "image super-resolution", "perspective rectification", "page-curl dewarping", "document segmentation", "voice user-interface"]}, {"id": "269", "title": "Top-K structural diversity search in large networks", "abstract": "Social contagion depicts a process of information (e.g., fads, opinions, news) diffusion in the online social networks. A recent study reports that in a social contagion process, the probability of contagion is tightly controlled by the number of connected components in an individuals neighborhood. Such a number is termed structural diversity of an individual, and it is shown to be a key predictor in the social contagion process. Based on this, a fundamental issue in a social network is to find top-(k) users with the highest structural diversities. In this paper, we, for the first time, study the top-(k) structural diversity search problem in a large network. Specifically, we study two types of structural diversity measures, namely, component-based structural diversity measure and core-based structural diversity measure. For component-based structural diversity, we develop an effective upper bound of structural diversity for pruning the search space. The upper bound can be incrementally refined in the search process. Based on such upper bound, we propose an efficient framework for top-(k) structural diversity search. To further speed up the structural diversity evaluation in the search process, several carefully devised search strategies are proposed. We also design efficient techniques to handle frequent updates in dynamic networks and maintain the top-(k) results. We further show how the techniques proposed in component-based structural diversity measure can be extended to handle the core-based structural diversity measure. Extensive experimental studies are conducted in real-world large networks and synthetic graphs, and the results demonstrate the efficiency and effectiveness of the proposed methods.", "keywords": ["structural diversity", "disjoint-set forest", " search", "dynamic graph"]}, {"id": "270", "title": "A unified approach for detecting and eliminating selfish nodes in MANETs using TBUT", "abstract": "Recent years have witnessed the increasing efforts toward making architecture standardization for the secured wireless mobile ad hoc networks. In this scenario when a node actively utilizes the other node resources for communicating and refuses to help other nodes in their transmission or reception of data, it is called a selfish node. As the entire mobile ad hoc network (MANETs) depends on cooperation from neighboring nodes, it is very important to detect and eliminate selfish nodes from being part of the network. In this paper, token-based umpiring technique (TBUT) is proposed, where every node needs a token to participate in the network and the neighboring nodes act as umpire. This proposed TBUT is found to be very efficient with a reduced detection time and less overhead. The security analysis and experimental results have shown that TBUT is feasible for enhancing the security and network performance of real applications.", "keywords": ["manet", "selfish node", "performance and token-based umpiring technique "]}, {"id": "271", "title": "Multiversion join index for multiversion data warehouse", "abstract": "The data warehouse (DW) technology is developed in order to support the integration of external data sources (EDSs) for the purpose of advanced data analysis by On-Line Analytical Processing (OLAP) applications. Since contents and structures of integrated EDSs may evolve in time, the content and schema of a DW must evolve too in order to correctly reflect the evolution of EDSs. In order to manage a DW evolution, we developed the multiversion data warehouse (MVDW) approach. In this approach, different states of a DW are represented by the sequence of persistent DW versions that correspond either to the real world state or to a simulation scenario. Typically, OLAP applications execute star queries that join multiple fact and dimension tables. An important optimization technique for this kind of queries is based on join indexes. Since in the MVDW fact and dimension data are physically distributed among multiple DW versions, standard join indexes need extensions. In this paper we present the concept of a multiversion join index (MVJI) applicable to indexing dimension and fact tables in the MVDW. The MVJI has a two-level structure, where an upper level is used for indexing attributes and a lower level is used for indexing DW versions. The paper also presents the theoretical upper bound (pessimistic) analysis of the MVJI performance characteristic with respect to I/O operations. The analysis is followed by experimental evaluation. It shows that the MVJI increases a system performance for queries addressing multiple DW versions with exact match and range predicates.  ", "keywords": ["star query", "join index", "multiversion data warehouse", "multiversion query", "multiversion join index"]}, {"id": "272", "title": "Reactive column profile map topology: Continuous distillation column with non-reversible kinetics", "abstract": "In this paper we present a topologically based approach to the analysis and synthesis of reactive distillation columns. We extend the definition of Tapp et al. [Tapp, M., Holland, S., Glasser, D., & Hildebrandt, D. (2004). Column profile maps part A: Derivation and interpretation. Industrial and Engineering Chemistry Research, 43, 364-374] of a column section in non-reactive distillation column to a reactive column section (RCS) in a reactive distillation column. A RCS is defined as a section of a reactive distillation column in which there is no addition or removal of material or energy. We introduce the concept of a reactive column profile map (RCPM) in which the profiles in the RCPM correspond to the liquid composition profiles in the RCS. By looking at the singular points in the RCPM, it is demonstrated that for a single chemical reaction with no net change in the total number of moles, the bifurcation of the singular points depends on both the difference point as introduced by Hauan et al. [Hauan, S., Ciric, A. R., Westerberg, A. W., & Lien, K. M. (2000). Difference points in extractive and reactive cascades I-Basic properties and analysis. Chemical Engineering Science, 55, 3145-3159] as well as the direction of the stoichiometric vector. These two vectors combine to define what we call the reactive difference point composition. We show that there only certain feasible topologies of the RCPM and these depend only on the position of the reactive difference point composition. We look at a simple example where the vapour liquid equilibrium (VLE) is ideal and show that we can classify regions of reactive difference point compositions that result in similar topology of the RCPM. Thus, by understanding the feasible topologies of the RCPM, one is able to identify profiles in the RCPM that are desirable and hence one is able to synthesize a reactive distillation column by combining RCS that correspond to the desired profile in the RCPM. We believe that this tool will help understand how and when reaction could introduce unexpected behaviors and this can be used as a complementary tool to existing methods used for synthesis of reactive distillation columns.  ", "keywords": ["reactive column profile map", "difference point", "reactive column section"]}, {"id": "273", "title": "Fault-tolerant hamiltonian laceability of hypercubes", "abstract": "It is known that every hypercube Q(n) is a bipartite graph. Assume that n greater than or equal to 2 and F is a subset of edges with F less than or equal to n - 2. We prove that there exists a hamiltonian path in Q(n) - F between any two vertices of different partite sets. Moreover, there exists a path of length 2(n) - 2 between any two vertices of the same partite set. Assume that n greater than or equal to 3 and F is a subset of edges with F less than or equal to n - 3. We prove that there exists a hamiltonian path in Q(n) - {v} - F between any two vertices in the partite set without v. Furthermore, all bounds are tight.  ", "keywords": ["hamiltonian laceable", "hypercube", "fault tolerance"]}, {"id": "274", "title": "incorporating user control into recommender systems based on naive bayesian classification", "abstract": "Recommender systems are increasingly being employed to personalize services, such as on the web, but also in electronics devices, such as personal video recorders. These recommenders learn a user profile, based on rating feedback from the user on, e.g., books, songs, or TV programs, and use machine learning techniques to infer the ratings of new items. The techniques commonly used are collaborative filtering and naive Bayesian classification, and they are known to have several problems, in particular the cold-start problem and its slow adaptivity to changing user preferences. These problems can be mitigated by allowing the user to set up or manipulate his profile. In this paper, we propose an extension to the naive Bayesian classifier that enhances user control. We do this by maintaining and flexibly integrating two profiles for a user, one learned by rating feedback, and one created by the user. We in particular show how the cold-start problem is mitigated.", "keywords": ["multi-valued features", "naive bayes", "user profile", "machine learning", "classification", "recommender", "user control"]}, {"id": "275", "title": "Population variation in genetic programming", "abstract": "A new population variation approach is proposed, whereby the size of the population is systematically varied during the execution of the genetic programming process with the aim of reducing the computational effort compared with standard genetic programming (SGP). Various schemes for altering population size under this proposal are investigated using a comprehensive range of standard problems to determine whether the nature of the population variation, i.e. the way the population is varied during the search, has any significant impact on GP performance. The initial population size is varied in relation to the initial population size of the SGP such that the worst case computational effort is never greater than that of the SGP. It is subsequently shown that the proposed population variation schemes do have the capacity to provide solutions at a lower computational cost compared with the SGP.", "keywords": ["genetic programming", "computational effort", "average number of evaluations", "convergence", "population variation"]}, {"id": "276", "title": "The Need for Power Debugging in the Multi-Core Environment", "abstract": "Debugging an application for power has a wide array of benefits ranging from minimizing the thermal hotspots to reducing the likelihood of CPU malfunction. In this work, we justify the need for power debugging, and show that performance debugging of a parallel application does not automatically guarantee power balance across multiple cores. We perform experiments and show our results using two case study benchmarks, Volrend from Splash-2 and Bodytrack from Parsec-1.0.", "keywords": ["multi-cores", "power debugging", "power imbalance"]}, {"id": "277", "title": "Human resource assignment system for distribution centers", "abstract": "Information technology and its wide range of applications have begun to make their presence in a new generation of logistic and distribution service industry. A more flexible breed of application packages Is emerging by the application of fourth generation language (4GL) technologies, which are able to provide foundations for true enterprise resource planning (ERP). There are many good reasons for adopting enterprise-wide resource planning systems. This research, however, focuses on the development of a human resource assignment module (HR module), usually considered as an essential part of an ERP system. This module provides crucial human resource data and supports decisions in human resource utilization in distribution center operations. We detail the crucial algorithm for the HR module, which provides efficient and effective manpower management for key logistic/distribution center operations.", "keywords": ["human resource management", "decision-support systems", "order picking"]}, {"id": "278", "title": "Quantized circulation in dilute Bose-Einstein condensates", "abstract": "We compute using a microscopic mean-field theory the structure and the quasiparticle excitation spectrum of a dilute. trapped Bose-Einstein condensate penetrated by an axisymmetric vortex line. The Gross-Pitaevskii equation for the condensate and the coupled Hartree-Fock-Bogoliubov-Popov equations describing the elementary excitations are solved self-consistently using finite-difference methods. We find locally stable vortex configurations at all temperatures below T-c.  ", "keywords": ["bose-einstein condensation", "vortices", "finite-difference methods"]}, {"id": "279", "title": "the design and evaluation of a high-performance soft keyboard", "abstract": "The design and evaluation of a high performance soft keyboard for mobile systems are described. Using a model to predict the upper-bound text entry rate for soft keyboards, we designed a keyboard layout with a predicted upper-bound entry rate of 58.2 wpm. This is about 35% faster than the predicted rate for a QWERTY layout. We compared our design (OPTI) with a QWERTY layout in a longitudinal evaluation using five participants and 20 45-minute sessions of text entry. Average entry rates for OPT1 increased from 17.0 wpm initially to 44.3 wpm at session 20. The average rates exceeded those for the QWERTY layout after the 10 session (about 4 hours of practice). A regression equation (R = .997) in the form of the power-law of learning predicts that our upper-bound prediction would be reach at about session 50.", "keywords": ["pen input", "regression", "power law", "design", "high-performance", "layout", "linguistic models", "digraph probabilities", "learning", "text entry", "model", "soft keyboards", "practical", "fitts' law", "stylus input", "evaluation", "participant", "mobile systems", "predict"]}, {"id": "280", "title": "Stereoscopic video coding and disparity estimation for low bitrate applications based on MPEG-4 multiple auxiliary components", "abstract": "Using an MPEG-4 MAC (multiple auxiliary component) system is a good way to encode stereoscopic video with existing standard CODECs, especially when it comes to low bitrate applications. In this paper, we discuss the properties and problems of MAC systems when encoding stereoscopic video, and propose an MAC-based stereoscopic video coder and disparity estimation scheme to solve those problems. We used a reconstructed disparity map during the disparity compensation process and took that disparity map into account while estimating the base-view sequence motion vectors. Moreover, we proposed a search range finding and illumination imbalance decision system. We also proposed a block-based disparity map regularization process as well as block splitting in the object boundary and occlusion regions (to reduce the number of bits to encode in both the disparity map and the residual image). Last, we compensated for the imbalance between two cameras with a novel system that used MAC characteristics. Experimental results indicate that the proposed MAC system outperformed conventional stereo coding systems by a maximum of 3.5dB in terms of the PSNR and 1018% in terms of bitsaving, especially in low bitrate applications.", "keywords": ["mpeg-4 mac", "stereoscopic video coder", "disparity estimation"]}, {"id": "281", "title": "Robust Recovery of Subspace Structures by Low-Rank Representation", "abstract": "In this paper, we address the subspace clustering problem. Given a set of data samples (vectors) approximately drawn from a union of multiple subspaces, our goal is to cluster the samples into their respective subspaces and remove possible outliers as well. To this end, we propose a novel objective function named Low-Rank Representation (LRR), which seeks the lowest rank representation among all the candidates that can represent the data samples as linear combinations of the bases in a given dictionary. It is shown that the convex program associated with LRR solves the subspace clustering problem in the following sense: When the data is clean, we prove that LRR exactly recovers the true subspace structures; when the data are contaminated by outliers, we prove that under certain conditions LRR can exactly recover the row space of the original data and detect the outlier as well; for data corrupted by arbitrary sparse errors, LRR can also approximately recover the row space with theoretical guarantees. Since the subspace membership is provably determined by the row space, these further imply that LRR can perform robust subspace clustering and error correction in an efficient and effective way.", "keywords": ["low-rank representation", "subspace clustering", "segmentation", "outlier detection"]}, {"id": "282", "title": "On an efficient CAD implementation of the distance term in Pelgrom's mismatch model", "abstract": "In 1989, Pelgrom et al published a mismatch model for MOS transistors, where the variation of parameter mismatch between two identical transistors is given by two independent terms: a size-dependent term and a distance-dependent term. Some CAD tools based on a nonphysical interpretation of Pelgrom's distance term result in excessive computationally expensive algorithms, which become nonviable even for circuits with a reduced number of transistors. Furthermore, some researchers are reporting new variations on the original nonphysically interpreted algorithms, which may render false results. The purpose of this paper is to clarify the physical interpretation of the distance term of Pelgrom et al. and indicate how to model it efficiently in prospective CAD tools.", "keywords": ["analog design", "mismatch gradient planes", "mismatch modeling", "pelgrom model", "sigma-space analysis"]}, {"id": "283", "title": "Vertex Ordering Characterizations of Graphs of Bounded Asteroidal Number", "abstract": "Asteroidal Triple-free (AT-free) graphs have received considerable attention due to their inclusion of various important graphs families, such as interval and cocomparability graphs. The asteroidal number of a graph is the size of a largest subset of vertices such that the removal of the closed neighborhood of any vertex in the set leaves the remaining vertices of the set in the same connected component. (AT-free graphs have asteroidal number at most 2.) In this article, we characterize graphs of bounded asteroidal number by means of a vertex elimination ordering, thereby solving a long-standing open question in algorithmic graph theory. Similar characterizations are known for chordal, interval, and cocomparability graphs.", "keywords": ["asteroidal triple", "at-free", "vertex elimination", "asteroidal number", ""]}, {"id": "284", "title": "Clustering validity checking methods: Part II", "abstract": "Clustering results validation is an important topic in the context of pattern recognition. We review approaches and systems in this context. In the first part of this paper we presented clustering validity checking approaches based on internal and external criteria. In the second, current part, we present a review of clustering validity approaches based on relative criteria. Also we discuss the results of an experimental study based on widely known validity indices. Finally the paper illustrates the issues that are under-addressed by the recent approaches and proposes the research directions in the field.", "keywords": ["clustering validation", "pattern discovery", "unsupervised learning"]}, {"id": "285", "title": "a training software model of an interrupt system", "abstract": "The paper justifies the necessity to introduce the students from the 'Computer Systems and Technologies' degree course to the structuce and way of operation of the interrupt system - one of the important components of the processor. Analysis of the basic funcionality of an example interrupt system is presented, an existing interrupt system is selected as a prototype of the training model and the arguments for its selection are proposed. The paper also describes the implemented model and its features. The work with the model will enable students to comprehend the way of operation of the interrupt system and it will be also used to check and assess their knowledge.", "keywords": ["interrupt system", "training software model", "virtual laboratory", "simulation"]}, {"id": "286", "title": "Efficient network QoS provisioning based on per node traffic shaping", "abstract": "This paper addresses the problem of providing per-connection end-to-end delay guarantees in a high-speed network, We assume that the network is connection oriented and enforces some admission control which ensures that the source traffic conforms to specified traffic characteristics. We concentrate on the class of rate-controlled service (RCS) disciplines, in which traffic from each connection is reshaped at every hop, and develop end-to-end delay bounds for the general case where different reshapers are used at each hop. In addition, we establish that these bounds can also be achieved when the shapers at each hop have the same ''minimal'' envelope. The main disadvantage of this class of service discipline is that the end-to-end delay guarantees are obtained as the sum of the worst-case delays at each node, but we show that this problem can be alleviated through ''proper'' reshaping of the traffic, We illustrate the impact of this reshaping by demonstrating its use in designing RCS disciplines that outperform service disciplines that are based on generalized processor sharing (GPS), Furthermore, we show that we fan restrict the space of ''good'' shapers to a family which is characterized by only one parameter, We also describe extensions to the service discipline that make it work conserving and as a result reduce the average end-to-end delays.", "keywords": ["qos provisioning", "real-time traffic", "traffic shaping", "atm", "scheduling", "end-to-end delay guarantees"]}, {"id": "287", "title": "AS-level source routing for multi-provider connection-oriented services", "abstract": "In this paper, we study the inter-domain Autonomous System (AS)-level routing problem within an alliance of ASs. We first describe the framework of our work, based on the introduction of a service plane for automatic multi-domain service provisioning. We adopt an abstract representation of domain relationships by means of directional metrics which are applied to a triplet (ingress point, transit AS, egress point) where the ingress and egress points can be ASs or routers. Then, we focus on the point-to-point and multipoint AS-level routing problems that arise in such an architecture. We propose an original approach that reaches near optimal solutions with tractable computation times. A further contribution of this paper is that a heavy step in the proposed heuristic can be precomputed, independently of the service demands. Moreover, we describe how in this context AS-level path diversity can be considered, and present the related extension of our heuristic. By extensive tests on AS graphs derived from the Internet, we show that our heuristic is often equal or a few percent close to the optimal, and that, in the case of precomputation, its time consumption can be much lower than with other well-known algorithms.", "keywords": ["inter-domain routing", "inter-as mpls", "qos routing", "multipoint routing", "as-level routing"]}, {"id": "288", "title": "Parallel and distributed simulation of sediment dynamics in shallow water using particle decomposition approach", "abstract": "This paper describes the parallel simulation of sediment dynamics in shallow water. By using a Lagrangian model, the problem is transformed to one in which a large number of independent particles must be tracked. This results in a technique that can be parallelised with high efficiency. We have developed a sediment transport model using three different sediment suspension methods. The first method uses a modified mean for the Poisson distribution function to determine the expected number of the suspended particles in each particular grid cell of the domain over all available processors. The second method determines the number of particles to suspend with the aid of the Poisson distribution function only in those grid cells which are assigned to that processor. The third method is based on the technique of using a synchronised pseudo-random-number generator to generate identical numbers of suspended particles in all valid grid cells for each processor. Parallel simulation experiments are performed in order to investigate the efficiency of these three methods. Also the parallel performance of the implementations is analysed. We conclude that the second method is the best method on distributed computing systems (e.g., a Beowulf cluster), whereas the third maintains the best load distribution.", "keywords": ["lagrangian particle model", "stochastic differential equation", "sediment transport", "parallel processing", "speed up", "load balance", "efficiency"]}, {"id": "289", "title": "A numerical technique to predict periodic and quasi-periodic response of nonlinear dynamic systems", "abstract": "A frequency domain based algorithm using Fourier approximation and Galerkin error minimization has been used to obtain the periodic orbits of large order nonlinear dynamic systems. The stability of these periodic response is determined through a bifurcation analysis using Floquet theory. This technique is applicable to dynamic systems having both analytic and nonanalytic nonlinearities. This technique is compared with numerical time integration and is found to be much faster in predicting the steady state periodic response.", "keywords": ["fouriergalerkinnewton technique", "floquet analysis", "bifurcations"]}, {"id": "290", "title": "Rough fuzzy approximations on two universes of discourse", "abstract": "In rough set theory, the lower and upper approximation operators can be constructed via a variety of approaches. Various fuzzy generalizations of rough approximation operators have been made over the years. This paper presents a framework for the study of rough fuzzy sets on two universes of discourse. By means of a binary relation between two universes of discourse, a covering and three relations are induced to a single universe of discourse. Based on the induced notions, four pairs of rough fuzzy approximation operators are proposed. These models guarantee that the approximating sets and the approximated sets are on the same universes of discourse. Furthermore, the relationship between the new approximation operators and the existing rough fuzzy approximation operators on two universes of discourse are scrutinized, and some interesting properties are investigated. Finally, the connections of these approximation operators are made, and conditions under which some of these approximation operators are equivalent are obtained.", "keywords": ["binary relations", "coverings", "fuzzy sets", "rough fuzzy approximation operators", "two universes"]}, {"id": "291", "title": "Identification of different stages of diabetic retinopathy using retinal optical images", "abstract": "Diabetes is a disease which occurs when the pancreas does not secrete enough insulin or the body is unable to process it properly. This disease affects slowly the circulatory system including that of the retina. As diabetes progresses, the vision of a patient may start to deteriorate and lead to diabetic retinopathy. In this study on different stages of diabetic retinopathy, 124 retinal photographs were analyzed. As a result, four groups were identified, viz., normal retina, moderate non-proliferative diabetic retinopathy, severe non-proliferative diabetic retinopathy and proliferative diabetic retinopathy. Classification of the four eye diseases was achieved using a three-layer feedforward neural network. The features are extracted from the raw images using the image processing techniques and fed to the classifier for classification. We demonstrate a sensitivity of more than 90% for the classifier with the specificity of 100%.", "keywords": ["eye", "normal", "features", "retinopathy", "neural network", "image processing", "feedforward", "classification"]}, {"id": "292", "title": "Constrained ZIP code segmentation by a PCNN-based thinning algorithm", "abstract": "This paper proposes a novel thinning algorithm and applies it to automatic constrained ZIP code segmentation. The segmentation method consists of two main stages: removal of rectangle boxes and location of ZIP code digits. Both the two stages are implemented on the skeleton of boxes, which is extracted by the proposed pulse coupled neural network (PCNN) based thinning algorithm. This algorithm is specially designed to merely skeletonize the boxes. At the second stage, a projection method is employed to segment ZIP code image into its constituent digits. Experimental results show that the proposed method is very efficient in segmenting ZIP code images even with noise.", "keywords": ["constrained zip code segmentation", "pulse coupled neural network", "skeleton", "projection"]}, {"id": "293", "title": "multimodal authentication based on random projections and source coding", "abstract": "In this paper, we consider an authentication framework for independent modalities based on binary hypothesis testing using source coding jointly with the random projections. The source coding ensures the multimodal signals reconstruction at the decoder based on the authentication data. The random projections are used to cope with the security, privacy, robustness and complexity issues. Finally, the authentication performance is investigated for both direct and random projections domains. The asymptotic performance approximation is derived and compared with the exact solutions. The impact of modality fusion on the authentication system performance is demonstrated.", "keywords": ["dimensionality reduction", "fusion", "hypothesis testing", "random projections", "multimodal authentication"]}, {"id": "294", "title": "Effects of additive elements on the phase formation and morphological stability of nickel monosilicide films", "abstract": "Alloying elements can substantially affect the formation and morphological stability of nickel monosilicide. A comprehensive study of phase formation was performed on 24 Ni alloys with varying concentrations of alloying elements. Silicide films have been used for more than 15 years to contact the source, drain and gate of state-of-the-art complementary-metal-oxide-semiconductor (CMOS) devices. In the past, the addition of alloying elements was shown to improve the transformation from the high resistivity C49 to the low resistivity C54-TiSi2 phase and to allow for the control of surface and interface roughness of CoSi2 films as well as produce significant improvements with respect to agglomeration of the films. Using simultaneous time-resolved X-ray diffraction (XRD), resistance and light scattering measurements, we follow the formation of the silicide phases in real time during rapid thermal annealing. Additions to the NiSi system lead to modifications in the phase formation sequence at low temperatures (metal-rich phases), to variations in the formation temperatures of NiSi and NiSi2, and to changes in the agglomeration behavior of the films formed. Of the 24 elements studied, additions of Mo, Re, Ta and W are amongst the most efficient to retard agglomeration while elements such as Pd, Pt and Rh are most efficient to retard the formation of NiSi2.", "keywords": ["nickel silicides", "nisi", "alloying", "agglomeration", "nisi2"]}, {"id": "295", "title": "HIGH-ORDER MULTIPLE-MODE AND TRANSADMITTANCE-MODE OTA-C UNIVERSAL FILTERS", "abstract": "This paper presents two new high-order OTA-C universal filters. The first proposed filter structure employs n + 3 operational transconductance amplifiers (OTAs) and n grounded capacitors, which can realize nth-order multiple-mode (including voltage, current, transadmittance, and transimpedance modes) universal filtering responses (lowpass, highpass, bandpass, bandreject, and allpass) from the same topology. Since the OTA has high input and output impedances, it is very suitable for transadmittance-mode circuit applications. Therefore, a new high-order transadmittance-mode OTA-C universal filter structure using the minimum components is introduced. The second proposed filter structure uses only n + 1 OTAs and n grounded capacitors, which are the minimum components necessary for realizing nth-order transadmittance-mode universal filtering responses (lowpass, highpass, bandpass, bandreject, and allpass) from the same topology. This represents the attractive feature from chip area and power consumption point of view. Moreover, the two new OTA-C universal filters still enjoy many important advantages: no need of extra inverting or double-type amplifiers for special input signals, using only n grounded capacitors, no need of any resistors, cascadably connecting the former voltage-mode stage and the latter current-mode stage, and low sensitivity performance. H-Spice simulations with TSMC 0.35 mu m process and +/- 1.65V supply voltages are included and confirm the theoretical predictions.", "keywords": ["operational transconductance amplifiers", "multiple-mode", "universal high-order filter", "transimpedance-mode", "transadmittance-mode"]}, {"id": "296", "title": "Random fuzzy fractional integral equations  theoretical foundations", "abstract": "This paper presents mathematical foundations for studies of random fuzzy fractional integral equations which involve a fuzzy integral of fractional order. We consider two different kinds of such equations. Their solutions have different geometrical properties. The equations of the first kind possess solutions with trajectories of nondecreasing diameter of their consecutive values. On the other hand, the solutions to equations of the second kind have trajectories with nonincreasing diameter of their consecutive values. Firstly, the existence and uniqueness of solutions is investigated. This is showed by using a method of successive approximations. An estimation of error of nth approximation is given. Also a boundedness of the solution is indicated. To show well-posedness of the considered theory, we prove that solutions depend continuously on the data of the equations. Some concrete examples of random fuzzy fractional integral equations are solved explicitly.", "keywords": ["random fuzzy fractional integral equation", "existence and uniqueness of solution", "uncertainty", "fuzzy differential equation", "set differential equation", "mathematical foundations"]}, {"id": "297", "title": "Efficient Opportunistic Routing in Utility-Based Ad Hoc Networks", "abstract": "Due to resource scarcity, a paramount concern in ad hoc networks is utilizing limited resources efficiently. The self-organized nature of ad hoc networks makes the network utility-based approach an efficient way to allocate limited resources. However, the effect of link instability has not yet been adequately addressed in literature. To efficiently address the routing problem in ad hoc networks, we integrate the cost and stability into a network utility metric, and adopt the metric to evaluate the routing optimality in a unified, opportunistic routing model. Based on this model, an efficient algorithm is designed, both centralized and distributed implementations are presented, and extensive simulations on NS-2 are conducted to verify our results.", "keywords": ["ad hoc networks", "distributed algorithms", "network utility", "opportunistic routing", "stability"]}, {"id": "298", "title": "A Computation to Integrate the Analysis of Genetic Variations Occurring within Regulatory Elements and Their Possible Effects", "abstract": "Single nucleotide polymorphisms (SNPs) and short tandem repeats (STRs) are the most common genetic variations, are widespread within genomes, and form the diversity within species. These genetic variations affect many regulatory elements such as transcription factor binding sites (TFBSs), DNA methylation sites on CpG islands, and microRNA target sites; these elements have been found to play major as well as indirect roles in regulating gene expression. Currently, systems are available to display such genetic variation occurring within regulatory elements. To understand and display all the potential variation described above, we have developed a web-based system tool, the Regulatory Element and Genetic Variation Viewer (REGV Viewer [REGV]), which provides a friendly web interface for users and shows genetic variation information within regulatory elements by either inputting a gene list or selecting a chromosome by name. Moreover, our tool not only supports logic operation queries, but after a query is submitted, it also shows a high-throughput simulation, including combined data, statistical graphs, and graphical views of the genetic variants and regulatory elements. Additionally, when the SNP variation occurs within TFBSs and if the SNP allele frequency and TFBS position weight matrices (PWMs) are available, our system will show the new putative TFBSs resulting from the SNP variation.", "keywords": ["genetic variation", "snp", "tfbs"]}, {"id": "299", "title": "Programs as visual, interactive documents", "abstract": "We present a novel approach to combined textual and visual programming by allowing visual, interactive objects to be embedded within textual source code and segments of source code to be further embedded within those objects. We retain the strengths of text-based source code, while enabling visual programming where it is beneficial. Additionally, embedded objects and code provide a simple object-oriented approach to adding a visual form of LISP-style macros to a language. The ability to freely combine source code and visual, interactive objects with one another allows for the construction of interactive programming tools and experimentation with novel programming language extensions. Our visual programming system is supported by a type coercion-based presentation protocol that displays normal Java and Python objects in a visual, interactive form. We have implemented our system within a prototype interactive programming environment called The Larch Environment. ", "keywords": ["java", "python", "environment", "visual programming", "interactive", "visualization", "implementation"]}, {"id": "300", "title": "The equilibrium limit of a constitutive model for two-phase granular mixtures and its numerical approximation", "abstract": "In this paper we analyze the equilibrium limit of the constitutive model for two-phase granular mixtures introduced in Papalexandris (2004) [13], and develop an algorithm for its numerical approximation. At, equilibrium, the constitutive model reduces to a strongly coupled, overdetermined system of quasilinear elliptic partial differential equations with respect to the pressure and the volume fraction of the solid granular phase. First we carry a perturbation analysis based on standard hydrostatic-type scaling arguments which reduces the complexity of the coupling of the equations. The perturbed system is then supplemented by an appropriate compatibility condition which arises from the properties of the gradient operator. Further, based on the Helmholtz decomposition and Ladyzhenskayas decomposition theorem, we develop a projection-type, Successive-Over-Relaxation numerical method. This method is general enough and can be applied to a variety of continuum models of complex mixtures and mixtures with micro-structure. We also prove that this method is both stable and consistent hence, under standard assumptions, convergent. The paper concludes with the presentation of representative numerical results.", "keywords": ["granular mixtures", "complex fluids", "overdetermined elliptic systems", "ladyzhenskayas theorem", "successive-over-relaxation", "predictorcorrector methods"]}, {"id": "301", "title": "Analysis of sonar targets by teager-huang transform (THT)", "abstract": "In this paper, an approach for Sonar targets analysis based on a new energy-time-frequency representation, called Teager-Huang Transform (THT), is presented. The THT is the combination of the empirical mode decomposition of Huang and the Teager-Kaiser signal demodulation method. The THT is free of interferences and does not requires basis functions for signals decomposition. The analysis is carried out, in free field, from the impulse responses of Sonar targets. We compare the analysis results of impulse responses of spherical and cylindrical targets given by THT to those of the smoothed Wigner-Ville transformation.", "keywords": ["time frequency analysis", "empirical mode decomposition", "teager-kaiser energy operator", "teager huang transform ", "sonar echos."]}, {"id": "302", "title": "International Standard Development for Knowledge Based Engineering Services for Product Lifecycle Management", "abstract": "In September 2005, the international information technology standard body Object Management Group (OMG) published a Request for Proposal (RFP) for an international standard for Knowledge Based Engineering (KBE) Services for Product Lifecycle Management (PLM). The standard aims to facilitate the integration of KBE applications in a PLM environment. KBE has been used in key engineering industry to deliver significant business benefits and has been a catalyst for changes in engineering processes. In recent years, mainstream CAD vendors begin to incorporate KBE functionalities in their solutions. PLM is evolving from the platform to manage engineering data to the repository of complete enterprise knowledge. As CAD becomes more knowledge based, the convergence of KBE and PLM is expected to happen soon. The OMG standard RFP is an action to accelerate this convergence. The RFP is the result of an international effort with a team that includes engineering end users, software vendors and researchers. This paper presents the essence and the development process of the RFP to widen the engagement with the engineering research community.", "keywords": ["knowledge based engineering", "product lifecycle management", "engineering knowledge management"]}, {"id": "303", "title": "An image topic model for image denoising", "abstract": "Topic model is a powerful tool for the basic document or image processing tasks. In this study we introduce a novel image topic model, called Latent Patch Model (LPM), which is a generative Bayesian model and assumes that the image and pixels are connected by a latent patch layer. Based on the LPM, we further propose an image denoising algorithm namely multiple estimate LPM (MELPM). Unlike other works, the proposed denoising framework is totally implemented on the latent patch layer, and it is effective for both Gaussian white noises and impulse noises. Experimental results demonstrate that LPM performs well in representing images. And its application in image denoising achieves competitive PSNR and visual quality with conventional algorithms.", "keywords": ["topic model", "denoising", "patch clustering", "semantic learning"]}, {"id": "304", "title": "improving test case generation for web applications using automated interface discovery", "abstract": "With the growing complexity of web applications, identifying web interfaces that can be used for testing such applications has become increasingly challenging. Many techniques that work effectively when applied to simple web applications are insufficient when used on modern, dynamic web applications, and may ultimately result in inadequate testing of the applications' functionality. To address this issue, we present a technique for automatically discovering web application interfaces based on a novel static analysis algorithm. We also report the results of an empirical evaluation in which we compare our technique against a traditional approach. The results of the comparison show that our technique can (1) discover a higher number of interfaces and (2) help generate test inputs that achieve higher coverage.", "keywords": ["interface extraction", "help", "web-application testing", "applications", "test", "dynamic", "functional", "addressing", "web interface", "static analysis", "test case generation", "interfaces", "discoveries", "web application", "complexity", "algorithm", "coverage", "comparisons", "empirical evaluation", "automation"]}, {"id": "305", "title": "Numerical study of stream-function formulation governing flows in multiply-connected domains by integrated RBFs and Cartesian grids", "abstract": "This paper describes a new numerical procedure, based on point collocation, integrated multiquadric functions and Cartesian grids, for the discretisation of the stream-function formulation for flows of a Newtonian fluid in multiply-connected domains. Three particular issues, namely (i) the derivation of the stream-function values on separate boundaries, (ii) the implementation of cross derivatives in irregular regions, and (iii) the treatment of double boundary conditions, are studied in the context of Cartesian grids and approximants based on integrated multiquadric functions in one dimension. Several test problems, i.e. steady flows between a rotating circular cylinder and a fixed square cylinder and also between eccentric cylinders maintained at different temperatures, are investigated. Results obtained are compared well with numerical data available in the literature.  ", "keywords": ["stream-function formulation", "multiply-connected domain", "integrated radial-basis-function network", "cartesian grid"]}, {"id": "306", "title": "Linear techniques to correct for temperature-induced spectral variation in multivariate calibration", "abstract": "The influence of external physical variation such as temperature fluctuations on near-infrared (NIR) spectra and their effect on the predictive power of calibration models such as PLS have been studied. Different methods to correct for the temperature effect by explicitly including the temperature in a calibration model have been tested. The results are compared to the implicit inclusion, which takes the temperature into account only through the calibration design. Two data sets are used, one well-designed data set measured in the laboratory and one industrial data set consisting of measurements for process samples. For both data sets, the explicit inclusion of the temperature in the calibration models did not result in an improvement of the prediction accuracy compared to implicit inclusion.  ", "keywords": ["spectral variation", "temperature", "nir spectra"]}, {"id": "307", "title": "Overlapping B+-trees: An implementation of a transaction time access method", "abstract": "A new variation of Overlapping B+ -trees is presented, which provides efficient indexing of transaction time and keys in a two dimensional key-time space. Modification operations (i.e. insertions, deletions and updates) are allowed at the current version, whereas queries are allowed to any temporal version, i.e. either in the current or in past versions. Using this structure, snapshot and range-timeslice queries can be answered optimally. However, the fundamental objective of the proposed method is to deliver efficient performance in case of a general pure-key query (i.e. 'history of a key'). The trade-off is a small increase in time cost for version operations and storage requirements.  ", "keywords": ["temporal databases", "transaction time", "access methods", "indexing", "algorithms", "time and space performance"]}, {"id": "308", "title": "Using a strategy-aligned fuzzy competitive analysis approach for market segment evaluation and selection", "abstract": "This study applies Five Forces Analysis to evaluate and select market segments for international business using a strategy-aligned fuzzy approach. An illustration segment evaluation procedure is used to demonstrate that our procedure is an effective quantification approach for integrating five forces, generic strategies and marketing information in a group decision-making process. The final decision-maker (DM) synthesizes the total crisp scores of individual alternatives by choosing judgmental coefficients ? based on individual attitude towards core business competitiveness and market risks to accommodate differences among market segments to the specific environment with a better understanding of the decision problem and individual decision-making behavior. In the illustration presented here, the final solution is then obtained by identifying the best market segment for further development and negotiation.", "keywords": ["market segment evaluation", "market segment selection", "fuzzy factor rating system", "strategy alignment", "multiple attributes decision-making"]}, {"id": "309", "title": "Using AdaBoost classifiers in a hierarchical framework for classifying surface images of marble slabs", "abstract": "In this paper, a new hierarchical classification method based on the use of various types of AdaBoost classification algorithms is proposed for automatic classification of marble slab images according to their quality. At first, features are extracted using the sum and difference histograms method and, at the second stage, different versions of the AdaBoost algorithms are used as classifiers together with those extracted features in a proposed hierarchical fashion. Performance of the proposed method is compared against performances of different types of neural network classifiers and a support vector machine (SVM) classifier. Computational results show that the proposed hierarchical structure employing AdaBoost algorithms performs superior to neural networks and the SVM classifier for classifying marble slab images in our large and diversified data set.  ", "keywords": ["classification of marble slab images", "hierarchical classification", "adaboost classification algorithms"]}, {"id": "310", "title": "On bridging the gap between stochastic integer programming and MIP solver technologies", "abstract": "Stochastic integer programs (SIPs) represent a very difficult class of optimization problems arising from the presence of both uncertainty and discreteness in planning and decision problems. Although applications of SIPs are abundant, nothing is available by way of computational software. On the other hand, commercial software packages for solving deterministic integer programs have been around for quite a few years, and more recently, a package for solving stochastic linear programs has been released. In this paper, we describe how these software tools can be integrated and exploited for the effective solution of general-purpose SIPs. We demonstrate these ideas on four problem classes from the literature and show significant computational advantages.", "keywords": ["stochastic programming", "integer programming", "branch and bound", "software"]}, {"id": "311", "title": "Historiographic mapping of knowledge domains literature", "abstract": "To better understand the topic of this colloquium, we have created a series of databases related to knowledge domains (dynamic systems [small world/Milgram], information visualization [Tufte], co-citation [Small], bibliographic coupling [Kessler], and scientometrics [Scientometrics]). I have used a software package called HistCite(TM) which generates chronological maps of subject (topical) collections resulting from searches of the ISI Web of Science(R) or ISI citation indexes (SCI, SSCI, and/or AHCI) on CD-ROM. When a marked list is created on WoS, an export file is created which contains all cited references for each source document captured. These bibliographic collections, saved as ASCII files, are processed by HistCite in order to generate chronological and other tables as well as historiographs which highlight the most-cited works in and outside the collection. HistCite also includes a module for detecting and editing errors or variations in cited references as well as a vocabulary analyzer which generates both ranked word lists and word pairs used in the collection. Ideally the system will be used to help the searcher quickly identify the most significant work on a topic and trace its year-by-year historical development. In addition to the collections mentioned above, historiographs based on collections of papers that cite the Watson-Crick 1953 classic paper identifying the helical structure of DNA were created. Both year-by-year as well as month-by-month displays of papers from 1953 to 1958 were necessary to highlight the publication activity of those years.", "keywords": ["mapping", "knowledge domains", "small world concept", "dna structure", "citation analysis", "historiography", "information visualization", "software", "histcite"]}, {"id": "312", "title": "Bot detection evasion: a case study on local-host alert correlation bot detection methods", "abstract": "Botnets have continuously evolved since their inception as a malicious entity. Attackers come up with new botnet designs that exploit the weaknesses in existing defense mechanisms and continue to evade detection. It is necessary to analyze the weaknesses of existing defense mechanisms to find out the lacunae in them. This research exposes a weakness found in an existing bot detection method (BDM) by implementing a specialized P2P botnet model and carrying out experiments on it. Weaknesses that are found and validated can be used to predict the development path of botnets, and as a result, detection and mitigation measures can be implemented in a proactive fashion. The main contribution of this work is to demonstrate the exploitation pattern of an inherent weakness in local-host alert correlation (LHAC) based methods and to assert that current LHAC implementations could allow pockets of cooperative bots to hide in an enterprise size network. This work suggests that additional monitoring capabilities must be added to current LHAC-based methods in order for them to remain a viable bot detection mechanism. ", "keywords": ["botnet", "p2p", "security", "network", "covert"]}, {"id": "313", "title": "New plating bath for electroless copper deposition on sputtered barrier layers", "abstract": "A new copper plating bath for electroless deposition directly on conductive copper-diffusion barrier layers has been developed. This plating bath can be operated at temperatures between 20 and 50C and has good stability. High temperature processing allows for increased deposition rates and decreased specific resistivity values for the deposited copper films. Electroless Cu films deposited from this bath showed a conformal step coverage in high aspect ratio trenches and, therefore, are promising as seed layers for copper electroplating. The effect of the bath composition, activation procedure and processing temperature on the plating rate and morphology of the deposited copper has been studied and is presented here.", "keywords": ["interconnection", "copper", "electroless deposition"]}, {"id": "314", "title": "An adaptive finite element procedure for fully-coupled point contact elastohydrodynamic lubrication problems", "abstract": "This paper presents an automatic locally adaptive finite element solver for the fully-coupled EHL point contact problems. The proposed algorithm uses a posteriori error estimation in the stress in order to control adaptivity in both the elasticity and lubrication domains. The implementation is based on the fact that the solution of the linear elasticity equation exhibits large variations close to the fluid domain on which the Reynolds equation is solved. Thus the local refinement in such region not only improves the accuracy of the elastic deformation solution significantly but also yield an improved accuracy in the pressure profile due to increase in the spatial resolution of fluid domain. Thus, the improved traction boundary conditions lead to even better approximation of the elastic deformation. Hence, a simple and an effective way to develop an adaptive procedure for the fully-coupled EHL problem is to apply the local refinement to the linear elasticity mesh. The proposed algorithm also seeks to improve the quality of refined meshes to ensure the best overall accuracy. It is shown that the adaptive procedure effectively refines the elements in the region(s) showing the largest local error in their solution, and reduces the overall error with optimal computational cost for a variety of EHL cases. Specifically, the computational cost of proposed adaptive algorithm is shown to be linear with respect to problem size as the number of refinement levels grows.", "keywords": ["elastohydrodynamic lubrication", "finite element method", "linear elasticity", "fully coupled approach", "adaptive h-refinement", "optimization of meshes"]}, {"id": "315", "title": "Allowing each node to communicate only once in a distributed system: shared whiteboard models", "abstract": "In this paper we study distributed algorithms on massive graphs where links represent a particular relationship between nodes (for instance, nodes may represent phone numbers and links may indicate telephone calls). Since such graphs are massive they need to be processed in a distributed way. When computing graph-theoretic properties, nodes become natural units for distributed computation. Links do not necessarily represent communication channels between the computing units and therefore do not restrict the communication flow. Our goal is to model and analyze the computational power of such distributed systems where one computing unit is assigned to each node. Communication takes place on a whiteboard where each node is allowed to write at most one message. Every node can read the contents of the whiteboard and, when activated, can write one small message based on its local knowledge. When the protocol terminates its output is computed from the final contents of the whiteboard. We describe four synchronization models for accessing the whiteboard. We show that message size and synchronization power constitute two orthogonal hierarchies for these systems. We exhibit problems that separate these models, i.e., that can be solved in one model but not in a weaker one, even with increased message size. These problems are related to maximal independent set and connectivity. We also exhibit problems that require a given message size independently of the synchronization model.", "keywords": ["distributed computing", "local computation", "graph properties", "bounded communication"]}, {"id": "316", "title": "A dual spinal cord lesion paradigm to study spinal locomotor plasticity in the cat", "abstract": "After a complete spinal cord injury (SCI) at the lowest thoracic level (T13), adult cats trained to walk on a treadmill can recover hindlimb locomotion within 23 weeks, resulting from the activity of a spinal circuitry termed the central pattern generator (CPG). The role of this spinal circuitry in the recovery of locomotion after partial SCIs, when part of descending pathways can still access the CPG, is not yet fully understood. Using a dual spinal lesion paradigm (first hemisection at T10 followed three weeks after by a complete spinalization at T13), we showed that major changes occurred in this locomotor spinal circuitry. These plastic changes at the spinal cord level could participate in the recovery of locomotion after partial SCI. This short review describes the main findings of this paradigm in adult cats.", "keywords": ["central pattern generator", "plasticity", "training", "spinal cord injury", "locomotion"]}, {"id": "317", "title": "Analysis of a sparse hypermatrix Cholesky with fixed-sized blocking", "abstract": "We present the way in which we have constructed an implementation of a sparse Cholesky factorization based on a hypermatrix data structure. This data structure is a storage scheme which produces a recursive 2D partitioning of a sparse matrix. It can be useful on some large sparse matrices. Subblocks are stored as dense matrices. Thus, efficient BLAS3 routines can be used. However, since we are dealing with sparse matrices some zeros may be stored in those dense blocks. The overhead introduced by the operations on zeros can become large and considerably degrade performance. We present the ways in which we deal with this overhead. Using matrices from different areas (Interior Point Methods of linear programming and Finite Element Methods), we evaluate our sequential in-core hypermatrix sparse Cholesky implementation. We compare its performance with several other codes and analyze the results. In spite of using a simple fixed-size partitioning of the matrix our code obtains competitive performance.", "keywords": ["sparse cholesky", "hypermatrix structure", "2d partitioning", "windows in submatrices", "small matrix library"]}, {"id": "318", "title": "Atomic precision patterning on Si: An opportunity for a digitized process", "abstract": "H depassivation lithography is a process by which a monolayer of H absorbed on a Si(100) 21 surface may be patterned by the removal of H atoms using a scanning tunneling microscope. This process can achieve atomic resolution where individual atoms are targeted and removed. This paper suggests that such a patterning process can be carried out as a digital process, where the pixels of the pattern are the individual H atoms. The goal is digital fabrication rather than digital information processing. The margins for the read and write operators appear to be sufficient for a digital process, and the tolerance for physical addressing of the atoms is technologically feasible. A digital fabrication process would enjoy some of the same advantages of digital computation; namely high reliability, error checking and correction, and the creation of complex systems.", "keywords": ["lithography", "scanning tunneling microscope", "si", "hydrogen depassivation", "digital process"]}, {"id": "319", "title": "Automated Worst-Case Execution Time Analysis Based on Program Modes", "abstract": "A program mode is a regular trajectory of the execution of a program that is determined by the values of its input variables. By exploiting program modes, we may make worst-case execution time (WCET) analysis more precise. This paper presents a novel method to automatically find program modes and calculate the WCET estimates of programs. First, the modes of a program will be identified automatically by mode-relevant program slicing, and the precondition will be calculated for each mode using a path-wise test data generation method. Then, for each feasible mode, we show how to calculate its WCET estimate for modern reduced instruction set computer (RISC) processors with caches and pipelines and for traditional complex instruction set computer (CISC) processors. We also present a method to obtain the symbolic expression for each mode for CISC processors. The experimental results show the effectiveness of the method.", "keywords": ["real-time systems", "wcet analysis", "program mode", "program slicing", "iterative relaxation method"]}, {"id": "320", "title": "Non-agricultural databases and thesauri Retrieval of subject headings and non-controlled terms in relation to agriculture", "abstract": "Purpose - The paper aims to assess the utility of non-agriculture-specific information systems, databases, and respective controlled vocabularies (thesauri) in organising and retrieving agricultural information. The purpose is to identify thesaurus-linked tree structures, controlled subject headings/terms (heading words, descriptors), and principal database-dependent characteristics and assess how controlled terms improve retrieval results (recall) in relation to free-text/uncontrolled terms in abstracts and document titles. Design/methodology/approach - Several different hosts (interfaces, platforms, portals) and databases were used: CSA Illumina.(ERIC, LISA), Ebscohost (Academic Search Complete, Medline, Political Science Complete), Ei-Engineering Village (Compendex, Inspec), OVID (PsycINFO), ProQuest (ABI/Inform Global). The search-terms agriculture and agricultural and truncated word-stem agricultur- were employed. Permuted (rotated index) search fields were used to retrieve terms from thesauri. Subject-heading search was assessed in relation to free-text search, based on abstracts and document titles. Findings - All thesauri contain agriculture-based headings; however, associative, hierarchical and synonymous relationships show important inter-database differences. Using subject headings along with abstracts and titles in search syntax (query) sometimes improves retrieval by up to 60 per cent. Retrieval depends on search fields and database-specifics, such as autostemming (lemmatization), explode function, word-indexing, or phrase-indexing. Research limitations/implications - Inter-database and host comparison, on consistent principles, can be limited because of some particular host- and database-specifics. Practical implications - End-users may exploit databases more competently and thus achieve better retrieval results in searching for agriculture-related information. Originality/value - The function of as many as ten databases in different disciplines in providing information relevant to subject matter that is not a topical focus of databases is assessed.", "keywords": ["thesauri", "controlled vocabularies", "indexing", "subject headings", "databases", "agriculture"]}, {"id": "321", "title": "Efficient Packet Classification with a Hybrid Algorithm", "abstract": "Packet classification categorizes incoming packets into multiple forwarding classes based on pre-defined filters. This categorization makes information accessible for quality of service or security handling in the network. In this paper, we propose a scheme which combines the Aggregate Bit Vector algorithm and the pruned Tuple Space Search algorithm to improve the performance of packet classification in terms of speed and storage. We also present the procedures of incremental update. Our scheme is evaluated with filter databases of varying sizes and characteristics. The experimental results demonstrate that our scheme is feasible and scalable.", "keywords": ["packet classification", "network intrusion defection systems", "firewalls", "qos", "packet forwarding"]}, {"id": "322", "title": "The Intra-Americas Sea Low-level Jet", "abstract": "A relevant climate feature of the Intra-Americas Sea (IAS) is the low-level jet (IALLJ) dominating the IAS circulation, both in summer and winter; and yet it is practically unknown with regard to its nature, structure, interactions with mid-latitude and tropical phenomena, and its role in regional weather and climate. This paper updates IALLJ current knowledge and its contribution to IAS circulationprecipitation patterns and presents recent findings about the IALLJ based on first in situ observations during Phase 3 of the Experimento Climtico en las Albercas de Agua Clida (ECAC), an international field campaign to study IALLJ dynamics during July 2001. Nonhydrostatic fifth-generation Pennsylvania State University National Center for Atmospheric Research Mesoscale Model (MM5) simulations were compared with observations and reanalysis. Large-scale circulation patterns of the IALLJ northern hemisphere summer and winter components suggest that trades, and so the IALLJ, are responding to landocean thermal contrasts during the summer season of each continent. The IALLJ is a natural component of the American monsoons as a result of the continent's approximate northsouth land distribution. During warm (cold) El NioSouthern Oscillation phases, winds associated with the IALLJ core (IALLJC) are stronger (weaker) than normal, so precipitation anomalies are positive (negative) in the western Caribbean near Central America and negative (positive) in the central IAS. During the ECAC Phase 3, strong surface winds associated with the IALLJ induced upwelling, cooling down the sea surface temperature by 12 C. The atmospheric mixed layer height reached 1 km near the surface wind maximum below the IALLJC. Observations indicate that primary water vapor advection takes place in a shallow layer between the IALLJC and the ocean surface. Latent heat flux peaked below the IALLJC. Neither the reanalysis nor MM5 captured the observed thermodynamic and kinematic IALLJ structure. So far, IALLJ knowledge is based on either dynamically initialized data or simulations of global (regional) models, which implies that a more systematic and scientific approach is needed to improve it. The Intra-Americas Study of Climate Processes is a great regional opportunity to address trough field work, modeling, and process studies, many of the IALLJ unknown features.", "keywords": ["intra-americas low-level jet", "tropical climate variability", "mm5 modeling", "el niosouthern oscillation", "enso"]}, {"id": "323", "title": "Operational knowledge representation for practical decision-making", "abstract": "For the design of an \"intelligent\" assistant system aimed at supporting operators' decision in subway control, we modeled operators' activity and know-how. As a result, we introduce the notion of a contextual graph, which appears as a simple solution to describe and manage operational decision-making.", "keywords": ["context representation", "contextual graphs", "decision tree", "knowledge representation", "operational knowledge"]}, {"id": "324", "title": "ORGANIZATIONAL STRUCTURE-SATISFACTORY SOCIAL LAW DETERMINATION IN MULTIAGENT WORKFLOW SYSTEMS", "abstract": "The multiagent workflow systems can be formalized from an organizational structure viewpoint, which includes three parts: the interaction structure among agents, the temporal flow of activities, and the critical resource sharing relations among activities. While agents execute activities, they should decide their strategies to satisfy the constraints brought by the organizational structure of multiagent workflow system. To avoid collisions in the multiagent workflow system, this paper presents a method to determine social laws in the system to restrict the strategies of agents and activities; the determined social laws can satisfy the characteristics of organization structures so as to minimize the conflicts among agents and activities. Moreover, we also deal with the social law adjustment mechanism for the alternations of interaction relations, temporal flows, and critical resource sharing relations. It is proved that our model can produce useful social laws for organizational structure of multiagent workflow systems, i.e., the conflicts brought by the constraints of organization structure can be minimized.", "keywords": ["multiagents", ".workflows", "coordination", "social laws", "social strategies", "organizational structures"]}, {"id": "325", "title": "CONTENT-AWARE RETARGETING FOR SOCCER VIDEO ADAPTATION", "abstract": "A content-aware retargeting method is proposed for adapting soccer video to heterogeneous terminals. According to domain-specific knowledge, ball, player and player's face are defined as user interested objects (UIOs) in different view-types. The UIOs are extracted by semantic analysis on soccer video, and then a region of interest (ROI) of each shot is determined jointly by three factors: terminal size, scaling factor and aspect ratio. The proposed method optimizes the retargeted region to contain more semantic content while adapting the constraint of terminal screen. The simulation results prove that the proposed CAR system wins better viewing experiences than the traditional methods such as resizing in a \"Letter box\" mechanism or cropping directly.", "keywords": ["video retargarting", "user interested object", "region of interest", "video analysis", "view-type"]}, {"id": "326", "title": "Ribosome kinetics and aa-tRNA competition determine rate and fidelity of peptide synthesis", "abstract": "It is generally accepted that the translation rate depends on the availability of cognate aa-tRNAs. In this study it is shown that the key factor that determines translation rate is the competition between near-cognate and cognate aa-tRNAs. The transport mechanism in the cytoplasm is diffusion, thus the competition between cognate, near-cognate and non-cognate aa-tRNAs to bind to the ribosome is a stochastic process. Two competition measures are introduced; C(i) and R(i) (i=1, 64) are quotients of the arrival frequencies of near-cognates vs. cognates and non-cognates vs. cognates, respectively. Furthermore, the reaction rates of bound cognates differ from those of bound near-cognates. If a near-cognate aa-tRNA binds to the A site of the ribosome, it may be rejected at the anti-codon recognition step or proofreading step or it may be accepted. Regardless of its fate, the near-cognates and non-cognates have caused delays of varying duration to the observed rate of translation. Rate constants have been measured at a temperature of 20C by (Gromadski, K.B., Rodnina, M.V., 2004. Kinetic determinants of high-fidelity tRNA discrimination on the ribosome. Mol. Cell 13, 191200). These rate constants have been re-evaluated at 37C, using experimental data at 24.5C and 37C (Varenne, S., et al., 1984. Translation in a non-uniform process: effect of tRNA availability on the rate of elongation of nascent polypeptide chains. J. Mol. Biol. 180, 549576). The key results of the study are: (i) the average time (at 37C) to add an amino acid, as defined by the ith codon, to the nascent peptide chain is: ?(i)=9.06+1.445[10.48C(i)+0.5R(i)] (in ms); (ii) the misreading frequency is directly proportional to the near-cognate competition, E(i)=0.0009C(i); (iii) the competition from near-cognates, and not the availability of cognate aa-tRNAs, is the most important factor that determines the translation rate  the four codons with highest near-cognate competition (in the case of E. coli) are [GCC]>[CGG]>[AGG]>[GGA], which overlap only partially with the rarest codons: [AGG]<[CCA]<[GCC]<[CAC]; (iv) based on the kinetic rates at 37C, the average time to insert a cognate amino acid is 9.06ms and the average delay to process a near-cognate aa-tRNA is 10.45ms and (vii) the model also provides estimates of the vacancy times of the A site of the ribosome  an important factor in frameshifting.", "keywords": ["ribosome kinetics", "translation", "trna availability", "mistranslation frequencies"]}, {"id": "327", "title": "An efficient and spectrally accurate numerical method for computing dynamics of rotating Bose-Einstein condensates", "abstract": "In this paper, we propose an efficient and spectrally accurate numerical method for computing the dynamics of rotating Bose-Einstein condensates (BEC) in two dimensions (2D) and 3D based on the Gross-Pitaevskii equation (GPE) with an angular momentum rotation term. By applying a time-splitting technique for decoupling the nonlinearity and properly using the alternating direction implicit (ADI) technique for the coupling in the angular momentum rotation term in the GPE, at every time step, the GPE in rotational frame is decoupled into a nonlinear ordinary differential equation (ODE) and two partial differential equations with constant coefficients. This allows us to develop new time-splitting spectral methods for computing the dynamics of BEC in a rotational frame. The new numerical method is explicit, unconditionally stable, and of spectral accuracy in space and second-order accuracy in time. Moreover, it is time reversible and time transverse invariant, and conserves the position density in the discretized level if the GPE does. Extensive numerical results are presented to confirm the above properties of the new numerical method for rotating BEC in 2D and 3D.  ", "keywords": ["rotating bose-einstein condensates", "gross-pitaevskii equation", "angular momentum rotation", "time spitting"]}, {"id": "328", "title": "The entropy of traces in parallel computation", "abstract": "The following problem arises in thtr context of parallel computation: how many bits of information are required to specify any one element from an arbitrary (nonempty) X-subset of a set? We characterize optimal coding techniques for this problem. We calculate the asymptotic behavior of the amount of information necessary, and construct an algorithm that specifies an element from a subset in an optimal manner.", "keywords": ["coding", "energy", "parallel computation", "traces"]}, {"id": "329", "title": "Proteomic Technologies to Study Diseases of the Lymphatic Vascular System", "abstract": "Now that the human genome has been mapped, a new challenge has emerged", "keywords": ["proteomics", "cancer", "lymph", "lymphatic endothelium", "lymphedema", "laser capture microdissection", "protein microarrays", "seldi-tof-mass spectroscopy"]}, {"id": "330", "title": "leveraging eclipse for integrated model-based engineering of web service compositions", "abstract": "In this paper we detail the design and implementation of an Eclipse plug-in for an integrated, model-based approach, to the engineering of web service compositions. The plug-in allows a designer to specify a service's obligations for coordinated web service compositions in the form of Message Sequence Charts (MSCs) and then generate policies in the form of WS-CDL and services in the form of BPEL4WS. The approach uses finite state machine representations of web service compositions and service choreography rules, and assigns semantics to the distributed process interactions. The move towards implementing web service choreography requires design time verification of these service interactions to ensure that service implementations fulfill requirements for multiple interested partners before such compositions and choreographies are deployed. The plug-in provides a tool for integrated specification, formal modeling, animation and providing verification results from choreographed web service interactions. The LTSA-Eclipse (for Web Services) plug-in is publicly available, along with other plug-ins, at: http://www.doc.ic.ac.uk/ltsa.", "keywords": ["eclipse plug-in", "verification", "service design", "web service choreography", "standards", "validation", "implementation", "model checking"]}, {"id": "331", "title": "Targeted Recruitment of Histone Modifications in Humans Predicted by Genomic Sequences", "abstract": "Histone modifications are important epigenetic regulators and play a critical role in development. The targeting mechanism for histone modifications is complex and still incompletely understood. Here we applied a computational approach to predict genome-scale histone modification targets in humans by the genomic DNA sequences using a set of recent ChIP-seq data. We found that a number of histone modification marks could be predicted with high accuracy. On the other hand, the impact of DNA sequences for each mark is intrinsically different dependent upon the target-and tissue-specificity. Diverse patterns are associated with different repetitive elements. Unexpectedly, we found that non-overlapping, functionally opposite histone modification marks could share similar sequence features. We propose that these marks may target a common set of loci but are mutually exclusive and that the competition may be important for developmental control. Taken together, we show that our computational approach has provided new insights into the targeting mechanism of histone modifications.", "keywords": ["dna sequence", "histone modification", "human"]}, {"id": "332", "title": "Normal vector voting: Crease detection and curvature estimation on large, noisy meshes", "abstract": "This paper describes a robust method for crease detection and curvature estimation on large, noisy triangle meshes. We assume that these meshes are approximations of piecewise-smooth surfaces derived from range or medical imaging systems and thus may exhibit measurement or even registration noise. The proposed algorithm, which we call normal vector voting, uses an ensemble of triangles in the geodesic neighborhood of a vertex-instead of its simple umbrella neighborhood-to estimate the orientation and curvature of the original surface at that point. With the orientation information, we designate a vertex as either lying on a smooth surface, following a crease discontinuity, or having no preferred orientation. For vertices on a smooth surface, the curvature estimation yields both principal curvatures and principal directions while for vertices on a discontinuity we estimate only the curvature along the crease. The last case for no preferred orientation occurs when three or more surfaces meet to form a corner or when surface noise is too large and sampling density is insufficient to determine orientation accurately. To demonstrate the capabilities of the method, we present results for both synthetic and real data and compare these results to the G. Taubin (1995, in Proceedings of the Fifth International Conference on Computer Vision, pp. 902-907) algorithm. Additionally, we show practical results for several large mesh data sets that are the motivation for this algorithm. ", "keywords": ["curvature estimation", "normal vector estimation", "crease detection", "dense triangle meshes", "piecewise-smooth surfaces"]}, {"id": "333", "title": "acoustic super models for large scale video event detection", "abstract": "Given the exponential growth of videos published on the Internet, mechanisms for clustering, searching, and browsing large numbers of videos have become a major research area. More importantly, there is a demand for event detectors that go beyond the simple finding of objects but rather detect more abstract concepts, such as \"feeding an animal\" or a \"wedding ceremony\". This article presents an approach for event classification that enables searching for arbitrary events, including more abstract concepts, in found video collections based on the analysis of the audio track. The approach does not rely on speech processing, and is language-indepent, instead it generates models for a set of example query videos using a mixture of two types of audio features: Linear-Frequency Cepstral Coefficients and Modulation Spectrogram Features. This approach can be used in complement with video analysis and requires no domain specific tagging. Application of the approach to the TRECVid MED 2011 development set, which consists of more than 4000 random \"wild\" videos from the Internet, has shown a detection accuracy of 64% including those videos which do not contain an audio track.", "keywords": ["multimedia event detection", "trecvid", "audio processing"]}, {"id": "334", "title": "Morphological influence of the beam overlap in focused ion beam induced deposition using raster scan", "abstract": "Material addition using focused ion beam induced deposition (FIBID) is a well-established local deposition technology in microelectronic engineering. We investigated FIBID characteristics as a function of beam overlap using phenanthrene molecules. To initiate the localization of gas molecules, we irradiated the ion beams using a raster scan. We varied the beam overlap between ?900% and 50% by adjusting the pixel size from 300nm to 15nm. We discuss the changes in surface morphologies and deposition rates due to delocalization by the range effect of excited surface atoms, the divided structure by continuous effect from the raster scan, enhanced localization by discrete effect from replenished gas molecules, the competition between deposition and sputtering processes, and the change in processing time with scan speed (smaller overlap case).", "keywords": ["fibid", "raster scan", "phenanthrene gas", "carbon", "beam overlap"]}, {"id": "335", "title": "Content-aware rate allocation for efficient video streaming via dynamic network utility maximization", "abstract": "Nowadays it is vital to design robust mechanisms to provide QoS for multimedia applications as an integral part of the network traffic. The main goal of this paper is to provide an efficient rate control scheme to support content-aware video transmission mechanism with buffer underflow avoidance at the receiver in congested networks. Towards this, we introduce a content-aware time-varying utility function, in which the quality impact of video content is incorporated into its mathematical expression. Moreover, we analytically model the buffer requirements of video sources in two ways: first as constraints of the optimization problem to guarantee a minimum rate demand for each source, and second as a penalty function embedded as part of the objective function attempting to achieve the highest possible rate for each source. Then, using the proposed analytical model, we formulate a dynamic network utility maximization problem, which aims to maximize the aggregate hybrid objective function of sources subject to capacity and buffer constraints. Finally, using primaldual method, we solve DNUM problem and propose a distributed algorithm called CA-DNUM that optimally allocates the shared bandwidth to video streams. The experimental results demonstrate the efficacy and performance improvement of the proposed content-aware rate allocation algorithm for video sources in different scenarios.", "keywords": ["video streaming", "dynamic network utility maximization", "content-aware video utility model", "buffer underflow avoidance", "convex optimization"]}, {"id": "336", "title": "INVERSE PROBLEM FOR FRICATIVES", "abstract": "Articulatory parameters, vocal tract shape and cross-sectional area function were determined from fricative spectra. A model of fricative generation was used for providing acoustical constraints for an optimization procedure with muscles work as the criterion of optimality. A distance between spectra was measured with the use of the Cauchy-Bounjakovsky non-equality. A proper initial approximation of articulatory parameters is required to obtain an accurate and stable solution of the inverse problem.", "keywords": ["speech", "inverse problem", "vocal tract shape", "fricatives", "optimization"]}, {"id": "337", "title": "efficient web usage mining process for sequential patterns", "abstract": "The tremendous growth in volume of web usage data results in the boost of web mining research with focus on discovering potentially useful knowledge from web usage data. This paper presents a new web usage mining process for finding sequential patterns in web usage data which can be used for predicting the possible next move in browsing sessions for web personalization. This process consists of three main stages: preprocessing web access sequences from the web server log, mining preprocessed web log access sequences by a tree-based algorithm, and predicting web access sequences by using a dynamic clustering-based model. It is designed based on the integration of the dynamic clustering-based Markov model with the Pre-Order Linked WAP-Tree Mining (PLWAP) algorithm to enhance mining performance. The proposed mining process is verified by experiments with promising results.", "keywords": ["sequential patterns", "web usage mining ", "pre-order linked wap-tree ", "markov model", "web access patterns "]}, {"id": "338", "title": "experimenting with an organic metaphor and hypervisual links for the interface of a video collection", "abstract": "In this paper we describe the prototype of an archive of short movies. The project proposes two original solutions for implementing the interface of this archive: an organic metaphor and a hypervisual navigation mechanism.", "keywords": ["user interfaces", "metaphors", "hyperlinks", "hypervideo"]}, {"id": "339", "title": "A Randomized Exhaustive Propositionalization Approach for Molecule Classification", "abstract": "Drug discovery is the process of designing compounds that have desirable properties, such as activity and nontoxicity. Molecule classification techniques are used along with this process to predict the properties of the compounds to expedite their testing. Ideally, the classification rules found should be accurate and reveal novel chemical properties, but current molecule representation techniques lead to less-than-adequate accuracy and knowledge discovery. This work extends the propositionalization approach recently proposed for multirelational data mining in two ways: it generates expressive attributes exhaustively, and it uses randomization to sample a limited set of complex (\"deep\") attributes. Our experimental tests show that the procedure is able to generate meaningful and interpretable attributes from molecular structural data, and that these features are effective for classification purposes.", "keywords": ["relational learning", "propositionalization", "molecule classification", "drug discovery"]}, {"id": "340", "title": "Error propagation analysis for underwater cooperative multi-hop communications", "abstract": "The potential gains of cooperative Communication and multi-hopping in underwater acoustic communication channels is examined. In particular, performance of such systems is compared to a comparable single hop system (direct transmission) with a common transmission distance. The effects of error propagation with decode and forward at each relay are explicitly treated and it is shown that strong gains can be achieved by multi-hopping (an effective SNR gain) is well as cooperation, which contributes to a diversity gain. We observe that cooperative diversity gains are retained even when considering error propagation. The analysis is done via a Markov chain analysis for both regular linear and grid networks. Our initial analysis is for single path channels; the effects of inter symbol interference as well as multi.-user interference are examined. It is found that due to the strong decay of signal power as a function of transmission distance, multi-user interference is not as significant as inter-symbol interference. In both cases, cooperative and multi-hopping gains are observed. ", "keywords": ["underwater acoustic communications", "cooperative communications", "multi-hopped networks", "error propagation analysis", "fading multipath channels", "diversity", "sensor networks"]}, {"id": "341", "title": "evolutionary algorithms for reasoning in fuzzy description logics with fuzzy quantifiers", "abstract": "The task of reasoning with fuzzy description logics with fuzzy quantification is approached by means of an evolutionary algorithm. An essential ingredient of the proposed method is a heuristic, implemented as an intelligent mutation operator, which observes the evolutionary process and uses the information gathered to guess at the mutations most likely to bring about an improvement of the solutions. The viability of the method is demonstrated by applying it to reasoning on a resource sheduling problem.", "keywords": ["fuzzy quantification", "fuzzy logic", "evolutionary algorithms", "description logics"]}, {"id": "342", "title": "Deadline-based scheduling in support of real-time data delivery", "abstract": "The use of deadline-based scheduling in support of real-time delivery of application data units (ADUs) in a packet-switched network is investigated. Of interest is priority scheduling where a packet with a smaller ratio of T/H (time until delivery deadline over number of hops remaining) is given a higher priority. We refer to this scheduling algorithm as the T/H algorithm. T/H has time complexity of O(logN) for a backlog of N packets and was shown to achieve good performance in terms of the percentage of ADUs that are delivered on-time. We develop a new and efficient algorithm, called T/H?p, that has O(1) time complexity. The performance difference of T/H, T/H?p and FCFS are evaluated by simulation. Implementations of T/H and T/H?p in high-speed routers are also discussed. We show through simulation that T/H?p is superior to FCFS but not as good as T/H. In view of the constant time complexity, T/H?p is a good candidate for high-speed routers when both performance and implementation cost are taken into consideration.", "keywords": ["real-time data delivery", "deadline-based scheduling", "packet-switched networks", "performance evaluation"]}, {"id": "343", "title": "Space hierarchy theorem revised", "abstract": "We show that, for an arbitrary function h(n) and each recursive function e(n), that are separated by a nondeterministically fully space constructible g(n), such that h(n) E Q(g(n)) but l(n) Q(g(n)), there exists a unary language L in NSPACE(h(n)) that is not contained in NSPACE(l(n)). The same holds for the deterministic case. The main contribution to the well-known Space Hierarchy Theorem is that (i) the language L separating the two space classes is unary (tally), (ii) the hierarchy is independent of whether h(n) or l(n) are in Omega(log n) or in o(log n), (iii) the functions h(n) or l(n) themselves need not be space constructible nor monotone increasing, (iv) the hierarchy is established both for strong and weak space complexity classes. This allows us to present unary languages in such complexity classes as, for example, NSPACE(log log n (.) log* n)  NSPACE(log log n), using a plain diagonalization.  ", "keywords": ["computational complexity", "space complexity"]}, {"id": "344", "title": "Non-dominated sorting genetic algorithm with decomposition to solve constrained optimisation problems", "abstract": "Pareto-domination was adopted to handle not only trade-off between objective and constraints but also trade-off between convergence and diversity on solving a constrained optimisation problem (COP) in this paper like many other researchers. But there are some differences. This paper converts a COP into an equivalent dynamic constrained multi-objective optimisation problem (DCMOP) first, then dynamic version of non-dominated sorting genetic algorithm with decomposition (NSGA/D) is designed to solve the equivalent DCMOP, consequently solve the COP. A key issue for the NSGA/D working effectively is that the environmental change should not destroy the feasibility of the population. With a feasible population, the NSGA/D could solve well the DCMOP just as a MOEA usually can solve well an unconstrained MOP. Experimental results show that the NSGA/D outperforms or performs similarly to other state-of-the-art algorithms referred to in this paper, especially in global search.", "keywords": ["evolutionary algorithm", "constrained optimisation", "multi-objective optimisation", "dynamic multi-objective optimisation", "dynamic optimisation"]}, {"id": "345", "title": "selecting for evolvable representations", "abstract": "Evolutionary algorithms tend to produce solutions that are not evolvable: Although current fitness may be high, further search is impeded as the effects of mutation and crossover become increasingly detrimental. In nature, in addition to having high fitness, organisms have evolvable genomes: phenotypic variation resulting from random mutation is structured and robust. Evolvability is important because it allows the population to produce meaningful variation, leading to efficient search. However, because evolvability does not improve immediate fitness, it must be selected for indirectly. One way to establish such a selection pressure is to change the fitness function systematically. Under such conditions, evolvability emerges only if the representation allows manipulating how genotypic variation maps onto phenotypic variation and if such manipulations lead to detectable changes in fitness. This research forms a framework for understanding how fitness function and representation interact to produce evolvability. Ultimately evolvable encodings may lead to evolutionary algorithms that exhibit the structured complexity and robustness found in nature.", "keywords": ["modularity", "estimation-of-distribution", "representations", "genetic algorithms", "indirect encodings", "evolvability", "development"]}, {"id": "346", "title": "Modeling biocomplexity  actors, landscapes and alternative futures", "abstract": "Increasingly, models (and modelers) are being asked to address the interactions between human influences, ecological processes, and landscape dynamics that impact many diverse aspects of managing complex coupled human and natural systems. These systems may be profoundly influenced by human decisions at multiple spatial and temporal scales, and the limitations of traditional process-level ecosystems modeling approaches for representing the richness of factors shaping landscape dynamics in these coupled systems has resulted in the need for new analysis approaches. New tools in the areas of spatial data management and analysis, multicriteria decision-making, individual-based modeling, and complexity science have all begun to impact how we approach modeling these systems. The term biocomplexity has emerged as a descriptor of the rich patterns of interactions and behaviors in human and natural systems, and the challenges of analyzing biocomplex behavior is resulting in a convergence of approaches leading to new ways of understanding these systems. Important questions related to system vulnerability and resilience, adaptation, feedback processing, cycling, non-linearities and other complex behaviors are being addressed using models employing new representational approaches to analysis. The complexity inherent in these systems challenges the modeling community to provide tools that capture sufficiently the richness of human and ecosystem processes and interactions in ways that are computationally tractable and understandable. We examine one such tool, EvoLand, which uses an actor-based approach to conduct alternative futures analyses in the Willamette Basin, Oregon.", "keywords": ["complexity", "resilience", "adaptation", "simulation"]}, {"id": "347", "title": "Computerised video tracking, movement analysis and behaviour recognition in insects", "abstract": "The need for automating behavioural observations and the evolution of systems developed for that purpose are outlined. Automatic video tracking systems enable behaviour to be studied in a reliable and consistent way, and over longer time periods than if it is manually recorded. To overcome limitations of currently available systems and to meet researchers' needs as these have been identified, we have developed an integrated system (EthoVision) for automatic recording of activity, movement and interactions of insects. The system is described here, with special emphasis on file management, experiment design, arena and zone definition, object detection, experiment control, visualisation of tracks and calculation of analysis parameters. A review of studies using our system is presented, to demonstrate its use in a variety of entomological applications. This includes research on beetles, fruit flies, soil insects, parasitic wasps, predatory mites, ticks, and spiders. Finally, possible future directions for development are discussed.", "keywords": ["video tracking", "movement analysis", "behaviour recognition", "ethovision"]}, {"id": "348", "title": "A novel similarity measure on intuitionistic fuzzy sets with its applications", "abstract": "The intuitionistic fuzzy set, as a generation of Zadeh fuzzy set, can express and process uncertainty much better, by introducing hesitation degree. Similarity measures between intuitionistic fuzzy sets (IFSs) are used to indicate the similarity degree between the information carried by IFSs. Although several similarity measures for intuitionistic fuzzy sets have been proposed in previous studies, some of those cannot satisfy the axioms of similarity, or provide counter-intuitive cases. In this paper, we first review several widely used similarity measures and then propose new similarity measures. As the consistency of two IFSs, the proposed similarity measure is defined by the direct operation on the membership function, non-membership function, hesitation function and the upper bound of membership function of two IFS, rather than based on the distance measure or the relationship of membership and non-membership functions. It proves that the proposed similarity measures satisfy the properties of the axiomatic definition for similarity measures. Comparison between the previous similarity measures and the proposed similarity measure indicates that the proposed similarity measure does not provide any counter-intuitive cases. Moreover, it is demonstrated that the proposed similarity measure is capable of discriminating the difference between patterns.", "keywords": ["intuitionistic fuzzy set", "distance measure", "similarity measure", "pattern recognition"]}, {"id": "349", "title": "An object-oriented simulation framework for real-time control of automated flexible manufacturing systems", "abstract": "This paper describes an object-oriented simulation approach for the design of a flexable, manufacturing system that allows the implementation of control logic during the system design phase. The object-oriented design approach is built around the formal theory of supervisory control based on Finite Automata. The formalism is used to capture inter-object relationships that are difficult to identify in the object-oriented design approach. The system resources are modeled as object classes based on the events that have to be monitored for real-time control. Real-time control issues including deadlock resolution, resource failures in various modes of operation and recovery from failures while sustaining desirable logical system properties are integrated into the logical design for simulating the supervisory controller.  ", "keywords": ["object-oriented simulation", "flexible manufacturing systems", "real-time control"]}, {"id": "350", "title": "A fully distributed architecture for large scale workflow enactment", "abstract": "Standard client-server workflow management systems are usually designed as client-server systems. The central server is responsible for the coordination of the workflow execution and, in some cases, may manage the activities database. This centralized control architecture may represent a single point of failure, which compromises the availability of the system. We propose a fully distributed and configurable architecture for workflow management systems. It is based on the idea that the activities of a case (an instance of the process) migrate from host to host, executing the workflow tasks, following a process plan. This core architecture is improved with the addition of other distributed components so that other requirements for Workflow Management Systems, besides scalability, are also addressed. The components of the architecture were tested in different distributed and centralized configurations. The ability to configure the location of components and the use of dynamic allocation of tasks were effective for the implementation of load balancing policies.", "keywords": ["large-scale workflow management systems", "fully distributed workflow architectures", "corba workflow implementation", "and mobile agents"]}, {"id": "351", "title": "Finite-difference model for one-dimensional electro-osmotic consolidation", "abstract": "Small strain consolidation theories treat soil properties as being constant and uniform in the course of consolidation, which is not true in the case of electro-osmosis-induced consolidation practices. Electro-osmotic consolidation leads to large strain, which physically and electro-chemically affects to a non-negligible extent the nonlinear changes of the soil properties. For the nonlinear changes, iterative computations provide a mathematical approximation of the soil consolidation when the time steps and spatial geometry are intensively meshed. In this context, this paper presents a finite-difference model, EC1, for one-dimensional electro-osmotic consolidation, and this model is developed based on a fixed Eulerian co-ordinate system and uses a piecewise linear approximation. The model is able to account for the large-strain-induced nonlinear changes of the physical and electro-chemical properties in a compressible mass subjected to electro-osmotic consolidation and to predict the consolidation characteristics of the compressible mass. EC1 is verified against exact analytical solutions and test results obtained from an experimental program. Example problems are illustrated with respect to the numerical solutions of large-strain electro-osmotic consolidation.", "keywords": ["electro-osmosis", "consolidation", "large strain", "nonlinear", "electrical potential", "pore pressure"]}, {"id": "352", "title": "Nonlinear characteristics of on-chip spiral inductors under high RF power", "abstract": "This paper explores silicon CMOS on-chip spiral inductors performance degradation under high RF power. A novel methodology to calibrate and characterize on-chip spiral inductor with large signal inputs (high/medium power) is presented. Experiments showed 12% degradation of quality factor in a particular inductor design when 34dBm RF power was applied. The degradation of quality factor of inductor can be attributed to a local self heating effect. Thermal imaging of such an inductor under high RF power validates the hypothesis.", "keywords": ["high rf power", "on-chip inductor", "quality factor"]}, {"id": "353", "title": "On d-Multiplicative Secret Sharing", "abstract": "A multiplicative secret sharing scheme allows players to multiply two secret-shared field elements by locally converting their shares of the two secrets into an additive sharing of their product. Multiplicative secret sharing serves as a central building block in protocols for secure multiparty computation (MPC). Motivated by open problems in the area of MPC, we introduce the more general notion of d-multiplicative secret sharing, allowing to locally multiply d shared secrets, and study the type of access structures for which such secret sharing schemes exist. While it is easy to show that d-multiplicative schemes exist if no d unauthorized sets of players cover the whole set of players, the converse direction is less obvious for da parts per thousand yen3. Our main result is a proof of this converse direction, namely that d-multiplicative schemes do not exist if the set of players is covered by d unauthorized sets. In particular, t-private d-multiplicative secret sharing among k players is possible if and only if k > dt. Our negative result holds for arbitrary (possibly inefficient or even nonlinear) secret sharing schemes and implies a limitation on the usefulness of secret sharing in the context of MPC. Its proof relies on a quantitative argument inspired by communication complexity lower bounds.", "keywords": ["secret sharing", "secure multiparty computation", "secure multiplication"]}, {"id": "354", "title": "shortest path amidst disc obstacles is computable", "abstract": "An open question in Exact Geometric Computation is whether there re transcendental computations that can be made \"geometrically exact\".Perhaps the simplest such problem in computational geometry is that of computing the shortest obstacle-avoiding path between two points p, q in the plane, where the obstacles re collection of n discs.This problem can be solved in O (n 2 log n)time in the Real RAM model, but nothing was known about its computability in the standard (Turing) model of computation. We first show the Turing-computability of this problem,provided the radii of the discs are rationally related. We make the usual assumption that the numerical input data are real algebraic numbers. By appealing to effective bounds from transcendental number theory, we further show single-exponential time upper bound when the input numbers are rational.Our result ppears to be the first example of non-algebraic combinatorial problem which is shown computable. It is also rare example of transcendental number theory yielding positive computational results.", "keywords": ["guaranteed precision computation", "exponential complexity", "real ram model", "exact geometric computation", "robust numerical algorithms", "disc obstacles", "shortest path"]}, {"id": "355", "title": "A uniform approach to constraint-solving for lists, multisets, compact lists, and sets", "abstract": "Lists, multisets, and sets are well-known data structures whose usefulness is widely recognized in various areas of computer science. They have been analyzed from an axiomatic point of view with a parametric approach in Dovier et al. [1998], where the relevant unification algorithms have been developed. In this article, we extend these results considering more general constraints, namely, equality and membership constraints and their negative counterparts.", "keywords": ["theory", "algorithms", "membership and equality constraints", "lists", "multisets", "compact lists", "sets"]}, {"id": "356", "title": "a competency framework for the stakeholders of a software process improvement initiative", "abstract": "The competencies (a set of specific knowledges, skills, attitudes and behaviors; e.g. stress handling, commitment, collaboration and identification of conflicts) of the employees of software organizations are a fundamental element for the success of a Software Process Improvement (SPI) initiative. We performed three case studies to identify the competencies required for the stakeholders in an SPI initiative. To identify these competencies, we observed the activities that each stakeholder performs and the interactions among them. We also identified the competencies that are required to perform those activities. We performed a classification of the identified competencies and integrated them into a framework. This framework defines the competencies for seven roles involved in an SPI initiative and defines the level of expertise required by each role for each competency. To evaluate the framework, we performed ten interviews and two empirical tests. Preliminary results show that this framework is relevant in SPI initiatives, the use of this framework can raise the awareness about the competencies, and it can support some SPI activities.", "keywords": ["software process improvement", "skills", "stakeholders", "knowledge", "competency framework", "competencies", "spi", "behavior", "roles"]}, {"id": "357", "title": "Remote information concentration via (W) state: reverse of ancilla-free phase-covariant telecloning", "abstract": "In this paper, we investigate generalized remote information concentration as the reverse process of ancilla-free phase-covariant telecloning (AFPCT) which is different from the reverse process of optimal universal telecloning. It is shown that the quantum information via (1rightarrow 2) AEPCT procedure can be remotely concentrated back to a single qubit with a certain probability by utilizing (non-)maximally entangled (W) states as quantum channels. Our protocols are the generalization of Wangs scheme (Open J Microphys 3:1821. doi:10.?4236/?ojm.?2013.?31004, 2013). And von Neumann measure and positive operator-valued measurement are performed in the maximal and non-maximal cases respectively. Relatively the former, the dimension of measurement space in the latter is greatly reduced. It makes the physical realization easier and suitable.", "keywords": ["ancilla-free phase-covariant teleclong", "remote information concentration", "three-qubit asymmetric entangled ", "nonmaximally entangled  state", "projective measurement", "povm"]}, {"id": "358", "title": "Simulation of continuous fibre reinforced thermoplastic forming using a shell finite element with transverse stress", "abstract": "A shell finite element with transverse stress is presented in this paper in order to simulate the forming of thermoplastic composites reinforced with continuous fibres. It is shown by an experimental work that many porosities occurs through the thickness of the composite during the heating and the forming process. Consequently the reconsolidation i.e. the porosity removing by applying a compressive stress through the thickness is a main point of the process. The presented shell finite element keeps the five degrees of freedom of the standard shell elements and adds a sixth one which is the variation in thickness. A locking phenomenon is avoided by uncoupling bending and pinching in the material law. A set of classical validation tests will prove the efficiency of this approach. Finally a forming process is simulated. It shows that the computed transverse stresses are in good agreement with porosity removing in the experiments.", "keywords": ["composites", "forming", "porosities", "shell finite element", "transverse stress", "locking"]}, {"id": "359", "title": "twiki a collaboration tool for the lhc", "abstract": "At the European Laboratory for High Energy Physics, CERN[1], the Large Hadron Collider (LHC)[2] accelerator is colliding beams of protons at energies of 3.5 TeV, recreating conditions close to those at the origin of the Universe. The four main LHC experiments, Alice, Atlas, CMS and LHCb are complex detectors with millions of output channels. These experiment detectors, \"large as cathedrals\", have been designed, built and are now operated by collaborations of physicists from universities and research institutes spread across the world. Wikis are a perfect match to the collaborative nature of CERN experiments and since TWiki[3] was installed at CERN in 2003 it has grown in popularity and the statistics from April 2011 show nearly 10000 registered editors and about 110000 topics (Figure 1). Since the start-up of the LHC more and more users are accessing TWiki requiring better server performance as well as finer control for read and write access and more features. This paper discusses the evolution of the use of TWiki at CERN.", "keywords": ["twiki", "cern", "lhc"]}, {"id": "360", "title": "A glimpse of symbolic-statistical modeling by PRISM", "abstract": "We give a brief overview of a logic-based symbolic modeling language PRISM which provides a unified approach to generative probabilistic models including Bayesian networks, hidden Markov models and probabilistic context free grammars. We include some experimental result with a probabilistic context free grammar extracted from the Penn Treebank. We also show EM learning of a probabilistic context free graph grammar as an example of exploring a new area.", "keywords": ["symbolic-statistical modeling", "prism", "probabilistic context free grammar"]}, {"id": "361", "title": "Improving validity and reliability in longitudinal case study timelines", "abstract": "Management Information Systems researchers rely on longitudinal case studies to investigate a variety of phenomena such as systems development, system implementation, and information systems-related organizational change. However, insufficient attention has been spent on understanding the unique validity and reliability issues related to the timeline that is either explicitly or implicitly required in a longitudinal case study. In this paper, we address three forms of longitudinal timeline validity: time unit validity (which deals with the question of how to segment the timeline - weeks, months, years, etc.), time boundaries validity (which deals with the question of how long the timeline should be), and time period validity (which deals with the issue of which periods should be in the timeline). We also examine timeline reliability, which deals with the question of whether another judge would have assigned the same events to the same sequence, categories, and periods. Techniques to address these forms of longitudinal timeline validity include: matching the unit of time to the pace of change to address time unit validity, use of member checks and formal case study protocol to address time boundaries validity, analysis of archival data to address both time unit and time boundary validity, and the use of triangulation to address timeline reliability. The techniques should be used to design, conduct, and report longitudinal case studies that contain valid and reliable conclusions.", "keywords": ["qualitative methods", "longitudinal case study", "timeline validity", "timeline reliability"]}, {"id": "362", "title": "Information privacy: Measuring individuals' concerns about organizational practices", "abstract": "Information privacy has been called one of the most important ethical issues of the information age. Public opinion polls show rising levels of concern about privacy among Americans. Against this backdrop, research into issues associated with information privacy is increasing. Based on a number of preliminary studies, it has become apparent that organizational practices, individuals' perceptions of these practices, and societal responses are inextricably linked in many ways. Theories regarding these relationships are slowly emerging. Unfortunately, researchers attempting to examine such relationships through confirmatory empirical approaches may be impeded by the lack of validated instruments for measuring individuals' concerns about organizational information privacy practices. To enable future studies in the information privacy research stream, we developed and validated an instrument that identifies and measures the primary dimensions of individuals' concerns about organizational information privacy practices. The development process included examinations of privacy literature; experience surveys and focus groups; and the use of expert judges. The result was a parsimonious 15-item instrument with four subscales tapping into dimensions of individuals' concerns about organizational information privacy practices. The instrument was rigorously tested and validated across several heterogenous populations, providing a high degree of confidence in the scales' validity, reliability, and generalizability.", "keywords": ["privacy", "lisrel", "ethical issues", "measures", "reliability", "validity"]}, {"id": "363", "title": "Using genetic programming and simulation to learn how to dynamically adapt the number of cards in reactive pull systems", "abstract": "We show how to learn dynamically to adapt the number of cards in real time in token-based pull systems. We propose a Simulation-based Genetic Programming approach which does not need training sets. We illustrate how the approach can be implemented using Arena and ?GP. A reactive ConWIP example show the efficiency of the approach and of the knowledge extracted. The resulting decision tree can be used online by production managers or for self-adaptation issues.", "keywords": ["kanban", "conwip", "manufacturing systems", "reactive pull systems", "self-adaptive systems", "learning", "simulation", "genetic programming"]}, {"id": "364", "title": "A development and verification framework for the SegBus platform", "abstract": "We describe the creation of a development framework for a platform-based design approach, in the context of the SegBus platform. The work intends to provide automated procedures for platform build-up and application mapping. The solution is based on a model-based process and heavily employs the UML. We develop a Domain Specific Language to support the platform modeling. An emulator is consequently introduced to allow an as much as possible accurate performance estimation of the solution, at high abstraction levels. Automated execution schedule generation is also featured. The resulting framework is applied to build actual design solutions for a MP3-decoder application.", "keywords": ["model-based engineering", "domain-specific languages", "system emulation", "code generation", "system-on-chip"]}, {"id": "365", "title": "A neuro-fuzzy controller for speed control of a permanent magnet synchronous motor drive", "abstract": "This paper introduces a neuro-fuzzy controller (NFC) for the speed control of a PMSM. A four layer neural network (NN) is used to adjust input and output parameters of membership functions in a fuzzy logic controller (FLC). The back propagation learning algorithm is used for training this network. The performance of the proposed controller is verified by both simulations and experiments. The hardware implementation of the controllers is made using a TMS320F240 DSP. The results are compared with the results obtain from a Proportional+Integral (PI) controller. Simulation and experimental results indicate that the proposed NFC is reliable and effective for the speed control of the PMSM over a wide range of operations of the PMSM drive.", "keywords": ["fuzzy logic control", "neural networks", "permanent magnet synchronous motor drive"]}, {"id": "366", "title": "On the sorting-complexity of suffix tree construction", "abstract": "The suffix tree of a string is the fundamental data structure of combinatorial pattern matching. We present a recursive technique for building suffix trees that yields optimal algorithms in different computational models. Sorting is an inherent bottleneck in building suffix trees and our algorithms match the sorting lower bound. Specifically, we present the following results. (1) Weiner [1973], who introduced the data structure, gave an optimal O(n)-time algorithm for building the suffix tree of an n-character string drawn from a constant-size alphabet. In the comparison model, there is a trivial n(n log n)-time lower bound based on sorting, and Weiner's algorithm matches this bound. For integer alphabets, the fastest known algorithm is the O(n log n) time comparison-based algorithm, but no super-linear lower bound is known. Closing this gap is the main open question in stringology. We settle this open problem by giving a linear time reduction to sorting for building suffix trees. Since sorting is a lower-bound for building suffix trees, this algorithm is time-optimal in every alphabet model, in particular, for an alphabet consisting of integers in a polynomial range we get the first known linear-time algorithm. (2) All previously known algorithms for building suffix trees exhibit a marked absence of locality of reference, and thus they tend to elicit many page faults (I/Os) when indexing very long strings. They are therefore unsuitable for building suffix trees in secondary storage devices, where I/Os dominate the overall computational cost. We give a linear-I/O reduction to sorting for suffix tree construction. Since sorting is a trivial I/O-lower bound for building suffix trees, our algorithm is I/O-optimal.", "keywords": ["algorithms", "design", "theory", "dam model", "external-memory data structures", "ram model", "sorting complexity", "suffix array", "suffix tree"]}, {"id": "367", "title": "Quasi-BirthDeath Processes, Tree-Like QBDs, Probabilistic 1-Counter Automata, and Pushdown Systems", "abstract": "We begin by observing that (discrete-time) Quasi-BirthDeath Processes (QBDs) are equivalent, in a precise sense, to probabilistic 1-Counter Automata (p1CAs), and both Tree-Like QBDs (TL-QBDs) and Tree-Structured QBDs (TS-QBDs) are equivalent to both probabilistic Pushdown Systems (pPDSs) and Recursive Markov Chains (RMCs). We then proceed to exploit these connections to obtain a number of new algorithmic upper and lower bounds for central computational problems about these models. Our main result is this: for an arbitrary QBD, we can approximate its termination probabilities (i.e.,its G matrix) to within i bits of precision (i.e.,within additive error 1/2i 1 / 2 i ), in time polynomial in both the encoding size of the QBD and in i , in the unit-cost rational arithmetic RAM model of computation. Specifically, we show that a decomposed Newtons method can be used to achieve this. We emphasize that this bound is very different from the well-known linear/quadratic convergence of numerical analysis, known for QBDs and TL-QBDs, which typically gives no constructive bound in terms of the encoding size of the system being solved. In fact, we observe (based on recent results) that for the more general TL-QBDs such a polynomial upper bound on Newtons method fails badly. Our upper bound proof for QBDs combines several ingredients: a detailed analysis of the structure of 1-Counter Automata, an iterative application of a classic condition number bound for errors in linear systems, and a very recent constructive bound on the performance of Newtons method for strongly connected monotone systems of polynomial equations. We show that the quantitative termination decision problem for QBDs (namely, is Gu,v?1/2 G u , v ? 1 / 2 ?) is at least as hard as long-standing open problems in the complexity of exact numerical computation, specifically the square-root sum problem. On the other hand, it follows from our earlier results for RMCs that any non-trivial approximation of termination probabilities for TL-QBDs is sqrt-root-sum-hard.", "keywords": ["quasi-birthdeath processes", "tree-like qbds", "probabilistic 1-counter automata", "pushdown systems", "newtons method"]}, {"id": "368", "title": "Composite kernel learning", "abstract": "The Support Vector Machine is an acknowledged powerful tool for building classifiers, but it lacks flexibility, in the sense that the kernel is chosen prior to learning. Multiple Kernel Learning enables to learn the kernel, from an ensemble of basis kernels, whose combination is optimized in the learning process. Here, we propose Composite Kernel Learning to address the situation where distinct components give rise to a group structure among kernels. Our formulation of the learning problem encompasses several setups, putting more or less emphasis on the group structure. We characterize the convexity of the learning problem, and provide a general wrapper algorithm for computing solutions. Finally, we illustrate the behavior of our method on multi-channel data where groups correspond to channels.", "keywords": ["supervized learning", "support vector machine", "kernel learning", "structured kernels", "feature selection and sparsity"]}, {"id": "369", "title": "recent progress in linear algebra and lattice basis reduction", "abstract": "A general goal concerning fundamental linear algebra problems is to reduce the complexity estimates to essentially the same as that of multiplying two matrices (plus possibly a cost related to the input and output sizes). Among the bottlenecks one usually finds the questions of designing a recursive approach and mastering the sizes of the intermediately computed data. In this talk we are interested in two special cases of lattice basis reduction. We consider bases given by square matrices over K[x] or Z, with, respectively, the notion of reduced form and LLL reduction. Our purpose is to introduce basic tools for understanding how to generalize the Lehmer and Knuth-Schnhage gcd algorithms for basis reduction. Over K[x] this generalization is a key ingredient for giving a basis reduction algorithm whose complexity estimate is essentially that of multiplying two polynomial matrices. Such a problem relation between integer basis reduction and integer matrix multiplication is not known. The topic receives a lot of attention, and recent results on the subject show that there might be room for progressing on the question.", "keywords": ["polynomial matrix", "matrix reduction", "lll basis reduction", "euclidean lattice"]}, {"id": "370", "title": "quantifying content consistency improvements through opportunistic contacts", "abstract": "Contacts between mobile users provide opportunities for data updates that supplement infrastructure-based mechanisms. While the benefits of such opportunistic sharing are intuitive, quantifying the capacity increase they give rise to is challenging because both contact rates and contact graphs depend on the structure of the social networks users belong to. Furthermore, social connectivity influences not only users' interests, i.e., the content they own, but also their willingness to share data with others. All these factors can have a significant effect on the capacity gains achievable through opportunistic contacts. This paper's main contribution is in developing a tractable model for estimating such gains in a content update system, where content originates from a server along multiple channels, with blocks of information in each channel updated at a certain rate, and users differ in their contact graphs, interests, and willingness to share content, e.g., only to the members of their own social networks. We establish that the added capacity available to improve content consistency through opportunistic sharing can be obtained by solving a convex optimization problem. The resulting optimal policy is evaluated using traces reflecting contact graphs in different social settings and compared to heuristic policies. The evaluation demonstrates the capacity gains achievable through opportunistic sharing, and the impact on those gains of the structure of the underlying social network.", "keywords": ["consistency", "optimization", "dynamic content", "dissemination", "delay-tolerant networks", "social networks"]}, {"id": "371", "title": "Study of the communication distance of a MEMS Pressure Sensor Integrated in a RFID Passive Tag", "abstract": "The performance of a MEMS (Micro Electro-Mechanical Systems) Sensor in a RFID system has been calculated, simulated and analyzed. It documents the viability from the power consumption point of view- of integrating a MEMS sensor in a passive tag maintaining its long range. The wide variety of sensors let us specify as many applications as the imagination is able to create. The sensor tag works without battery, and it is remotely powered through a commercial reader accomplishing the EPC standard Class 1 Gen 2. The key point is the integration in the tag of a very low power consumption pressure MEMS sensor. The power consumption of the sensor is 12.5 mu W. The specifically developed RFID CMOS passive module, with an integrated temperature sensor, is able to communicate up to 2.4 meters. Adding the pressure MEMS sensor - an input capacity, a maximum range of 2 meters can be achieved between the RFID sensor tag and a commercial reader (typical reported range for passive pressure sensors are in the range of a few centimeters). The RFID module has been fabricated with a CMOS process compatible with a bulk micromachining MEMS process. So, the feasibility of a single chip is presented.", "keywords": ["radiofrequency identification", "sensor systems", "low power electronics", "wireless sensor networks"]}, {"id": "372", "title": "A hardware architecture for real-time image compression using a searchless fractal image coding method", "abstract": "In this paper we present a novel hardware architecture for real-time image compression implementing a fast, searchless iterated function system (SIFS) fractal coding method. In the proposed method and corresponding hardware architecture, domain blocks are fixed to a spatially neighboring area of range blocks in a manner similar to that given by Furao and Hasegawa. A quadtree structure, covering from 3232 blocks down to 22 blocks, and even to single pixels, is used for partitioning. Coding of 22 blocks and single pixels is unique among current fractal coders. The hardware architecture contains units for domain construction, zig-zag transforms, range and domain mean computation, and a parallel domain-range match capable of concurrently generating a fractal code for all quadtree levels. With this efficient, parallel hardware architecture, the fractal encoding speed is improved dramatically. Additionally, attained compression performance remains comparable to traditional search-based and other searchless methods. Experimental results, with the proposed hardware architecture implemented on an Altera APEX20K FPGA, show that the fractal encoder can encode a 5125128 image in approximately 8.36ms operating at 32.05MHz. Therefore, this architecture is seen as a feasible solution to real-time fractal image compression.", "keywords": ["fractal image encoding", "quadtree", "searchless", "real-time image compression"]}, {"id": "373", "title": "Distinguishing views in symmetric networks: A tight lower bound", "abstract": "The view of a node in a port-labeled network is an infinite tree encoding all walks in the network originating from this node. We prove that for any integers n?D?1 n ? D ? 1 , there exists a port-labeled network with at most n nodes and diameter at most D  which contains a pair of nodes whose (infinite) views are different, but whose views truncated to depth ?(Dlog?(n/D)) ? ( D log ? ( n / D ) ) are identical.", "keywords": ["anonymous network", "port-labeled network", "view", "quotient graph"]}, {"id": "374", "title": "cache injection for parallel applications", "abstract": "For two decades, the memory wall has affected many applications in their ability to benefit from improvements in processor speed. Cache injection addresses this disparity for I/O by writing data into a processor's cache directly from the I/O bus. This technique reduces data latency and, unlike data prefetching, improves memory bandwidth utilization. These improvements are significant for data-intensive applications whose performance is dominated by compulsory cache misses. We present an empirical evaluation of three injection policies and their effect on the performance of two parallel applications and several collective micro-benchmarks. We demonstrate that the effectiveness of cache injection on performance is a function of the communication characteristics of applications, the injection policy, the target cache, and the severity of the memory wall. For example, we show that injecting message payloads to the L3 cache can improve the performance of network-bandwidth limited applications. In addition, we show that cache injection improves the performance of several collective operations, but not all-to-all operations (implementation dependent). Our study shows negligible pollution to the target caches.", "keywords": ["memory wall", "cache injection"]}, {"id": "375", "title": "Cryptanalysis and improvement of an access control in user hierarchy based on elliptic curve cryptosystem", "abstract": "In a key management scheme for hierarchy based access control, each security class having higher clearance can derive the cryptographic secret keys of its other security classes having lower clearances. In 2008, Chung et al. proposed an efficient scheme on access control in user hierarchy based on elliptic curve cryptosystem [Information Sciences 178 (1) (2008) 230243]. Their scheme provides solution of key management efficiently for dynamic access problems. However, in this paper, we propose an attack on Chung et al.s scheme to show that Chung et al.s scheme is insecure against the exterior root finding attack. We show that under this attack, an attacker (adversary) who is not a user in any security class in a user hierarchy attempts to derive the secret key of a security class by using the root finding algorithm. In order to remedy this attack, we further propose a simple improvement on Chung et al.s scheme. Overall, the main theme of this paper is very simple: a security flaw is presented on Chung et al.s scheme and then a fix is provided in order to remedy the security flaw found in Chung et al.s scheme.", "keywords": ["key management", "elliptic curve", "hierarchical access control", "polynomial interpolation", "security", "exterior root finding attacks"]}, {"id": "376", "title": "Use of molecular modeling, docking, and 3D-QSAR studies for the determination of the binding mode of benzofuran-3-yl-(indol-3-yl)maleimides as GSK-3 beta inhibitors", "abstract": "Molecular modeling and docking studies along with three-dimensional quantitative structure relationships (3D-QSAR) studies have been used to determine the correct binding mode of glycogen synthase kinase 3 beta (GSK-3 beta) inhibitors. The approaches of comparative molecular field analysis (CoMFA) and comparative molecular similarity index analysis (CoMSIA) are used for the 3D-QSAR of 51 substituted benzofuran-3-yl-(indol-3-yl)maleimides as GSK-3 beta inhibitors. Two binding modes of the inhibitors to the binding site of GSK-3 beta are investigated. The binding mode 1 yielded better 3D-QSAR correlations using both CoMFA and CoMSIA methodologies. The three-component CoMFA model from the steric and electrostatic fields for the experimentally determined pIC(50) values has the following statistics: R(2)(cv) = 0.386 nd SE(cv) = 0.854 for the cross-validation, and R(2) = 0.811 and SE = 0.474 for the fitted correlation. F (3,47) = 67.034, and probability of R(2) = 0 (3,47) = 0.000. The binding mode suggested by the results of this study is consistent with the preliminary results of X-ray crystal structures of inhibitor-bound GSK-3 beta. The 3D-QSAR models were used for the estimation of the inhibitory potency of two additional compounds.", "keywords": ["benzofuran-3-yl- maleimides", "binding mode", "comfa", "comsia", "docking", "gsk-3beta inhibitors", "3d-qsar", "x-ray"]}, {"id": "377", "title": "On the numerical solution of some semilinear elliptic problems - II", "abstract": "In the earlier paper [6], a Galerkin method was proposed and analyzed for the numerical solution of a Dirichlet problem for a semi-linear elliptic boundary value problem of the form -Delta U = F((.),U). This was converted to a problem on a standard domain and then converted to an equivalent integral equation. Galerkina's method was used to solve the integral equation, with the eigenfunctions of the Laplacian operator on the standard domain D as the basis functions. In this paper we consider the implementing of this scheme, and we illustrate it for some standard domains D.", "keywords": ["elliptic", "nonlinear", "integral equation", "galerkin method"]}, {"id": "378", "title": "supporting mobile work processes in logistics with wearable computing", "abstract": "Logistics is a very dynamic and heterogeneous application area which generates complex requirements regarding the development of information and communication technologies (ICT). For this area, it is a challenge to support mobile workers on-site in an unobtrusive manner. In this contribution, wearable computing technologies are investigated as basis for a \"mobile worker supporting system\" for tasks at an automobile terminal. The features of wearable computing technologies are checked against the requirements of the application area to come to an usable and acceptable mobile solution in an user-centred design process.", "keywords": ["mobile usability", "logistics", "wearable computing", "mobile work processes", "autonomous control", "user-centred design"]}, {"id": "379", "title": "A nonlinear domain decomposition formulation with application to granular dynamics", "abstract": "Simulation of granular media undergoing dynamic evolution involves nonsmooth problems when grains are modeled as rigid bodies. With dense samples, this nonsmoothness occurs everywhere in the studied domain, and large sized systems lead to computationally intensive simulations. In this article, we combine domain decomposition approaches and nonsmooth contact dynamics. Unlike the smooth continuum media case, a coarse space problem does not trivially increase the convergence rate, as it is exemplified in this article, with semi-analytical examples and real size numerical simulations. Nevertheless, the description of an underlying force network in the samples may guide the analysis for new approximation schemes or algorithms.", "keywords": ["nonsmooth contact dynamics", "multicontact systems", "scalability", "multiscale", "asymptotic analysis"]}, {"id": "380", "title": "Minimum cut linear arrangement of p-q dags for VLSI layout of adder trees", "abstract": "Two algorithms for minimum cut linear arrangement of a class of graphs called p-q dags are proposed. A p-q dag represents the connection scheme of an adder tree, such as Wallace tree, and the VLSI layout problem of a bit slice of an adder tree is treated as the minimum cut linear arrangement problem of its corresponding p-q dag. One of the two algorithms is based on dynamic programming. It calculates an exact minimum solution within n(O(1)) time and space, where n is the size of a given graph. The other algorithm is an approximation algorithm which calculates a solution with O(log n) cutwidth. It requires O(n log n) time.", "keywords": ["graph algorithm", "minimum cut linear arrangement", "vlsi layout", "adder tree", "multiplier"]}, {"id": "381", "title": "OPTIMIZATION OF ILLUMINATION ENVIRONMENTAL FACTORS BASED ON ORTHOGONAL TEST", "abstract": "This paper presents a comparative research of nine different combinations of imaging environmental factors using orthogonal test approach to gain optimal illumination in an image acquisition device which was self-designed. The effect of four different environmental factors such as shoot distance, lamp number, lamp height, lamp side distance have been investigated on the key parameters. Experimental results based on L(9)(3)(4) orthogonal test design shows that under different combination of environmental factors, there are obvious differences between illumination intensity and illumination uniformity of images and which are mainly affected by the shoot distance and lamp number. Based on these experiments, we get two preferable combinations. Attention is concentrated on finding the best. Through further analysis and discussion, the best combination is identified. Our experimental results indicate that orthogonal test here is very suitable for gaining optimal environmental factors.", "keywords": ["image acquisition", "diffuse reflection", "orthogonal test", "illumination intensity", "illumination uniformity"]}, {"id": "382", "title": "Linear B cell epitope prediction for epitope vaccine design against meningococcal disease and their computational validations through physicochemical properties", "abstract": "Neisseria meningitidis serogroup B is predominantly known for its leading role in bacterial meningitis and septicemia worldwide. Although, polysaccharide conjugate vaccines have been developed and used successfully against many of the serogroups of N. meningitidis, such strategy has proved ineffective against group B meningococci. Here, we proposed to develop peptide epitope-based vaccine candidates from outer membrane (OM) protein contained in the outer membrane vesicles (OMV) based on our in silico analysis. In OMV, a total of 236 proteins were identified, only 15 (6.4%) of which were predicted to be located in the outer membrane. For the preparation of specific monoclonal antibodies against pathogenic bacterial protein, identification and selection of B cell epitopes that act as a vaccine target are required. We selected 13 outer membrane proteins from OMV proteins while taking into consideration the removal of cross-reactivity. Epitopia web server was used for the prediction of B cell epitopes. Epitopes are distinguished from non-epitopes by properties such as amino acid preference on the basis of amino acid composition, secondary structure composition, and evolutionary conservation. Predicted results were subject to verification with experimental data and we performed string-based search through IEDB. Our finding shows that epitopes have general preference for charged and polar amino acids; epitopes are enriched with loop as a secondary structure element that renders them flexible and also exposes another view of antibodyantigen interaction.", "keywords": ["b cell epitopes", "meningococcal", "neisseria meningitides", "structural features", "vaccine candidates"]}, {"id": "383", "title": "Object recognition using proportion-based prior information: Application to fisheries acoustics", "abstract": "This paper addresses the inference of probabilistic classification models using weakly supervised learning. The main contribution of this work is the development of learning methods for training datasets consisting of groups of objects with known relative class priors. This can be regarded as a generalization of the situation addressed by Bishop and Ulusoy (2005), where training information is given as the presence or absence of object classes in each set. Generative and discriminative classification methods are conceived and compared for weakly supervised learning, as well as a non-linear version of the probabilistic discriminative models. The considered models are evaluated on standard datasets and an application to fisheries acoustics is reported. The proposed proportion-based training is demonstrated to outperform model learning based on presence/absence information and the potential of the non-linear discriminative model is shown.", "keywords": ["weakly supervised learning", "generative classification model", "discriminative classification model"]}, {"id": "384", "title": "Hes frequencyamplitude formulation for nonlinear oscillators with an irrational force", "abstract": "In this paper, Hes frequencyamplitude formulation is applied to determine the periodic solution for a nonlinear oscillator system with an irrational force. Comparison with the exact solution shows that the result obtained is of high accuracy.", "keywords": ["nonlinear oscillators", "hes frequency formulation", "periodic solution"]}, {"id": "385", "title": "The effectiveness of a training method using self-modeling webcam photos for reducing musculoskeletal risk among office workers using computers", "abstract": "An intervention study was conducted to examine the effectiveness of an innovative self-modeling photo-training method for reducing musculoskeletal risk among office workers using computers. Sixty workers were randomly assigned to either: 1) a control group; 2) an office training group that received personal, ergonomic training and workstation adjustments or 3) a photo-training group that received both office training and an automatic frequent-feedback system that displayed on the computer screen a photo of the workers current sitting posture together with the correct posture photo taken earlier during office training. Musculoskeletal risk was evaluated using the Rapid Upper Limb Assessment (RULA) method before, during and after the six weeks intervention. Both training methods provided effective short-term posture improvement; however, sustained improvement was only attained with the photo-training method. Both interventions had a greater effect on older workers and on workers suffering more musculoskeletal pain. The photo-training method had a greater positive effect on women than on men.", "keywords": ["occupational exposure", "ergonomics", "telemedicine", "feedback", "task performance and analysis", "algorithm", "posture"]}, {"id": "386", "title": "the influence of feedback on egocentric distance judgments in real and virtual environments", "abstract": "A number of investigators have reported that distance judgments in virtual environments (VEs) are systematically smaller than distance judgments made in comparably-sized real environments. Many variables that may contribute to this difference have been investigated but none of them fully explain the distance compression. One approach to this problem that has implications for both VE applications and the study of perceptual mechanisms is to examine the influence of the feedback available to the user. Most generally, we asked whether feedback within a virtual environment would lead to more accurate estimations of distance. Next, given the prediction that some change in behavior would be observed, we asked whether specific adaptation effects would generalize to other indications of distance. Finally, we asked whether these effects would transfer from the VE to the real world. All distance judgments in the head-mounted display (HMD) became near accurate after three different forms of feedback were given within the HMD. However, not all feedback sessions within the HMD altered real world distance judgments. These results are discussed with respect to the perceptual and cognitive mechanisms that may be involved in the observed adaptation effects as well as the benefits of feedback for VE applications.", "keywords": ["adaptation", "virtual environments", "space perception", "feedback"]}, {"id": "387", "title": "synchronous versus asynchronous collaboration in situated multi-agent systems", "abstract": "According to the taxonomy for agent activity, proposed by V. Parunak, a collaboration is an interaction between agents of a multi-agent system (MAS) whereby the agents explicitly coordinate their actions before they cooperate. We discuss two sub-types of collaboration in the context of situated MASs, namely asynchronous and synchronous collaboration. After setting up collaboration, the interaction between the agents in an asynchronous collaboration happens indirectly through the environment. Agents direct their actions via the perceived state change of their environment. On the other hand, during a synchronous collaboration agents have to act simultaneously and this requires an additional agreement about which actions should be executed. Although they both fit the characteristics of collaboration, the requirements for their implementation is quite different. Whereas agents in an asynchronous collaboration can be implemented as separate processes that act directly into the environment, the implementation of synchronous collaboration is more complex since it requires support for simultaneous actions. In the paper we give examples of both kinds of collaborations and outline the necessary support for their implementation.", "keywords": ["synchronization", "collaboration", "interaction"]}, {"id": "388", "title": "Web Services Compositions Modelling and Choreographies Analysis", "abstract": "In Rouached et al. (2006) and Rouached and Godart (2007) the authors described the semantics of WSBPEL by way of mapping each of the WSBPEL (Arkin et al., 2004) constructs to the EC algebra and building a model of the process behaviour. With these mapping rules, the authors describe a modelling approach of a process defined for a single Web service composition. However, this modelling is limited to a local view and can only be used to model the behaviour of a single process. The authors further the semantic mapping to include Web service composition interactions through modelling Web service conversations and their choreography. This paper elaborates the models to support a view of interacting Web service compositions extending the mapping from WSBPEL to EC, and including Web service interfaces (WSDL) for use in modelling between services. The verification and validation techniques are also exposed while automated induction-based theorem prover is used as verification back-end.", "keywords": ["choreography", "orchestration", "semantic mapping", "verification and validation", "web service composition"]}, {"id": "389", "title": "PnP problem revisited", "abstract": "Perspective-n-Point camera pose determination, or the PnP problem, has attracted much attention in the literature. This paper gives a systematic investigation on the PnP problem from both geometric and algebraic standpoints, and has the following contributions: Firstly, we rigorously prove that the PnP problem under distance-based definition is equivalent to the PnP problem under orthogonal-transformation-based definition when n > 3, and equivalent to the PnP problem under rotation-transformation-based definition when n = 3. Secondly, we obtain the upper bounds of the number of solutions for the PnP problem under different definitions. In particular, we show that for any three non-collinear control points, we can always find out a location of optical center such that the P3P problem formed by these three control points and the optical center can have 4 solutions, its upper bound. Additionally a geometric way is provided to construct these 4 solutions. Thirdly, we introduce a depth-ratio based approach to represent the solutions of the whole PnP problem. This approach is shown to be advantageous over the traditional elimination techniques. Lastly, degenerated cases for coplanar or collinear control points are also discussed. Surprisingly enough, it is shown that if all the control points are collinear, the PnP problem under distance-based definition has a unique solution, but the PnP problem under transformation-based definition is only determined up to one free parameter.", "keywords": ["perspective-n-point camera pose determination", "distance-based definition", "transformation-based definition", "depth-ratio based equation", "upper bound of the number of solutions"]}, {"id": "390", "title": "Example-driven animation synthesis", "abstract": "We introduce an easy and intuitive approach to create animations by assembling existing animations. Using our system, the user needs only to simply scribble regions of interest and select the example animations that he/she wants to apply. Our system will then synthesize a transformation for each triangle and solve an optimization problem to compute the new animation for this target mesh. Like playing a jigsaw puzzle game, even a novice can explore his/her creativity by using our system without learning complicated routines, but just using a few simple operations to achieve the goal.", "keywords": ["animation synthesis", "warping", "intelligent scribbling"]}, {"id": "391", "title": "Macroblock and Motion Feature Analysis to H.264/AVC Fast Inter Mode Decision", "abstract": "One fast inter mode decision algorithm is proposed in this paper. The whole algorithm is divided into two stages. In the pre-stage, by exploiting spatial and temporal information of encoded macrobocks (MBs), a skip mode early detection scheme is proposed. The homogeneity of current MB is also analyzed to filter out small inter modes in this stage. Secondly, during the block matching stage, a motion feature based inter mode decision scheme is introduced by analyzing the motion vector predictor's accuracy, the block overlapping situation and the smoothness of SAD (sum of absolute difference) value. Moreover, the rate distortion cost is checked in an early stage and we set some constraints to speed up the whole decision flow. Experiments show that our algorithm can achieve a speed up factor of up to 53.4% for sequences with different motion type. The overall bit increment and quality degradation is negligible compared with existing works.", "keywords": ["mode decision", "h.264/avc", "feature analysis"]}, {"id": "392", "title": "the design of parsers for incremental language processors", "abstract": "An incremental language processor is one that accepts as input a sequence of substrings of the source language and maps them independently onto fragments in some object code. The ordered sequence of these object code fragments are then either compiled, in which case we have an incremental compiler, or interpreted. In the first case the advantage resulting is that subsequent changes in the source program entail only reprocessing the source fragments affected and recompiling the updated collection of object code fragments. In an environment where small changes are made frequently to large programs, e.g. debugging, the curtailment of reprocessing is attractive. In the second case the object code fragments are the actual run-time program representation, and hence inter-fragment relations are transiently evaluated as needed in the process of execution, with no long-term preservation of these relationships beyond the scope of their immediate need in execution time. This permits the possibility of program recomposition in the midst of execution, one of the principal characteristics of conversational computing. Many conversational language processors execute a program representation functionally analogous to parse trees, i.e. the syntax analysis of a fragment, insofar as it is possible, is done at fragment load time. This representation choice is popular because many of the expensive aspects of interpretation, including character string scanning, symbol table lookup, and parsing, are performed once only and do not contribute to the execution overhead. This paper is devoted to examining the question of the construction of such a parser in a general manner for an arbitrary source language.", "keywords": ["parser", "input", "analysis", "maps", "design", "collect", "computation", "object", "trees", "case", "general", "timing", "paper", "representation", "scan", "strings", "change", "incremental", "fragmentation", "processor", "program", "character", "aspect", "preservation", "relationships", "environments", "interpretation", "code", "language", "process", "compilation", "sequence", "debugging", "parsing"]}, {"id": "393", "title": "On the computation of all supported efficient solutions in multi-objective integer network flow problems", "abstract": "This paper presents a new algorithm for identifying all supported non-dominated vectors (or outcomes) in the objective space, as well as the corresponding efficient solutions in the decision space, for multi-objective integer network flow problems. Identifying the set of supported non-dominated vectors is of the utmost importance for obtaining a first approximation of the whole set of non-dominated vectors. This approximation is crucial, for example, in two-phase methods that first compute the supported non-dominated vectors and then the unsupported non-dominated ones. Our approach is based on a negative-cycle algorithm used in single objective minimum cost flow problems, applied to a sequence of parametric problems. The proposed approach uses the connectedness property of the set of supported non-dominated vectors/efficient solutions to find all integer solutions in maximal non-dominated/efficient facets.", "keywords": ["multi-objective linear and integer programming", "multi-objective network flows", "negative-cycle algorithms", "parametric programming"]}, {"id": "394", "title": "Two-objective method for crisp and fuzzy interval comparison in optimization", "abstract": "In real optimization we always meet two main groups of criteria: requirements of useful outcomes increasing or expenses decreasing and demands of lower uncertainty or, in other words, risk minimization. Therefore, it seems advisable to formulate optimization problem under conditions of uncertainty, at least, two-objective on the basis of local criteria of outcomes increasing or expenses reduction and risk minimization. Generally, risk may be treated as the uncertainty of obtained result. In the considered situation, the degree of risk (uncertainty) may be defined in a natural way through the width of final interval objective function at the point of optimum achieved. To solve the given problem, the two-objective interval comparison technique has been developed taking into account the probability of supremacy of one interval over the other one and relation of compared widths of intervals. To illustrate the efficiency of the proposed method, the simple examples of minimization of interval double-extreme discontinuous cost function and fuzzy extension of Rosenbrock's test function are presented.", "keywords": ["crisp interval", "fuzzy interval", "interval comparison", "probabilistic approach", "optimization"]}, {"id": "395", "title": "Development of head detection and tracking systems for visual surveillance", "abstract": "This paper proposes a technique for the detection of head nod and shake gestures based on eye tracking and head motion decision. The eye tracking step is divided into face detection and eye location. Here, we apply a motion segmentation algorithm that examines differences in moving peoples faces. This system utilizes a Hidden Markov Model-based head detection module that carries out complete detection in the input images, followed by the eye tracking module that refines the search based on a candidate list provided by the preprocessing module. The novelty of this paper is derived from differences in real-time input images, preprocessing to remove noises (morphological operators and so on), detecting edge lines and restoration, finding the face area, and cutting the head candidate. Moreover, we adopt a K-means algorithm for finding the head region. Real-time eye tracking extracts the location of eyes from the detected face region and is performed at close to a pair of eyes. After eye tracking, the coordinates of the detected eyes are transformed into a normalized vector of x-coordinate and y-coordinate. Head nod and shake detector uses three hidden Markov models (HMMs). HMM representation of the head detection can estimate the underlying HMM states from a sequence of face images. Head nod and shake can be detected by three HMMs that are adapted by a directional vector. The directional vector represents the direction of the head movement. The vector is HMMs for determining neutral as well as head nod and shake. These techniques are implemented on images, and notable success is notified.", "keywords": ["head detection", "head location", "eye location", "hidden markov models"]}, {"id": "396", "title": "Scheduling of printed circuit board (PCB) assembly systems with heterogeneous processors using simulation-based intelligent optimization methods", "abstract": "The complexity of printed circuit boards (PCBs), as an important sector of the electronics manufacturing industry, has increased over the last three decades. This paper focuses on a practical application observed at a PCB assembly line of electronics manufacturing facility. It is shown that this problem is equivalent to a flowshop scheduling with multiple heterogeneous batch processors where processors can perform multiple tasks as long as the sizes of jobs in a batch do not violate the processors capacity. The equivalent problem is mathematically formulated as a mixed integer programming model. Then, a Monte Carlo simulation is incorporated into high-level genetic algorithm-based intelligent optimization techniques to assess the performance of makespan-oriented system under uncertain processing times. At each iteration of algorithm, the output of simulator is used by optimizers to provide online feedbacks on the progress of the search and direct the search toward a promising solution zone. Furthermore, various parameters and operators of the algorithm are discussed and calibrated by means of Taguchi statistical technique. The result of extensive computational experiments shows that the solution approach gives high-quality solutions in reasonable computational time.", "keywords": ["electronics manufacturing", "pcb assembly", "monte carlo simulation", "genetic algorithms"]}, {"id": "397", "title": "Multi-agent systems with reinforcement hierarchical neuro-fuzzy models", "abstract": "This paper introduces a new multi-agent model for intelligent agents, called reinforcement learning hierarchical neuro-fuzzy multi-agent system. This class of model uses a hierarchical partitioning of the input space with a reinforcement learning algorithm to overcome limitations of previous RL methods. The main contribution of the new system is to provide a flexible and generic model for multi-agent environments. The proposed generic model can be used in several applications, including competitive and cooperative problems, with the autonomous capacity to create fuzzy rules and expand their own rule structures, extracting knowledge from the direct interaction between the agents and the environment, without any use of supervised algorithms. The proposed model was tested in three different case studies, with promising results. The tests demonstrated that the developed system attained good capacity of convergence and coordination among the autonomous intelligent agents.", "keywords": ["multi-agent systems ", "hierarchical neuro-fuzzy", "intelligent agents", "reinforcement learning"]}, {"id": "398", "title": "Possibilistic meanvariance models and efficient frontiers for portfolio selection problem", "abstract": "In this paper, it is assumed that the rates of return on assets can be expressed by possibility distributions rather than probability distributions. We propose two kinds of portfolio selection models based on lower and upper possibilistic means and possibilistic variances, respectively, and introduce the notions of lower and upper possibilistic efficient portfolios. We also present an algorithm which can derive the explicit expression of the possibilistic efficient frontier for the possibilistic mean-variance portfolio selection problem dealing with lower bounds on asset holdings.", "keywords": ["possibility theory", "possibilistic mean", "possibilistic variance", "portfolio selection", "optimization"]}, {"id": "399", "title": "Millennials among the Professional Workforce in Academic Libraries: Their Perspective on Leadership", "abstract": "This study explores possible leadership perceptions of Millennials working in academic libraries, specifically their definition, the attributes they associate with leadership, whether they want to assume formal leadership roles, whether they perceive themselves as leaders, and whether they perceive leadership opportunities within their organizations and LIS professional associations. An online survey was utilized to gather the responses and the study participants comprised of Millennials (born 1982 or after) currently working full-time in libraries that were a member of the Committee on Institutional Cooperation (CIC), a consortium of the Big Ten universities and the University of Chicago in 201112.", "keywords": ["leadership", "millennials", "academic", "leaders", "perceptions", "management"]}, {"id": "400", "title": "Mesh fusion using functional blending on topologically incompatible sections", "abstract": "Three-dimensional mesh fusion provides an easy and fast way to create new mesh models from existing ones. We introduce a novel approach of mesh fusion in this paper based on functional blending. Our method has no restriction of disk-like topology or one-ring opening on the meshes to be merged. First of all, sections with boundaries of the under-fusing meshes are converted into implicit representations. An implicit transition surface, which joins the sections together while keeping smoothness at the boundaries, is then created based on cubic Hermite functional blending. Finally, the implicit surface is tessellated to form the resultant mesh. Our scheme is both efficient and simple, and with it users can easily construct interesting, complex 3D models.", "keywords": ["mesh fusion", "functional blending", "interactive modeling tool"]}, {"id": "401", "title": "Imaging spectrometry and asphalt road surveys", "abstract": "This study integrates ground spectrometry, imaging spectrometry, and in situ pavement condition surveys for asphalt road assessment. Field spectra showed that asphalt aging and deterioration produce measurable changes in spectra as these surfaces undergo a transition from hydrocarbon dominated new roads to mineral dominated older roads. Several spectral measures derived from field and image spectra correlated well with pavement quality indicators. Spectral confusion between pavement material aging and asphalt mix erosion on the one hand, and structural road damages (e.g. cracking) on the other, poses some limits to remote sensing based mapping. Both the common practice methods (Pavement management system-PMS, in situ vehicle inspections), and analysis using imaging spectrometry are effective in identifying roads in good and very good condition. Variance and uncertainty in all survey data (PMS, in situ vehicle inspections, remote sensing) increases for road surfaces in poor condition and clear determination of specific (and expensive) surface treatment decisions remains problematic from these methods.", "keywords": ["asphalt road survey", "imaging spectrometry", "pavement management", "spectral library", "remote sensing", "hyperspectral"]}, {"id": "402", "title": "Conceptions of learning versus conceptions of web-based learning: The differences revealed by college students", "abstract": "Past research has shown the variations of students' conceptions of learning, but little has been especially undertaken to address students' conceptions of web-based learning and to make comparisons between students' conceptions of learning in general and their conceptions of web-based learning in particular. By interviewing 83 Taiwanese college students with some web-based learning experiences, this study attempted to investigate the students' conceptions of learning, conceptions of web-based learning, and the differences between these conceptions. Using the phenomenographic method of analyzing student interview transcripts, several categories of conceptions of learning and of web-based learning were revealed. The analyses of interview results suggested that the conceptions of web-based learning were often more sophisticated than those of learning. For example, much more students conceptualized learning in web-based context as pursuing real understanding and seeing in a new way than those for learning in general. This implies that the implementation of web-based instruction may be a potential avenue for promoting students' conceptions of learning. By gathering questionnaire responses from the students, this study further found that the sophistication of the conceptions toward web-based learning was associated with better searching strategies as well as higher self-efficacy for web-based learning.  ", "keywords": ["post-secondary education", "distance education and telelearning"]}, {"id": "403", "title": "on the behaviour of weighted multi-recombination evolution strategies optimising noisy cigar functions", "abstract": "Cigar functions are convex quadratic functions that are characterised by the presence of only two distinct eigenvalues of their Hessian, the smaller one of which occurs with multiplicity one. Their ridge-like topology makes them a useful test case for optimisation strategies. This paper extends previous work on modelling the behaviour of evolution strategies with isotropically distributed mutations optimising cigar functions by considering weighted recombination as well as the effects of noise on optimisation performance. It is found that the same weights that have previously been seen to be optimal for the sphere and parabolic ridge functions are optimal for cigar functions as well. The influence of the presence of noise on optimisation performance depends qualitatively on the trajectory of the search point, which in turn is determined by the strategy's mutation strength as well as its population size and recombination weights. Analytical results are obtained for the case of cumulative step length adaptation.", "keywords": ["evolution strategy", "weighted recombination", "cumulative step length adaptation", "cigar function", "noise"]}, {"id": "404", "title": "Human emotion recognition from videos using spatio-temporal and audio features", "abstract": "In this paper, we present human emotion recognition systems based on audio and spatio-temporal visual features. The proposed system has been tested on audio visual emotion data set with different subjects for both genders. The mel-frequency cepstral coefficient (MFCC) and prosodic features are first identified and then extracted from emotional speech. For facial expressions spatio-temporal features are extracted from visual streams. Principal component analysis (PCA) is applied for dimensionality reduction of the visual features and capturing 97% of variances. Codebook is constructed for both audio and visual features using Euclidean space. Then occurrences of the histograms are employed as input to the state-of-the-art SVM classifier to realize the judgment of each classifier. Moreover, the judgments from each classifier are combined using Bayes sum rule (BSR) as a final decision step. The proposed system is tested on public data set to recognize the human emotions. Experimental results and simulations proved that using visual features only yields on average 74.15% accuracy, while using audio features only gives recognition average accuracy of 67.39%. Whereas by combining both audio and visual features, the overall system accuracy has been significantly improved up to 80.27%.", "keywords": ["human computer interface ", "multimodal system", "human emotions", "support vector machines ", "spatio-temporal features"]}, {"id": "405", "title": "Fortification of rice: technologies and nutrients", "abstract": "This article provides a comprehensive review of the currently available technologies for vitamin and mineral rice fortification. It covers currently used technologies, such as coating, dusting, and the various extrusion technologies, with the main focus being on cold, warm, and hot extrusion technologies, including process flow, required facilities, and sizes of operation. The advantages and disadvantages of the various processing methods are covered, including a discussion on micronutrients with respect to their technical feasibility during processing, storage, washing, and various cooking methods and their physiological importance. The microstructure of fortified rice kernels and their properties, such as visual appearance, sensory perception, and the impact of different micronutrient formulations, are discussed. Finally, the article covers recommendations for quality control and provides a summary of clinical trials.", "keywords": ["rice fortification", "technologies", "nutrients", "vitamins", "minerals"]}, {"id": "406", "title": "A perceptual mapping procedure for analysis of proximity data to determine common and unique product-market structures", "abstract": "Traditional techniques of perceptual mapping hypothesize that products are differentiated in a common perceptual space of attributes. This paper suggests that each product is differentiated not only in a common perceptual space, but also a unique perceptual space consisting of as many dimensions as the number of products. It provides a model and estimation procedure based on alternating least squares for estimating the model parameters.", "keywords": ["product differentiation", "product uniqueness", "brand image", "three-way data", "multidimensional scaling", "proximities"]}, {"id": "407", "title": "On the Design and Prototype Implementation of a Multimodal Situation Aware System", "abstract": "In this paper we describe the design concepts and prototype implementation of a situation aware ubiquitous computing system using multiple modalities such as National Marine Electronics Association (NMEA) data from global positioning system (GPS) receivers, text, speech, environmental audio, and handwriting inputs. While most mobile and communication devices know where and who they are, by accessing context information primarily in the form of location, time stamps, and user identity, the concept of sharing of this information in a reliable and intelligent fashion is crucial in many scenarios. A framework which takes the concept of context aware computing to the level of situation aware computing by intelligent information exchange between context aware devices is designed and implemented in this work. Four sensual modes of contextual information like text, speech, environmental audio, and handwriting are augmented to conventional contextual information sources like location from GPS, user identity based on IP addresses (IPA), and time stamps. Each device derives its context not necessarily using the same criteria or parameters but by employing selective fusion and fission of multiple modalities. The processing of each individual modality takes place at the client device followed by the summarization of context as a text file. Exchange of dynamic context information between devices is enabled in real time to create multimodal situation aware devices. A central repository of all user context profiles is also created to enable self-learning devices in the future. Based on the results of simulated situations and real field deployments it is shown that the use of multiple modalities like speech, environmental audio, and handwriting inputs along with conventional modalities can create devices with enhanced situational awareness.", "keywords": ["bidirectional ftp synchronization", "environmental audio", "gps", "speech recognition", "ubiquitous computing"]}, {"id": "408", "title": "Neural-association of microcalcification patterns for their reliable classification in digital mammography", "abstract": "Breast cancer continues to be the most common cause of cancer deaths in women. Early detection of breast cancer is significant for better prognosis. Digital Mammography currently offers the best control strategy for the early detection of breast cancer. The research work in this paper investigates the significance of neural-association of microcalcification patterns for their reliable classification in digital mammograms. The proposed technique explores the auto-associative abilities of a neural network approach to regenerate the composite of its learned patterns most consistent with the new information, thus the regenerated patterns can uniquely signify each input class and improve the overall classification. Two types of features: computer extracted (gray level based statistical) features and human extracted (radiologists' interpretation) features are used for the classification of calcification type of breast abnormalities. The proposed technique attained the highest 90.5% classification rate on the calcification testing dataset.", "keywords": ["neural networks", "auto-associator", "classifier", "feature extraction", "digital mammography"]}, {"id": "409", "title": "Modeling hemodynamics in an unoccluded and partially occluded inferior vena cava under rest and exercise conditions", "abstract": "Pulmonary embolism is the third leading cause of death in hospitalized patients in the US. Vena cava filters are medical devices inserted into the inferior vena cava (IVC) and are designed to trap thrombi before they reach the lungs. Once trapped in a filter, however, thrombi disturb otherwise natural flow patterns, which may be clinically significant. The goal of this work is to use computational modeling to study the hemodynamics of an unoccluded and partially occluded IVC under rest and exercise conditions. A realistic, three-dimensional model of the IVC, iliac, and renal veins represents the vessel geometry and spherical clots represent thombi trapped by several conical filter designs. Inflow rates correspond to rest and exercise conditions, and a transitional turbulence model captures transitional flow features, if they are present. The flow equations are discretized and solved using a second-order finite-volume method. No significant regions of transitional flow are observed. Nonetheless, the volume of stagnant and recirculating flow increases with partial occlusion and exercise. For the partially occluded vessel, large wall shear stresses are observed on the IVC and on the model thrombus, especially under exercise conditions. These large wall shear stresses may have mixed clinical implications: thrombotic-like behavior may initiate on the vessel wall, which is undesirable; and thrombolysis may be accelerated, which is desirable.", "keywords": ["cfd", "filter", "thrombosis", "vena cava", "wall shear stress"]}, {"id": "410", "title": "Recent advances in direct methods for solving unsymmetric sparse systems of linear equations", "abstract": "During the past few years, algorithmic improvements alone have reduced the time required for the direct solution of unsymmetric sparse systems of linear equations by almost an order of magnitude. This paper compares the performance of some well-known software packages for solving general sparse systems. In particular, it demonstrates the consistently high level of performance achieved by WSMP-the most recent of such solvers. It compares the various algorithmic components of these solvers and discusses their impact on solver performance. Our experiments show that the algorithmic choices made in WSMP enable it to run more than twice as fast as the best among similar solvers and that WSMP can factor some of the largest sparse matrices available from real applications in only a few seconds on a 4-CPU workstation. Thus, the combination of advances in hardware and algorithms makes it possible to solve those general sparse linear systems quickly and easily that might have been considered too large until recently.", "keywords": ["algorithms", "performance", "sparse matrix factorization", "sparse lu decomposition", "multifrontal method", "parallel sparse solvers"]}, {"id": "411", "title": "A linear goal programming approach to determining the relative importance weights of customer requirements in quality function deployment", "abstract": "Quality function deployment (QFD) is a planning tool used in new product development and quality management. It aims at achieving maximum customer satisfaction by listening to the voice of customers. To implement QFD, customer requirements (CRs) should be identified and assessed first. The current paper proposes a linear goal programming (LGP) approach to assess the relative importance weights of CRs. The LGP approach enables customers to express their preferences on the relative importance weights of CRs in their preferred or familiar formats, which may differ from one customer to another but have no need to be transformed into the same format, thus avoiding information loss or distortion. A numerical example is tested with the LGP approach to demonstrate its validity, effectiveness and potential applications in QFD practice.  ", "keywords": ["quality function deployment", "customer requirement", "customer preference", "preference format", "goal programming", "group decision making"]}, {"id": "412", "title": "Local, deformable precomputed radiance transfer", "abstract": "Precomputed radiance transfer (PRT) captures realistic lighting effects from distant, low-frequency environmental lighting but has been limited to static models or precomputed sequences. We focus on PRT for local effects such as bumps, wrinkles, or other detailed features, but extend it to arbitrarily deformable models. Our approach applies zonal harmonics (ZH) which approximate spherical functions as sums of circularly symmetric Legendre polynomials around different axes. By spatially varying both the axes and coefficients of these basis functions, we can fit to spatially varying transfer signals. Compared to the spherical harmonic (SH) basis, the ZH basis yields a more compact approximation. More important, it can be trivially rotated whereas SH rotation is expensive and unsuited for dense per-vertex or per-pixel evaluation. This property allows, for the first time, PRT to be mapped onto deforming models which re-orient the local coordinate frame. We generate ZH transfer models by fitting to PRT signals simulated on meshes or simple parametric models for thin membranes and wrinkles. We show how shading with ZH transfer can be significantly accelerated by specializing to a given lighting environment. Finally, we demonstrate real-time rendering results with soft shadows, inter-reflections, and subsurface scatter on deforming models.", "keywords": ["lighting environments", "nonlinear optimization", "spherical", "harmonics", "soft shadows", "subsurface scattering", "texture maps", "zonal harmonics"]}, {"id": "413", "title": "A mimetic mass, momentum and energy conserving discretization for the shallow water equations", "abstract": "A spatial semi-discretization is developed for the two-dimensional depth-averaged shallow water equations on a non-equidistant structured and staggered grid. The vector identities required for energy conservation in the continuous case are identified. Discrete analogues are developed, which lead to a finite-volume semi-discretisation which conserves mass, momentum, and energy simultaneously. The key to discrete energy conservation for the shallow water equations is to numerically distinguish storage of momentum from advective transport of momentum. Simulation of a large-amplitude wave in a basin confirms the conservative properties of the new scheme, and demonstrates the enhanced robustness resulting from the compatibility of continuity and momentum equations. The scheme can be used as a building block for constructing fully conservative curvilinear, higher order, variable density, and non-hydrostatic discretizations.  ", "keywords": ["shallow water equations", "energy-conservation", "fully conservative", "mimetic", "finite-volume", "finite-difference", "symmetry preservation", "staggered", "c-grid"]}, {"id": "414", "title": "3d object search through semantic component", "abstract": "In this paper, we present a novel concept named semantic component for 3D object search which describes a key component that semantically defines a 3D object. In most cases, the semantic component is intra-category stable and therefore can be used to construct an efficient 3D object retrieval scheme. By segmenting an object into segments and learning the similar segments shared by all the objects in the same category, we can summarise what human uses for object recognition, from the analysis of which we develop a method to find the semantic component of an object. In our experiments, the proposed method is justified and the effectiveness of our algorithm is also demonstrated.", "keywords": ["semantic component", "3d object search"]}, {"id": "415", "title": "Formal semantics and efficient analysis of Timed Rebeca in Real-Time Maude", "abstract": "Provides an executable formal Real-Time Maude semantics for Timed Rebeca. Integrates Real-Time Maude analysis into the Rebeca toolchain. Provides an efficient semantics using partial-order-reduction-like techniques. Shows the performance gained by this optimization.", "keywords": ["real-time actors", "timed rebeca", "formal semantics", "model checking", "real-time maude"]}, {"id": "416", "title": "A new bonding-tool solution to improve stitch bondability", "abstract": "A new bonding-tool solution is proposed to improve stitch bondability by creating a new surface morphology on the tip surface of a wire-bonding tool (capillary). The surface has relatively deep lines with no fixed directions. This new capillary has less slipping between the wire and the capillary tip surface and provides better coupling effect between them. Experiments of wire bonding on unstable lead frames/substrates, alloyed wire (2N gold wire) bonding, and copper wire bonding were carried out to confirm the effect of the new capillary on the stitch bondability. The experimental results are promising and have proved that the use of the new capillary could improve the bondability of the stitch bond and minimize the occurrence of short tail defects and non-sticking on lead during bonding.", "keywords": ["microelectronics packaging", "wire bond", "stitch bondability", "capillary", "copper wire bonding"]}, {"id": "417", "title": "optimal instruction scheduling using integer programming", "abstract": "This paper presents a new approach to local instruction scheduling based on integer programming that produces optimal instruction schedules in a reasonable time, even for very large basic blocks. The new approach first uses a set of graph transformations to simplify the data-dependency graph while preserving the optimality of the final schedule. The simplified graph results in a simplified integer program which can be solved much faster. A new integer-programming formulation is then applied to the simplified graph. Various techniques are used to simplify the formulation, resulting in fewer integer-program variables, fewer integer-program constraints and fewer terms in some of the remaining constraints, thus reducing integer-program solution time. The new formulation also uses certain adaptively added constraints (cuts) to reduce solution time. The proposed optimal instruction scheduler is built within the Gnu Compiler Collection (GCC) and is evaluated experimentally using the SPEC95 floating point benchmarks. Although optimal scheduling for the target processor is considered intractable, all of the benchmarks' basic blocks are optimally scheduled, including blocks with up to 1000 instructions, while total compile time increases by only 14%.", "keywords": ["processor", "variability", "instruction scheduling", "scheduling", "optimality", "benchmark", "data dependence", "collect", "instruction", "compilation", "graph", "floating point", "timing", "graph transformation", "paper", "constraint", "integer programming", "locality"]}, {"id": "418", "title": "Web Services Discovery with Rough Sets", "abstract": "Web services are emerging as a major technology for building service-oriented distributed systems. Potentially, various resources on the Internet can be virtualized as Web services for a wider use by their communities. Service discovery becomes an issue of vital importance for Web services applications. This article presents ROSSE, a Rough Sets based Search Engine for Web service discovery. One salient feature of ROSSE lies in its capability to deal with uncertainty of service properties when matching services. A use case is presented to demonstrate the use of ROSSE for discovery of car services. ROSSE is evaluated in terms of its accuracy and efficiency in service discovery.", "keywords": ["owl-s", "rough sets", "service matchmaking", "web service discovery"]}, {"id": "419", "title": "Stability-Optimized Time Adjustment for a Networked Computer Clock", "abstract": "We propose an optimal time adjustment method from the viewpoint of frequency stability, which is defined as the Allan deviation. When time adjustment is needed for a clock in a networked computer, it is made over a period called a time adjustment period. The proposed method optimizes frequency stability for a given time adjustment period. This method has been evaluated and compared with the adjtime() system call in UNIX systems in terms of frequency stability and duration of time adjustment period needed for achieving particular values of frequency stability. For time intervals from 1 to 1; 000 s, the frequency stability achieved by the proposed method was about 0.01-0.5 of that achieved by the adjtime() system call. The evaluation also showed that the duration of a time adjustment period needed for achieving the frequency stability of 1.0 x 10(-10) in the proposed method was less than 1/12 (1/6) that of the period in the adjtime() system call when we optimized frequency stability for a 60 (3,600) s time interval under the condition that the duration of the time-adjustment period was 12 h.", "keywords": ["clock synchronization", "allan deviation", "allan variance", "frequency stability"]}, {"id": "420", "title": "Improving mobile services design: A QFD approach", "abstract": "The quality of mobile services in the mobile and wireless world is ultimately judged in terms of customer satisfaction. This is particularly true for the third generation (3G) and beyond multimedia mobile services which should meet or exceed customer expectations. In this study Quality Function Deployment (QFD) is used for the first time as a quality improvement approach for building customers' requirements into mobile services. Traditionally QFD approach is adopted in product and manufacturing industries. In this paper QFD is extended to mobile service industry which is such a promising industry in today's information society. This paper proposes a generic framework based on QFD concepts and practices to improve mobile service design and development. An example is presented to illustrate the use of QFD for mobile e-learning services for university students and lecturers. The data transmission speed is found to be the most critical requirement in mobile e-learning services. By the use of QFD the developed mobile services can best meet customers' requirements or even exceed their expectations. At the end of this paper some benefits as well as further improvements regarding QFD approach are discussed and concluded.", "keywords": ["mobile service", "e-learning", "quality function deployment", "customer requirement", "voice of customer", "house of quality"]}, {"id": "421", "title": "International research collaborations of ASEAN Nations in economics, 19792010", "abstract": "This study examines the research performance and international research collaborations (IRC) of ASEAN nations in the area of economics. Over the last 3 decades international collaborated papers have increased in the region, while locally-co-authored papers have declined. Singapore towered among ASEAN nations in research efficiency based on geographical area, population and GDP. Vietnam performed relatively better in research efficiency than research productivity (number of papers produced), while Indonesia performed poorly. Overall, internationally co-authored papers were cited twice as often as locally authored papers except that both The Philippines and Indonesia exhibited almost no difference in how their local and internationally co-authored papers were cited. The study also examined IRC from the perspective of social networks. Centrality had a strong correlation with research performance; however, vertex tie-strength (a result of repeat collaboration) showed maximum correlation with research performance. While Malaysia emerged as the nation with the highest betweenness centrality or bridging power, the US emerged as the most favoured international partner of ASEAN nations. However, collaboration between ASEAN countries accounted for just 4% of all international collaborations. Increased academic mobility and more joint scientific works are suggestions to consider to boost educational co-operation among the ASEAN nations.", "keywords": ["international research collaborations", "research efficiency", "social networks", "asean", "economics"]}, {"id": "422", "title": "Automated assistants for analyzing team behaviors", "abstract": "Multi-agent teamwork is critical in a large number of agent applications, including training, education, virtual enterprises and collective robotics. The complex interactions of agents in a team as well as with other agents make it extremely difficult for human developers to understand and analyze agent-teambehavior. It has thus become increasingly important to develop tools that can help humans analyze, evaluate, and understand team behaviors. However, the problem of automated team analysis is largely unaddressed in previous work. In this article, we identify several key constraints faced by team analysts. Most fundamentally, multiple types of models of team behavior are necessary to analyze different granularities of team events, including agent actions, interactions, and global performance. In addition, effective ways of presenting the analysis to humans is critical and the presentation techniques depend on the model being presented. Finally, analysis should be independent of underlying team architecture and implementation. We also demonstrate an approach to addressing these constraints by building an automated team analyst called ISAAC for post-hoc, off-line agent-team analysis. ISAAC acquires multiple, heterogeneous team models via machine learning over teams' external behavior traces, where the specific learning techniques are tailored to the particular model learned. Additionally, ISAAC employs multiple presentation techniques that can aid human understanding of the analyses. ISAAC also provides feedback on team improvement in two novel ways: (i) It supports principled \"what-if'' reasoning about possible agent improvements; (ii) It allows the user to compare different teams based on their patterns of interactions. This paper presents ISAAC's general conceptual framework, motivating its design, as well as its concrete application in two domains: ( i) RoboCup Soccer; ( ii) software agent teams participating in a simulated evacuation scenario. In the RoboCup domain, ISAAC was used prior to and during the RoboCup '99 tournament, and was awarded the RoboCup Scientific Challenge Award. In the evacuation domain, ISAAC was used to analyze patterns of message exchanges among software agents, illustrating the generality of ISAAC's techniques. We present detailed algorithms and experimental results from ISAAC's application.", "keywords": ["teamwork", "analysis", "multiagent systems"]}, {"id": "423", "title": "Adaptive clustering in wireless sensor networks by mining sensor energy data", "abstract": "Clustering has been well received as one of the effective solutions to enhance energy efficiency and scalability of large-scale wireless sensor networks. The goal of clustering is to identify a subset of nodes in a wireless sensor network, then all the other nodes communicate with the network sink via these selected nodes. However, many current clustering algorithms are tightly coupled with exact sensor locations derived through either triangulation methods or extra hardware such as GPS equipment. However, in practice, it is very difficult to know sensor location coordinates accurately due to various factors such as random deployment and low-power, low-cost sensing devices. Therefore, how to develop an adaptive clustering algorithm without relying on exact sensor location information is a very important yet challenging problem. In this paper, we try to address this problem by proposing a new adaptive clustering algorithm for energy efficiency of wireless sensor networks. Compared with other work having been done in this area, our proposed adaptive clustering algorithm is original because of its capability to infer the location information by mining wireless sensor energy data. Furthermore, based on the inferred location information and the remaining (residual) energy level of each node, the proposed clustering algorithm will dynamically change cluster heads for energy efficacy. Simulation results show that the proposed adaptive clustering algorithm is efficient and effective for energy saving in wireless sensor networks.  ", "keywords": ["adaptive clustering", "wireless sensor networks", "network management", "data mining"]}, {"id": "424", "title": "To feature space and back: Identifying top-weighted features in polynomial Support Vector Machine models", "abstract": "Polynomial Support Vector Machine models of degree d are linear functions in a feature space of monomials of at most degree d. However, the actual representation is stored in the form of support vectors and Lagrange multipliers that is unsuitable for human understanding. An efficient, heuristic method for searching the feature space of a polynomial Support Vector Machine model for those features with the largest absolute weights is presented. The time complexity of this method is Theta(dms(2) + sdp), where m is the number of variables, d the degree of the kernel, s the number of support vectors, and p the number of features the algorithm is allowed to search. In contrast, the brute force approach of constructing all weights and then selecting the largest weights has complexity Theta(sd((m+d)(d))). The method is shown to be effective in identifying the top-weighted features on several simulated data sets, where the true weight vector is known. Additionally, the method is run on several high-dimensional, real world data sets where the features returned may be used to construct classifiers with classification performances similar to models built with all or subsets of variables returned by variable selection methods. This algorithm provides a new ability to understand, conceptualize, visualize, and communicate polynomial SVM models and has implications for feature construction, dimensionality reduction, and variable selection.", "keywords": ["support vector machines", "classification", "variable selection"]}, {"id": "425", "title": "Sound direction estimation using an artificial ear for robots", "abstract": "We propose a novel design of an artificial robot ear for sound direction estimation using two measured outputs only. The spectral features in the interaural transfer functions (ITFs) of the proposed artificial ears are distinctive and move monotonically according to the sound direction. Thus, these features provide effective sound cues to estimate sound direction using the measured two output signals. Bilateral asymmetry of microphone positions can enhance the estimation performance even in the median plane where interaural differences vanish. We propose a localization method to estimate the lateral and vertical angles simultaneously. The lateral angle is estimated using interaural time difference and Woodworth and Schlosberg's formula, and the front-back discrimination is achieved by finding the spectral features in the ITF estimated from two measured outputs. The vertical angle of a sound source in the frontal region is estimated by comparing the spectral features in the estimated ITF with those in the database built in an anechoic chamber. The feasibility of the designed artificial ear and the estimation method were verified in a real environment. In the experiment, it was shown that both the front-back discrimination and the sound direction estimation in the frontal region can be achieved with reasonable accuracy. Thus, we expect that robots with the proposed artificial ear can estimate the direction of speaker from two output signals only.  ", "keywords": ["sound direction estimation", "artificial ear", "human-robot interaction", "head-related transfer function", "interaural transfer function"]}, {"id": "426", "title": "Alcohol, Type of Alcohol, and All-Cause and Coronary Heart Disease Mortality", "abstract": "Many studies from a variety of countries have shown a U- or J-shaped relation between alcohol intake and mortality from all causes. It is now quite well documented from epidemiologic as well as clinical and experimental studies that the descending leg of the curve results from a decreased risk of cardiovascular disease among those with light-to-moderate alcohol consumption. The findings that wine drinkers are at a decreased risk of mortality from cardiovascular disease compared to non-wine drinkers suggest that substances present in wine are responsible for a beneficial effect on the outcome, in addition to that from a light intake of ethanol. Several potential confounding factors still remain to be excluded, however.", "keywords": ["coronary heart disease", "alcohol intake", "flavonoids", "wine"]}, {"id": "427", "title": "Rigorous development of an embedded fault-tolerant system based on coordinated atomic actions", "abstract": "This paper describes our experience using coordinated atomic (CA) actions as a system structuring tool to design and validate a sophisticated and embedded control system for a complex industrial application that has high reliability and safety requirements. Our study is based on an extended production cell model, the specification and simulator for which were defined and developed by FZI (Forschungszentrum Informatik, Germany). This \"Fault-Tolerant Production Cell\" represents a manufacturing process involving redundant mechanical devices (provided in order to enable continued production in the presence of machine faults). The challenge posed by the model specification is to design a control system that maintains specified safety and liveness properties even in the presence of a large number and variety of device and sensor failures. Based on an analysis of such failures, we provide in this paper details of: 1) a design for a control program that uses CA actions to deal with both safety-related and fault tolerance concerns and 2) the formal verification of this design based on the use of model-checking. We found that CA action structuring facilitated both the design and verification tasks by enabling the various safety problems (involving possible clashes of moving machinery) to be treated independently. Even complex situations involving the concurrent occurrence of any pairs of the many possible mechanical and sensor failures can be handled simply yet appropriately. The formal verification activity was performed in parallel with the design activity and the interaction between them resulted in a combined exercise in \"design for validation\"; formal verification was very valuable in identifying some very subtle residual bugs in early versions of our design which would have been difficult to detect otherwise.", "keywords": ["concurrency", "coordinated atomic  actions", "embedded fault-tolerant systems", "exception handling", "object orientation", "formal verification", "model checking", "reliability", "safety"]}, {"id": "428", "title": "Ventricular shape visualization using selective volume rendering of cardiac datasets", "abstract": "In this paper, we present a novel technique of improving volume rendering quality and speed by integrating original volume data and global model information attained by segmentation. The segmentation information prevents object occlusions that may appear when volume rendering is based on local image features only. Thus the presented visualization technique provides meaningful visual results that enable a clear understanding of complex anatomical structures. In the first part, we describe a segmentation technique for extracting the region of interest based on an active contour model. In the second part, we propose a volume rendering method for visualizing the selected portions of fuzzy surfaces extracted by local image processing methods. We show the results of selective volume rendering of left and right ventricle based on cardiac datasets from clinical routines. Our method offers an accelerated technique to accurately visualize the surfaces of segmented objects.", "keywords": ["cardiology", "medical imaging", "selective volume rendering", "direct volume rendering", "distance transform", "segmentation"]}, {"id": "429", "title": "Mutual information for Lucas-Kanade tracking (MILK): An inverse compositional formulation", "abstract": "Mutual Information (MI) is popular for registration via function optimization. This work proposes an inverse compositional formulation of MI for Levenberg-Marquardt optimization. This yields a constant Hessian, which may be precomputed. Speed improvements of 15 percent were obtained, with convergence accuracies similar those of the standard formulation.", "keywords": ["mutual information", "registration", "newton optimization", "tracking"]}, {"id": "430", "title": "A Comparative Study of Software Model Checkers as Unit Testing Tools: An Industrial Case Study", "abstract": "Conventional testing methods often fail to detect hidden flaws in complex embedded software such as device drivers or file systems. This deficiency incurs significant development and support/maintenance cost for the manufacturers. Model checking techniques have been proposed to compensate for the weaknesses of conventional testing methods through exhaustive analyses. Whereas conventional model checkers require manual effort to create an abstract target model, modern software model checkers remove this overhead by directly analyzing a target C program, and can be utilized as unit testing tools. However, since software model checkers are not fully mature yet, they have limitations according to the underlying technologies and tool implementations, potentially critical issues when applied in industrial projects. This paper reports our experience in applying Blast and CBMC to testing the components of a storage platform software for flash memory. Through this project, we analyzed the strong and weak points of two different software model checking technologies in the viewpoint of real-world industrial application-counterexample-guided abstraction refinement with predicate abstraction and SAT-based bounded analysis.", "keywords": ["embedded software verification", "software model checking", "bounded model checking", "cegar-based model checking", "flash file systems"]}, {"id": "431", "title": "CMATERdb1: a database of unconstrained handwritten Bangla and Bangla-English mixed script document image", "abstract": "In this paper, we have described the preparation of a benchmark database for research on off-line Optical Character Recognition (OCR) of document images of handwritten Bangla text and Bangla text mixed with English words. This is the first handwritten database in this area, as mentioned above, available as an open source document. As India is a multi-lingual country and has a colonial past, so multi-script document pages are very much common. The database contains 150 handwritten document pages, among which 100 pages are written purely in Bangla script and rests of the 50 pages are written in Bangla text mixed with English words. This database for off-line-handwritten scripts is collected from different data sources. After collecting the document pages, all the documents have been preprocessed and distributed into two groups, i.e., CMATERdb1.1.1, containing document pages written in Bangla script only, and CMATERdb1.2.1, containing document pages written in Bangla text mixed with English words. Finally, we have also provided the useful ground truth images for the line segmentation purpose. To generate the ground truth images, we have first labeled each line in a document page automatically by applying one of our previously developed line extraction techniques [Khandelwal et al., PReMI 2009, pp. 369-374] and then corrected any possible error by using our developed tool GT Gen 1.1. Line extraction accuracies of 90.6 and 92.38% are achieved on the two databases, respectively, using our algorithm. Both the databases along with the ground truth annotations and the ground truth generating tool are available freely at http://code.google.com/p/cmaterdb.", "keywords": ["unconstrained handwritten document image database", "text line extraction", "ground truth preparation", "ocr of multi-script document"]}, {"id": "432", "title": "A time domain boundary element method for modeling the quasi-static viscoelastic behavior of asphalt pavements", "abstract": "A time domain boundary element method (BEM) is presented to model the quasi-static linear viscoelastic behavior of asphalt pavements. In the viscoelastic analysis, the fundamental solution is derived in terms of elemental displacement discontinuities (DDs) and a boundary integral equation is formulated in the time domain. The unknown DDs are assumed to vary quadratically in the spatial domain and to vary linearly in the time domain. The equation is then solved incrementally through the whole time history using an explicit time-marching approach. All the spatial and temporal integrations can be performed analytically, which guarantees the accuracy of the method and the stability of the numerical procedure. Several viscoelastic models such as Boltzmann, Burgers, and power-law models are considered to characterize the time-dependent behavior of linear viscoelastic materials. The numerical method is applied to study the load-induced stress redistribution and its effects on the cracking performance of asphalt pavements. Some benchmark problems are solved to verify the accuracy and efficiency of the approach. Numerical experiments are also carried out to demonstrate application of the method in pavement engineering.", "keywords": ["time domain bem", "viscoelasticity", "displacement discontinuities", "time marching", "viscoelastic models", "asphalt pavements", "stress redistribution"]}, {"id": "433", "title": "A decision support system for animated film selection based on a multi-criteria aggregation of referees' ordinal preferences", "abstract": "This paper presents a decision support system devoted to the selection of films for the International Animated Film Festival organized at Annecy, France, every year. It deals with the representation and aggregation of referees' preferences along predefined criteria in addition to their overall selection point of view. The practical requirements associated with this application (often encountered in social or cultural areas as well) are: a common ordinal scale for the criteria scores, a procedure to deal with inconsistencies between criteria and overall scores, explanation tools of each referee's preference model in order to facilitate the deliberation process and also to argument the selection decision. The processing of referees' preferences is achieved thanks to a recent method which consists in finding a generalized mean aggregation operator representing the preferences of a referee, in a finite ordinal scale context. The method allows to deal with consistency conditions on referees' behaviour in order to highlight the criteria or pair of criteria which are the most influential for each of the referees. All the functionalities have been implemented in an interactive decision software that facilitates a shared selection decision. Results issued from the 2007 selection are presented and analysed from the preference representation and processing point of view.  ", "keywords": ["animated film selection", "ordinal scale", "expert preference representation", "multi-criteria aggregation"]}, {"id": "434", "title": "Semantic Web search based on rough sets and Fuzzy Formal Concept Analysis", "abstract": "Fuzzy Formal Concept Analysis (FFCA) is a generalization of Formal Concept Analysis (FCA) for modeling uncertainty information. FFCA provides a mathematical framework which can support the construction of formal ontologies in the presence of uncertainty data for the development of the Semantic Web. In this paper, we show how rough set theory can be employed in combination with FFCA to perform Semantic Web search and discovery of information in the Web.", "keywords": ["semantic web", "formal concept analysis", "fuzzy information", "rough set theory", "formal concept"]}, {"id": "435", "title": "Constrained consensus of asynchronous discrete-time multi-agent systems with time-varying topology", "abstract": "The union graph is assumed to be strongly connected over each finite interval. An approach is proposed to transform the original network to a synchronous one. We show that the linear part converges and the projection error vanishes over time.", "keywords": ["constrained consensus", "multi-agent system", "asynchronous communication", "distributed control"]}, {"id": "436", "title": "Axiomatic theory of intuitionistic fuzzy sets", "abstract": "A Bernays-like axiomatic theory of intuitionistic fuzzy sets involving five primitives and seven axioms is presented.  ", "keywords": ["fuzzy sets", "intuitionistic fuzzy sets", "axiomatic theory"]}, {"id": "437", "title": "Top-down delayering to expose large inspection area on die side-edge with Platinum (Pt) deposition technique", "abstract": "Methodology to increase the flat inspection area Useful in exposition for die side edge. Platinum (Pt) deposition technique to form a protection mask Pt deposition to slow down the edging effect", "keywords": ["top-down polishing method", "large exposition", "edging effect", "platinum  deposition"]}, {"id": "438", "title": "Short-term robustness of production management systems: A case study", "abstract": "Whereas Operations Research concentrates on optimization, practitioners find the robustness of a proposed solution more important. Therefore this paper presents a practical methodology that is a stagewise combination of four proven techniques: (1) simulation, (2) optimization, (3) risk or uncertainty analysis, and (4) bootstrapping. This methodology is illustrated through a production-control study. That illustration defines robustness as the capability to maintain short-term service, in a variety of environments (scenarios); that is, the probability of the short-term fill-rate remains within a prespecified range. Besides satisfying this probabilistic constraint, the system minimizes expected long-term work-in-process. Actually, the example compares four systemsnamely, Kanban, Conwip, Hybrid, and Genericfor the well-known case of a production line with four stations and a single product. The conclusion is that in this particular example, Hybrid is best when risk is not ignored; otherwise Generic is best; that is, risk considerations do make a difference.", "keywords": ["risk analysis", "robustness and sensitivity analysis", "scenarios", "manufacturing", "inventory"]}, {"id": "439", "title": "The variety generated by semi-Heyting chains", "abstract": "The purpose of this paper was to investigate the structure of semi-Heyting chains and the variety ({{mathcal{CSH}}}) generated by them. We determine the number of non-isomorphic n-element semi-Heyting chains. As a contribution to the study of the lattice of subvarieties of ({{mathcal{CSH}}},) we investigate the inclusion relation between semi-Heyting chains. Finally, we provide equational bases for ({{mathcal{CSH}}}) and for the subvarieties of ({{mathcal{CSH}}}) introduced in [5].", "keywords": ["heyting algebras", "varieties", "semi-heyting algebras"]}, {"id": "440", "title": "Stochastic fault tree analysis with self-loop basic events", "abstract": "This paper presents an analytical approach for performing fault tree analysis (FTA) with stochastic self-loop events. The proposed approach uses the flow-graph concept, and moment generating function (MGF) to develop a new stochastic FTA model for computing the probability, mean time to occurrence, and standard deviation time to occurrence of the top event. The application of the method is demonstrated by solving one example.", "keywords": ["fault tree", "flow-graph", "reliability model", "stochastic failure analysis"]}, {"id": "441", "title": "global-view abstractions for user-defined reductions and scans", "abstract": "Since APL, reductions and scans have been recognized as powerful programming concepts. Abstracting an accumulation loop (reduction) and an update loop (scan), the concepts have efficient parallel implementations based on the parallel prefix algorithm. They are often included in high-level languages with a built-in set of operators such as sum, product, min, etc. MPI provides library routines for reductions that account for nearly nine percent of all MPI calls in the NAS Parallel Benchmarks (NPB) version 3.2. Some researchers have even advocated reductions and scans as the principal tool for parallel algorithm design.Also since APL, the idea of applying the reduction control structure to a user-defined operator has been proposed, and several implementations (some parallel) have been reported. This paper presents the first global-view formulation of user-defined scans and an improved global-view formulation of user-defined reductions, demonstrating them in the context of the Chapel programming language. Further, these formulations are extended to a message passing context (MPI), thus transferring global-view abstractions to local-view languages and perhaps signaling a way to enhance local-view languages incrementally. Finally, examples are presented showing global-view user-defined reductions \"cleaning up\" and/or \"speeding up\" portions of two NAS benchmarks, IS and MG. In consequence, these generalized reduction and scan abstractions make the full power of the parallel prefix technique available to both global- and local-view parallel programming.", "keywords": ["reductions", "mpi", "parallel programming", "scans", "parallel prefix", "chapel"]}, {"id": "442", "title": "Partial Derivative Guidance for Weak Classifier Mining in Pedestrian Detection", "abstract": "Boosting over weak classifiers is widely used in pedestrian detection. As the number of weak classifiers is large, researchers always use a sampling method over weak classifiers before training. The sampling makes the boosting process harder to reach the fixed target. In this paper, we propose a partial derivative guidance for weak classifier mining method which can be used in conjunction with a boosting algorithm. Using weak classifier mining method makes the sampling less degraded in the performance. It has the same effect as testing more weak classifiers while using acceptable time. Experiments demonstrate that our algorithm can process quicker than [1] algorithm in both training and testing, without any performance decrease. The proposed algorithms is easily extending to any other boosting algorithms using a window-scanning style and HOG-like features.", "keywords": ["pedestrian detection", "partial derivative", "classifier mining", "hog", "boosting"]}, {"id": "443", "title": "the role of operators in apl", "abstract": "Operators, which apply to functions to produce functions, are an important component of APL. Despite their importance, their role is not well understood, and they are often lumped with functions in expositions of the language. This paper attempts to clarify the role of operators in APL by tracing their development, outlining possible future directions, and commenting briefly on their roles in other languages, both natural and programming.", "keywords": ["role", "program", "direct", "operability", "traces", "paper", "future", "developer", "language", "roles", "component"]}, {"id": "444", "title": "Theoretical modeling of micro-scale biological phenomena in human coronary arteries", "abstract": "This paper presents a mathematical model of biological structures in relation to coronary arteries with atherosclerosis. A set of equations has been derived to compute blood flow through these transport vessels with variable axial and radial geometries. Three-dimensional reconstructions of diseased arteries from cadavers have shown that atherosclerotic lesions spiral through the artery. The theoretical framework is able to explain the phenomenon of lesion distribution in a helical pattern by examining the structural parameters that affect the flow resistance and wall shear stress. The study is useful for connecting the relationship between the arterial wall geometries and hemodynamics of blood. It provides a simple, elegant and non-invasive method to predict flow properties for geometrically complex pathology at micro-scale levels and with low computational cost.", "keywords": ["atherosclerosis", "axial and radial asymmetry", "spiraling lesion", "resistance to flow ratio", "wall shear stress"]}, {"id": "445", "title": "Unanticipated partial behavioral reflection: Adapting applications at runtime", "abstract": "Dynamic, unanticipated adaptation of running systems is of interest in a variety of situations, ranging from functional upgrades to on-the-fly debugging or monitoring of critical applications. In this paper we study a particular form of computational reflection, called unanticipated partial behavioral reflection (UPBR), which is particularly well suited for unanticipated adaptation of real-world systems. Our proposal combines the dynamicity of unanticipated reflection, i.e., reflection that does not require preparation of the code of any sort, and the selectivity and efficiency of partial behavioral reflection (PBR). First, we propose unanticipated partial behavioral reflection which enables the developer to precisely select the required reifications, to flexibly engineer the metalevel and to introduce the metabehavior dynamically. Second, we present a system supporting unanticipated partial behavioral reflection in Squeak Smalltalk, called GEPPETTO, and illustrate its use with a concrete example of a web application. Benchmarks validate the applicability of our proposal as an extension to the standard reflective abilities of Smalltalk.", "keywords": ["reflection", "metaprogramming", "metaobject protocol", "smalltalk"]}, {"id": "446", "title": "A weighted additive fuzzy programming approach for multi-criteria supplier selection", "abstract": "In supply chain management, to build strategic and strong relationships, firms should select best suppliers by applying appropriate method and selection criteria. In this paper, to handle ambiguity and fuzziness in supplier selection problem effectively, a new weighted additive fuzzy programming approach is developed. Firstly, linguistic values expressed as trapezoidal fuzzy numbers are used to assess the weights of the factors. By applying the distances of each factor between Fuzzy Positive Ideal Rating and Fuzzy Negative Ideal Rating, weights are obtained. Then applying suppliers' constraints, goals and weights of the factors, a fuzzy multi-objective linear model is developed to overcome the selection problem and assign optimum order quantities to each supplier. The proposed model is explained by a numerical example.  ", "keywords": ["supplier selection", "fuzzy multi-objective linear model", "linguistic variables", "multi-criteria decision making"]}, {"id": "447", "title": "FireGrid: An e-infrastructure for next-generation emergency response support", "abstract": "The FireGrid project aims to harness the potential of advanced forms of computation to support the response to large-scale emergencies (with an initial focus on the response to fires in the built environment). Computational models of physical phenomena are developed, and then deployed and computed on High Performance Computing resources to infer incident conditions by assimilating live sensor data from an emergency in real timeor, in the case of predictive models, faster-than-real time. The results of these models are then interpreted by a knowledge-based reasoning scheme to provide decision support information in appropriate terms for the emergency responder. These models are accessed over a Grid from an agent-based system, of which the human responders form an integral part. This paper proposes a novel FireGrid architecture, and describes the rationale behind this architecture and the research results of its application to a large-scale fire experiment.", "keywords": ["emergency response", "grid", "high performance computing", "multi-agent system", "knowledge-based reasoning", "fire simulation model"]}, {"id": "448", "title": "A branch and bound algorithm for minimizing total completion time on a single batch machine with incompatible job families and dynamic arrivals", "abstract": "In this paper, we consider a single batch machine scheduling problem with incompatible job families and dynamic job arrivals. The objective is to minimize the total completion time. This problem is known to be strongly NP-hard. We present several dominance properties and two types of lower bounds, which are incorporated to construct a basic branch and bound algorithm. Furthermore, according to the characteristics of dynamic job arrivals, a decomposed branch and bound algorithm is proposed to improve the efficiency. The proposed algorithms are tested on a large set of randomly generated problem instances.", "keywords": ["branch and bound algorithm", "dynamic arrivals", "batch scheduling", "incompatible job families"]}, {"id": "449", "title": "Predicting saturates of sour vacuum gas oil using artificial neural networks and genetic algorithms", "abstract": "Accurate predictions of chemical composition by physical properties of sour vaccum gas oil (VGO) fractions are important for the refinery. In this paper, a feed-forward type network based on genetic algorithm (GA), was developed and used for predicting saturates of sour vacuum gas oil. The number of neurons in the hidden layer, the momentum and the learning rates were determined by using the genetic algorithm. The five physical properties of sour VGO, namely, average boiling point, density at 20C, molecular weight, kinematic viscosity at 100C and refractive index at 70C were considered as input variables of the ANN and the saturates of sour VGO was used as output variable. The study shows that genetic algorithm could find the optimal networks architecture and parameters of the back-propagation algorithm. Further, the artificial neural network models based on genetic algorithm are tested and the results indicate that the adopted model is very suitable for the forecasting of saturates of sour VGO. Compared with other forecasting models, it can be found that this model can improve prediction accuracy.", "keywords": ["saturates", "sour", "vacuum gas oil", "prediction", "artificial neural networks", "genetic algorithm"]}, {"id": "450", "title": "Implementing a hardware-embedded reactive agents platform based on a service-oriented architecture over heterogeneous wireless sensor networks", "abstract": "Wireless Sensor Networks (WSNs) represent a key technology for collecting important information from different sources in context-aware environments. Unfortunately, integrating devices from different architectures or wireless technologies into a single sensor network is not an easy task for designers and developers. In this sense, distributed architectures, such as service-oriented architectures and multi-agent systems, can facilitate the integration of heterogeneous sensor networks. In addition, the sensors capabilities can be expanded by means of intelligent agents that change their behavior dynamically. This paper presents the Hardware-Embedded Reactive Agents (HERA) platform. HERA is based on Services laYers over Light PHysical devices (SYLPH), a distributed platform which integrates a service-oriented approach into heterogeneous WSNs. As SYLPH, HERA can be executed over multiple devices independently of their wireless technology, their architecture or the programming language they use. However, HERA goes one step ahead of SYLPH and adds reactive agents to the platform and also a reasoning mechanism that provides HERA Agents with Case-Based Planning features that allow solving problems considering past experiences. Unlike other approaches, HERA allows developing applications where reactive agents are directly embedded into heterogeneous wireless sensor nodes with reduced computational resources.", "keywords": ["distributed architectures", "multi-agent systems", "heterogeneous wireless sensor networks", "embedded agents", "case-based planning"]}, {"id": "451", "title": "efficient algorithms for stream mining of constrained frequent patterns in a limited memory environment", "abstract": "As technology advances, streams of data can be rapidly generated in many real-life applications. This calls for stream mining, which searches for implicit, previously unknown, and potentially useful information---such as frequent patterns---that might be embedded in continuous data streams. However, most of the existing algorithms do not allow users to express the patterns to be mined according to their intentions, via the use of constraints. As a result, these unconstrained mining algorithms can yield numerous patterns that are not interesting to the users. Moreover, many existing tree-based algorithms assume that all the trees constructed during the mining process can fit into memory. While this assumption holds for many situations, there are many other situations in which it does not hold. Hence, in this paper, we develop efficient algorithms for stream mining of constrained frequent patterns in a limited memory environment . Our algorithms allow users to impose a certain focus on the mining process, discover from data streams all those frequent patterns that satisfy the user constraints, and handle situations where the available memory space is limited.", "keywords": ["limited memory space", "frequent itemsets", "data streams", "constraints", "data mining"]}, {"id": "452", "title": "Synthesis of topology and sizing of analog electrical circuits by means of genetic programming", "abstract": "The design (synthesis) of an analog electrical circuit entails the creation of both the topology and sizing (numerical values) of all of the circuit's components. There has previously been no general automated technique for automatically creating the design for an analog electrical circuit from a high-level statement of the circuit's desired behavior. This paper shows how genetic programming can be used to automate the design of eight prototypical analog circuits, including a lowpass filter, a highpass filter, a bandstop filter, a tri-state frequency discriminator circuit, a frequency-measuring circuit, a 60 dB amplifier, a computational circuit for the square root function, and a time-optimal robot controller circuit.", "keywords": ["genetic programming", "genetic algorithms", "circuit synthesis", "electrical circuits", "design"]}, {"id": "453", "title": "A spectrum of compromise aggregation operators for multi-attribute decision making", "abstract": "In many decision making problems, a number of independent attributes or criteria are often used to individually rate an alternative from an agent's local perspective and then these individual ratings are combined to produce an overall assessment. Now, in cases where these individual ratings are not in complete agreement, the overall rating should be somewhere in between the extremes that have been suggested. However, there are many possibilities for the aggregated value. Given this, this paper systematically explores the space of possible compromise operators for such multi-attribute decision making problems. Specifically, we axiomatically identify the complete spectrum of such operators in terms of the properties they should satisfy, and show the main ones that are widely used-namely averaging operators, uninorms and nullnorms-represent only three of the nine types we identify. For each type, we then go onto analyse their properties and discuss how specific instances can actually be developed. Finally, to illustrate the richness of our framework, we show how a wide range of operators are needed to model the various attitudes that a user may have for aggregation in a given scenario (bidding in multi-attribute auctions).  ", "keywords": ["aggregation operator", "uninorm", "nullnorm risk", "multi-attribute decision making", "multi-attribute auction"]}, {"id": "454", "title": "Detecting categorical perception in continuous discrimination data", "abstract": "We present a method for assessing categorical perception from continuous discrimination data. Until recently, categorical perception of speech has exclusively been measured by discrimination and identification experiments with a small number of different stimuli, each of which is presented multiple times. Experiments by Rogers and Davis (2009), however, suggest that using non-repeating stimuli yields a more reliable measure of categorization. If this idea is applied to a single phonetic continuum, the continuum has to be densely sampled and the obtained discrimination data is nearly continuous. In the present study, we describe a maximum-likelihood method that is appropriate for analysing such continuous discrimination data.", "keywords": ["categorical perception", "dense sampling", "discrimination", "maximum likelihood"]}, {"id": "455", "title": "simplified similarity scoring using term ranks", "abstract": "We propose a method for document ranking that combines a simple document-centric view of text, and fast evaluation strategies that have been developed in connection with the vector space model. The new method defines the importance of a term within a document qualitatively rather than quantitatively, and in doing so reduces the need for tuning parameters. In addition, the method supports very fast query processing, with most of the computation carried out on small integers, and dynamic pruning an effective option. Experiments on a wide range of TREC data show that the new method provides retrieval effectiveness as good as or better than the Okapi BM25 formulation, and variants of language models.", "keywords": ["vector space model", "method", "strategies", "dynamic", "computation", "retrieval", "text", "language model", "data", "efficiency/scale: architectures", "efficient query evaluation", "experience", "similarity", "tuning", "documentation", "connection", "effect", "evaluation", "prune", "query processing", "text representation and indexing", "compression", "ranking"]}, {"id": "456", "title": "Midgar: Generation of heterogeneous objects interconnecting applications. A Domain Specific Language proposal for Internet of Things scenarios", "abstract": "Smart Objects and Internet of Things are two ideas that describe the future. The interconnection of objects can make them intelligent or expand their intelligence. This is achieved by a network that connects all the objects in the world. A network where most of the data traffic comes from objects instead of people. Cities, houses, cars or any other objects that come to life, respond, work and make their owners life easier. This is part of that future. But first, there are many basic problems that must be solved. In this paper we propose solutions for many of these problems: the interconnection of ubiquitous, heterogeneous objects and the generation of applications allow inexperienced people to interconnect them. For that purpose, we present three possible solutions: a Domain Specific Language capable of abstracting the application generation problem; a graphic editor that simplifies the creation of that DSL; and an IoT platform (Midgar) able to interconnect different objects between them. Through Midgar, you can register objects and create interconnection between ubiquitous and heterogeneous objects through a graphic editor that generates a model defined by the DSL. From this model, Midgar generates the interconnection defined by the user with the graphical editor.", "keywords": ["internet of things", "ubiquitous computing", "sensor network", "model driven engineering", "domain specific language", "smart objects"]}, {"id": "457", "title": "ROC curves in cost space", "abstract": "ROC curves and cost curves are two popular ways of visualising classifier performance, finding appropriate thresholds according to the operating condition, and deriving useful aggregated measures such as the area under the ROC curve (AUC) or the area under the optimal cost curve. In this paper we present new findings and connections between ROC space and cost space. In particular, we show that ROC curves can be transferred to cost space by means of a very natural threshold choice method, which sets the decision threshold such that the proportion of positive predictions equals the operating condition. We call these new curves rate-driven curves, and we demonstrate that the expected loss as measured by the area under these curves is linearly related to AUC. We show that the rate-driven curves are the genuine equivalent of ROC curves in cost space, establishing a point-point rather than a point-line correspondence. Furthermore, a decomposition of the rate-driven curves is introduced which separates the loss due to the threshold choice method from the ranking loss (Kendall tau distance). We also derive the corresponding curve to the ROC convex hull in cost space; this curve is different from the lower envelope of the cost lines, as the latter assumes only optimal thresholds are chosen.", "keywords": ["cost curves", "roc curves", "cost-sensitive evaluation", "ranking performance", "operating condition", "kendall tau distance", "area under the roc curve "]}, {"id": "458", "title": "On the characteristics of growing cell structures (GCS) neural network", "abstract": "In this paper, a self-developing neural network model, namely the Growing Cell Structures (GCS) is characterized. In GCS each node (or cell) is associated with a local resource counter tau (t). We show that GCS has the conservation property by which the summation of all resource counters always equals s(1 - alpha)/alpha, thereby s is the increment added to tau (t) of the wining node after each input presentation and alpha (0 < alpha < 1.0) is the forgetting (i.e., decay) factor applied to tau (t) of non-wining nodes. The conservation property provides an insight into how GCS can maximize information entropy. The property is also employed to unveil the chain-reaction effect and race-condition which can greatly influence the performance of GCS. We show that GCS can perform better in terms of equi-probable criterion if the resource counters are decayed by a smaller alpha.", "keywords": ["self-developing neural network", "competitive learning", "race-condition", "topology", "equi-probable criterion", "chain-reaction effect"]}, {"id": "459", "title": "limitations of multivariable controller tuning using genetic algorithms", "abstract": "In recent years Evolutionary Computation has come of age, with Genetic Algorithms (GA) being possibly the most popular technique. A study is presented revealing the performance of a GA in determining the PID tuning parameters for a multivariable process, including decoupling controllers. The process used for this investigation is a distillation column which is a MIMO high-order, nonlinear system. The results indicate some limitations of using GAs for controller tuning when MIMO systems are involved.", "keywords": ["genetic algorithm", "tuning", "pid", "multivariable"]}, {"id": "460", "title": "Collaborative real-time traffic information generation and sharing framework for the intelligent transportation system", "abstract": "Real-time traffic information collection and data fusion is one of the most important tasks in the advanced traffic management system (ATMS), and sharing traffic information to users is an essential part of the advance traveler information system (ATIS) among the intelligent transportation systems (ITS). Traditionally, sensor-based schemes or probing-vehicle based schemes have been used for collecting traffic information, but the coverage, cost, and real-time issues have remained unsolved. In this paper, a wiki-like collaborative real-time traffic information collection, fusion and sharing framework is proposed, which includes user-centric traffic event reacting mechanism, and automatic agent-centric traffic information aggregating scheme. Smart traffic agents (STA) developed for various front-end devices have the location-aware two-way real-time traffic exchange capability, and built-in event-reporting mechanism to allow users to report the real-time traffic events around their locations. In addition to collecting traffic information, the framework also integrates heterogeneous external real-time traffic information data sources and internal historical traffic information database to predict real-time traffic status by knowledge base system technique.", "keywords": ["collective intelligence", "traffic status prediction", "smart traffic agent", "intelligent transportation system ", "knowledge-based system"]}, {"id": "461", "title": "Perspectives on wellness self-monitoring tools for older adults", "abstract": "Compared older adults and healthcare providers perceptions on self-monitoring. Explored advantages in older adults voluntary use of self-monitoring. Identified challenges in older adults voluntary use of self-monitoring. Suggested design implications for older adults self-monitoring tools.", "keywords": ["consumer health information", "health communication", "self management", "independent living"]}, {"id": "462", "title": "Exploration of term relationship for Bayesian network based sentence retrieval", "abstract": "Sentence retrieval is to retrieve query-relevant sentences in response to user query. However, limited information contained in sentence always incurs a lot of uncertainties, which heavily influence the retrieval performance. To solve this problem, Bayesian network, which has been accepted as one of the most promising methodologies to deal with information uncertainty, is explored. Correspondingly, three sentence retrieval models based on Bayesian network are proposed, i.e. BNSR model, BNSR_TR model and BNSR_CR model. BNSR model assumes independency between terms and shows certain improvement in retrieval performance. BNSR_TR and BNSR_CR models relax the assumption of term independency but consider term relationships from two different points of view, namely term and term context. Experiments verify the performance improvements produced by these two models, but BNSR_CR shows more advantages than BNSR_TR model, because of its more accurate identification of term dependency.", "keywords": ["sentence retrieval", "bayesian network", "term relationship", "association rule mining"]}, {"id": "463", "title": "animation aerodynamics", "abstract": "Methods based on aerodynamics are developed to simulate and control the motion of objects in fluid flows. To simplify the physics for animation, the problem is broken down into two parts: a fluid flow regime and an object boundary regime. With this simplification one can approximate the realistic behaviour of objects moving in liquids or air. It also enables a simple way of designing and controlling animation sequences: from a set of flow primitives, an animator can design the spatial arrangement of flows, create flows around obstacles and direct flow timing. The approach is fast, simple, and is easily fitted into simulators that model objects governed by classical mechanics. The methods are applied to an animation that involves hundreds of flexible leaves being blown by wind currents.", "keywords": ["spatial", "simplification", "aerodynamics", "method", "simulation", "approximation", "arrangement", "design", "fluid mechanics", "leaves", "object", "direct", "flow primitives", "flow", "flexibility", "control motion design", "physical", "timing", "model", "animation", "sequence", "control", "motion"]}, {"id": "464", "title": "A Fourier-Analytic Approach to Reed-Muller Decoding", "abstract": "We present a Fourier-analytic approach to list-decoding Reed-Muller codes over arbitrary finite fields. We use this to show that quadratic forms over any field are locally list-decodable up to their minimum distance. The analogous statement for linear polynomials was proved in the celebrated works of Goldreich et al. Previously, tight bounds for quadratic polynomials were known only for q = 2 and 3; the best bound known for other fields was the Johnson radius. Departing from previous work on Reed-Muller decoding which relies on some form of self-corrector, our work applies ideas from Fourier analysis of Boolean functions to low-degree polynomials over finite fields, in conjunction with results about the weight-distribution. We believe that the techniques used here could find other applications, we present some applications to testing and learning.", "keywords": ["codes", "computational complexity", "fourier transforms", "polynomials"]}, {"id": "465", "title": "monotonic solution concepts in coevolution", "abstract": "Assume a coevolutionary algorithm capable of storing and utilizing all phenotypes discovered during its operation, for as long as it operates on a problem; that is, assume an algorithm with a monotonically increasing knowledge of the search space. We ask: If such an algorithm were to periodically report, over the course of its operation, the best solution found so far, would the quality of the solution reported by the algorithm improve monotonically over time? To answer this question, we construct a simple preference relation to reason about the goodness of different individual and composite phenotypic behaviors. We then show that whether the solutions reported by the coevolutionary algorithm improve monotonically with respect to this preference relation depends upon the solution concept implemented by the algorithm. We show that the solution concept implemented by the conventional coevolutionary algorithm does not guarantee monotonic improvement; in contrast, the game-theoretic solution concept of Nash equilibrium does guarantee monotonic improvement. Thus, this paper considers 1) whether global and objective metrics of goodness can be applied to coevolutionary problem domains (possibly with open-ended search spaces), and 2) whether coevolutionary algorithms can, in principle, optimize with respect to such metrics and find solutions to games of strategy.", "keywords": ["coevolution", "solution concepts", "monotonic progress"]}, {"id": "466", "title": "Building roadmaps of minima and transitions in visual models", "abstract": "Becoming trapped in suboptimal local minima is a perennial problem when optimizing visual models, particularly in applications like monocular human body tracking where complicated parametric models are repeatedly fitted to ambiguous image measurements. We show that trapping can be significantly reduced by building 'roadmaps' of nearby minima linked by transition pathways-paths leading over low 'mountain passes' in the cost surface-found by locating the transition state (codimension-1 saddle point) at the top of the pass and then sliding downhill to the next minimum. We present two families of transition-state-finding algorithms based on local optimization. In eigenvector tracking, unconstrained Newton minimization is modified to climb uphill towards a transition state, while in hypersurface sweeping, a moving hypersurface is swept through the space and moving local minima within it are tracked using a constrained Newton method. These widely applicable numerical methods, which appear not to be known in vision and optimization, generalize methods from computational chemistry where finding transition states is critical for predicting reaction parameters. Experiments on the challenging problem of estimating 3D human pose from monocular images show that our algorithms find nearby transition states and minima very efficiently, but also underline the disturbingly large numbers of minima that can exist in this and similar model based vision problems.", "keywords": ["model based vision", "global optimization", "saddle points", "3d human tracking"]}, {"id": "467", "title": "dynamic computational geometry on parallel computers", "abstract": "This paper surveys our parallel algorithms for determining geometric properties of systems of moving objects . The properties investigated include nearest (farthest) neighbor, closest (farthest) pair, collision, convex hull, diameter, and containment. The models of computation include the CREW PRAM, mesh, and hypercube.", "keywords": ["collision", "mesh", "systems", "survey", "parallel algorithm", "convex hull", "dynamic", "parallel computation", "hypercube", "paper", "model of computation", "computational geometry", "container"]}, {"id": "468", "title": "Why does the single neuron activity change from trial to trial during sensory-motor task", "abstract": "Single neuron activities from cortical areas of a monkey were recorded while performing a sensory-motor task (a choice reaction time task). Quantitative trial-by-trial analysis revealed that the timing of peak activity exhibited large variation from trial to trial, compared to the variation in the behavioral reaction time of the task. Therefore, we developed a multi-unit dynamic neural network model to investigate the effects of structure of neural connections on the variation of the timing of peak activity. Computer simulation of the model showed that, even though the units are connected in a cascade fashion, a wide variation exists in the timing of peak activity of neurons because of parallel organization of neural network within each unit.", "keywords": ["single neuron", "peak activity", "neural model", "simulation"]}, {"id": "469", "title": "Preventing design conflicts in distributed design systems composed of heterogeneous agents", "abstract": "We model the uncertainty in distributed design. We model the attitudes of design agents to develop novel collaboration indicators. Monte Carlo simulation is performed for heterogeneous design agents. Design conflicts of heterogeneous design agents are prevented. Design agent dominations are reduced to be coherent with agent characters.", "keywords": ["collaborative design", "distributed design", "set-based design", "conflict prevention", "constraint satisfaction problem", "agent attitude model"]}, {"id": "470", "title": "Survivability and performance optimization of mobile wireless communication networks in the event of base station failure", "abstract": "In this paper, we investigate the survivability of mobile wireless communication networks in the event of base station (BS) failure. A survivable network is modeled as a mathematical optimization problem in which the objective is to minimize the total amount of blocked traffic. We apply Lagrangean relaxation as a solution approach and analyze the experiment results in terms of the blocking rate, service rate, and CPU time. The results show that the total call blocking rate (CBR) is much less sensitive to the call blocking probability (CBP) threshold of each BS when the load is light, rather than heavy; therefore, the more traffic loaded, the less the service rate will vary. BS recovery is much more important when the network load is light. However, the BS recovery ratio (BSRR), which is a key factor in reducing the blocking rate for a small number of BSs, is more important when a system is heavily loaded. The proposed model provides network survivability subject to available resources. The model also fits capacity expansion requirements by locating mobile/portable BSs in the places they are most needed.", "keywords": ["base station recovery", "lagrangean relaxation", "mathematical modeling", "network survivability", "performance evaluation", "quality of service"]}, {"id": "471", "title": "2D shallow water flow model for the hydraulic jump", "abstract": "A flow model is presented for predicting a hydraulic jump in a straight open channel. The model is based on the general 2D shallow water equations in strong conservation form, without artificial viscosity, which is usually incorporated into the flow equations to capture a hydraulic jump. The equations are discretised using the finite volume method. The results are compared with experimental data and available numerical results, and have shown that the present model can provide good results. The model is simple and easy to implement. To demonstrate the potential application of the model, several hydraulic jumps occurring in different situations are simulated, and the predictions are in good agreement with standard solution for open channel hydraulics. ", "keywords": ["shallow water flow", "finite volume method", "hydraulic jump", "open channel flow"]}, {"id": "472", "title": "Using multiple query representations in patent prior-art search", "abstract": "Before a patent application is made, it is important to search the appropriate databases for prior-art (i.e., pre-existing patents that may affect the validity of the application). Previous work on prior-art search has concentrated on single query representations of the patent application. In the following paper, we describe an approach which uses multiple query representations. We evaluate our technique using a well-known test collection (CLEF-IP 2011). Our results suggest that multiple query representations significantly outperform single query representations.", "keywords": ["patent search", "prior-art", "collaborative filtering"]}, {"id": "473", "title": "A genetic algorithm with tabu search procedure for flexible job shop scheduling with transportation constraints and bounded processing times", "abstract": "In this paper, we propose a model for Flexible Job Shop Scheduling Problem (FJSSP) with transportation constraints and bounded processing times. This is a NP hard problem. Objectives are to minimize the makespan and the storage of solutions. A genetic algorithm with tabu search procedure is proposed to solve both assignment of resources and sequencing problems on each resource. In order to evaluate the proposed algorithm's efficiency, five types of instances are tested. Three of them consider sequencing problems with or without assignment of processing or/and transport resources. The fourth and fifth ones introduce bounded processing times which mainly characterize Surface Treatment Facilities (STFs). Computational results show that our model and method are efficient for solving both assignment and scheduling problems in various kinds of systems.", "keywords": ["flexible job shop scheduling problem with transportation", "bounded processing times", "genetic algorithm", "tabu search", "flexible manufacturing system", "robotic cell", "surface treatment facility", "disjunctive graph"]}, {"id": "474", "title": "Intrusion detection by integrating boosting genetic fuzzy classifier and data mining criteria for rule pre-screening", "abstract": "The purpose of the work described in this paper is to provide an intelligent intrusion detection system (IIDS) that uses two of the most popular data mining tasks, namely classification and association rules mining together for predicting different behaviors in networked computers. To achieve this, we propose a method based on iterative rule learning using a fuzzy rule-based genetic classifier. Our approach is mainly composed of two phases. First, a large number of candidate rules are generated for each class using fuzzy association rules mining, and they are pre-screened using two rule evaluation criteria in order to reduce the fuzzy rule search space. Candidate rules obtained after pre-screening are used in genetic fuzzy classifier to generate rules for the classes specified in IIDS: namely Normal, PRB-probe, DOS-denial of service, U2R-user to root and R2L-remote to local. During the next stage, boosting genetic algorithm is employed for each class to find its fuzzy rules required to classify data each time a fuzzy rule is extracted and included in the system. Boosting mechanism evaluates the weight of each data item to help the rule extraction mechanism focus more on data having relatively more weight, i.e., uncovered less by the rules extracted until the current iteration. Each extracted fuzzy rule is assigned a weight. Weighted fuzzy rules in each class are aggregated to find the vote of each class label for each data item.", "keywords": ["intrusion detection", "genetic classifier", "fuzziness", "data mining", "weighted fuzzy rules"]}, {"id": "475", "title": "An Objective Perceptual Quality-Based ADTE for Adapting Mobile SVC Video Content", "abstract": "In this paper, we propose an Adaptation Decision-Taking Engine (ADTE) that targets the delivery of scalable video content in mobile usage environments. Our ADTE design relies on an objective perceptual quality metric in order to achieve video adaptation according to human visual perception, thus allowing to maximize the Quality of Service (QoS). To describe the characteristics of a particular usage environment, as well as the properties of the scalable video content, MPEG-21 Digital Item Adaptation (DIA) is used. Our experimental results show that the proposed ADTE design provides video content with a higher subjective quality than an ADTE using the conventional maximum-bit-allocation method.", "keywords": ["adaptation", "adte", "quality metric", "subjective quality", "svc"]}, {"id": "476", "title": "A Memory-Efficient Pipelined Implementation of the Aho-Corasick String-Matching Algorithm", "abstract": "With rapid advancement in Internet technology and usages, some emerging applications in data communications and network security require matching of huge volume of data against large signature sets with thousands of strings in real time. In this article, we present a memory-efficient hardware implementation of the well-known Aho-Corasick (AC) string-matching algorithm using a pipelining approach called P-AC. An attractive feature of the AC algorithm is that it can solve the string-matching problem in time linearly proportional to the length of the input stream, and the computation time is independent of the number of strings in the signature set. A major disadvantage of the AC algorithm is the high memory cost required to store the transition rules of the underlying deterministic finite automaton. By incorporating pipelined processing, the state graph is reduced to a character trie that only contains forward edges. Together with an intelligent implementation of look-up tables, the memory cost of P-AC is only about 18 bits per character for a signature set containing 6,166 strings extracted from Snort. The control structure of P-AC is simple and elegant. The cost of the control logic is very low. With the availability of dual-port memories in FPGA devices, we can double the system throughput by duplicating the control logic such that the system can process two data streams concurrently. Since our method is memory-based, incremental changes to the signature set can be accommodated by updating the look-up tables without reconfiguring the FPGA circuitry.", "keywords": ["algorithms", "design", "performance", "security", "string-matching", "deterministic and nondeterministic finite automaton", "pipelined processing", "intrusion detection system"]}, {"id": "477", "title": "Development of online suites of social science-based resources for health researchers and practitioners", "abstract": "The burgeoning of the Internet has enormous potential for bringing scientific research into the hands of both health practitioners and health researchers to enhance their job performance. In this article, the authors give two examples of how carefully developed and organized online resources can leverage the engaging multimedia formats, ubiquitous access, and low cost of the Internet to address this goal. The article describes two new online suites of social and behavioral science-based resources designed for those in the HIV/AIDS and teen pregnancy prevention fields: HIV Research and Practice Resources and Teen Pregnancy Research and Practice Resources. Each online suite includes research data, survey instruments, prevention resources, and evaluation-related publications and tools that can enhance prevention research and practice. The article ends by peering into the future at how the field of health-related prevention and research might be further advanced using the Internet.", "keywords": ["science-based resources", "internet", "hiv/aids", "prevention", "health", "research", "practice"]}, {"id": "478", "title": "Deferring elimination of design alternatives in object-oriented methods", "abstract": "While developing systems, software engineers generally have to deal with a large number of design alternatives. Current object-oriented methods aim to eliminate design alternatives whenever they are generated. Alternatives, however, should be eliminated only when sufficient information to take such a decision is available. Otherwise, alternatives have to be preserved to allow further refinements along the development process. Too early elimination of alternatives results in loss of information and excessive restriction of the design space. This paper aims to enhance the current object-oriented methods by modeling and controlling the design alternatives through the application of fuzzy-logic-based techniques. By using an example method, it is shown that the proposed approach increases the adaptability and reusability of design models. The method has been implemented and tested in our experimental CASE environment. ", "keywords": ["design alternatives", "object-oriented methods", "fuzzy logic", "adaptable design models", "case environments", "software artifacts"]}, {"id": "479", "title": "Fiber reinforced concrete properties - a multiscale approach", "abstract": "This paper describes the development of a fiber reinforced concrete (FRC) unit cell for analyzing concrete structures by executing a multiscale analysis procedure using the theory of homogenization. This was achieved through solving a periodic unit cell problem of the material in order to evaluate its macroscopic properties. Our research describes the creation of an FRC unit cell through the use of concrete paste generic information e.g. the percentage of aggregates, their distribution, and the percentage of fibers in the concrete. The algorithm presented manipulates the percentage and distribution of these aggregates along with fiber weight to create a finite element unit cell model of the FRC which can be used in a multiscale analysis of concrete structures.", "keywords": ["frc-fibered reinforced concrete", "multiscale analysis", "concrete unit cell", "elastic properties", "mesoscale concrete finite element model"]}, {"id": "480", "title": "NEW SPLINE SPACES WITH GENERALIZED TENSION PROPERTIES", "abstract": "The paper describes a new space of variable degree polynomials. This space is isomorphic to P(6), possesses a Bernstein like basis and has generalized tension properties in the sense that, for limit values of the degrees, its functions approximate quadratic polynomials. The corresponding space of C(3), variable degree splines is also studied. This spline space can be profitably used in the construction of shape preserving curves or surfaces.", "keywords": ["variable degree polynomials", "bernstein basis", "b-splines", "shape preservation"]}, {"id": "481", "title": "ANTHEPROT: An integrated protein sequence analysis software with client/server capabilities", "abstract": "Programs devoted to the analysis of protein sequences exist either as stand-alone programs or as Web servers. However, stand-alone programs can hardly accommodate for the analysis that involves comparisons on databanks, which require regular updates. Moreover, Web servers cannot be as efficient as stand-alone programs when dealing with real-time graphic display. We describe here a stand-alone software program called ANTHEPROT, which is intended to perform protein sequence analysis with a high integration level and clients/server capabilities. It is an interactive program with a graphical user interface that allows handling of protein sequence and data in a very interactive and convenient manner. It provides many methods and tools, which are integrated into a graphical user interface. ANTHEPROT is available for Windows-based systems. It is able to connect to a Web server in order to perform large-scale sequence comparison on up-to-date databanks. ANTHEPROT is freely available to academic users and may be downloaded at http://pbil.ibcp.fr/ANTHEPROT.", "keywords": ["protein sequence analysis", "multiple alignment", "secondary structure prediction", "web server"]}, {"id": "482", "title": "Cardioids-based faster authentication and diagnosis of remote cardiovascular patients", "abstract": "In recent times, dealing with deaths associated with cardiovascular diseases (CVD) has been one of the most challenging issues. The usage of mobile phones and portable Electrocardiogram (ECG) acquisition devices can mitigate the risks associated with CVD by providing faster patient diagnosis and patient care. The existing technologies entail delay in patient authentication and diagnosis. However, for the cardiologists minimizing the delay between a possible CVD symptom and patient care is crucial, as this has a proven impact in the longevity of the patient. Therefore, every seconds counts in terms of patient authentication and diagnosis. In this paper, we introduce the concept of Cardioid based patient authentication and diagnosis. According to our experimentations, the authentication time can be reduced from 30.64 s (manual authentication in novice mobile user) to 0.4398 s (automated authentication). Our ECG based patient authentication mechanism is up to 4878 times faster than conventional biometrics like, face recognition. The diagnosis time could be improved from several minutes to less than 0.5 s (cardioid display on a single screen). Therefore, with our presented mission critical alerting mechanism on wireless devices, minute's worth of tasks can be reduced to second's, without compromising the accuracy of authentication and quality of diagnosis. ", "keywords": ["mission critical alerting", "cardiovascular disease detection", "remote monitoring", "wireless monitoring", "patient authentication", "cardioid"]}, {"id": "483", "title": "A multiprocessor system-on-chip for real-time biomedical monitoring and analysis: ECG prototype architectural design space exploration", "abstract": "In this article we focus on multiprocessor system-on- chip ( MPSoC) architectures for human heart electrocardiogram ( ECG) real time analysis as a hardware/ software ( HW/SW) platform offering an advance relative to state-of-the- art solutions. This is a relevant biomedical application with good potential market, since heart diseases are responsible for the largest number of yearly deaths. Hence, it is a good target for an application-specific system-on- chip (SoC) and HW/ SW codesign. We investigate a symmetric multiprocessor architecuture based on STMicroelecronics VLIW DSPs that process in real time 12-lead ECG signals. This architecture improves upon state-of-the-art SoC designs for ECG analysis in its ability to analyze the full 12 leads in real time, even with high sampling frequencies, and its ability to detect heart malfunction for the whole ECG signal interval. We explore the design space by considering a number of hardware and software architectural options. Comparing our design with present-day solutions from an SoC and application point-ofview shows that our platform can be used in real time and without failures.", "keywords": ["performance", "design", "experimentation"]}, {"id": "484", "title": "Accurate Approximation of the Earth Mover's Distance in Linear Time", "abstract": "Color descriptors are one of the important features used in content-based image retrieval. The dominant color descriptor (DCD) represents a few perceptually dominant colors in an image through color quantization. For image retrieval based on DCD, the earth mover's distance (EMD) and the optimal color composition distance were proposed to measure the dissimilarity between two images. Although providing good retrieval results, both methods are too time-consuming to be used in a large image database. To solve the problem, we propose a new distance function that calculates an approximate earth mover's distance in linear time. To calculate the dissimilarity in linear time, the proposed approach employs the space-filling curve for multidimensional color space. To improve the accuracy, the proposed approach uses multiple curves and adjusts the color positions. As a result, our approach achieves order-of-magnitude time improvement but incurs small errors. We have performed extensive experiments to show the effectiveness and efficiency of the proposed approach. The results reveal that our approach achieves almost the same results with the EMD in linear time.", "keywords": ["earth mover's distance", "approximation", "content-based image retrieval"]}, {"id": "485", "title": "Variable selection in linear regression: Several approaches based on normalized maximum likelihood", "abstract": "The use of the normalized maximum likelihood (NML) for model selection in Gaussian linear regression poses troubles because the normalization coefficient is not finite. The most elegant solution has been proposed by Rissanen and consists in applying a particular constraint for the data space. In this paper, we demonstrate that the methodology can be generalized, and we discuss two particular cases, namely the rhomboidal and the ellipsoidal constraints. The new findings are used to derive four NML-based criteria. For three of them which have been already introduced in the previous literature, we provide a rigorous analysis. We also compare them against five state-of-the-art selection rules by conducting Monte Carlo simulations for families of models commonly used in signal processing. Additionally, for the eight criteria which are tested, we report results on their predictive capabilities for real life data sets.", "keywords": ["gaussian linear regression", "model selection", "normalized maximum likelihood", "rhomboidal constraint", "ellipsoidal constraint"]}, {"id": "486", "title": "Spacetime adaptive finite difference method for European multi-asset options", "abstract": "The multi-dimensional BlackScholes equation is solved numerically for a European call basket option using a prioria posteriori error estimates. The equation is discretized by a finite difference method on a Cartesian grid. The grid is adjusted dynamically in space and time to satisfy a bound on the global error. The discretization errors in each time step are estimated and weighted by the solution of the adjoint problem. Bounds on the local errors and the adjoint solution are obtained by the maximum principle for parabolic equations. Comparisons are made with Monte Carlo and quasi-Monte Carlo methods in one dimension, and the performance of the method is illustrated by examples in one, two, and three dimensions.", "keywords": ["blackscholes equation", "finite difference method", "space adaptation", "time adaptation", "maximum principle"]}, {"id": "487", "title": "alpha-words and the radix order", "abstract": "Let alpha = (a(1), a(2),...) be a sequence (finite or infinite) of integers with a(1) >= 0 and a(n) >= 1, for all n >= 2. Let {a, b} be an alphabet. For n >= 1, and r = r(1)r(2)...r(n) is an element of N(n) with 0  = 2. Many interesting combinatorial properties of alpha-words have been studied by Chuan. In this paper, we obtain some new methods of generating the distinct alpha-words of the same order in lexicographic order. Among other results, we consider another function r bar right arrow w[r] from the set of labels of alpha-words to the set of alpha-words. The string r is called a new label of the alpha-word w[r]. Using any new label of an nth-order alpha-word w, we can compute the number of the nth-order alpha-words that are less than w in the lexicographic order. With the radix orders <(r) on N(n) (regarding N as an alphabet) and {a, b}(+) with a <(r) b, we prove that there exists a subset D of the set of all labels such that w[r] <(r) w[s] whenever r, s is an element of D and r <(r) S.  ", "keywords": ["alpha-word", "radix order", "lexicographic order"]}, {"id": "488", "title": "A buyerseller game model for selection and negotiation of purchasing bids: Extensions and new models", "abstract": "A number of efficiency-based vendor selection and negotiation models have been developed to deal with multiple attributes including price, quality and delivery performance. The efficiency is defined as the ratio of weighted outputs to weighted inputs. By minimizing the efficiency, Talluri [Eur. J. Operat. Res. 143(1) (2002) 171] proposes a buyerseller game model that evaluates the efficiency of alternative bids with respect to the ideal target set by the buyer. The current paper shows that this buyerseller game model is closely related to data envelopment analysis (DEA) and can be simplified. The current paper also shows that setting the (ideal) target actually incorporates implicit tradeoff information on the multiple attributes into efficiency evaluation. We develop a new buyerseller game model where the efficiency is maximized with respect to multiple targets set by the buyer. The new model allows the buyer to evaluate and select the vendors in the context of best-practice. By both minimizing and maximizing efficiency, the buyer can obtain an efficiency range within which the true efficiency lies given the implicit tradeoff information characterized by the targets. The current study establishes the linkage between buyerseller game models and DEA. Such a linkage can provide the buyer with correct evaluation methods based upon existing DEA models regarding the nature of bidding.", "keywords": ["game models", "linear programming", "efficiency", "data envelopment analysis"]}, {"id": "489", "title": "A Bayesian latent variable model with classification and regression tree approach for behavior and credit scoring", "abstract": "A Bayesian latent variable model with classification and regression tree approach is built to overcome three challenges encountered by a bank in credit-granting process. These three challenges include (1) the bank wants to predict the future performance of an applicant accurately; (2) given current information about cardholders credit usage and repayment behavior, financial institutions would like to determine the optimal credit limit and APR for an applicant; and (3) the bank would like to improve its efficiency by automating the process of credit-granting decisions. Data from a leading bank in Taiwan is used to illustrate the combined approach. The data set consists of each credit card holders credit usage and repayment data, demographic information, and credit report. Empirical study shows that the demographic variables used in most credit scoring models have little explanatory ability with regard to a cardholders credit usage and repayment behavior. A cardholders credit history provides the most important information in credit scoring. The continuous latent customer quality from the Bayesian latent variable model allows considerable latitude for producing finer rules for credit granting decisions. Compared to the performance of discriminant analysis, logistic regression, neural network, multivariate adaptive regression splines (MARS) and support vector machine (SVM), the proposed model has a 92.9% accuracy rate in predicting customer types, is less impacted by prior probabilities, and has a significantly low Type I errors in comparison with the other five approaches.", "keywords": ["behavior scoring", "credit scoring", "bayesian", "latent variable model", "classification and regression tree"]}, {"id": "490", "title": "The representation of manufacturing requirements in model-driven parts manufacturing", "abstract": "Today there is a need to make process and production planning more cost-effective while not compromising the quality of the product. Manufacturing requirements are used to ensure producibility in early development phases and also as a source for continuous improvement of the manufacturing system. To make this possible it is essential to have correct, updated information available and to be able to trace the relations between requirements and their origin and subjects. To trace requirements' origin in resources or processes is today very difficult owing to system integration problems. This article discusses the relations that need to be represented and proposes the use of model-based methods to enable traceability of requirements. Because requirements are a collaborative effort a standard for information exchange is needed. The ISO10303 STEP application protocol AP233 System Engineering is proposed for this purpose.", "keywords": ["information management", "manufacturing", "requirements"]}, {"id": "491", "title": "Baldwinian learning utilizing genetic and heuristic algorithms for logic synthesis and minimization of incompletely specified data with Generalized ReedMuller (ANDEXOR) forms", "abstract": "This research applies a new heuristic combined with a genetic algorithm (GA) to the task of logic minimization for incompletely specified data, with both single and multi-outputs, using the Generalized ReedMuller (GRM) equation form. The GRM equation type is a canonical expression of the Exclusive-Or Sum-of-Products (ESOPs) type, in which for every subset of input variables there exists not more than one term with arbitrary polarities of all variables. This ANDEXOR implementation has been shown to be economical, generally requiring fewer gates and connections than that of ANDOR logic. GRM logic is also highly testable, making it desirable for FPGA designs. The minimization results of this new algorithm tested on a number of binary benchmarks are given. This minimization algorithm utilizes a GA with a two-level fitness calculation, which combines human-designed heuristics with the evolutionary process, employing Baldwinian learning. In this algorithm, first a pure GA creates certain constraints for the selection of chromosomes, creating only genotypes (polarity vectors). The phenotypes (GRMs) are then learned in the environment and contribute to the GA fitness (which is the total number of terms of the best GRM for each output), providing indirect feedback as to the quality of the genotypes (polarity vectors) but the genotype chromosomes (polarity vectors) remain unchanged. In this process, the improvement in genotype chromosomes (polarity vectors) is the product of the evolutionary processes from the GA only. The environmental learning is achieved using a human-designed GRM minimization heuristic. As much previous research has presented the merit of ANDEXOR logic for its high density and testability, this research is the first application of the GRM (a canonical ANDEXOR form) to the minimization of incompletely specified data.", "keywords": ["incompletely specified generalized reedmuller forms", "andexor forms", "logic synthesis and minimization", "baldwinian learning", "genetic algorithms"]}, {"id": "492", "title": "Energy stable numerical methods for hyperbolic partial differential equations using overlapping domain decomposition", "abstract": "Overlapping domain decomposition methods, otherwise known as overset grid or chimera methods, are useful for simplifying the discretization of partial differential equations in or around complex geometries. Though in wide use, such methods are prone to numerical instability unless numerical diffusion or some other form of regularization is used, especially for higher-order methods. To address this shortcoming, high-order, provably energy stable, overlapping domain decomposition methods are derived for hyperbolic initial boundary value problems. The overlap is treated by splitting the domain into pieces and using generalized summation-by-parts derivative operators and polynomial interpolation. New implicit and explicit operators are derived that do not require regularization for stability in the linear limit. Applications to linear and nonlinear problems in one and two dimensions are presented, where it is found the explicit operators are preferred to the implicit ones.", "keywords": ["high order finite difference methods", "overlapping domain decomposition", "numerical stability", "generalized summation-by-parts"]}, {"id": "493", "title": "The Impact of Cluster Representatives on the Convergence of the K-Modes Type Clustering", "abstract": "As a leading partitional clustering technique, k-modes is one of the most computationally efficient clustering methods for categorical data. In the k-modes, a cluster is represented by a \"mode,\" which is composed of the attribute value that occurs most frequently in each attribute domain of the cluster, whereas, in real applications, using only one attribute value in each attribute to represent a cluster may not be adequate as it could in turn affect the accuracy of data analysis. To get rid of this deficiency, several modified clustering algorithms were developed by assigning appropriate weights to several attribute values in each attribute. Although these modified algorithms are quite effective, their convergence proofs are lacking. In this paper, we analyze their convergence property and prove that they cannot guarantee to converge under their optimization frameworks unless they degrade to the original k-modes type algorithms. Furthermore, we propose two different modified algorithms with weighted cluster prototypes to overcome the shortcomings of these existing algorithms. We rigorously derive updating formulas for the proposed algorithms and prove the convergence of the proposed algorithms. The experimental studies show that the proposed algorithms are effective and efficient for large categorical datasets.", "keywords": ["clustering", "k-modes type clustering algorithms", "categorical data", "weighted cluster prototype", "convergence"]}, {"id": "494", "title": "Service quality and ERP implementation: A conceptual and empirical study of semiconductor-related industries in Taiwan", "abstract": "This paper examines the effectiveness of the implementation of enterprise resource planning (ERP) in improving service quality in the Taiwanese semiconductor industry by assessing the expectations and the perceptions of service quality from the perspectives of both upstream manufacturers and downstream customers. The study first establishes a modified service quality gap model incorporating: (i) the downstream customers' expectations and perceptions, and (ii) the upstream manufacturers' perceptions of the customers' expectations and perceptions. An empirical study by questionnaire survey is then undertaken to investigate the gaps proposed in the research model. The results show that service quality gaps do exist in the Taiwanese semiconductor industry between upstream manufacturers that are implementing ERP and their downstream customers. The study shows that the proposed model provides valuable guidance to manufacturers with respect to the prevention, detection, and elimination of the demonstrated service quality gaps. The model thus helps manufacturers to evaluate the contribution of various ERP modules to improved customer satisfaction with service quality and also provides guidance on improvement strategies to enhance service quality by eliminating quality gaps.  ", "keywords": ["enterprise resource planning ", "semiconductor industry", "service quality gaps", "erp implementation"]}, {"id": "495", "title": "Largest inscribed rectangles in convex polygons", "abstract": "We consider approximation algorithms for the problem of computing an inscribed rectangle having largest area in a convex polygon on n  vertices. If the order of the vertices of the polygon is given, we present a randomized algorithm that computes an inscribed rectangle with area at least (1) ( 1) times the optimum with probability t  in time O ( 1 ? log n ) for any constant t<1 t < 1 . We further give a deterministic approximation algorithm that computes an inscribed rectangle of area at least (1) ( 1) times the optimum in running time O ( 1 ? 2 log n ) and show how this running time can be slightly improved.", "keywords": ["approximation algorithms", "geometric algorithms", "largest area rectangle", "inscribed rectangles in polygons"]}, {"id": "496", "title": "An effective node-selection scheme for the energy efficiency of solar-powered WSNs in a stream environment", "abstract": "We propose an effective node-selection scheme in the stream environment of solar-powered WSNs. We analyzed the stream environment including single stream and cross-stream cases. The deployment conditions are appropriate to each stream case. Based on the node selection scheme, the number of active nodes and transmitted packets is minimized. The proposed scheme prolongs the lifetime of the solar-powered WSN in a stream environment.", "keywords": ["sensor deployment", "node-selection", "stream environment", "solar-powered sensor", "wireless sensor network"]}, {"id": "497", "title": "The concept of a quasi-particle and the non-probabilistic interpretation of wave mechanics", "abstract": "In recent works of the author [found Phys 36 (2006) 1701-1717, Math Comput simul 74 (2007) 93-103], the argument has been made that Hertz's equations of electrodynamics reflect the material invariance (indifference) of the latter. Then the principle of material invariance was postulated in heu of Lorentz covariance. and the respective absolute medium wits named the metacontinuum Here. we go further to assume that the metacontinuum is a very thin but very stuff 3D hypershell in the 4D space The equation for the deflection of the shell along the fourth dimension is the \"master\" nonlinear dispersive equation of wave mechanics whose linear part (Euler-Bernoulli equation) is nothing else but the Schrodinger wave equation written for the real or the imaginary part of the wave function. The wave function has a clear non-probabilistic interpretation as the actual amplitude of the flexural deformation The \"master\" equation admits solitary-wave solutions/solutions that behave as quasi-particles (QPs). We stipulate that particles are our perception of the QPs (schaumkommen in Schrodinger's own words). We show the passage from the continuous Lagrangian of the field to the discrete Lagrangian of the centers of QPs and introduce the concept of (pseudo)mass. We interpret the membrane tension as all attractive (gravitational?) force acting between the QPs. Thus. it self-consistent unification of electrodynamics, wave mechanics, gravitation. and the wave-particle duality is achieved  ", "keywords": ["luminiferous metacontinuum", "maxwell-hertz electrodynamics", "schrodinger wave mechanics", "quasi-particles", "particle-wave duality"]}, {"id": "498", "title": "An empirical study of the expressiveness of the functional basis", "abstract": "Function models are frequently used in engineering design to describe the technical functions that a product performs. This paper investigates the use of the functional basis, a function vocabulary developed to aid in communication and archiving of product function information, in describing consumer products that have been decomposed, analyzed, modeled functionally, and stored in a Web-based design repository. The frequency of use of function terms and phrases in 11 graphical and 110 list-based representations in the repository is examined and used to analyze the organization and expressiveness of the functional basis and function models. Within the context of reverse engineering, we determined that the modeling resolution provided by the hierarchical levels, especially the tertiary level, is inadequate for function modeling; the tertiary terms are inappropriate for capturing sufficient details desired by modelers for archiving and reuse, and there is a need for a more expressive flow terms and flow qualifiers in the vocabulary. A critical comparison is also presented of two representations in the design repository: function structures and function lists. The conclusions are used to identify new research opportunities, including the extension of the vocabulary to incorporate flow qualifiers in addition to more expressive terms.", "keywords": ["functional basis", "function model", "function representation", "vocabulary"]}, {"id": "499", "title": "A 6.7 kbps vector sum excited linear prediction on TMS320C54X digital signal processor", "abstract": "In this paper, a 6.7-kbps vector sum excited linear prediction (VSELP) coder with less computational complexity is presented. A very efficient VSELP codebook with nine basis vectors and a heuristic K-selection method (to reduce the search space and complexity) is constructed to obtain the stochastic codebook vector. The nine basis vectors are obtained by optimizing a set of randomly generated basis vectors. During the optimization process, we have trained the basis vectors to give the system apriori knowledge of the characteristics of the input. The coder is implemented on a TMS320C541 digital signal processor. The performance is evaluated by testing the 6.7-kbps VSELP coder with different test speech data taken from different speakers. The quality of the coder is estimated by comparing the performance of the 6.7-kbps VSELP coder with an 8-kbps VSELP speech coder based on the IS-54 standards.  ", "keywords": ["vector sum excited linear prediction", "code excited linear prediction", "linear predictive coding", "digital signal processor"]}]