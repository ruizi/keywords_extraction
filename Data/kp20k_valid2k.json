[
  {
    "id": "0",
    "title": "Real-Time Data Aggregation in Contention-Based Wireless Sensor Networks",
    "abstract": "We investigate the problem of delay constrained maximal information collection for CSMA-based wireless sensor networks. We study how to allocate the maximal allowable transmission delay at each node, such that the amount of information collected at the sink is maximized and the total delay for the data aggregation is within the given bound. We formulate the problem by using dynamic programming and propose an optimal algorithm for the optimal assignment of transmission attempts. Based on the analysis of the optimal solution, we propose a distributed greedy algorithm. It is shown to have a similar performance as the optimal one.",
    "keywords": [
      "algorithms",
      "design",
      "performance",
      "sensor networks",
      "data aggregation",
      "real-time traffic",
      "csma/ca",
      "delay constrained transmission"
    ]
  },
  {
    "id": "1",
    "title": "word sense disambiguation for event trigger word detection",
    "abstract": "This paper describes a method for detecting event trigger words in biomedical text based on a word sense disambiguation (WSD) approach. We first investigate the applicability of existing WSD techniques to trigger word disambiguation in the BioNLP 2009 shared task data, and find that we are able to outperform a traditional CRF-based approach for certain word-types. On the basis of this finding, we combine the WSD approach with the CRF, and obtain significant improvements over the standalone CRF, gaining particularly in recall.",
    "keywords": [
      "biomedical text",
      "machine learning",
      "information extraction"
    ]
  },
  {
    "id": "2",
    "title": "composing architectural aspects based on style semantics",
    "abstract": "The lack of architecturally-significant mechanisms for aspectual composition might artificially hinder the specification of stable and reusable design aspects. Current aspect-oriented approaches at the architecture-level tend to mimic programming language join point models while overlooking mainstream architectural concepts such as styles and their semantics. Syntax-based pointcuts are typically used to select join points based on the names of architectural elements, exposing architecture descriptions to pointcut fragility and reusability problems. This paper presents style-based composition, a new flavor of aspect composition at the architectural level based on architectural styles. We propose style-based join point models and provide a pointcut language that supports the selection of join points based on style-constrained architectural models. Stability and reusability assessments of the proposed style-based composition model were carried out through three case studies involving different styles. The interplay of style-based pointcuts and some style composition techniques is also discussed.",
    "keywords": [
      "architectural styles",
      "architectural aspects",
      "pointcut languages",
      "style-based composition"
    ]
  },
  {
    "id": "3",
    "title": "using pen-based computers across the computer science curriculum",
    "abstract": "This paper describes our use of pen-based electronic classrooms to enhance several computer science courses. After presenting our motivation for undertaking this work, and its relevance to the growing interest in using tablet PC's in the classroom, we present an overview of our use of this technology to engage students during class. Finally, we present the students' reaction to the approach as measured through attitude surveys and a focus group.",
    "keywords": [
      "computer science",
      "present",
      "groupware",
      "use",
      "technologies",
      "pen",
      "pen-based computing",
      "motivation",
      "survey",
      "tablet",
      "computation",
      "tablet pcs",
      "relevance",
      "computer science curriculum",
      "paper",
      "focus-group",
      "attitude",
      "collaborative computing",
      "class",
      "student"
    ]
  },
  {
    "id": "4",
    "title": "On the syntactic and functional correspondence between hybrid (or layered) normalisers and abstract machines",
    "abstract": "We show how to connect the syntactic and the functional correspondence for normalisers and abstract machines implementing hybrid (or layered) reduction strategies, that is, strategies that depend on subsidiary sub-strategies. Many fundamental strategies in the literature are hybrid, in particular, many full-reducing strategies, and many full-reducing and complete strategies that deliver a fully reduced result when it exists. If we follow the standard program-transformation steps the abstract machines obtained for hybrids after the syntactic correspondence cannot be refunctionalised, and the junction with the functional correspondence is severed. However, a solution is possible based on establishing the shape invariant of well-formed continuation stacks. We illustrate the problem and the solution with the derivation of substitution-based normalisers for normal order, a hybrid, full-reducing, and complete strategy of the pure lambda calculus. The machine we obtain is a substitution-based, eval/apply, open-terms version of Pierre Crgut's full-reducing Krivine machine KN.",
    "keywords": [
      "operational semantics",
      "program transformation",
      "reduction strategies",
      "abstract machines",
      "full reduction"
    ]
  },
  {
    "id": "5",
    "title": "A case-based method for service-oriented value chain and sustainable network design",
    "abstract": "The purpose of this research is to present a case-based analytic method for a service-oriented value chain and a sustainable network design considering customer, environmental and social values. Enterprises can enhance competitive advantage by providing more values to all stakeholders in the network. Our model employs a stylized database to identify successful cases of value chain application under similar company marketing conditions, illustrating potential value chains and sustainable networks as references. This work first identifies economic benefits, environmental friendliness and social contribution values based on prior studies. Next, a search engine which is developed based on the rough set theory will search and map similarities to find similar or parallel cases in the database. Finally, a visualized network mapping will be automatically generated to possible value chains. This study applies a case-based methodology to assist enterprises in developing a service-oriented value chain design. For decision makers, this can reduce survey time and inspire innovative works based on previous successful experience. Besides, successful ideas from prior cases can be reused. In addition to customer values, this methodology incorporates environment and social values that may encourage a company to build their value chain in a more comprehensive and sustainable manner. This is a pilot study which attempts to utilize computer-aided methodology to assist in service or value-related design. The pertinent existing solutions can be filtered from an array of cases to engage the advantages from both product-oriented and service-oriented companies. Finally, the visualized display of value network is formed to illustrate the results. A customized service-oriented value chains which incorporates environment and social values can be designed according to different conditions. Also, this system engages the advantages from both product-oriented and service-oriented companies to build a more comprehensive value network. Apart from this, the system can be utilized as a benchmarking tool, and it could remind the decision makers to consider potential value from a more multifaceted perspective. This is the first paper that applied a computer-aided method to design service-oriented value chains. This work also can serve as a decision support and benchmarking system because decision makers can develop different value networks according to various emphasized values. Finally, the visualized display of value network can improve the communication among stakeholders.",
    "keywords": [
      "value chain design",
      "sustainable network design",
      "case-based reasoning"
    ]
  },
  {
    "id": "6",
    "title": "volume subdivision based hexahedral finite element meshing of domains with interior 2-manifold boundaries",
    "abstract": "We present a subdivision based algorithm for multi-resolution Hexahedral meshing. The input is a bounding rectilinear domain with a set of embedded 2-manifold boundaries of arbitrary genus and topology. The algorithm first constructs a simplified Voronoi structure to partition the object into individual components that can be then meshed separately. We create a coarse hexahedral mesh for each Voronoi cell giving us an initial hexahedral scaffold. Recursive hexahedral subdivision of this hexahedral scaffold yields adaptive meshes. Splitting and Smoothing the boundary cells makes the mesh conform to the input 2-manifolds. Our choice of smoothing rules makes the resulting boundary surface of the hexahedral mesh as C 2 continuous in the limit (C 1 at extra-ordinary points), while also keeping a definite bound on the condition number of the Jacobian of the hexahedral mesh elements. By modifying the crease smoothing rules, we can also guarantee that the sharp features in the data are captured. Subdivision guarantees that we achieve a very good approximation for a given tolerance, with optimal mesh elements for each Level of Detail (LoD).",
    "keywords": [
      "hexahedral meshing",
      "mesh generation",
      "subdivision meshes",
      "3d meshing"
    ]
  },
  {
    "id": "7",
    "title": "Methods for evaluating and creating data quality",
    "abstract": "This paper provides a survey of two classes of methods that can be used in determining and improving the quality of individual files or groups of files. The first are edit/imputation methods for maintaining business rules and for imputing for missing data. The second are methods of data cleaning for finding duplicates within files or across files.",
    "keywords": [
      "integer programming",
      "set covering",
      "data cleaning",
      "approximate string comparison",
      "unsupervised and supervised learning"
    ]
  },
  {
    "id": "8",
    "title": "Nonlinear magnetostatic BEM formulation using one unknown double layer charge",
    "abstract": "Purpose - The purpose of this paper is to solve generic magnetostatic problems by BEM, by studying how to use a boundary integral equation (BIE) with the double layer charge as unknown derived from the scalar potential. Design/methodology/approach - Since the double layer charge produces only the potential gap without disturbing the normal magnetic flux density, the field is accurately formulated even by one BIE with one unknown. Once the double layer charge is determined, Biot-Savart's law gives easily the magnetic flux density. Findings - The BIE using double layer charge is capable of treating robustly geometrical singularities at edges and corners. It is also capable of solving the problems with extremely high magnetic permeability. Originality/value - The proposed BIE contains only the double layer charge while the conventional equations derived from the scalar potential contain the single and double layer charges as unknowns. In the multiply connected problems, the excitation potential in the material is derived from the magnetomotive force to represent the circulating fields due to multiply connected exciting currents.",
    "keywords": [
      "boundary integral equation",
      "double layer charge",
      "multiply connected problem",
      "nonlinear magnetostatic analysis",
      "scalar potential",
      "integral equations",
      "electric current"
    ]
  },
  {
    "id": "9",
    "title": "Mathematical modeling of electrical activity of uterine muscle cells",
    "abstract": "The uterine electrical activity is an efficient parameter to study the uterine contractility. In order to understand the ionic mechanisms responsible for its generation, we aimed at building a mathematical model of the uterine cell electrical activity based upon the physiological mechanisms. First, based on the voltage clamp experiments found in the literature, we focus on the principal ionic channels and their cognate currents involved in the generation of this electrical activity. Second, we provide the methodology of formulations of uterine ionic currents derived from a wide range of electrophysiological data. The model is validated step by step by comparing simulated voltage-clamp results with the experimental ones. The model reproduces successfully the generation of single spikes or trains of action potentials that fit with the experimental data. It allows analyzing ionic channels implications. Likewise, the calcium-dependent conductance influences significantly the cellular oscillatory behavior.",
    "keywords": [
      "myometrial ionic currents",
      "uterine excitability",
      "voltage clamp",
      "action potential",
      "electrophysiological model"
    ]
  },
  {
    "id": "10",
    "title": "Sweep synchronization as a global propagation mechanism",
    "abstract": "This paper presents a new generic filtering algorithm which simultaneously considers n conjunctions of constraints as well as those constraints mentioning some variables Yk Y k of the pairs X , Y k ( 1 ? k ? n ) occurring in these conjunctions. The main benefit of this new technique comes from the fact that, for adjusting the bounds of a variable X according to n conjunctions, we do not perform n sweeps in an independent way but rather synchronize them. We then specialize this technique to the non-overlapping rectangles constraint where we consider the case where several rectangles of height one have the same X coordinate for their origin as well as the same length. For this specific constraint we come up with an incremental bipartite matching algorithm which is triggered while we sweep over the time axis. We illustrate the usefulness of this new pruning method on a timetabling problem, where each task cannot be interrupted and requires the simultaneous availability of n distinct persons. In addition each person has his own periods of unavailability and can only perform one task at a time.",
    "keywords": [
      "global constraint",
      "filtering algorithm",
      "sweep",
      "timetabling"
    ]
  },
  {
    "id": "11",
    "title": "A structural approach to reversible computation",
    "abstract": "Reversibility is a key issue in the interface between computation and physics, and of growing importance as miniaturization progresses towards its physical limits. Most foundational work on reversible computing to date has focussed on simulations of low-level machine models. By contrast, we develop a more structural approach. We show how high-level functional programs can be mapped compositionally (i.e. in a syntax-directed fashion) into a simple kind of automata which are immediately seen to be reversible. The size of the automaton is linear in the size of the functional term. In mathematical terms, we are building a concrete model of functional computation. This construction stems directly from ideas arising in Geometry of Interaction and Linear Logic-but can be understood without any knowledge of these topics. In fact, it serves as an excellent introduction to them. At the same time, an interesting logical delineation between reversible and irreversible forms of computation emerges from our analysis.  ",
    "keywords": [
      "reversible computation",
      "linear combinatory algebra",
      "term-rewriting",
      "automata",
      "geometry of interaction"
    ]
  },
  {
    "id": "12",
    "title": "UBL: The DNA of next generation e-Business",
    "abstract": "This paper introduces into the evolution of Electronic Data Interchange (EDI) and the Universal Business Language (UBL). an OASIS standard to encode and customize business documents. It shows its peculiarities and also sets it into a broader picture showing where UBL is positioned in relationship to business processes and standards like BPEL and BPMN.",
    "keywords": [
      "universal business language ",
      "electronic data interchange ",
      "e-business"
    ]
  },
  {
    "id": "13",
    "title": "K-GENI testbed deployment and federated meta operations experiment over GENI and KREONET",
    "abstract": "The classical Internet has confronted many drawbacks in terms of network security, scalability, and performance, although it has strongly influenced the development and evolution of diverse network technologies, applications, and services. Therefore, new innovative research on the Future Internet has been performed to resolve the inherent weaknesses of the traditional Internet, which, in turn, requires new at-scale network testbeds and research infrastructure for large-scale experiments. In this context, K-GENI has been developed as an international programmable Future Internet testbed in the GENI spiral-2 program, and it has been operational between the USA (GENI) and Korea (KREONET) since 2010. The K-GENI testbed and the related collaborative efforts will be introduced with two major topics in this paper: (1) the design and deployment of the K-GENI testbed and (2) the federated meta operations between the K-GENI and GENI testbeds. Regarding the second topic in particular, we will describe how meta operations are federated across K-GENI between GMOC (GENI Meta Operations Center) and DvNOC (Distributed virtual Network Operations Center on KREONET/K-GENI), which is the first trial of an international experiment on the federated network operations over GENI.",
    "keywords": [
      "geni",
      "k-geni",
      "kreonet",
      "federation",
      "dvnoc"
    ]
  },
  {
    "id": "14",
    "title": "Conceptions and modeling for transmitted information evaluation by ANN",
    "abstract": "In this paper, the main measure, an amount of information, of the information theory is analyzed and corrected. The three conceptions of the theory on the microstate, dissipation pathways, and self-organization levels with a tight connection to the statistical physics are discussed. The concepts of restricted information were introduced as well as the proof of uniqueness of the entropy function, when the probabilities are rational numbers, is presented. The artificial neural network (ANN) model for mapping the evaluation of transmitted information has been designed and experimentally approbated in the biological area.",
    "keywords": [
      "information theory",
      "entropy",
      "amount of information",
      "artificial neural networks"
    ]
  },
  {
    "id": "15",
    "title": "FMESP: Framework for the modeling and evaluation of software processes",
    "abstract": "Nowadays, organizations face with a very high competitiveness and for this reason they have to continuously improve their processes. Two key aspects to be considered in the software processes management in order to promote their improvement are their effective modeling and evaluation. The integrated management of these key aspects is not a trivial task, the huge number and diversity of elements to take into account makes it complex the management of software processes. To ease and effectively support this management, in this paper we propose FMESP: a framework for the integrated management of the modeling and measurement of software processes. FMESP incorporates the conceptual and technological elements necessary to ease the integrated management of the definition and evaluation of software processes. From the measurement perspective of the framework and in order to provide the support for the software process measurement at model level a set of representative measures have been defined and validated.",
    "keywords": [
      "software process modeling",
      "software measurement",
      "conceptual framework",
      "software engineering environment"
    ]
  },
  {
    "id": "16",
    "title": "ANTICIPATED FUNCTION SYNCHRONIZATION WITH UNKNOWN PARAMETERS OF DISCRETE-TIME CHAOTIC SYSTEMS",
    "abstract": "In this paper, firstly, the control problem for the chaos synchronization of discrete-time chaotic (hyperchaotic) systems with unknown parameters are considered. Next, back-stepping control law is derived to make the error signals between drive 2D discrete-time chaotic system and response 2D discrete-time chaotic system with two uncertain parameters asymptotically synchronized. Finally, the approach is extended to the synchronization problem for 3D discrete-time chaotic system with two unknown parameters. Numerical simulations are presented to show the effectiveness of the proposed chaos synchronization scheme.",
    "keywords": [
      "anticipated function synchronization",
      "backstepping design",
      "fold maps",
      "henon maps"
    ]
  },
  {
    "id": "17",
    "title": "Methods of assessing spinal radiographs in scoliosis are functions of its geometry",
    "abstract": "The most important feature of scoliosis is the lateral curvature of the spine. It can be treated either conservatively or by surgery; however, treatment choice depends mainly on curve progression which is determined by frequent curve assessment. This is a review of methods of curve measurement and proof of the relationship between them.",
    "keywords": [
      "scoliosis",
      "curve progression",
      "curve measurement"
    ]
  },
  {
    "id": "18",
    "title": "Feedback control of natural convection",
    "abstract": "An applicable method is developed for the identification and feedback control of natural convection. The Boussinesq equation is reduced to a small set of ordinary differential equations by means of the KarhunenLove Galerkin procedure [Int. J. Heat Mass Transfer 39 (1996) 3311]. Based on this low-dimensional dynamic model, a feedback control synthesis is constructed by first performing an extended Kalman filter estimate of the velocity and temperature fields to treat the measurement errors and then developing the optimal feedback law by means of the linear quadratic regulator theory. The present method allows for the practical implementation of modern control concepts to many flow systems including natural convection.",
    "keywords": [
      "karhunenlove galerkin procedure",
      "feedback control",
      "natural convection"
    ]
  },
  {
    "id": "19",
    "title": "Demand assigned capacity management (DACM) in IP over optical (IPO) networks",
    "abstract": "The demand assigned capacity management (DACM) problem in IP over optical (IPO) network aims at devising efficient bandwidth replenishment schedules from the optical domain conditioned upon traffic evolution processes in the IP domain. A replenishment schedule specifies the location, sizing, and sequencing of link capacity expansions to support the growth of Internet traffic demand in the IP network subject to economic considerations. A major distinction in the approach presented in this paper is the focus of attention on the economics of \"excess bandwidth\" in the IP domain, which can be viewed as an inventory system that is endowed with fixed and variable costs and depletes with increase in IP traffic demand requiring replenishment from the optical domain. We, develop mathematical models to address the DACM problem in IPO networks based on a class of inventory management replenishment methods. We apply the technique to IPO networks that implement capacity adaptive routing in the IP domain and networks without capacity adaptive routing. We analyze the performance characteristics under both scenarios, in terms of minimizing cumulative replenishment cost over an interval of time. For the non-capacity adaptive routing scenario, we consider a shortest path approach in the IP domain, specifically OSPF. For the capacity adaptive scenario, we use an online constraint-based routing scheme. This study. represents an application of integrated traffic engineering which concerns collaborative decision making targeted towards network performance improvement that takes into consideration traffic demands, control capabilities, and network assets at different levels in the network hierarchy.",
    "keywords": [
      "ason",
      "bandwidth replenishment",
      "capacity management",
      "demand assigned capacity management",
      "gmpls",
      "integrated traffic engineering",
      "inventory management",
      "ip over optical networks",
      "ipo",
      "mpls",
      "network performance optimization",
      "traffic engineering"
    ]
  },
  {
    "id": "20",
    "title": "Parallel algorithm for finding modules of large-scale coherent fault trees",
    "abstract": "We propose a new parallel algorithm to find all modules of a large fault tree. An experiment is used to compare the linear time algorithm and parallel algorithm. The result shows that our method is efficient in handling large-scale fault trees.",
    "keywords": [
      "modularization",
      "parallel algorithm",
      "fault tree",
      "directed acyclic graph"
    ]
  },
  {
    "id": "21",
    "title": "Finding relevant clustering directions in high-dimensional data using Particle Swarm Optimization",
    "abstract": "A method based on Particle Swarm Optimization (PSO) is proposed and described for finding subspaces that carry meaningful information about the presence of groups in high-dimensional data sets. The advantage of using PSO is that not only the variables that are responsible for the main data structure are identified but also other subspaces corresponding to local optima. The characteristics of the method are shown on two simulated data sets and on a real matrix coming from the analysis of genomic microarrays. In all cases, PSO allowed to explore different subspaces and to discover meaningful structures in the analyzed data. ",
    "keywords": [
      "variable selection",
      "clustering",
      "particle swarm optimization ",
      "swarm intelligence"
    ]
  },
  {
    "id": "22",
    "title": "Neural network learning of optimal Kalman prediction and control",
    "abstract": "Although there are many neural network (NN) algorithms for prediction and for control, and although methods for optimal estimation (including filtering and prediction) and for optimal control in linear systems were provided by Kalman in 1960 (with nonlinear extensions since then), there has been, to my knowledge, no NN algorithm that learns either Kalman prediction or Kalman control (apart from the special case of stationary control). Here we show how optimal Kalman prediction and control (KPC), as well as system identification, can be learned and executed by a recurrent neural network composed of linear-response nodes, using as input only a stream of noisy measurement data. The requirements of KPC appear to impose significant constraints on the allowed NN circuitry and signal flows. The NN architecture implied by these constraints bears certain resemblances to the local-circuit architecture of mammalian cerebral cortex. We discuss these resemblances, as well as caveats that limit our current ability to draw inferences for biological function. It has been suggested that the local cortical circuit (LCC) architecture may perform core functions (as yet unknown) that underlie sensory, motor, and other cortical processing. It is reasonable to conjecture that such functions may include prediction, the estimation or inference of missing or noisy sensory data, and the goal-driven generation of control signals. The resemblances found between the KPC NN architecture and that of the LCC are consistent with this conjecture.",
    "keywords": [
      "kalman filter",
      "kalman control",
      "recurrent neural network",
      "local cortical circuit"
    ]
  },
  {
    "id": "23",
    "title": "Learning point-to-point movements on an elastic limb using dynamic movement primitives",
    "abstract": "Expansion of the DMP approach for gravitation compensation in elastic robots. Grid-based mixture approach based on bilinear interpolation of learned trajectories. Model-free gravitation compensation in directed limb movements.",
    "keywords": [
      "passive compliance",
      "compliant robotics",
      "movement primitives",
      "reinforcement learning",
      "robot arm",
      "directed limb movement"
    ]
  },
  {
    "id": "24",
    "title": "Audio dual watermarking scheme for copyright protection and content authentication",
    "abstract": "We propose a new multipurpose audio watermarking scheme in which two watermarks are used. For intellectual property protection, audio clip is divided into frames and robust watermark is embedded. At the same time, the feature of each frame is extracted, and it is quantized as semi-fragile watermark. Then, the frame is cut into sections and the semi-fragile watermark bits are embedded into these sections. For content authentication, the semi-fragile watermark extracted from each frame is compared with the watermark generated from the same frame to judge whether the watermarked audio is tampered, and locate the tampered position. Experimental results show that our scheme is inaudibility. The two watermark schemes are all robust to common signal processing operations such as additive noise, resampling, re-quantization and low-pass filtering, and the semi-fragile watermark scheme can achieve tampered detection and location.",
    "keywords": [
      "multipurpose audio watermarking",
      "robust watermark",
      "copyright protection",
      "semi-fragile watermark",
      "content authentication"
    ]
  },
  {
    "id": "25",
    "title": "A comparison of path planning strategies for autonomous exploration and mapping of unknown environments",
    "abstract": "To date, a large number of algorithms to solve the problem of autonomous exploration and mapping has been presented. However, few efforts have been made to compare these techniques. In this paper, an extensive study of the most important methods for autonomous exploration and mapping of unknown environments is presented. Furthermore, a representative subset of these techniques has been chosen to be analysed. This subset contains methods that differ in the level of multi-robot coordination and in the grade of integration with the simultaneous localization and mapping (SLAM) algorithm. These exploration techniques were tested in simulation and compared using different criteria as exploration time or map quality. The results of this analysis are shown in this paper. The weaknesses and strengths of each strategy have been stated and the most appropriate algorithm for each application has been determined.",
    "keywords": [
      "autonomous exploration",
      "mapping of unknown environments",
      "path planning for multiple mobile robot systems"
    ]
  },
  {
    "id": "26",
    "title": "Improving reliable multicast using active parity encoding services",
    "abstract": "We propose and evaluate novel reliable multicast protocols that combine active repair service (a.k.a. local recovery) and parity encoding (a.k.a. forward error correction or FEC) techniques. We show that, compared to other repair service protocols, our protocols require less buffer inside the network, maintain the low bandwidth requirements of previously proposed repair service/FEC combination protocols, and reduce the amount of FEC processing at repair servers, moving more of this processing to the end-hosts. We also examine repair service/FEC combination protocols in an environment where loss rates differ across domains within the network. We find that repair services are more effective than FEC at reducing bandwidth utilization in such environments. Furthermore, we show that adding FEC to a repair services protocol not only reduces buffer requirements at repair servers, but also reduces bandwidth utilization in domains with high loss, or in domains with large populations of receivers.",
    "keywords": [
      "reliable multicast",
      "forward error correction",
      "repair services",
      "active services",
      "performance analysis"
    ]
  },
  {
    "id": "27",
    "title": "A comparative study on concept drift detectors",
    "abstract": "We evaluated eight different concept drift detectors. A 2k factorial design was used to indicate the best parameters for each method. Tests compared accuracy, evaluation time, false alarm and miss detection rates. A Mahalanobis distance is proposed as a metric to compare drift methods. DDM was the method that presented the best average results in all tested datasets.",
    "keywords": [
      "data streams",
      "time-changing data",
      "concept drift detectors",
      "comparison"
    ]
  },
  {
    "id": "28",
    "title": "automatic photo pop-up",
    "abstract": "This paper presents a fully automatic method for creating a 3D model from a single photograph. The model is made up of several texture-mapped planar billboards and has the complexity of a typical children's pop-up book illustration. Our main insight is that instead of attempting to recover precise geometry, we statistically model geometric classes defined by their orientations in the scene. Our algorithm labels regions of the input image into coarse categories: \"ground\", \"sky\", and \"vertical\". These labels are then used to \"cut and fold\" the image into a pop-up model using a set of simple assumptions. Because of the inherent ambiguity of the problem and the statistical nature of the approach, the algorithm is not expected to work on every image. However. it performs surprisingly well for a wide range of scenes taken from a typical person's photo album.",
    "keywords": [
      "single-view reconstruction",
      "image-based rendering",
      "machine learning",
      "image segmentation"
    ]
  },
  {
    "id": "29",
    "title": "H and H2 stabilisers via static output feedback based on coordinate transformations with free variables",
    "abstract": "This article designs H and H2 stabilisers, respectively, for linear time-invariant systems via static output feedback (SOF). A state coordinate transformation of controlled system generates a dummy system with lower dimension, which cannot be directly influenced by the SOF stabiliser. Then the H (H2) stabiliser via SOF may be obtained by solving proper linear matrix inequality (LMI). This LMI is feasible only if the dummy system has a state feedback stabiliser with the same H (H2) index. Meanwhile, a free matrix variable in coordinate transformation can act as the state feedback gain matrix. Hence after the design of dummy system, the SOF stabiliser can be determined if certain LMI is feasible. This method does not concern any conservative reduction or enlargement of matrix inequalities. Numerical examples show the validity of the proposed algorithms.",
    "keywords": [
      "h2",
      "static output feedback",
      "coordinates transformation",
      "lmi",
      "optimal control",
      "h control"
    ]
  },
  {
    "id": "30",
    "title": "Observation of linear systems with unknown inputs via high-order sliding-modes",
    "abstract": "A high- order sliding- mode observer is designed for linear time invariant systems with single output and unknown bounded single input. It provides for the global observation of the state and the output under sufficient and necessary conditions of strong observability or strong detectability. The observation is finite- time- convergent and exact in the strong observability case. The accuracy of the proposed observation and identification schemes is estimated via the sampling step or magnitude of deterministic noises. The results are extended to the multi- input multi- output case.",
    "keywords": [
      "high order sliding modes",
      "observation",
      "identification"
    ]
  },
  {
    "id": "31",
    "title": "A Mathematical Model of Penile Vascular Dysfunction and Its Application to a New Diagnostic Technique",
    "abstract": "A noninvasive diagnostic device was developed to assess the vascular origin and severity of penile dysfunction. It was designed and studied using both a mathematical model of penile hemodynamics and preliminary experiments on healthy young volunteers. The device is based on the application of an external pressure (or vacuum) perturbation to the penis following the induction of erection. The rate of volume change while the penis returns to its natural condition is measured using a noninvasive system that includes a volume measurement mechanism that has very low friction, thereby not affecting the measured system. The rate of volume change (net flow) is obtained and analyzed. Simulations using a mathematical model show that the device is capable of differentiating between arterial insufficiency and venous leak and indicate the severity of each. In preliminary measurements on young healthy volunteers, the feasibility of the measurement has been demonstrated. More studies are required to confirm the diagnostic value of the measurements",
    "keywords": [
      "erectile dysfunction",
      "arterial insufficiency",
      "venous leak",
      "veno-occlusive mechanism",
      "mathematical model",
      "hemodynamics"
    ]
  },
  {
    "id": "32",
    "title": "The spectra of irreducible matrices over completed idempotent semifields",
    "abstract": "Motivated by some spectral results in the characterization of concept lattices we investigate the spectra of reducible matrices over complete idempotent semifields in the framework of naturally-ordered semirings, or dioids. We find non-null eigenvectors for every non-null element in the semifield and conclude that the notion of spectrum has to be refined to encompass that of the incomplete semifield case so as to include only those eigenvalues with eigenvectors that have finite coordinates. Considering special sets of eigenvectors brings out finite complete lattices in the picture and we contend that such structure may be more important than standard eigenspaces for matrices over completed idempotent semifields.",
    "keywords": [
      "matrix spectra",
      "dioids",
      "complete idempotent semifields",
      "complete idempotent semimodules",
      "spectral order lattices"
    ]
  },
  {
    "id": "33",
    "title": "interactive technologies for health special interest group",
    "abstract": "Health and how to support it with interactive computer systems, networks, and devices is a global and, for many countries, an explicit national priority. Significant interest in issues related to interactive systems for health has been demonstrated repeatedly within SIGCHI. A community focused on health started in 2010, fostering collaboration and dissemination of research findings as well as bridging with practitioners. As part of this community's on-going efforts, we will hold a special interest group session during ACM CHI 2011 to discuss, prioritize, and promote some of these most pressing issues facing the community.",
    "keywords": [
      "fitness",
      "assistive technologies",
      "medicine",
      "telecare",
      "wellness",
      "health",
      "nutrition",
      "health informatics"
    ]
  },
  {
    "id": "34",
    "title": "Cyclic sequence alignments: Approximate versus optimal techniques",
    "abstract": "The problem of cyclic sequence alignment is considered. Most existing optimal methods for comparing cyclic sequences are very time consuming. For applications where these alignments are intensively used, optimal methods axe seldom a feasible choice. The alternative to an exact and costly solution is to use a close-to-optimal but cheaper approach. In previous works, we have presented three suboptimal techniques inspired on the quadratic-time suboptimal algorithm proposed by Bunke and Buhler. Do these approximate approaches come sufficiently close to the optimal solution, with a considerable reduction in computing time? Is it thus worthwhile investigating these approximate methods? This paper shows that approximate techniques are good alternatives to optimal methods.",
    "keywords": [
      "cyclic sequences",
      "cyclic string matching",
      "structural pattern analysis"
    ]
  },
  {
    "id": "35",
    "title": "Direct AuAu bonding technology for high performance GaAs/AlGaAs quantum cascade lasers",
    "abstract": "In this paper we investigate chip bonding technology of GaAs/AlGaAs quantum cascade lasers (QCLs). Its results have strong influence on final performance of devices and are essential for achieving room temperature operation. Various solders were investigated and compared in terms of their thermal resistance and induced stress. The spatially resolved photoluminescence technique has been applied for a device thermal analysis. The soldering quality was also investigated by means of a scanning acoustic microscopy. The particular attention has been paid to AuAu die bonding, which seems to be a promising alternative to the choice between hard and soft solder bonding of GaAs/AlGaAs QCLs operating from cryogenic temperatures up to room temperatures. A good quality direct AuAu bonding was achieved for bonding parameters comparable with the ones typical for AuSn eutectic bonding process. High performance room temperature operation of GaAs/AlGaAs QCLs has been achieved with the state-of-the-art parameters.",
    "keywords": [
      "gaas/algaas quantum cascade laser",
      "mounting technology",
      "die-bonding",
      "packaging",
      "scanning accustic microscopy"
    ]
  },
  {
    "id": "36",
    "title": "Harnessing Cloud Technologies for a Virtualized Distributed Computing Infrastructure",
    "abstract": "The InterGrid system aims to provide an execution environment for running applications on top of interconnected infrastructures. The system uses virtual machines as building blocks to construct execution environments that span multiple computing sites. Such environments can be extended to operate on cloud infrastructures, such as Amazon EC2. This article provides an abstract view of the proposed architecture and its implementation; experiments show the scalability of an InterGrid-managed infrastructure and how the system can benefit from using the cloud.",
    "keywords": [
      "amazon ec2",
      "cloud computing",
      "grid computing",
      "distributed systems",
      "scheduling",
      "resource management",
      "virtualization",
      "intergrid gateway"
    ]
  },
  {
    "id": "37",
    "title": "A method of MPEG2-TS test stream generation for digital TV software",
    "abstract": "Input of the digital TV software is a transport stream (TS) in MPEG (Moving Picture Expert Group)-2 format, a standard specification for moving picture compression. We propose a method to thoroughly generate MPEG-2 TS test data, namely, a test stream based on the black-box test concept for digital TV software. We also introduce a tool to automate the test stream generation known as Auto-TEst data generator from Protocol standard (ATEP). This empirical study of the application of an ATEP-derived test stream to an actual digital TV software settop box should benefit digital TV software developers as well as other testers.",
    "keywords": [
      "mpeg2-ts test stream",
      "digital tv software test",
      "black-box test"
    ]
  },
  {
    "id": "38",
    "title": "UNIFIED REPRESENTATION OF ZIPF DISTRIBUTIONS",
    "abstract": "Certain discrete probability distributions, used independently from each other in linguistics and other sciences, can be considered as special cases of the distribution based on the Lerch zeta function. We will list the probability functions for some of the most important cases. Moments and estimators are derived for the general Lerch distribution.",
    "keywords": [
      "lerch zeta function",
      "zipf distributions",
      "estimators",
      "nonlinear equation systems"
    ]
  },
  {
    "id": "39",
    "title": "Physical quantum algorithms",
    "abstract": "I review the differences between classical and quantum systems, emphasizing the connection between no-hidden variable theorems and superior computational power of quantum computers. Using quantum lattice gas automata as examples, I describe possibilities for efficient simulation of quantum and classical systems with a quantum computer. I conclude with a list of research directions. ",
    "keywords": [
      "quantum simulation",
      "quantum lattice gas automata"
    ]
  },
  {
    "id": "40",
    "title": "A slack-diversifying nonlinear fluctuation smoothing rule for job dispatching in a wafer fabrication factory",
    "abstract": "This study proposes a slack-diversifying nonlinear fluctuation smoothing rule to reduce the average cycle time in a wafer fabrication factory. The slack-diversifying nonlinear fluctuation smoothing rule is derived from the one-factor tailored nonlinear fluctuation smoothing rule for cycle time variation (1f-TNFSVCT) by dynamically maximizing the standard deviation of the slack, which has been shown to improve scheduling performance in several previous studies. The effectiveness of the proposed rule has been validated via using it with a simulated data set. Based on the findings in this research we also derived several directions that can be exploited in the future.",
    "keywords": [
      "wafer fabrication",
      "dispatching rule",
      "slack",
      "diversify",
      "fluctuation smoothing"
    ]
  },
  {
    "id": "41",
    "title": "Linear and compact floating node voltage-controlled variable resistor circuit",
    "abstract": "In this letter, my proposals for a Floating node voltagecontrolled Variable Resistor circuit (FVR) are based upon its advantages as linear and compact. The performance of the proposed circuit was confirmed by PSpice simulation. The simulation results are reported in this letter.",
    "keywords": [
      "analog integrated circuits",
      "floating node",
      "voltage-controlled variable resistor circuit"
    ]
  },
  {
    "id": "42",
    "title": "The management information systems (MIS) job market late 1970s-late 1990s",
    "abstract": "The rapidly changing information technology (IT) environment continues to pose a challenging dilemma for both management information systems (MIS) managers and MIS educators at all levels, especially the collegiate level. This research examines the content of MIS-related job advertisements over a 20-year period: late 1970s-late 1990s. It is the continuation of a study initially published in The Journal of Computer Information Systems (6) and includes the data that represents the late 1990s timeframe. Results trace the rise and fall in demand for certain IT skills and knowledge and identify the growing strength or stability of others. The study clearly exposes the great diversity in the MIS job market. This diversity is the root cause of the dilemma confronting MIS managers and MIS educators as they try to recruit workers from or prepare students for the changing IT environment.",
    "keywords": [
      "mis job market",
      "mis job market diversity",
      "mis skills",
      "mis knowledge"
    ]
  },
  {
    "id": "43",
    "title": "Development of a ceramic bolus for the permanent electronic identification of sheep, goat and cattle",
    "abstract": "Retention rate and digestive and performance effects of ceramic boluses (6620 mm, 65 g) enclosing passive transponders (32.53.8 mm) were studied in three experiments. Reading distances of transponders inside and outside the boluses (n=10) did not vary. In the first experiment, a total of 2452 boluses were applied to 74 lambs and 808 ewes, 16 young and 67 adult goats, 1138 calves and 349 cows. Plastic balling guns were used to insert the boluses and their effects were evaluated during 3 years or until slaughter. Time needed for application and recommended live-weights (LW) depended on animal category (sheep, 24 s and >25 kg; goats, 26 s and >20 kg; cattle, 19240 s and >30 kg). Application in calves was possible during the first week of life. Retention rates were 100, 98.8 and 99.7% in sheep, goats and cattle, respectively. The location of boluses in the reticulum was checked with hand-held readers and verified by X-ray in a sample (n=4) of each animal category or directly in cannulated cows (n=3). Transceivers were interfaced with electronic scales for automatic weight recording. Dynamic reading efficiency was 100% in race-ways with a frame antenna (9452 cm). Health and performances were not modified by boluses. An average of 93% of boluses were found in the reticulum at slaughter. Recovery rates and times varied according to animal category (lambs, 100% and 5 s; ewes and goats, 100% and 8 s; fattened calves, 91.3% and 12 s; dairy cows, 72% and 14 s). In the second experiment, two groups of adult ewes (control, n=5; bolus, n=5) were housed in individual pens and fed forage ad libitum. Mean forage intake and nutrient digestibility were not varied by the ceramic boluses. In the third experiment 45 fattening male lambs (20 kg LW) and 20 replacement ewe-lambs (30 kg LW) were used. Fattening lambs were divided into two groups and assigned to the treatments (control, n=25; bolus, n=20) until slaughter (25 kg LW). In spite of the difficulties observed in the force-feeding of boluses in eight lambs (40%), average daily gain and reticulum-rumen mucosa were not altered. Ewe-lambs were also assigned to the treatments in two groups (control, n=10; bolus, n=10) and monitored until first lambing or 1 year old. The weight, body condition score and reproductive performance were not affected by boluses. In conclusion, the use of the ceramic bolus is recommended as a safe and tamper-proof method for electronic identification of ruminants once the animals have reached a weight where successful administration is possible. Moreover, boluses proved to be useful for dynamic reading and automatic weight recording on farm conditions.",
    "keywords": [
      "animal identification",
      "ceramic bolus",
      "reading range"
    ]
  },
  {
    "id": "44",
    "title": "OWL-Eu: Adding customised datatypes into OWL",
    "abstract": "Although OWL is rather expressive, it has a very serious limitation on datatypes; i.e., it does not support customised datatypes. It has been pointed out that many potential users will not adopt OWL unless this limitation is overcome, and the W3C Semantic Web Best Practices and Development Working Group has set up a task force to address this issue. This paper makes the following two contributions: (i) it provides a brief summary of OWL-related datatype formalisms, and (ii) it provides a decidable extension of OWL DL, called OWL-Eu, that supports customised datatypes. A detailed proof of the decidability of OWL-Eu is presented.",
    "keywords": [
      "ontologies",
      "semantic web",
      "description logics",
      "customised datatypes",
      "unary datatype groups"
    ]
  },
  {
    "id": "45",
    "title": "Turning a community into a market: A practice perspective on information technology use in boundary spanning",
    "abstract": "This paper examines how information technology (IT) transforrns relations across fields of practice within organizations. Drawing on Bourdieu's practice theory, we argue that the production of any practice involves varying degrees of embodiment (i.e., relying on personal relationships) and objectification (i.e., relying on the exchange of objects). We subsequently characterize boundary-spanning practices according to their relative degrees of embodiment and objectification. We distinguish between \"market-like\" boundary-spanning practices, which rely primarily on an objectified mode of practice production, from \"community-like\" practices, which involve mostly the embodied mode of practice production. IT is then conceptualized as a medium for sharing objects in the production of practices. As such, IT use allows for the sharing of objects without relying on embodied relationships. We use data from an in-depth ethnographic case study to investigate how IT was used to transform community-like boundary-spanning practices within an organization into market-like ones. Moreover, we demonstrate how, as IT was used to support the exchange and combination of depersonalized objects, other aspects of the practice (such as the roles of intermediaries and the nature of meetings) also changed. The related changes in these diverse aspects of a boundary-spanning practice supported the trend toward greater objectification. IT use also increased visibility of the terms associated with object exchange. This increased visibility exposed the inequity of the exchange and encouraged the disadvantaged party to renegotiate the relationship.",
    "keywords": [
      "boundary objects",
      "boundary spanners",
      "boundary spanning",
      "communities of practice",
      "coordination mechanisms",
      "information technology use",
      "practice theory",
      "qualitative methods"
    ]
  },
  {
    "id": "46",
    "title": "Integration of Different Risk Assessment Tools to Improve Stratification of Patients with Coronary Artery Disease",
    "abstract": "Cardiovascular disease (CVD) causes unaffordable social and health costs that tend to increase as the European population ages. In this context, clinical guidelines recommend the use of risk scores to predict the risk of a cardiovascular disease event. Some useful tools have been developed to predict the risk of occurrence of a cardiovascular disease event (e.g. hospitalization or death). However, these tools present some drawbacks. These problems are addressed through two methodologies: (i) combination of risk assessment tools: fusion of nave Bayes classifiers complemented with a genetic optimization algorithm and (ii) personalization of risk assessment: subtractive clustering applied to a reduced-dimensional space to create groups of patients. Validation was performed based on two ACS-NSTEMI patient data sets. This work improved the performance in relation to current risk assessment tools, achieving maximum values of sensitivity, specificity, and geometric mean of, respectively, 79.8, 83.8, and 80.9%. Additionally, it assured clinical interpretability, ability to incorporate of new risk factors, higher capability to deal with missing risk factors and avoiding the selection of a standard CVD risk assessment tool to be applied in the clinical practice.",
    "keywords": [
      "information and knowledge management",
      "management of cardiovascular diseases",
      "decision-support systems"
    ]
  },
  {
    "id": "47",
    "title": "Segmenting ideal morphologies of sewer pipe defects on CCTV images for automated diagnosis",
    "abstract": "Several literatures presented automated systems for detecting or classifying sewer pipe defects based on morphological features of pipe defects. In those automated systems, however, the morphologies of the darker center or some uncertain objects on CCTV images are also segmented and become noises while morphology-based pipe defect segmentation is implemented. In this paper, the morphology-based pipe defect segmentation is proposed and discussed to be an improved approach for automated diagnosis of pipe defects on CCTV images. The segmentation of pipe defect morphologies is first to implement an opening operation for gray-level CCTV images to distinguish pipe defects. Then, Otsu's technique is used to segment pipe defects by determining the optimal thresholds for gray-level CCTV images of opening operation. Based on the segmentation results of CCTV images, the ideal morphologies of four typical pipe defects are defined. If the segmented CCTV images match the definition of those ideal morphologies, the pipe defects on those CCTV images can be successfully identified by a radial basis network (RBN) based diagnostic system. As for the rest CCTV images failing to match the ideal morphologies, the failure causes was discussed so to suggest a regulation for imaging conditions, such as camera pose and light source, in order to obtain CCTV images for successful segmentation.  ",
    "keywords": [
      "cctv",
      "image processing",
      "morphologies of pipe defects",
      "diagnostic system"
    ]
  },
  {
    "id": "48",
    "title": "A new fuzzy rule-based classification system for word sense disambiguation",
    "abstract": "Word sense disambiguation (WSD) can be thought of as the most challenging task in the process of machine translation. Various supervised and unsupervised learning methods have already been proposed for this purpose. In this paper, we propose a new efficient fuzzy classification system in order to be applied for WSD. In order to optimize the generalization accuracy, we use rule-weight as a simple mechanism to tune the classifier and propose a new learning method to iteratively adjust the weight of fuzzy rules. Through computer simulations on TWA data as a standard corpus, the proposed scheme shows a uniformly good behavior and achieves results which are comparable or better than other classification systems, proposed in the past.",
    "keywords": [
      "word sense disambiguation",
      "machine translation",
      "fuzzy systems",
      "classification",
      "rule-weight",
      "generalization accuracy"
    ]
  },
  {
    "id": "49",
    "title": "On the hierarchy of conservation laws in a cellular automaton",
    "abstract": "Conservation laws in cellular automata (CA) are studied as an abstraction of the conservation laws observed in nature. In addition to the usual real-valued conservation laws we also consider more general group-valued and semigroup-valued conservation laws. The (algebraic) conservation laws in a CA form a hierarchy, based on the range of the interactions they take into account. The conservation laws with smaller interaction ranges are the homomorphic images of those with larger interaction ranges, and for each specific range there is a most general law that incorporates all those with that range. For one-dimensional CA, such a most general conservation law haseven in the semigroup-valued casean effectively constructible finite presentation, while for higher-dimensional CA such effective construction exists only in the group-valued case. It is even undecidable whether a given two-dimensional CA conserves a given semigroup-valued energy assignment. Although the local properties of this hierarchy are tractable in the one-dimensional case, its global properties turn out to be undecidable. In particular, we prove that it is undecidable whether this hierarchy is trivial or unbounded. We point out some interconnections between the structure of this hierarchy and the dynamical properties of the CA. In particular, we show that positively expansive CA do not have non-trivial real-valued conservation laws.",
    "keywords": [
      "cellular automata",
      "conservation laws",
      "energy",
      "reversibility",
      "undecidability",
      "dynamical systems",
      "chaos"
    ]
  },
  {
    "id": "50",
    "title": "Positive periodic solutions for the neutral ratio-dependent predatorprey model",
    "abstract": "By using a continuation theorem based on coincidence degree theory, we obtain some new sufficient conditions for the existence of positive periodic solutions for the neutral ratio-dependent predatorprey model with Holling type II functional response.",
    "keywords": [
      "predatorprey model",
      "ratio-dependent",
      "periodic solution",
      "neutral",
      "coincidence degree"
    ]
  },
  {
    "id": "51",
    "title": "Impact of dual-k spacer on analog performance of underlap FinFET",
    "abstract": "Multigate structures have better short channel control than conventional bulk devices due to increased gate electrostatic control. FinFET is a promising candidate among multigate structures due to its ease of manufacturability. The RF performance of FinFET is affected by gate controlled parameters such as transconductance, output conductance and total gate capacitance. In this paper we have used dual-k spacers in underlap FinFETs to improve the gate electrostatic integrity. The inner high-k spacer helps in better screening out the gate sidewall fringing fields, thereby, increasing transconductance and reducing output conductance with increase in total gate capacitance. At 16nm gate lengths, we have observed that, the intrinsic gain of dual-k spacer based FinFET can be increased by more than 100% (>6dB) without affecting cutoff frequency and maximum oscillation frequency, as compared to conventional single spacer based FinFET. Improvement in cutoff frequency by 11% and maximum oscillation frequency by 5% can be achieved, when the gate lengths are scaled down to 12nm, in addition to 2.75 times (8.8dB) increase in intrinsic gain.",
    "keywords": [
      "short channel effect ",
      "dual-k spacer",
      "figures of merit ",
      "electrostatic integrity ",
      "intrinsic gain",
      "cutoff frequency"
    ]
  },
  {
    "id": "52",
    "title": "3D depth estimation for visual inspection using in wavelet transform modulus maxima",
    "abstract": "A vision based approach for calculating accurate 3D models of the objects is presented. Generally industrial visual inspection systems capable of accurate 3D depth estimation rely on extra hardware tools like laser scanners or light pattern projectors. These tools improve the accuracy of depth estimation but also make the vision system costly and cumbersome. In the proposed algorithm, depth and dimensional accuracy of the produced 3D depth model depends on the existing reference model instead of the information from extra hardware tools. The proposed algorithm is a simple and cost effective software based approach to achieve accurate 3D depth estimation with minimal hardware involvement. The matching process uses the well-known coarse to fine strategy, involving the calculation of matching points at the coarsest level with consequent refinement up to the finest level. Vector coefficients of the wavelet transform-modulus are used as matching features, where wavelet transform-modulus maxima defines the shift invariant high-level features with phase pointing to the normal of the feature surface. The technique addresses the estimation of optimal corresponding points and the corresponding 2D disparity maps leading to the creation of accurate depth perception model.  ",
    "keywords": [
      "wavelet transform modulus",
      "coarse to fine",
      "disparity",
      "3d depth"
    ]
  },
  {
    "id": "53",
    "title": "Adaptive FEC-based packet loss resilience scheme for supporting voice communication over ad hoc wireless networks",
    "abstract": "Providing real-time voice support over multihop ad hoc wireless networks (AWNS) is a challenging task. The standard retransmission-based strategies proposed in the literature are poorly matched to voice applications because of timeliness and large overheads involved in transmitting small-sized voice packets. To make a voice application feasible over AWNS, the perceived voice quality must be improved while not significantly increasing the packet overhead. We suggest packet-level media-dependent adaptive forward error correction (FEC) at the application layer in tandem with multipath transport for improving the voice quality. Since adaptive FEC masks packet losses in the network, at the medium access control (MAC) layer, we avoid retransmissions (hence, no acknowledgments) in order to reduce the control overhead and end-to-end delay. Further, we exploit the combined strengths of layered coding and multiple description (MD) coding for supporting error-resilient voice communication in AWNS. We propose an efficient packetization scheme in which the important substream of the voice stream is protected adaptively with FEC depending on the loss rate present in the network and is transmitted over two maximally node-disjoint paths. The less important substream of the voice stream is encoded into two descriptions, which are then transmitted over two maximally node-disjoint paths. The performance of our scheme (packet-level media-dependent adaptive FEC scheme) is evaluated in terms of two parameters: residual packet loss rate (RPLR, packet loss rate after FEC recovery) and average burst length (ABL, average length of consecutive packet losses after FEC recovery) of voice data after FEC recovery. The sets of equations leading to the analytical formulation of both RPLR and ABL are first given for a renewal error process. The values of both these parameters depend on FEC-Offset (r, the distance between original voice frame and piggybacked redundant voice frame) and loss rate present in the network. Then, these parameters are computed for a Gilbert-Elliott (GE) two-state Markov error model and compared with experimental data. Our scheme adaptively selects the FEC-Offset (it chooses r that minimizes RPLR and ABL as much as possible) based on the loss rate feedback obtained from the destination. The proposed scheme achieves significant gains in terms of reduced frame loss rate (FLR), reduced control overhead, and minimum end-to-end delay and almost doubles the perceived voice quality compared to the existing approaches.",
    "keywords": [
      "ad hoc networks",
      "voice frame",
      "layered coding",
      "multiple description coding",
      "forward error correction",
      "packetization scheme",
      "voice quality",
      "multipath transport",
      "multimedia"
    ]
  },
  {
    "id": "54",
    "title": "Introduction of a data schema to support a design repository",
    "abstract": "This paper presents the data schema required to capture fundamental elements of design information in a heterogeneous repository supporting design reuse. Design information captured by the repository can be divided into seven main categories of artifact-, function-, failure-, physical-, performance-, sensory- and media-related information types. Each of the seven types of design information is described in detail. The repository schema is specific to a relational database system driving the implemented design repository; however, the types of design information recorded are applicable to any implementation of a design repository. The aim of this paper is to fully describe the data schema such that it could be recreated or specialized for industrial or research applications. The result is a complete description of fundamental design knowledge to support design reuse and a data schema specification. The data schema has been vetted with the implemented design repository that contains design information for over 100 consumer electro-mechanical products.",
    "keywords": [
      "design repository schema",
      "conceptual design"
    ]
  },
  {
    "id": "55",
    "title": "Distributivity in lattices of fuzzy subgroups",
    "abstract": "The main goal of this paper is to study the finite groups whose lattices of fuzzy subgroups are distributive. We obtain a characterization of these groups which is similar to a well-known result of group theory.  ",
    "keywords": [
      "fuzzy subgroup lattices",
      "subgroup lattices",
      "distributivity",
      "finite cyclic groups",
      "equivalence relations"
    ]
  },
  {
    "id": "56",
    "title": "on delegation and workflow execution models",
    "abstract": "Workflow systems have long been of interest to computer science researchers due to their practical relevance. Supporting delegation mechanisms in workflow systems is receiving increasing research interest. In this paper, we conduct a comprehensive study of user delegation operations in computerized workflow systems. In a workflow system, the semantics of a delegation operation are largely based on three factors: the underlying workflow execution model, task type and delegation type. We describe three different workflow execution models and examine the effect of various delegation operations in each workflow execution model. We then extend our workflow execution models to examine the effect of various delegation operations in different role-based workflow execution models.",
    "keywords": [
      "delegation",
      "workflow management systems"
    ]
  },
  {
    "id": "57",
    "title": "On-line anomaly detection and resilience in classifier ensembles",
    "abstract": "Detection of anomalies is a broad field of study, which is applied in different areas such as data monitoring, navigation, and pattern recognition. In this paper we propose two measures to detect anomalous behaviors in an ensemble of classifiers by monitoring their decisions; one based on Mahalanobis distance and another based on information theory. These approaches are useful when an ensemble of classifiers is used and a decision is made by ordinary classifier fusion methods, while each classifier is devoted to monitor part of the environment. Upon detection of anomalous classifiers we propose a strategy that attempts to minimize adverse effects of faulty classifiers by excluding them from the ensemble. We applied this method to an artificial dataset and sensor-based human activity datasets, with different sensor configurations and two types of noise (additive and rotational on inertial sensors). We compared our method with two other well-known approaches, generalized likelihood ratio (GLR) and One-Class Support Vector Machine (OCSVM), which detect anomalies at data/feature level. We found that our method is comparable with GLR and OCSVM. The advantages of our method compared to them is that it avoids monitoring raw data or features and only takes into account the decisions that are made by their classifiers, therefore it is independent of sensor modality and nature of anomaly. On the other hand, we found that OCSVM is very sensitive to the chosen parameters and furthermore in different types of anomalies it may react differently. In this paper we discuss the application domains which benefit from our method.",
    "keywords": [
      "anomaly detection",
      "classifier ensemble",
      "decision fusion",
      "human activity recognition"
    ]
  },
  {
    "id": "58",
    "title": "Scheduling unrelated parallel machines to minimize total weighted tardiness",
    "abstract": "This article considers the problem of scheduling a given set of independent jobs on unrelated parallel machines to minimize the total weighted tardiness. The problem is known to be NP-hard in the strong sense. Efficient lower and upper bounds are developed. The lower bound is based on the solution of an assignment problem, while the upper bound is obtained by a two-phase heuristic. A branch-and-bound algorithm that incorporates various dominance rules is presented. Computational experiments are conducted to demonstrate the performance of the proposed algorithm. Scope and purpose Parallel machine scheduling models are important from both the theoretical and practical points of view. From the theoretical point of view, they generalize the single machine scheduling models. From the practical point of view, they are important because the occurrence of a bank of machines in parallel is common in industries. In this article, the unrelated parallel machine total weighted tardiness scheduling problem is examined. The tardiness criterion has many applications in real world. This problem is difficult to solve. A branch-and-bound algorithm that incorporates various dominance rules along with efficient lower and upper bounds is proposed to find an optimal solution.",
    "keywords": [
      "scheduling",
      "parallel machines",
      "branch-and-bound",
      "tardiness"
    ]
  },
  {
    "id": "59",
    "title": "Lock-free deques and doubly linked lists",
    "abstract": "We present a practical lock-free shared data structure that efficiently implements the operations of a concurrent deque as well as a general doubly linked list. The implementation supports parallelism for disjoint accesses and uses atomic primitives which are available in modern computer systems. Previously known lock-free algorithms of doubly linked lists are either based on non-available atomic synchronization primitives, only implement a subset of the functionality, or are not designed for disjoint accesses. Our algorithm only requires single-word compare-and-swap atomic primitives, supports fully dynamic list sizes, and allows traversal also through deleted nodes and thus avoids unnecessary operation retries. We have performed an empirical study of our new algorithm on two different multiprocessor platforms. Results of the experiments performed under high contention show that the performance of our implementation scales linearly with increasing number of processors. Considering deque implementations and systems with low concurrency, the algorithm by Michael shows the best performance. However, as our algorithm is designed for disjoint accesses, it performs significantly better on systems with high concurrency and non-uniform memory architecture.",
    "keywords": [
      "deque",
      "doubly linked list",
      "non-blocking",
      "lock-free",
      "shared data structure",
      "multi-thread",
      "concurrent"
    ]
  },
  {
    "id": "60",
    "title": "An optimal deployable bandwidth aggregation system",
    "abstract": "The explosive increase in data demand coupled with the rapid deployment of various wireless access technologies have led to the increase of number of multi-homed or multi-interface enabled devices. Fully exploiting these interfaces has motivated researchers to propose numerous solutions that aggregate their available bandwidths to increase overall throughput and satisfy the end-users growing data demand. These solutions, however, do not utilize their interfaces to the maximum without network support, and more importantly, have faced a steep deployment barrier. In this paper, we propose an optimal deployable bandwidth aggregation system (DBAS) for multi-interface enabled devices. We present the DBAS architecture that does not introduce any intermediate hardware, modify current operating systems, modify socket implementations, nor require changes to current applications or legacy servers. The DBAS architecture is designed to automatically estimate the characteristics of applications and dynamically schedule various connections and/or packets to different interfaces. We also formulate our optimal scheduler as a mixed integer programming problem yielding an efficient solution. We evaluate DBAS via implementation on the Windows OS and further verify our results with simulations on NS2. Our evaluation shows that, with current Internet characteristics, DBAS reaches the throughput upper bound with no modifications to legacy servers. It also highlights the significant enhancements in the response time introduced by DBAS, which directly enhances the user experience.",
    "keywords": [
      "bandwidth aggregation",
      "multiple network interfaces",
      "throughput",
      "optimization",
      "multihoming"
    ]
  },
  {
    "id": "61",
    "title": "Dynamic bandwidth allocation in heterogeneous WDM EPONs",
    "abstract": "Several dynamic bandwidth allocation algorithms have been introduced to schedule upstream wavelength channels in wavelength division multiplexing Ethernet passive optical networks (WDM EPONs), but mostly for homogenous WDM EPON networks with the same distance between each optical network unit (ONU) and the optical line terminal. For WDM EPON with heterogeneous round trip times (RTTs), we propose two algorithms for ONU scheduling, called nearest first and early allocation (EA); and a wavelength assignment algorithm, called best fit (BF). Both ONU scheduling algorithms take RTT dissimilarities into account, thus minimizing packet delay and packet drop ratio at ONUs. Additionally, EA can relive the common drawback of offline scheduling, i.e., channel idle time. On the other hand, the BF wavelength assignment is proposed that assigns the best wavelength to each ONU in order to improve network performances in terms of packet queuing delay and packet drop ratio at ONU sides.",
    "keywords": [
      "heterogeneous wdm eopn",
      "dynamic bandwidth allocation",
      "nearest first",
      "early allocation",
      "wavelength assignment"
    ]
  },
  {
    "id": "62",
    "title": "A layered framework to study collaboration as a form of knowledge sharing and diffusion",
    "abstract": "Collaboration is presented as a form of knowledge sharing and hence of knowledge diffusion. A layered framework for collaboration studies is proposed. The notions of relative and absolute proper essential node (PEN) centrality are introduced as indicators of a node's importance for diffusion of knowledge through collaboration.",
    "keywords": [
      "collaboration",
      "diffusion",
      "layered systems",
      "networks",
      "centrality indicators"
    ]
  },
  {
    "id": "63",
    "title": "Classes of tree languages determined by classes of monoids",
    "abstract": "In this paper finite state recognizers are considered as unary tree recognizers with unary operational symbols. We introduce translation recognizers of a tree recognizer, which are finite state recognizers whose operations are the elementary translations of the underlying algebra of the considered tree recognizer. In terms of translation recognizers we give general conditions under which a class of recognizable tree languages with a given property can be determined by a class of monoids determining the class of string languages having the same property.",
    "keywords": [
      "tree automata",
      "tree languages",
      "syntactic monoids"
    ]
  },
  {
    "id": "64",
    "title": "On a transfer theorem for the P not equal NP conjecture",
    "abstract": "A model of computation is defined over the algebraic numbers and over number fields. This model is non-uniform, and the cost of operations depends on the height of the operands and on the degree of the extension of the rational defined by those operands. A transfer theorem for the P not equal NP Conjecture is proved, namely: P not equal NP in this model over the real algebraic numbers if and only if P not equal NP in the classical setting. ",
    "keywords": [
      "np-completeness",
      "computability over a ring",
      "height",
      "transfer theorem"
    ]
  },
  {
    "id": "65",
    "title": "The design and implementation of a future Internet live TV system over 4G networks",
    "abstract": "The emerging 4G (fourth generation) networks featuring wider coverage, higher transmission bandwidth and easier deployment have a desirable potential to serve ubiquitous and pervasive multimedia applications in creating new user-centric communication services. However, the practical implementation of 4G network to demonstrate such potential, especially for delivering real-time and high-quality video services, is scarce. This paper therefore provides the design and implementation of a future Internet live TV system over 4G networks to achieve cost-effectiveness, instead of using expensive satellite news gathering (SNG) vehicle and costly satellite transmissions in traditional TV stations. To effectively provide live TV services, we apply not only the hybrid duplex modes but also the port-based VLAN on the deployed networks for maximizing bandwidth, minimizing signal interference, and guaranteeing QoS of differentiated services. Performance metrics are applied to demonstrate that the proposed solution is cost-effective and is feasible for live TV services in future Internet.",
    "keywords": [
      "future internet",
      "4g networks",
      "live tv system",
      "satellite news gathering  service",
      "vlan"
    ]
  },
  {
    "id": "66",
    "title": "Numerical study of a stochastic particle method for homogeneous gas-phase reactions",
    "abstract": "In this paper, we study a stochastic particle system that describes homogeneous gasphase reactions of a number of chemical species. First, we introduce the system as a Markov jump process and discuss how relevant physical quantities are represented in terms of appropriate random variables. Then, we show how various deterministic equations, used in the literature, are derived from the stochastic system in the limit when the number of particles goes to infinity. Finally, we apply the corresponding stochastic algorithm to a toy problem, a simple formal reaction mechanism, and a real combustion problem. This problem is given by the isothermal combustion of a homogeneous mixture of heptane and air modelled by a detailed reaction mechanism with 107 chemical species and 808 reversible reactions. Heptane as described in this chemical mechanism serves as model-fuel for different types of internal combustion engines. In particular, we study the order of convergence with respect to the number of simulation particles, and illustrate the limitations of the method.  ",
    "keywords": [
      "stochastic particle method",
      "combustion",
      "convergence",
      "efficiency"
    ]
  },
  {
    "id": "67",
    "title": "A Nitsche stabilized finite element method for frictional sliding on embedded interfaces. Part II: Intersecting interfaces",
    "abstract": "Developed a weighted Nitsche stabilized method for embedded interfaces with junctions. Provided an explicit expression for the method parameter for lower order elements in the presence of junctions. Examples highlight the method capabilities in modeling grain-boundary sliding behavior.",
    "keywords": [
      "frictional contact",
      "grain-boundary sliding",
      "junctions",
      "nitsche",
      "polycrystalline",
      "x-fem"
    ]
  },
  {
    "id": "68",
    "title": "A class of aggregation functions encompassing two-dimensional OWA operators",
    "abstract": "In this paper we prove that, under suitable conditions, Atanassovs K? operators, which act on intervals, provide the same numerical results as OWA operators of dimension two. On one hand, this allows us to recover OWA operators from K? operators. On the other hand, by analyzing the properties of Atanassovs operators, we can generalize them. In this way, we introduce a class of aggregation functions  the generalized Atanassov operators  that, in particular, include two-dimensional OWA operators. We investigate under which conditions these generalized Atanassov operators satisfy some properties usually required for aggregation functions, such as bisymmetry, strictness, monotonicity, etc. We also show that if we apply these aggregation functions to interval-valued fuzzy sets, we obtain an ordered family of fuzzy sets.",
    "keywords": [
      "OWA operators",
      "Interval-valued fuzzy sets operators",
      "Generalized K",
      "operators",
      "Dispersion"
    ]
  },
  {
    "id": "69",
    "title": "Data Structures on Event Graphs",
    "abstract": "We investigate the behavior of data structures when the input and operations are generated by an event graph. This model is inspired by Markov chains. We are given a fixed graph G, whose nodes are annotated with operations of the type insert, delete, and query. The algorithm responds to the requests as it encounters them during a (random or adversarial) walk in G. We study the limit behavior of such a walk and give an efficient algorithm for recognizing which structures can be generated. We also give a near-optimal algorithm for successor searching if the event graph is a cycle and the walk is adversarial. For a random walk, the algorithm becomes optimal.",
    "keywords": [
      "successor searching",
      "markov chain",
      "low entropy",
      "data structure"
    ]
  },
  {
    "id": "70",
    "title": "Radar detection algorithm for GARCH clutter model",
    "abstract": "We propose a GARCH model to represent the clutter in radar applications. We fit this model to real sea clutter data and we show that it represents adequately the statistics of the data. Then, we develop a detection test based on this model. Using synthetic and real radar data, we evaluate its performance and we show that the proposed detector offers higher probability of detection for a specified value of probability of false alarm than tests based on Gaussian and Weibull models, especially for low signal to clutter ratios.",
    "keywords": [
      "radar",
      "detection",
      "non-gaussian clutter",
      "garch processes"
    ]
  },
  {
    "id": "71",
    "title": "Particle-based non-photorealistic volume visualization",
    "abstract": "Non-photorealistic techniques are usually applied to produce stylistic renderings. In visualization, these techniques are often able to simplify data, producing clearer images than traditional visualization methods. We investigate the use of particle systems for visualizing volume datasets using non-photorealistic techniques. In our VolumeFlies framework, user-selectable rules affect particles to produce a variety of illustrative styles in a unified way. The techniques presented do not require the generation of explicit intermediary surfaces.",
    "keywords": [
      "visualization",
      "non-photorealistic rendering",
      "volume rendering",
      "particle systems"
    ]
  },
  {
    "id": "72",
    "title": "A framework for sequential multiblock component methods",
    "abstract": "Multiblock or multiset methods are starting to be used in chemistry and biology to study complex data sets. In chemometrics, sequential multiblock methods are popular; that is, methods that calculate one component at a time and use deflation for finding the next component. In this paper a framework is provided for sequential multiblock methods, including hierarchical PCA (HPCA; two versions), consensus PCA (CPCA; two versions) and generalized PCA (GPCA). Properties of the methods are derived and characteristics of the methods are discussed. All this is illustrated with a real five-block example from chromatography. The only methods with clear optimization criteria are GPCA and one version of CPCA. Of these, GPCA is shown to give inferior results compared with CPCA. ",
    "keywords": [
      "multiblock methods",
      "hierarchical pca",
      "consensus pca",
      "generalized pca",
      "multiway methods",
      "stationary phases",
      "reversed phase liquid chromatography"
    ]
  },
  {
    "id": "73",
    "title": "Of the rich and the poor and other curious minds: on open access and \"development\"",
    "abstract": "Purpose - The paper seeks to reconsider open access and its relation to issues of \"development\" by highlighting the ties the open access movement has with the hegemonic discourse of development and to question some of the assumptions about science and scientific communication upon which the open access debates are based. The paper also aims to bring out the conflict arising from the convergence of the hegemonic discourses of science and development with the contemporary discourse of openness. Design/methodology/approach - The paper takes the form of a critical reading of a range of published work on open access and the so-called \"developing world\" as well as of various open access declarations. The argument is supported by insights from post-development studies. Findings - Open access is presented as an issue of moral concern beyond the narrow scope of scholarly communication. Claims are made based on hegemonic discourses that are positioned as a priori and universal. The construction of open access as an issue of unquestionable moral necessity also impedes the problematisation of its own heritage. Originality/value - This paper is intended to open up the view for open access's less obvious alliances and conflicting discursive ties and thus to initiate a politisation, which is necessary in order to further the debate in a more fruitful way.",
    "keywords": [
      "developing countries",
      "sciences",
      "communication technologies",
      "journal publishers"
    ]
  },
  {
    "id": "74",
    "title": "geometric surrogate-based optimisation for permutation-based problems",
    "abstract": "In continuous optimisation, surrogate models (SMs) are used when tackling real-world problems whose candidate solutions are expensive to evaluate. In previous work, we showed that a type of SMs - radial basis function networks (RBFNs) - can be rigorously generalised to encompass combinatorial spaces based in principle on any arbitrarily complex underlying solution representation by extending their natural geometric interpretation from continuous to general metric spaces. This direct approach to representations does not require a vector encoding of solutions, and allows us to use SMs with the most natural representation for the problem at hand. In this work, we apply this framework to combinatorial problems using the permutation representation and report experimental results on the quadratic assignment problem.",
    "keywords": [
      "radial-basis functions",
      "representations",
      "surrogate model optimization"
    ]
  },
  {
    "id": "75",
    "title": "Computational approach to ensure the stability of the favorable ATP binding site in E. coli Hfq",
    "abstract": "Bacterial Hfq is a highly conserved thermostable protein of about 10kDa. The Hfq protein was discovered in 1968 as an E. coli host factor that was essential for replication of the bacteriophage Q?. It is now clear that Hfq has many important physiological roles. In E. coli, Hfq mutants show a multiple stress response related phenotypes. Hfq is now known to regulate the translation of two major stress transcription factors RpoS and RpoE in Enterobacteria and mediates its plieotrophic effects through several mechanisms. It interacts with regulatory sRNA and facilitates their antisense interaction with their targets. It also acts independently to modulate mRNA decay and in addition acts as a repressor of mRNA translation. Recent paper from Arluison et al. [9] provided the first evidence indicating that Hfq is an ATP-binding protein. They determined a plausible ATP-binding site in Hfq and tested Hfq's ATP-binding affinity and stoichiometry. Experimental data suggest that the ATP-binding by the HfqRNA complex results in its significant destabilization of the protein and the result also proves important role of Tyr25 that flanks the cleft and stabilizes the adenine portion of ATP, possibly via aromatic stacking. In our study, the ATP molecule was docked into the predicted binding cleft using GOLD docking software. The binding nature of ATP and its effect on HfqRNA complex was studied using molecular dynamics simulations. Importance of Tyr25 residue was monitored and revealed using mutational study on the modeled systems. Our data and the corresponding results point to one of Hfq functional structural consequences due to ATP binding and Tyr25Ala mutation.",
    "keywords": [
      "host factor protein-hfq",
      "atp",
      "oligoribonucleotide",
      "rna",
      "post-transcriptional regulation",
      "molecular dynamics simulation",
      "mutation",
      "aromatic stacking",
      "destabilization"
    ]
  },
  {
    "id": "76",
    "title": "The SDL pattern approach  a reuse-driven SDL design methodology",
    "abstract": "There are several SDL methodologies that offer full system life-cycle support. Only few of them consider software reuse, not to mention high-level reuse of architecture and design. However, software reuse is a proven software engineering paradigm leading to high quality and reduced development effort. Experience made it apparent that  beyond the more traditional reuse of code  especially high-level reuse of architecture and design (as in the case of design patterns or frameworks) has the potential of achieving more systematic and widespread reuse. This paper presents the SDL pattern approach, a design methodology for distributed systems which integrates SDL-based system development with the pattern paradigm. It supports reuse of design knowledge modeled as SDL patterns and concentrates on the design phase of SDL-based system development. In order to get full life-cycle support, the pattern-based design process can be integrated within existing SDL methodologies.",
    "keywords": [
      "sdl",
      "design methodology",
      "software reuse",
      "patterns",
      "distributed systems",
      "process model"
    ]
  },
  {
    "id": "77",
    "title": "Database repairing using updates",
    "abstract": "Repairing a database means bringing the database in accordance with a given set of integrity constraints by applying some minimal change. If a database can be repaired in more than one way, then the consistent answer to a query is defined as the intersection of the query answers on all repaired versions of the database. Earlier approaches have confined the repair work to deletions and insertions of entire tuples. We propose a theoretical framework that also covers updates as a repair primitive. Update-based repairing is interesting in that it allows rectifying an error within a tuple without deleting the tuple, thereby preserving consistent values in the tuple. Another novel idea is the construct of nucleus: a single database that yields consistent answers to a class of queries, without the need for query rewriting. We show the construction of nuclei for full dependencies and conjunctive queries. Consistent query answering and constructing nuclei is generally intractable under update-based repairing. Nevertheless, we also show some tractable cases of practical interest.",
    "keywords": [
      "theory",
      "consistent query answering",
      "database repairing"
    ]
  },
  {
    "id": "78",
    "title": "Simulation of face/hairstyle swapping in photographs with skin texture synthesis",
    "abstract": "The modern trend of diversification and personalization has encouraged people to boldly express their differentiation and uniqueness in many aspects, and one of the noticeable evidences is the wide variety of hairstyles that we could observe today. Given the needs for hairstyle customization, approaches or systems, ranging from 2D to 3D, or from automatic to manual, have been proposed or developed to digitally facilitate the choice of hairstyles. However, nearly all existing approaches suffer from providing realistic hairstyle synthesis results. By assuming the inputs to be 2D photos, the vividness of a hairstyle re-synthesis result relies heavily on the removal of the original hairstyle, because the co-existence of the original hairstyle and the newly re-synthesized hairstyle may lead to serious artifact on human perception. We resolve this issue by extending the active shape model to more precisely extract the entire facial contour, which can then be used to trim away the hair from the input photo. After hair removal, the facial skin of the revealed forehead needs to be recovered. Since the skin texture is non-stationary and there is little information left, the traditional texture synthesis and image inpainting approaches do not fit to solve this problem. Our proposed method yields a more desired facial skin patch by first interpolating a base skin patch, and followed by a non-stationary texture synthesis. In this paper, we also would like to reduce the user assistance during such a process as much as possible. We have devised a new and friendly facial contour and hairstyle adjusting mechanism that make it extremely easy to manipulate and fit a desired hairstyle onto a face. In addition, our system is also equipped with the functionality of extracting the hairstyle from a given photo, which makes our work more complete. Moreover, by extracting the face from the input photo, our system allows users to exchange faces as well. In the end of this paper, our re-synthesized results are shown, comparisons are made, and user studies are conducted as well to further demonstrate the usefulness of our system.",
    "keywords": [
      "active shape model",
      "skin texture synthesis",
      "hairstyle extraction"
    ]
  },
  {
    "id": "79",
    "title": "Ubiquitous provision of context-aware web services",
    "abstract": "Providing context-aware Web services refers to an adaptive process of delivering contextually matched Web services to meet service requesters needs at the moment. This article presents an ontology-based context model that enables formal description and acquisition of contextual information pertaining to both service requesters and services. The context model is supported by context query and phased acquisition techniques. Re also report two context-aware Web services built on top of our context model to demonstrate how the model can be used to facilitate Web services discovery and Web content adaptation. Implementation details of the context elicitation system and the evaluation results of context-aware services provision are also reported.",
    "keywords": [
      "context-aware",
      "owl-s",
      "portable devices",
      "service oriented architecture",
      "ubiquitous",
      "web services"
    ]
  },
  {
    "id": "80",
    "title": "Extracting Representative Information to Enhance Flexible Data Queries",
    "abstract": "Extracting representative information is of great interest in data queries and web applications nowadays, where approximate match between attribute values/records is an important issue in the extraction process. This paper proposes an approach to extracting representative tuples from data classes under an extended possibility-based data model, and to introducing a measure (namely, relation compactness) based upon information entropy to reflect the degree that a relation is compact in light of information redundancy. Theoretical analysis and data experiments show that the approach has desirable properties that: 1) the set of representative tuples has high degrees of compactness (less redundancy) and coverage (rich content); 2) it provides a way to obtain data query outcomes of different sizes in a flexible manner according to user preference; and 3) the approach is also meaningful and applicable to web search applications.",
    "keywords": [
      "flexible data queries",
      "information equivalence",
      "relation compactness",
      "representativeness",
      "web search"
    ]
  },
  {
    "id": "81",
    "title": "points-to analysis using bdds",
    "abstract": "This paper reports on a new approach to solving a subset-based points-to analysis for Java using Binary Decision Diagrams (BDDs). In the model checking community, BDDs have been shown very effective for representing large sets and solving very large verification problems. Our work shows that BDDs can also be very effective for developing a points-to analysis that is simple to implement and that scales well, in both space and time, to large programs.The paper first introduces BDDs and operations on BDDs using some simple points-to examples. Then, a complete subset-based points-to algorithm is presented, expressed completely using BDDs and BDD operations. This algorithm is then refined by finding appropriate variable orderings and by making the algorithm propagate sets incrementally, in order to arrive at a very efficient algorithm. Experimental results are given to justify the choice of variable ordering, to demonstrate the improvement due to incrementalization, and to compare the performance of the BDD-based solver to an efficient hand-coded graph-based solver. Finally, based on the results of the BDD-based solver, a variety of BDD-based queries are presented, including the points-to query.",
    "keywords": [
      "point",
      "order",
      "communities",
      "examples",
      "efficiency",
      "analysis",
      "performance",
      "points-to analysis",
      "timing",
      "paper",
      "binary decision diagrams",
      "incremental",
      "program",
      "variability",
      "model checking",
      "experimentation",
      "space",
      "verification",
      "operability",
      "queries",
      "demonstrate",
      "graph",
      "algorithm",
      "effect",
      "query",
      "completeness"
    ]
  },
  {
    "id": "82",
    "title": "Fuzzy Q-Learning Admission Control for WCDMA/WLAN Heterogeneous Networks with Multimedia Traffic",
    "abstract": "In this paper, admission control by a fuzzy Q-learning technique is proposed for WCDMA/WLAN heterogeneous networks with multimedia traffic. The fuzzy Q-learning admission control (FQAC) system is composed of a neural-fuzzy inference system (NFIS) admissibility estimator, an NFIS dwelling estimator, and a decision maker. The NFIS admissibility estimator takes essential system measures into account to judge how each reachable subnetwork can support the admission request's required QoS and then output admissibility costs. The NFIS dwelling estimator considers the Doppler shift and the power strength of the requested user to assess his/her dwell time duration in each reachable subnetwork and then output dwelling costs. Also, in order to minimize the expected maximal cost of the user's admission request, a minimax theorem is applied in the decision maker to determine the most suitable subnetwork for the user request or to reject. Simulation results show that FQAC can always maintain the system QoS requirement up to a traffic intensity of 1.1 because it can appropriately admit or reject the users' admission requests. Also, the FQAC can achieve lower blocking probabilities than conventional JSAC proposed in [20] and can significantly reduce the handoff rate by 15-20 percent.",
    "keywords": [
      "fuzzy q-learning",
      "admission control",
      "handoff",
      "heterogeneous network"
    ]
  },
  {
    "id": "83",
    "title": "A numerical study of three-dimensional liquid sloshing in tanks",
    "abstract": "A numerical model NEWTANK (Numerical Wave TANK) has been developed to study three-dimensional (3-D) non-linear liquid sloshing with broken free surfaces. The numerical model solves the spatially averaged NavierStokes equations, which are constructed on a non-inertial reference frame having arbitrary six degree-of-freedom (DOF) of motions, for two-phase flows. The large-eddy-simulation (LES) approach is adopted to model the turbulence effect by using the Smagorinsky sub-grid scale (SGS) closure model. The two-step projection method is employed in the numerical solutions, aided by the Bi-CGSTAB technique to solve the pressure Poisson equation for the filtered pressure field. The second-order accurate volume-of-fluid (VOF) method is used to track the distorted and broken free surface. Laboratory experiments are conducted for both 2-D and 3-D non-linear liquid sloshing in a rectangular tank. A linear analytical solution of 3-D liquid sloshing under the coupled surge and sway excitation is also developed in this study. The numerical model is first validated against the available analytical solution and experimental data for 2-D liquid sloshing of both inviscid and viscous fluids. The validation is further extended to 3-D liquid sloshing. The numerical results match with the analytical solution when the excitation amplitude is small. When the excitation amplitude is large where sloshing becomes highly non-linear, large discrepancies are developed between the numerical results and the analytical solutions, the former of which, however, agree well with the experimental data. Finally, as a demonstration, a violent liquid sloshing with broken free surfaces under six DOF excitations is simulated and discussed.",
    "keywords": [
      "liquid sloshing",
      "three-dimensional numerical model",
      "navierstokes equations",
      "non-inertial reference frame",
      "vof method",
      "broken free surface",
      "analytical solution"
    ]
  },
  {
    "id": "84",
    "title": "Spike-based VLSI modeling of the ILD system in the echolocating bat",
    "abstract": "The azimuthal localization of objects by echolocating bats is based on the difference of echo intensity received at the two ears, known as the interaural level difference (ILD). Mimicking the neural circuitry in the bat associated with the computation of ILD, we have constructed a spike-based VLSI model that can produce responses similar to those seen in the lateral superior olive (LSO) and some parts of the inferior colliculus (IC). We further explore some of the interesting computational consequences of the dynamics of both synapses and cellular mechanisms.",
    "keywords": [
      "echolocation",
      "spike-based",
      "vlsi",
      "azimuthal localization",
      "hardware model",
      "masking"
    ]
  },
  {
    "id": "85",
    "title": "Having expectations of information systems benefits that match received benefits: does it really matter",
    "abstract": "A study was conducted to examine the effect of implementing a new system on its users, specifically, the relationship between pre-implementation expectations and their perceived benefits based on post-implementation experience. Disconfirmation theory was used as the theoretical basis; this predicts that unrealistically high expectations will result in lower levels of perceived benefit than those associated with realistic expectations (i.e. where expectations match experience). Support was found for this prediction, refuting the predictions of dissonance theory. In addition to examining expectations of system use generally, six expectation categories were examined to identify the critical categories where managers should keep expectations from becoming unrealistically high. Significant relationships were found for three expectation categories: system usefulness, ease of use, and information quality. The results indicate that creating and maintaining realistic expectations of future system benefits really does matter.",
    "keywords": [
      "information systems success",
      "end-user satisfaction",
      "user expectations",
      "disconfirmation theory"
    ]
  },
  {
    "id": "86",
    "title": "Online updating belief rule based system for pipeline leak detection under expert intervention",
    "abstract": "A belief rule base inference methodology using the evidential reasoning approach (RIMER) has been developed recently, where a new belief rule base (BRB) is proposed to extend traditional IF-THEN rules and can capture more complicated causal relationships using different types of information with uncertainties, but these models are trained off-line and it is very expensive to train and re-train them. As such, recursive algorithms have been developed to update the BRB systems online and their calculation speed is very high, which is very important, particularly for the systems that have a high level of real-time requirement. The optimization models and recursive algorithms have been used for pipeline leak detection. However, because the proposed algorithms are both locally optimal and there may exist some noise in the real engineering systems, the trained or updated BRB may violate some certain running patterns that the pipeline leak should follow. These patterns can be determined by human experts according to some basic physical principles and the historical information. Therefore, this paper describes under expert intervention, how the recursive algorithm update the BRB system so that the updated BRB cannot only be used for pipeline leak detection but also satisfy the given patterns. Pipeline operations under different conditions are modeled by a BRB using expert knowledge, which is then updated and fine tuned using the proposed recursive algorithm and pipeline operating data, and validated by testing data. All training and testing data are collected from a real pipeline. The study demonstrates that under expert intervention, the BRB expert system is flexible, can be automatically tuned to represent complicated expert systems, and may be applied widely in engineering. It is also demonstrated that compared with other methods such as fuzzy neural networks (FNNs), the RIMER has a special characteristic of allowing direct intervention of human experts in deciding the internal structure and the parameters of a BRB expert system.",
    "keywords": [
      "belief rule base",
      "expert system",
      "evidential reasoning",
      "recursive algorithm",
      "leak detection"
    ]
  },
  {
    "id": "87",
    "title": "Negative concord with polyadic quantifiers",
    "abstract": "In this paper we develop a syntax-semantics of negative concord in Romanian within a constraint-based lexicalist framework. We show that n-words in Romanian are best treated as negative quantifiers which may combine by resumption to form polyadic negative quantifiers. Optionality of resumption explains the existence of simple sentential negation readings alongside double negation readings. We solve the well-known problem of defining general semantic composition rules for translations of natural language expressions in a logical language with polyadic quantifiers by integrating our higher-order logical object language in Lexical Resource Semantics (LRS), whose constraint-based composition mechanisms directly support a systematic syntax-semantics for negative concord with polyadic quantification in Head-driven Phrase Structure Grammar (HPSG).",
    "keywords": [
      "negative concord",
      "romanian",
      "polyadic quantifiers",
      "head-driven phrase structure grammar",
      "lexical resource semantics"
    ]
  },
  {
    "id": "88",
    "title": "Psychological reactance to online recommendation services",
    "abstract": "Adoption of online recommendation services can improve the quality of decision making or it can pose threats to free choice. When people perceive that their freedom is reduced or threatened by others, they are likely to experience a psychological reactance where they attempt to restore the freedom. We performed an experimental study to determine whether users expectation of personalization increased their intention to use recommendation services, because their perception of expected threat to freedom caused by the recommendations reduced their intention to participate. An analysis based on subjects responses after using a hypothetical shopping website confirmed the two-sided nature of personalized recommendations, suggesting that the approach and avoidance strategies in persuasive communications can be effectively applied to personalized recommendation services on the web. Theoretical and practical implications are discussed.",
    "keywords": [
      "threat to freedom",
      "psychological reactance",
      "recommendation",
      "personalization",
      "online shopping"
    ]
  },
  {
    "id": "89",
    "title": "Enhanced ab initio protein folding simulations in PoissonBoltzmann molecular dynamics with self-guiding forces",
    "abstract": "We have investigated the sampling efficiency in molecular dynamics with the PB implicit solvent when self-guiding forces are added. Compared with a high-temperature dynamics simulation, the use of self-guiding forces in room-temperature dynamics is found to be rather efficient as measured by potential energy fluctuation, gyration radius fluctuation, backbone RMSD fluctuation, number of unique clusters, and distribution of low RMSD structures over simulation time. Based on the enhanced sampling method, we have performed ab initio folding simulations of two small proteins,1 and villin headpiece. The preliminary data for the folding simulations is presented. It is found that1 folding proceeds by initiation of the turn and the helix. The hydrophobic collapse seems to be lagging behind or at most concurrent with the formation of the helix. The hairpin stability is weaker than the helix in our simulations. Its role in the early folding events seems to be less important than the more stable helix. In contrast, villin headpiece folding proceeds first by hydrophobic collapse. The formation of helices is later than the collapse phase, different from the1 folding.",
    "keywords": [
      "poissonboltzmann",
      "molecular dynamics",
      "self-guiding forces",
      "protein folding",
      "bba1",
      "villin headpiece"
    ]
  },
  {
    "id": "90",
    "title": "An implicit Galerkin finite element RungeKutta algorithm for shock-structure investigations",
    "abstract": "This paper introduces an implicit high-order Galerkin finite element RungeKutta algorithm for efficient computational investigations of shock structures. The algorithm induces no spatial-discretization artificial diffusion, relies on cubic and higher-degree elements for an accurate resolution of the steep shock gradients, uses an implicit time integration for swift convergence to steady states, and employs original Neumann-type outlet boundary conditions in the form of generalized RankineHugoniot conditions on normal stress and balance of heat flux and deviatoric-stress work per unit time. The formulation automatically calculates the spatial extent of the shock and employs the single non-dimensional (0,1) computational domain for the determination of any shock structure. Since it is implicit, the algorithm rapidly generates steady shock structures, in at most 150 time steps for any upstream Mach number considered in this study. The finite element discretization is shown to be asymptotically convergent under progressive grid refinements, in respect of both the H0 H 0 and H1 H 1 error norms, with an H0 H 0 accuracy order as high as 6 and reduction of the discretization error to the round-off-error threshold of 110?9 with just 420 computational cells and 5th-degree elements. For upstream Mach numbers in the range 1.05?M?10.0, the computational results satisfy the RankineHugoniot conditions and reflect independently published NavierStokes results.",
    "keywords": [
      "normal shocks",
      "shock-wave structures",
      "computational stability",
      "finite elements",
      "implicit rungekutta"
    ]
  },
  {
    "id": "91",
    "title": "A contemporary view of organizational safety: variability and interactions of organizational processes",
    "abstract": "Studies of qualitative assessment of organizational processes (e.g., safety audits and performance indicators) and their incorporation into risk models have been based on a normative view that decomposes organizations into separate processes that are likely to fail and lead to accidents. This paper discusses a control theoretic framework of organizational safety that views accidents as a result of performance variability of human behaviors and organizational processes whose complex interactions and coincidences lead to adverse events. Safety-related tasks managed by organizational processes are examined from the perspective of complexity and coupling. This allows safety analysts to look deeper into the complex interactions of organizational processes and how these may remain hidden or migrate toward unsafe boundaries. A taxonomy of variability of organizational processes is proposed and challenges in managing adaptability are discussed. The proposed framework can be used for studying interactions between organizational processes, changes of priorities over time, delays in effects, reinforcing influences, and long-term changes of processes. These dynamic organizational interactions are visualized with the use of system dynamics. The framework can provide a new basis for modeling organizational factors in risk analysis, analyzing accidents and designing safety reporting systems.",
    "keywords": [
      "organizational safety",
      "systems theory",
      "variability",
      "complexity and coupling",
      "safety management",
      "system dynamics"
    ]
  },
  {
    "id": "92",
    "title": "Robust image-based visual servoing using invariant visual information",
    "abstract": "Image-based visual servoing from spherical projection. Decoupled control using invariant features. A near-linear behavior is obtained thanks to the proposed features. The sensitivity to image noise is taken into account.",
    "keywords": [
      "robust visual servoing",
      "spherical projection"
    ]
  },
  {
    "id": "93",
    "title": "a distributed numerical/simulative algorithm for the analysis of large continuous time markov chains",
    "abstract": "A distributed algorithm is introduced for the analysis of large continuous time Markov chains (CTMCs) by combining in some sense numerical solution techniques and simulation. CTMCs are described as a set of processes communicating via message passing. The state of a process is described by a probability distribution over a set of reachable states rather than by a single state. Simulation is used to determine event times and messages types to be exchanged, whereas transitions are realized by vector matrix products as in iterative numerical analysis techniques. In this way, the state space explosion of numerical analysis is avoided, but it is still possible to determine more detailed results than with simulation. Parallelization of the algorithm is realized applying a conservative synchronization scheme, which exploits the possibility of precomputing event times as already proposed for parallel simulation of CTMCs. In contrast to a pure simulation approach, the amount of computation is increased, whereas the amount of communication keeps constant. Hence it is possible to achieve even on a workstation cluster a significant speedup.",
    "keywords": [
      "communication",
      "speedup",
      "distributed algorithms",
      "simulation",
      "parallel simulation",
      "synchronization",
      "event",
      "analysis",
      "computation",
      "probability",
      "timing",
      "combinational",
      "product",
      "vectorization",
      "transit",
      "space",
      "exploit",
      "parallel",
      "message",
      "types",
      "process",
      "message-passing",
      "algorithm",
      "distributed",
      "continuation",
      "iter",
      "scheme",
      "cluster"
    ]
  },
  {
    "id": "94",
    "title": "Properties of prediction sorting",
    "abstract": "One of the major sources of unwanted variation in an industrial process is the raw material quality. However, if the raw materials are sorted into more homogeneous groups before production, each group can be treated differently. In this way the raw materials can be better utilized and the stability of the end product may be improved. Prediction sorting is a methodology for doing this. The procedure is founded on the fuzzy c-means algorithm where the distance in the objective function is based on the predicted end product quality. Usually empirical models such as linear regression are used for predicting the end product quality. By using simulations and bootstrapping, this paper investigates how the uncertainties connected with empirical models affect the optimization of the splitting and the corresponding process variables. The results indicate that the practical consequences of uncertainties in regression coefficients are small. ",
    "keywords": [
      "raw material variability",
      "sorting",
      "robustness",
      "fuzzy clustering"
    ]
  },
  {
    "id": "95",
    "title": "Interstitial fluid glucose time-lag correction for real-time continuous glucose monitoring",
    "abstract": "Time lag between subcutaneous interstitial fluid and plasma glucose decreases the accuracy of real-time continuous glucose monitors. However, inverse filters can be designed to correct time lag and attenuate noise enabling the bloodglucose profile to be reconstructed in real time from continuous measurements of the interstitial-fluid glucose. We designed and tested a Wiener filter using a set of 20 sensor-glucose tracings (?30h each) with a 1-min sample interval. Delays of 102min (meanSD) were introduced into each signal with additive Gaussian white noise (SNR=40dB). Performance of the filter was compared to conventional causal and non-causal seventh-order finite-impulse response (FIR) filters. Time lags introduced an error of 5.32.7%. The error increased in the presence of noise (to 5.72.6%) and attempts to remove the noise with conventional low-pass filtering increased the error still further (to 7.03.5%). In contrast, the Wiener filter decreased the error attributed to time delay by ?50% in the presence of noise (from 5.7% to 2.601.26%) and by ?75% in the absence of noise (5.3% to 1.31%). Introducing time-lag correction without increasing sensitivity to noise can increase CGM accuracy.",
    "keywords": [
      "continuous glucose monitoring",
      "wiener filter",
      "time-lag",
      "interstitial fluid"
    ]
  },
  {
    "id": "96",
    "title": "Fast data access and energy-efficient protocol for wireless data broadcast",
    "abstract": "In wireless mobile computing environments, broadcasting is an effective and scalable technique to disseminate information to a massive number of clients, wherein the energy usage and responsiveness are considered major concerns. Existing air indexing schemes for data broadcast have focused on energy efficiency (reducing tuning time) only. On the other hand, existing broadcast scheduling schemes have aimed at reducing access latency through nonflat data broadcast to improve responsiveness only. Not much work has addressed the energy efficiency and responsiveness issues concurrently. In this paper, we propose a fast data access scheme concurrently supporting energy saving protocol that constructs the broadcast channels according to the access frequency of each type of message in order to improve energy efficiency in mobile devices (MDs). The pinwheel scheduling algorithm (PSA) presented in this paper is used to organize all types of messages in the broadcast channel in the most symmetrical distribution in order to reduce both the tuning and access time. The performance of the proposed mechanism is analyzed, and the improvement over existing methods is demonstrated numerically. The results show that the proposed mechanism is capable of improving both the tuning and access time due to the presence of skewness in the access distribution among the disseminated messages. ",
    "keywords": [
      "mobile computing",
      "wireless broadcast system",
      "energy saving",
      "access time",
      "tuning time"
    ]
  },
  {
    "id": "97",
    "title": "ONOFF retinal ganglion cells temporally encode OFF/ON sequence",
    "abstract": "While the functions of ON and OFF retinal ganglion cells have been intensively investigated, that of ONOFF cells has not. In the present study, the temporal properties of spike trains emitted from ON-OFF cells in response to randomly flickering or multiphase ramp stimuli were examined in the Japanese quail. The results indicate that the firing of ON-spikes was influenced by the recent firing of OFF-spikes, and vice versa. As a result of this interaction, OFF/ON sequence of light intensity change was encoded with a spike pair with an interval of 20ms, indicating that temporal coding is utilized in the vertebrate visual system as early as the retina. Thus, the present results suggest that retinal neuronal circuits may detect specific sequential features of stimuli.",
    "keywords": [
      "onoff retinal ganglion cells",
      "spike train",
      "stimulus sequence",
      "temporal coding",
      "retina",
      "optic nerve",
      "quail"
    ]
  },
  {
    "id": "98",
    "title": "Noise reduction for magnetic resonance images via adaptive multiscale products thresholding",
    "abstract": "Edge-preserving denoising is of great interest in medical image processing. This paper presents a wavelet-based multiscale products thresholding scheme for noise suppression of magnetic resonance images. A Canny edge detector-like dyadic wavelet transform is employed. This results in the significant features in images evolving with high magnitude across wavelet scales, while noise decays rapidly. To exploit the wavelet interscale dependencies we multiply the adjacent wavelet subbands to enhance edge structures while weakening noise. In the multiscale products, edges can be effectively distinguished from noise. Thereafter, an adaptive threshold is calculated and imposed on the products, instead of on the wavelet coefficients, to identify important features. Experiments show that the proposed scheme better suppresses noise and preserves edges than other wavelet-thresholding denoising methods.",
    "keywords": [
      "denoising",
      "magnetic resonance image",
      "multiscale products",
      "thresholding",
      "wavelet transform"
    ]
  },
  {
    "id": "99",
    "title": "Integration of semi-fuzzy SVDD and CC-Rule method for supplier selection",
    "abstract": "A model based on semi-fuzzy support vector domain description (semi-fuzzy SVDD) is put forward to address multi-classification problem involved in supplier selection. By preprocessing using semi-fuzzy kernel clustering algorithm, original samples are divided into two subsets: deterministic samples and fuzzy samples. Only the fuzzy samples, rather than all original ones, require expert judgment to decide their categories and are selected as training samples to accomplish SVDD specification. Therefore, the samples preprocessing method can not only decrease experts working strength, but also achieve less computational consumption and better performance of the classifier. Nevertheless, in order to accomplish practical decision making, another condition has to be met: good explanations to the decision. A rule extraction method based on cooperative coevolution algorithm (CCEA), is introduced to achieve the target. To validate the proposed methodology, samples from real world were employed for experiments, with results compared with conventional multi-classification support vector machine approaches and other artificial intelligence techniques. Moreover, in terms of rule extraction, experiments on key parameters, different methods including decompositional and pedagogical ones etc. were also conducted.",
    "keywords": [
      "supplier selection",
      "semi-fuzzy kernel clustering algorithm",
      "support vector domain description",
      "cooperative coevolution algorithm"
    ]
  },
  {
    "id": "100",
    "title": "A distributed disk layer for mass storage at DESY",
    "abstract": "For the mass storage system at DESY, a disk layer is under development. Decoupling the client request queue and access to the mass storage system by means of migration, staging and prefetching it shall provide full utilization of robot and drive resources. By managing distributed disk resources in the heterogeneous computing environment of DESY, optimized data access shall be given. ",
    "keywords": [
      "mass storage",
      "disk layer",
      "client-server architecture",
      "network file system"
    ]
  },
  {
    "id": "101",
    "title": "On the learning machine for three dimensional mapping",
    "abstract": "In this paper, we investigate the neural network with three-dimensional parameters for applications like 3D image processing, interpretation of 3D transformations, and 3D object motion. A 3D vector represents a point in the 3D space, and an object might be represented with a set of these points. Thus, it is desirable to have a 3D vector-valued neural network, which deals with three signals as one cluster. In such a neural network, 3D signals are flowing through a network and are the unit of learning. This article also deals with a related 3D back-propagation (3D-BP) learning algorithm, which is an extension of conventional back-propagation algorithm in the single dimension. 3D-BP has an inherent ability to learn and generalize the 3D motion. The computational experiments presented in this paper evaluate the performance of considered learning machine in generalization of 3D transformations and 3D pattern recognition.",
    "keywords": [
      "3d back-propagation algorithm",
      "3d real-valued vector",
      "orthogonal matrix",
      "3d face"
    ]
  },
  {
    "id": "102",
    "title": "A unified approach to the analysis of Horton-Strahler parameters of binary tree structures",
    "abstract": "The Horton-Strahler number naturally arose from problems in various fields, e.g., geology, molecular biology and computer science. Consequently, detailed investigations of related parameters for different classes of binary tree structures are of interest. This paper shows one possibility of how to perform a mathematical analysis for parameters related to the Horton-Strahler number in a unified way such that only a single analysis is needed to obtain results for many different classes of trees. The method is explained by the examples of the expected Horton-Strahler number and the related rth moments, the average number of critical nodes, and the expected distance between critical nodes. ",
    "keywords": [
      "average-case analysis",
      "combinatorial structures",
      "horton-strahler numbers",
      "analytic combinatorics"
    ]
  },
  {
    "id": "103",
    "title": "Visitors of two types of museums: A segmentation study",
    "abstract": "Market segmentation comprises a wide range of measurement tools that are useful for the sake of supporting marketing and promotional policies also in the sector of cultural economics. This paper aims to contribute to the literature on segmenting cultural visitors by using the Bagged Clustering method, as an alternative and effective strategy to conduct cluster analysis when binary variables are used. The technique is a combination of hierarchical and partitioning methods and presents several advantages with respect to more standard techniques, such as k-means and LVQ. For this purpose, two ad hoc surveys were conducted between June and September 2011 in the two principal museums of the two provinces of the Trentino-South Tyrol region (Bolzano and Trento), Northern Italy: the South Tyrol Museum of Archaeology in Bolzano (TZI), hosting the permanent exhibition of the Iceman tzi, and the Museum of Modern and Contemporaneous Art of Trento and Rovereto (MART). The segmentation analysis was conducted separately for the two kinds of museums in order to find similarities and differences in behaviour patterns and characteristics of visitors. The analysis identified three and two cluster segments respectively for the MART and TZI visitors, where two TZI clusters presented similar characteristics to two out of three MART groups. Conclusions highlight marketing and managerial implications for a better direction of the museums.",
    "keywords": [
      "bagged clustering",
      "logit models",
      "museum",
      "segmentation",
      "motivation"
    ]
  },
  {
    "id": "104",
    "title": "Grid Resource Availability Prediction-Based Scheduling and Task Replication",
    "abstract": "The frequent and volatile unavailability of volunteer-based Grid computing resources challenges Grid schedulers to make effective job placements. The manner in which host resources become unavailable will have different effects on different jobs, depending on their runtime and their ability to be checkpointed or replicated. A multi-state availability model can help improve scheduling performance by capturing the various ways a resource may be available or unavailable to the Grid. This paper uses a multi-state model and analyzes a machine availability trace in terms of that model. Several prediction techniques then forecast resource transitions into the model's states. We analyze the accuracy of our predictors, which outperform existing approaches. We also propose and study several classes of schedulers that utilize the predictions, and a method for combining scheduling factors. We characterize the inherent tradeoff between job makespan and the number of evictions due to failure, and demonstrate how our schedulers can navigate this tradeoff under various scenarios. Lastly, we propose job replication techniques, which our schedulers utilize to replicate those jobs that are most likely to fail. Our replication strategies outperform others, as measured by improved makespan and fewer redundant operations. In particular, we define a new metric for replication efficiency, and demonstrate that our multi-state availability predictor can provide information that allows our schedulers to be more efficient than others that blindly replicate all jobs or some static percentage of jobs.",
    "keywords": [
      "multi-state",
      "prediction",
      "availability",
      "characterization",
      "scheduling",
      "replication"
    ]
  },
  {
    "id": "105",
    "title": "On the number of optimal identifying codes in a twin-free graph",
    "abstract": "Let G be a simple, undirected graph with vertex set V . For v?V v ? V and r?1 r ? 1 , we denote by BG,r(v) B G , r ( v ) the ball of radius r and centre v . A set C?V C ? V is said to be an r-identifying code  in G if the sets BG,r(v)?C B G , r ( v ) ? C , v?V v ? V , are all nonempty and distinct. A graph G which admits an r-identifying code is called r-twin-free  or r-identifiable , and in this case the smallest size of an r-identifying code in G is denoted by ? r I D ( G ) . We study the number of different optimal r-identifying codes C , i.e., such that | C | = ? r I D ( G ) , that a graph G can admit, and try to construct graphs having many such codes.",
    "keywords": [
      "graph theory",
      "twin-free graphs",
      "identifiable graphs",
      "identifying codes"
    ]
  },
  {
    "id": "106",
    "title": "On the impact of triangle shapes for boundary layer problems using high-order finite element discretization",
    "abstract": "The impact of triangle shapes, including angle sizes and aspect ratios, on accuracy and stiffness is investigated for simulations of highly anisotropic problems. The results indicate that for high-order discretizations, large angles do not have an adverse impact on solution accuracy. However, a correct aspect ratio is critical for accuracy for both linear and high-order discretizations. Large angles are also found to be not problematic for the conditioning of the linear systems arising from the discretizations. Further, when choosing preconditioning strategies, coupling strengths among elements rather than element angle sizes should be taken into account. With an appropriate preconditioner, solutions on meshes with and without large angles can be achieved within a comparable time.",
    "keywords": [
      "triangle shape",
      "large angle",
      "aspect ratio",
      "high-order finite element",
      "anisotropic problems",
      "ilu-factorization"
    ]
  },
  {
    "id": "107",
    "title": "Tree-walking automata cannot be determinized",
    "abstract": "Tree-walking automata are a natural sequential model for recognizing languages of finite trees. Such automata walk around the tree and may decide in the end to accept it. It is shown that deterministic tree-walking automata are weaker than nondeterministic tree-walking automata.  ",
    "keywords": [
      "tree-walking automata",
      "deterministic tree-walking automata"
    ]
  },
  {
    "id": "108",
    "title": "ADAPTABLE MULTI-AGENT SYSTEMS: THE CASE OF THE GAIA METHODOLOGY",
    "abstract": "Changes and adaptations are always necessary after the deployment of a multi-agent system (MAS), as well as of any other type of software systems. Some of these changes may be simply perfective and have local impact only. However, adaptive changes to meet new situations in the operational environment of the MAS may impact globally on the overall design. More specifically, those changes usually affect the organizational structure of the MAS. In this paper we analyze the issue of design change/adaptation in a MAS organization, and the specific problem of how to properly model/design a MAS so as to make it ready for adaptation. Special attention is paid to the Gaia methodology, whose suitability in dealing with adaptable MAS organizations is also discussed with the help of an illustrative application example.",
    "keywords": [
      "agent-oriented methodologies",
      "adaptive/adaptable organizations",
      "design for changes",
      "gaia methodology"
    ]
  },
  {
    "id": "109",
    "title": "On the multicriteria allocation problem",
    "abstract": "We consider multicriteria allocation problems with linear sum objectives. Despite the fact that the single objective allocation problem is easily solvable, we show that already in the bicriteria case the problem becomes intractable, is NP-hard and has a non-connected efficient set in general. Using the equivalence to appropriately defined multiple criteria multiple-choice knapsack problems, an algorithm is suggested that uses partial dominance conditions to save computational time. Different types of enumeration schemes are discussed, for example, with respect to the number of necessary filtering operations and with regard to possible parallelizations of the procedure.",
    "keywords": [
      "multicriteria optimization",
      "combinatorial optimization",
      "location-allocation problem"
    ]
  },
  {
    "id": "110",
    "title": "Optimization and correction of the tool path of the five-axis milling machine - Part 2: Rotations and setup",
    "abstract": "We present two new algorithms (which supplement Algorithms 1, 2 and 3 presented in part 1) to optimize the tool path of the five-axis numerically controlled milling machine. Algorithm 4 optimizes a set of feasible rotations. Algorithm 5 presents a least-square optimization with regard to a setup of the machine  ",
    "keywords": [
      "nc-programming",
      "cad/cam",
      "optimization",
      "milling machines"
    ]
  },
  {
    "id": "111",
    "title": "teaching about the risks of electronic voting technology",
    "abstract": "In these interesting times computer scientists are increasingly called upon to help concerned citizens understand the risks involved in the current generation of electronic voting machines. These risks and the concurrent escalation of legal challenges to the election system in the United States have shaken the confidence of many Americans that a fair and accurate election is even possible. As computer science educators we have an opportunity to add breadth and depth to our curriculum by using these issues to show how existing concepts can be applied to new problems, and how new problems extend our field. In this paper we identify some of the main problems with e-voting machines and vote-counting technology and suggest ways that discussions of the risks and the attendant societal and ethical issues might be incorporated into the computer science curriculum.",
    "keywords": [
      "electronic voting"
    ]
  },
  {
    "id": "112",
    "title": "Effectiveness of bibliographic searches performed by paediatric residents and interns assisted by librarians. A randomised controlled trial",
    "abstract": "Background:? Considerable barriers still prevent paediatricians from successfully using information retrieval technology.",
    "keywords": [
      "decision support",
      "evidence based library and information practice",
      "evidence based practice",
      "evidence-based medicine",
      "health science",
      "health services research",
      "information seeking behaviour",
      "librarians",
      "library and information science",
      "reflective practice"
    ]
  },
  {
    "id": "113",
    "title": "A genetic programming model for bankruptcy prediction: Empirical evidence from Iran",
    "abstract": "Prediction of corporate bankruptcy is a phenomenon of increasing interest to investors/creditors, borrowing firms, and governments alike. Timely identification of firms impending failure is indeed desirable. By this time, several methods have been used for predicting bankruptcy but some of them suffer from underlying shortcomings. In recent years, Genetic Programming (GP) has reached great attention in academic and empirical fields for efficient solving high complex problems. GP is a technique for programming computers by means of natural selection. It is a variant of the genetic algorithm, which is based on the concept of adaptive survival in natural organisms. In this study, we investigated application of GP for bankruptcy prediction modeling. GP was applied to classify 144 bankrupt and non-bankrupt Iranian firms listed in Tehran stock exchange (TSE). Then a multiple discriminant analysis (MDA) was used to benchmarking GP model. Genetic model achieved 94% and 90% accuracy rates in training and holdout samples, respectively; while MDA model achieved only 77% and 73% accuracy rates in training and holdout samples, respectively. McNemar test showed that GP approach outperforms MDA to the problem of corporate bankruptcy prediction.",
    "keywords": [
      "bankruptcy prediction",
      "financial ratios",
      "genetic programming",
      "multiple discriminant analysis",
      "iranian companies"
    ]
  },
  {
    "id": "114",
    "title": "A queueing model for general group screening policies and dynamic item arrivals",
    "abstract": "Classification of items as good or bad can often be achieved more economically by examining the items in groups rather than individually. If the result of a group test is good, all items within it can be classified as good, whereas one or more items are bad in the opposite case. Whether it is necessary to identify the bad items or not, and if so, how, is described by the screening policy. In the course of time, a spectrum of group screening models has been studied, each including some policy. However, the majority ignores that items may arrive at random time epochs at the testing center in real life situations. This dynamic aspect leads to two decision variables: the minimum and maximum group size. In this paper, we analyze a discrete-time batch-service queueing model with a general dependency between the service time of a batch and the number of items within it. We deduce several important quantities, by which the decision variables can be optimized. In addition, we highlight that every possible screening policy can, in principle, be studied, by defining the dependency between the service time of a batch and the number of items within it appropriately.",
    "keywords": [
      "queueing",
      "group screening policies",
      "dynamic item arrivals"
    ]
  },
  {
    "id": "115",
    "title": "efficient character-level taint tracking for java",
    "abstract": "Over 80% of web services are vulnerable to attack, and much of the danger arises from command injection vulnerabilities. We present an efficient character-level taint tracking system for Java web applications and argue that it can be used to defend against command injection vulnerabilities. Our approach involves modification only to Java library classes and the implementation of the Java servlets framework, so it requires only a one-time modification to the server without any subsequent modifications to a web application's bytecode or access to the web application's source code. This makes it easy to deploy our technique and easy to secure legacy web software. Our preliminary experiments with the JForum web application suggest that character-level taint tracking adds 0-15% runtime overhead.",
    "keywords": [
      "information flow",
      "java",
      "dynamic taint tracking",
      "web applications"
    ]
  },
  {
    "id": "116",
    "title": "How Accurate Can Block Matches Be in Stereo Vision",
    "abstract": "This article explores the subpixel accuracy attainable for the disparity computed from a rectified stereo pair of images with small baseline. In this framework we consider translations as the local deformation model between patches in the images. A mathematical study first shows how discrete block-matching can be performed with arbitrary precision under Shannon-Whittaker conditions. This study leads to the specification of a block-matching algorithm which is able to refine disparities with subpixel accuracy. Moreover, a formula for the variance of the disparity error caused by the noise is introduced and proved. Several simulated and real experiments show a decent agreement between this theoretical error variance and the observed root mean squared error in stereo pairs with good signal-to-noise ratio and low baseline. A practical consequence is that under realistic sampling and noise conditions in optical imaging, the disparity map in stereo-rectified images can be computed for the majority of pixels (but only for those pixels with meaningful matches) with a 1/20 pixel precision.",
    "keywords": [
      "block-matching",
      "subpixel accuracy",
      "noise error estimate"
    ]
  },
  {
    "id": "117",
    "title": "The Cognitive Internet of Things: A Unified Perspective",
    "abstract": "In this article, we present a unified perspective on the cognitive internet of things (CIoT). It is noted that within the CIoT design we observe the convergence of energy harvesting, cognitive spectrum access and mobile cloud computing technologies. We unify these distinct technologies into a CIoT architecture which provides a flexible, dynamic, scalable and robust network design road-map for large scale IoT deployment. Since the prime objective of the CIoT network is to ensure connectivity between things, we identify key metrics which characterize the network design space. We revisit the definition of cognition in the context of IoT networks and argue that both the energy efficiency and the spectrum efficiency are key design constraints. To this end, we define a new performance metric called the overall link success probability which encapsulates these constraints. The overall link success probability is characterized by both the self-sustainablitiy of the link through energy harvesting and the availability of spectrum for transmissions. With the help of a reference scenario, we demonstrate that well-known tools from stochastic geometry can be employed to investigate both the node and the network level performance. In particular, the reference scenario considers a large scale deployment of a CIoT network empowered by solar energy harvesting deployed along with the centralized CIoT device coordinators. It is assumed that CIoT network is underlaid with a cellular network, i.e., CIoT nodes share spectrum with mobile users subject to a certain co-existence constraint. Considering the dynamics of both energy harvesting and spectrum sharing, the overall link success probability is then quantified. It is shown that both the self-sustainability of the link, and the availability of transmission opportunites, are coupled through a common parameter, i.e., the node level transmit power. Furthermore, provided the co-existence constraint is satisfied, the link level success in the presence of both the inter-network and intra-network interference is an increasing function of the transmit power. We demonstrate that the overall link level success probability can be maximized by employing a certain optimal transmit power. Characterization of such an optimal operational point is presented. Finally, we highlight some of the future directions which can benefit from the analytical framework developed in this paper.",
    "keywords": [
      "internet-of-things",
      "cognitive radios",
      "solar energy harvesting",
      "stochastic cloud cover",
      "shared spectrum",
      "underlay",
      "interference"
    ]
  },
  {
    "id": "118",
    "title": "Lower bounds for modular counting by circuits with modular gates",
    "abstract": "We prove that constant depth circuits, with one layer of MODm gates at the inputs, followed by a fixed number of layers of MODp gates, where p is prime, require exponential size to compute the MODq function, if q is a prime that divides neither p nor m.",
    "keywords": [
      "circuit complexity",
      "modular counting"
    ]
  },
  {
    "id": "119",
    "title": "On the physical implementation of logical transformations: Generalized L-machines",
    "abstract": "Any account of computation in a physical system, whether an artificial computing device or a natural system considered from a computational point of view, invokes some notion of the relationship between the abstract-logical and concrete-physical aspects of computation. In a recent paper, James Ladyman explored this relationship using a \"hybrid physical-logical entity\" - the L-machine - and the general account of computation that it supports [J. Ladyman, What does it mean to say that a physical system implements a computation?, Theoretical Computer Science 410 (2009) 376-383]. The underlying L-machine of Ladyman's analysis is, however, classical and highly idealized, and cannot capture essential aspects of computation in important classes of physical systems (e.g. emerging nanocomputing devices) where logical states do not have faithful physical representations and where noise and quantum effects prevail. In this work we generalize the L-machine to allow for generally unfaithful and noisy implementations of classical logical transformations in quantum mechanical systems. We provide a formal definition and physical-information-theoretic characterization of generalized quantum L-machines (QLMs), identify important classes of QLMs, and introduce new efficacy measures that quantify the faithfulness and fidelity with which logical transformations are implemented by these machines. Two fundamental issues emphasized by Ladyman - realism about computation and the connection between logical and physical irreversibility - are reconsidered within the more comprehensive account of computation that follows from our generalization of the L-machine.  ",
    "keywords": [
      "implementation",
      "l-machine",
      "landauer's principle",
      "physical information theory",
      "physics of computation",
      "realism about computation",
      "representation"
    ]
  },
  {
    "id": "120",
    "title": "Tangibles for learning: a representational analysis of physical manipulation",
    "abstract": "Manipulativesphysical learning materials such as cubes or tilesare prevalent in educational settings across cultures and have generated substantial research into how actions with physical objects may support childrens learning. The ability to integrate digital technology into physical objectsso-called digital manipulativeshas generated excitement over the potential to create new educational materials. However, without a clear understanding of how actions with physical materials lead to learning, it is difficult to evaluate or inform designs in this area. This paper is intended to contribute to the development of effective tangible technologies for childrens learning by summarising key debates about the representational advantages of manipulatives under two key headings: offloading cognitionwhere manipulatives may help children by freeing up valuable cognitive resources during problem solving, and conceptual metaphorswhere perceptual information or actions with objects have a structural correspondence with more symbolic concepts. The review also indicates possible limitations of physical objectsmost importantly that their symbolic significance is only granted by the context in which they are used. These arguments are then discussed in light of tangible designs drawing upon the authors current research into tangibles and young childrens understanding of number.",
    "keywords": [
      "tangible technologies",
      "physical manipulatives",
      "mathematics learning",
      "educational technology",
      "virtual manipulatives"
    ]
  },
  {
    "id": "121",
    "title": "Identifying Spurious Interactions and Predicting Missing Interactions in the Protein-Protein Interaction Networks via a Generative Network Model",
    "abstract": "With the rapid development of high-throughput experiment techniques for protein-protein interaction (PPI) detection, a large amount of PPI network data are becoming available. However, the data produced by these techniques have high levels of spurious and missing interactions. This study assigns a new reliably indication for each protein pairs via the new generative network model (RIGNM) where the scale-free property of the PPI network is considered to reliably identify both spurious and missing interactions in the observed high-throughput PPI network. The experimental results show that the RIGNM is more effective and interpretable than the compared methods, which demonstrate that this approach has the potential to better describe the PPI networks and drive new discoveries.",
    "keywords": [
      "protein-protein interaction network",
      "generative network model",
      "ppi data denoising"
    ]
  },
  {
    "id": "122",
    "title": "Similarities between powersets of terms",
    "abstract": "Generalisation of the foundational basis for many-valued logic programming builds upon generalised terms in the form of powersets of terms. A categorical approach involving set and term functors as monads allows for a study of monad compositions that provide variable substitutions and compositions thereof. In this paper, substitutions and unifiers appear as constructs in Kleisli categories related to particular composed powerset term monads. Specifically, we show that a frequently used similarity-based approach to fuzzy unification is compatible with the categorical approach, and can be adequately extended in this setting; also some examples are included in order to illuminate the definitions.  ",
    "keywords": [
      "similarities",
      "fuzzy unification",
      "category theory and unification",
      "generalised terms"
    ]
  },
  {
    "id": "123",
    "title": "Feedback vertex sets on restricted bipartite graphs",
    "abstract": "A feedback vertex set (FVS) in a graph is a subset of vertices whose complement induces a forest. Finding a minimum FVS is N P-complete on bipartite graphs, but tractable on convex bipartite graphs and on chordal bipartite graphs. A bipartite graph is called tree convex, if a tree is defined on one part of the vertices, such that for every vertex in the other part, its neighborhood induces a subtree. When the tree is a path, a triad or a star, the bipartite graph is called convex bipartite, triad convex bipartite or star convex bipartite, respectively. We show that: (I) FVS is tractable on triad convex bipartite graphs; (2) FVS is N P-complete on star convex bipartite graphs and on tree convex bipartite graphs where the maximum degree of vertices on the tree is at most three.  ",
    "keywords": [
      "feedback vertex set",
      "tree convex bipartite",
      "polynomial time",
      "n p-complete"
    ]
  },
  {
    "id": "124",
    "title": "The extended Delaunany tessellation",
    "abstract": "The extended Delaunay tessellation (EDT) is presented in this paper as the unique partition of a node set into polyhedral regions defined by nodes lying on the nearby Voronoi spheres. Until recently, all the FEM mesh generators were limited to the generation of tetrahedral or hexahedral elements (or triangular and quadrangular in 2D problems). The reason for this limitation was the lack of any acceptable shape function to be used in other kind of geometrical elements. Nowadays, there are several acceptable shape functions for a very large class of polyhedra. These new shape functions, together with the EDT, gives an optimal combination and a powerful tool to solve a large variety of physical problems by numerical methods. The domain partition into polyhedra presented here does not introduce any new node nor change any node position. This makes this Process suitable for Lagrangian problems and meshless methods in which only the connectivity information is used and there is no need for any expensive smoothing process.",
    "keywords": [
      "mesh generation",
      "delaunayl/voronoi tessellations"
    ]
  },
  {
    "id": "125",
    "title": "interlaced euler scheme for stiff systems of stochastic differential equations",
    "abstract": "In deterministic as well as stochastic models, stiff systems, i.e., systems with vastly different time scales where the fast scales are stable, are very common. It is well known that the implicit Euler method is well suited for stiff deterministic equations (modeled by ODEs) while the explicit Euler is not. In particular, once the fast transients are over, the implicit Euler allows for the choice of time steps comparable to the slowest time scales of the system. In stochastic systems (modeled by SDEs) the picture is more complex. While the implicit Euler has better stability properties over the explicit Euler, it underestimates the stationary variance. In general, one may not expect any method to work successfully by taking time steps of the order of the slowest time scale. We explore the idea of interlacing large implicit Euler steps with a sequence of small explicit Euler steps. In particular, we present our study of a linear test system of SDEs and demonstrate that such interlacing could effectively deal with stiffness. We also discuss the uniform convergence of mean and variance.",
    "keywords": [
      "explicit euler method",
      "stochastic differential equations",
      "implicit euler method",
      "uniform convergence",
      "stiffness"
    ]
  },
  {
    "id": "126",
    "title": "Inverse problem for wave propagation in a perturbed layered half-space",
    "abstract": "This paper is concerned with the inverse medium scattering problem in a perturbed, layered, half-space, which is a problem related to the seismologial investigation of inclusions inside the earth's crust. A wave penetrable object is located in a layer where the refraction index is different from the other part of the half-space. Wave propagation in such a layered half-space is different from that in a homogeneous half-space. In a layered half-space, a scattered wave consists of a free wave and a guided wave. In many cases, only the free-wave far-field or only the guided-wave far-field can be measured. We establish mathematical formulas for relations between the object, the incident wave and the scattered wave. In the ideal condition where exact data are given, we prove the uniqueness of the inverse problem. A numerical example is presented for the reconstruction of a penetrable object from simulated noise data.  ",
    "keywords": [
      "inverse problems"
    ]
  },
  {
    "id": "127",
    "title": "an explorative analysis of user evaluation studies in information visualisation",
    "abstract": "This paper presents an analysis of user studies from a review of papers describing new visualisation applications and uses these to highlight various issues related to the evaluation of visualisations. We first consider some of the reasons why the process of evaluating visualisations is so difficult. We then dissect the problem by discussing the importance of recognising the nature of experimental design, datasets and participants as well as the statistical analysis of results. We propose explorative evaluation as a method of discovering new things about visualisation techniques, which may give us a better understanding of the mechanisms of visualisations. Finally we give some practical guidance on how to do evaluation correctly.",
    "keywords": [
      "explorative evaluation",
      "case study",
      "evaluation",
      "information visualisation"
    ]
  },
  {
    "id": "128",
    "title": "An investigation the factors affecting MIS student burnout in technical-vocational college",
    "abstract": "Management information system (MIS) students are one of the most important information system (IS) employee sources. However, the determinants of student's burnout for MIS major students have received little attentions, despite their importance as indicator in predicting professional burnout and their working intention after their graduation and becoming IS professionals. This study explores the antecedents of student burnout for MIS major at technical-vocational college. Self-efficacy, social support, and sex-role were considered as antecedents to MIS student burnout. A questionnaire method by self-administered technique was used in this study. Multiple regression analysis was used to analyze the hypotheses. Statistical results displayed that MIS students with social support, self-efficacy and femininity have predictive power over student burnout. MIS students with social support and masculinity also have predictive power over self-efficacy.",
    "keywords": [
      "technical-vocational education",
      "student burnout",
      "self-efficacy",
      "social support",
      "sex-role"
    ]
  },
  {
    "id": "129",
    "title": "BEM calculation of the complex thermal impedance of microelectronic devices",
    "abstract": "This paper presents a numerical method for modelling the dynamic thermal behaviour of microelectronic structures in the frequency domain. A boundary element method (BEM) based on a Green's function solution is proposed for solving the 3D heat equation in phasor notation. The method is capable of calculating the AC temperature and heat flux distributions and complex thermal impedance for packages composed of an arbitrary number of bar-shaped components. Various types of boundary conditions, including thermal contact resistance and convective cooling, can be taken into account. A simple benchmark case is investigated and a good convergence towards the analytical solution is obtained. Simulation results for a thin plate under convective cooling are compared with a theoretical model and an excellent agreement is observed. In a second example a more complicated three-layer structure is investigated. The BEM is used to analyse the thermal behaviour if delamination of the package occurs, and a physical explanation for the results is given.",
    "keywords": [
      "thermal impedance",
      "microelectronics",
      "boundary element method",
      "green's function",
      "nyquist plot",
      "heat transfer",
      "phasor notation"
    ]
  },
  {
    "id": "130",
    "title": "Directional texture transfer for video",
    "abstract": "Texture transfer is a method that copies the texture of a reference image to a target image. This technique has an advantage in that various styles can be expressed according to the reference image, in a single framework. However, in this technique, it is not easy to control the effect of each style. In addition, when this technique is extended to processing video images, maintaining temporal coherence is very difficult. In this paper, we propose an algorithm that transfers the texture of a reference image to a target video while retaining the directionality of the target video. The algorithm maintains the temporal coherency of the transferred texture, and controls the style of the texture transfer.",
    "keywords": [
      "texture transfer",
      "temporal coherence",
      "example-based rendering",
      "video processing"
    ]
  },
  {
    "id": "131",
    "title": "Wind parameters extraction from aircraft trajectories",
    "abstract": "High altitude wind parameters can be extracted from recorded aircraft positions. This method gives the best results when aircraft are stabilized (no turn, no climb, no descend) This method can extract wind dynamics, e.g. how wind parameters change over time. Our method performs well thanks to a mix of automatic wind extraction and direct manipulation technique.",
    "keywords": [
      "wind extraction",
      "least squares approximation",
      "air traffic control",
      "data exploration",
      "visual analytics"
    ]
  },
  {
    "id": "132",
    "title": "A three-dimensional model of the human transglutaminase 1: insights into the understanding of lamellar ichthyosis",
    "abstract": "The stratum corneum, the outer layer of the epidermis, serves as a protective barrier to isolate the skin from the external environment. Keratinocyte transglutaminase 1 (TGase 1) catalyzes amide crosslinking between glutamine and lysine residues on precursor proteins forming the impermeable layers of the epidermal cell envelopes (CE), the highly insoluble membranous structures of the stratum corneum. Patients with the autosomal recessive skin disorder lamellar ichthyosis (LI) appear to have deficient cross-linking of the cell envelope due to mutations identified in TGase 1, linking this enzyme to LI. In the absence of a crystal structure, molecular modeling was used to generate the structure of TGase 1. We have mapped the known mutations of TGase 1 from our survey obtained from a search of PubMed and successfully predicted the impact of these mutations on LI. Furthermore, we have identified Ca(2+) binding sites and propose that Ca(2+) induces a cis to trans isomerization in residues near the active site as part of the enzyme transamidation activation. Docking experiments suggest that substrate binding subsequently induces the reverse cis to trans isomerization, which may be a significant part of the catalytic process. These results give an interpretation at the molecular level of previously reported mutations and lead to further insights into the structural model of TGase 1, providing a new basis for understanding LI.",
    "keywords": [
      "keratinocyte transglutaminase 1",
      "lamellar ichthyosis",
      "mutations",
      "metal ions",
      "isomerization",
      "molecular modeling"
    ]
  },
  {
    "id": "133",
    "title": "Understanding continuance usage of mobile sites",
    "abstract": "Purpose - The purpose of this research is to draw on both perspectives of technological perceptions and flow experience to examine continuance usage of mobile sites. Design/methodology/approach - Based on the valid responses collected from a. survey questionnaire, structural equation modeling technology was employed to examine the research model. Findings - The results indicated that both perspectives of technological perceptions and flow experience have effects on satisfaction, which in turn affects continuance usage. Technological perceptions include system quality and information quality, whereas flow experience includes perceived enjoyment and attention focus. Among them, perceived enjoyment has the largest effect on satisfaction. Research limitations/implications - This research is conducted in China, where mobile internet is still in its early stage. Thus, the results need to be generalized to other countries that had developed mobile internet. Originality/value - Previous research has focused on the effects of instrumental beliefs such as perceived usefulness on mobile user continuance. However, user behavior may be also affected by intrinsic motivations such as flow. This research tries to fill the gap.",
    "keywords": [
      "information quality",
      "system quality",
      "mobile sites"
    ]
  },
  {
    "id": "134",
    "title": "Interconnection analysis for standard cell layouts",
    "abstract": "We present an accurate model and procedures for predicting the common physical design characteristics of standard cell layouts (i.e., the interconnection length and the chip area). The predicted results are obtained from analysis of the net list only, that is, no prior knowledge of the functionality of the design is used, Random and optimized placements, global routing, and detailed routing are each abstracted by procedural models that capture the important features of these processes, and closed-form expressions that define these procedural models are presented. We have verified both the global characteristics (total interconnection length and layout area) and the detailed characteristics (wire length and feedthrough distributions) of the model, On the designs in our test suite, the estimates are very close to the actual layouts.",
    "keywords": [
      "global route modeling",
      "interconnection length estimation",
      "layout area estimation",
      "placement modeling",
      "standard cell layout"
    ]
  },
  {
    "id": "135",
    "title": "MISEP - Linear and nonlinear ICA based on mutual information",
    "abstract": "Linear Independent Components Analysis (ICA) has become an important signal processing and data analysis technique, the typical application being blind source separation in a wide range of signals, such as biomedical, acoustical and astrophysical ones. Nonlinear ICA is less developed, but has the potential to become at least as powerful. This paper presents MISEP, an ICA technique for linear and nonlinear mixtures, which is based on the minimization of the mutual information of the estimated components. MISEP is a generalization of the popular INFOMAX technique, which is extended in two ways: (1) to deal with nonlinear mixtures, and (2) to be able to adapt to the actual statistical distributions of the sources, by dynamically estimating the nonlinearities to be used at the outputs. The resulting MISEP method optimizes a network with a specialized architecture, with a single objective function: the output entropy. The paper also briefly discusses the issue of nonlinear source separation. Examples of linear and nonlinear source separation performed by MISEP are presented.",
    "keywords": [
      "ica",
      "blind source separation",
      "nonlinear ica",
      "mutual information"
    ]
  },
  {
    "id": "136",
    "title": "Beyond state-of-the-art topology as normative ground for decison-making systems",
    "abstract": "Reviews experiments in design and urbanism, intervening in the development of transdisciplinary systems theory for decision-making organizations. Presents beyond state-of-the-art phenomena, of a morphological and topological type (out of architecture), and advocates harnessing such creativity power to problem solving in informatics.",
    "keywords": [
      "architecture",
      "cybernetics",
      "perception"
    ]
  },
  {
    "id": "137",
    "title": "Fuzzy-hybrid modelling of an Ackerman steered electric vehicle",
    "abstract": "Physical system modelling with known parameters together with 2-D or high order look-up tables (obtained from experimental data), have been the preferred method for simulating electric vehicles. The non-linear phenomena which are present at the vehicle tyre patch and ground interface have resulted in it quantitative understanding of this phenomena. However, nowadays, there is it requirement for a deeper understanding of the vehicle sub-models which previously used look-up tables. In this paper the hybrid modelling methodology used for electric vehicle systems offers a two-stage advantage: firstly, the vehicle model retains a comprehensive analytical formulation and secondly, the 'fuzzy' element offers, in addition to the quantitative results, a qualitative understanding of specific vehicle sub-models. In the literature several hybrid topologies are reported, sequential, auxiliary, and embedded. In this paper, the hybrid model topology selected is auxiliary and within the same hybrid model, the first paradigm used is the vehicle dynamics together With the actuator/gearbox system. The second paradigm is the non-linear fuzzy tyre model for each wheel. In particular, conventional physical system dynamic modelling has been combined with the fuzzy logic type-II or type-III methodology. The resulting hybrid-fuzzy tyre models were estimated for a-priori number of rules from experimental data. The physical system modelling required the available vehicle parameters such as the overall mass, wheel radius and chassis dimensions. The suggested synergetic fusion of the two methods, (hybrid-fuzzy), allowed the vehicle planar trajectories to be obtained prior to the hardware development of the entire vehicle. The strength of this methodology is that it requires localised system experimental data rather than global system data. The disadvantage in obtaining global experimental data is the requirement for comprehensive testing of it vehicle prototype which is both time consuming process and requires extensive resources. In this paper the authors have proposed the use of existing experimental rigs which are available from the leading automotive manufacturers. Hence, for the 'hybrid' modelling, localised data sets were used. In particular, wheel-tyre experimental data were obtained from the University Tyre Rig experimental facilities. Tyre forces acting on the tyre patch are mainly responsible for the overall electric vehicle motion. In addition, tyre measurement rigs are a well known method for obtaining localised data thus allowing the effective simulation of more detailed mathematical models. These include, firstly, physical system modelling (conventional vehicle dynamics), secondly, fuzzy type II or III modelling (for the tyre characteristics), and thirdly, electric drive modelling within the context of electric vehicles. The proposed hybrid model synthesis has resulted in simulation results which are similar to piece-wise 'look-up' table solutions. In addition, the strength of the 'hybrid' synthesis is that the analyst has a set of rules which clearly show the reasoning behind the complex development of the vehicle tyre forces. This is due to the inherent transparency of the type II and type III methodologies. Finally, the authors discussed the reasons for selecting a type-III framework. The paper concludes with a plethora of simulation results.  ",
    "keywords": [
      "hybrid model synthesis-",
      "type-ii and type-iii fuzzy systems",
      "parameter estimation",
      "electric drives"
    ]
  },
  {
    "id": "138",
    "title": "On the assessment and evaluation of voice hoarseness",
    "abstract": "This article presents a non-invasive speech processing method for the assessment and evaluation of voice hoarseness. A technique based on time-scale analysis of the voice signal is used to decompose the signal into a suitable number of high-frequency details and extract the high-frequency bands of the signal. A discriminating measure, which measures the roll-off in power in the high-frequency bands of the signal, with respect to the decomposition index, is developed. The measure reflects the presence and degree of severity of hoarseness in the analyzed voice signals. The discriminating measure is supported by frequency-domain and time-series analyses of the high-frequency bands of normal and hoarse voice signals to provide a visual aid to the clinician or therapist. A database of sustained long vowels of normal and hoarse voices is created and used to assess the presence and degree of severity of hoarseness. The results obtained by the proposed method are compared to results obtained by perturbation analysis.",
    "keywords": [
      "voice hoarseness",
      "speech pathology",
      "speech analysis",
      "time-scale and time-series analysis"
    ]
  },
  {
    "id": "139",
    "title": "probabilistic models of computer systems",
    "abstract": "We develop a method based on diffusion approximations in order to compute, under some general conditions, the queue length distribution for a queue in a network. Applications to computer networks and to time-sharing systems are presented.",
    "keywords": [
      "sharing",
      "network",
      "order",
      "applications",
      "method",
      "systems",
      "diffuse",
      "computer network",
      "computation",
      "general",
      "timing",
      "model",
      "distributed"
    ]
  },
  {
    "id": "140",
    "title": "Integrating synchronization with priority into a Kronecker representation",
    "abstract": "The compositional representation of a Markov chain using Kronecker algebra, according to a compositional model representation as a superposed generalized stochastic Petri net or a stochastic automata network, has been studied for a while. In this paper we describe a Kronecker expression and associated data structures, that allows to handle nets with synchronization over activities of different levels of priority. New algorithms for these structures are provided to perform an iterative solution method of Jacobi or GaussSeidel type. These algorithms are implemented in the APNN Toolbox. We use this implementation in combination with GreatSPN and exercise an example that illustrates characteristics of the presented algorithms.",
    "keywords": [
      "stochastic petri nets",
      "performance evaluation tools",
      "numerical algorithms"
    ]
  },
  {
    "id": "141",
    "title": "Non-monotone trust region methods for nonlinear equality constrained optimization without a penalty function",
    "abstract": "We propose and analyze a class of penalty-function-free nonmonotone trust-region methods for nonlinear equality constrained optimization problems. The algorithmic framework yields global convergence without using a merit function and allows nonmonotonicity independently for both, the constraint violation and the value of the Lagrangian function. Similar to the Byrd-Omojokun class of algorithms, each step is composed of a quasi-normal and a tangential step. Both steps are required to satisfy a decrease condition for their respective trust-region subproblems. The proposed mechanism for accepting steps combines nonmonotone decrease conditions on the constraint violation and/or the Lagrangian function, which leads to a flexibility and acceptance behavior comparable to filter-based methods. We establish the global convergence of the method. Furthermore, transition to quadratic local convergence is proved. Numerical tests are presented that confirm the robustness and efficiency of the approach.",
    "keywords": [
      "nonmonotone trust-region methods",
      "sequential quadratic programming",
      "penalty function",
      "global convergence",
      "equality constraints",
      "local convergence",
      "large-scale optimization"
    ]
  },
  {
    "id": "142",
    "title": "Numerical computation of the helical Chandrasekhar-Kendall modes",
    "abstract": "A new formulation is presented for numerically computing the helical Chandrasekhar-Kendall modes in an axisymmetric torus. It explicitly imposes del . B = 0 and yields a standard matrix eigenvalue problem, which can then be solved by standard matrix eigenvalue techniques. Numerical implementation and computational results are shown for an axisymmetric torus typical of reversed field pinch and spherical tokamak.  ",
    "keywords": [
      "magnetic relaxation",
      "taylor state",
      "chandrasekhar-kendall modes",
      "eigenvalue",
      "spherical tokamak",
      "reversed field pinch",
      "helicity injection"
    ]
  },
  {
    "id": "143",
    "title": "Fuzzy adaptive neural network approach to path loss prediction in urban areas at GSM-900 band",
    "abstract": "This paper presents the results of the Adaptive-Network Based Fuzzy Inference System (ANFIS) for the prediction of path loss in a specific urban environment. A new algorithm based ANFIS for tuning the path loss model is introduced in this work. The performance of the path loss model which is obtained from proposed algorithm is compared to the Bertoni-Walfisch model, which is one of the best studied for propagation analysis involving buildings. This comparison is based on the mean square error between predicted and measured values. According to the indicated error criterion, the errors related to the predictions that are obtained from the algorithm are less than the errors that are obtained from the Bertoni-Walfisch Model. In this study, propagation measurements were carried out in the 900 MHz band in the city of Istanbul, Turkey.",
    "keywords": [
      "anfis",
      "propagation measurements",
      "path loss",
      "urban environment"
    ]
  },
  {
    "id": "144",
    "title": "Optimization of microwave devices combining topology gradient and genetic algorithm",
    "abstract": "Topology optimization can be seen as optimizing a distribution of small topological elements within a domain with respect to given specifications. A numerical topology gradient (TG) algorithm is applied in the context of electromagnetism for optimizing microwave devices, computing the sensitivity on adding or removing small metallic elements. This method leads to an optimum topology with very little initial information in acceptable time consumption. The method is applied to the design of a microstrip component in which the topology gradient is directly used as a direction of descent. However, in some ill-behavior problems, topology gradient is not sufficient to converge to the global optimum. In the latter case, the basic TG is coupled with a genetic algorithm (G.A) to make a more suitable algorithm for solving local optima problems. ",
    "keywords": [
      "shape optimization",
      "topology gradient",
      "genetic algorithm"
    ]
  },
  {
    "id": "145",
    "title": "A layout algorithm for undirected compound graphs",
    "abstract": "We present an algorithm for the layout of undirected compound graphs, relaxing restrictions of previously known algorithms in regards to topology and geometry. The algorithm is based on the traditional force-directed layout scheme with extensions to handle multi-level nesting, edges between nodes of arbitrary nesting levels, varying node sizes, and other possible application-specific constraints. Experimental results show that the execution time and quality of the produced drawings with respect to commonly accepted layout criteria are quite satisfactory. The algorithm has also been successfully implemented as part of a pathway integration and analysis toolkit named PATIKA, for drawing complicated biological pathways with compartmental constraints and arbitrary nesting relations to represent molecular complexes and various types of pathway abstractions.  ",
    "keywords": [
      "information visualization",
      "graph drawing",
      "force-directed graph layout",
      "compound graphs",
      "bioinformatics"
    ]
  },
  {
    "id": "146",
    "title": "A knowledge-based scheduling system for Emergency Departments",
    "abstract": "A knowledge-based reactive scheduling system is proposed to answer the requirements of Emergency Departments (EDs). The algorithm includes detailed patient priority, arrival time, flow time and doctor load. The main aim is to determine the patients who have higher priorities initially, and then minimize their waiting times. To achieve this aim, physicians and the other related workers can use an interactive system. In this study, we evaluated the existing system by comparing the proposed system. Also, reactive scheduling cases were evaluated for some items such as decreasing the number of doctors, changing durations and entering of an urgent patient to the system. All experiments were performed with proposed algorithm and right shift rescheduling approach.",
    "keywords": [
      "knowledge-based system",
      "reactive scheduling",
      "emergency department",
      "health care system",
      "patient priorities"
    ]
  },
  {
    "id": "147",
    "title": "Specifying and proving properties of timed I/O automata using Tempo",
    "abstract": "Timed I/O automata (TIOA) is a mathematical framework for modeling and verification of distributed systems that involve discrete and continuous dynamics. TIOA can be used for example, to model a real-time software component controlling a physical process. The TIOA model is sufficiently general to subsume other models in use for timed systems. The Tempo Toolset, currently under development, is aimed at supporting system development based on TIOA specifications. The Tempo Toolset is an extension of the IOA toolkit, which provides a specification simulator, a code generator, and both model checking and theorem proving support for analyzing specifications. This paper focuses on the modeling of timed systems and their properties with TIOA and on the use of TAME4TIOA, the TAME (Timed Automata Modeling Environment) based theorem proving support provided in Tempo, for proving system properties, including timing properties. Several examples are provided by way of illustration.",
    "keywords": [
      "system development frameworks",
      "modeling environments",
      "tool suites",
      "automata models",
      "timed automata",
      "hybrid systems",
      "formal methods",
      "specification",
      "verification",
      "theorem proving"
    ]
  },
  {
    "id": "148",
    "title": "Distributed virtual backbone construction in sensor networks with asymmetric links",
    "abstract": "In this paper, we study the problem of distributed virtual backbone construction in sensor networks, where the coverage area of nodes are disks with different radii. This problem is modeled by the construction of a minimum connected dominating set (MCDS) in geometric k-disk graphs. We derive the size relationship of any maximal independent set (MIS) and MCDS in geometric k-disk graphs, and apply it to analyze the performances of two distributed connected dominating set (CDS) algorithms we propose in this paper. These algorithms have bounded performance ratio and low communication overhead. To the best of our knowledge, the results reported in this paper represent the state-of-the-art. ",
    "keywords": [
      "sensor networks with asymmetric transmission links",
      "connected dominating set",
      "geometric k-disk graphs",
      "maximal independent set"
    ]
  },
  {
    "id": "149",
    "title": "A Real-Time Java Chip-Multiprocessor",
    "abstract": "Chip-multiprocessors are an emerging trend for embedded systems. In this article, we introduce a real-time Java multiprocessor called JopCMP. It is a symmetric shared-memory multiprocessor, and consists of up to eight Java Optimized Processor (JOP) cores, an arbitration control device, and a shared memory. All components are interconnected via a system on chip bus. The arbiter synchronizes the access of multiple CPUs to the shared main memory. In this article, three different arbitration policies are presented, evaluated, and compared with respect to their real-time and average-case performance: a fixed priority, a fair-based, and a time-sliced arbiter. Tasks running on different CPUs of a chip-multiprocessor (CMP) influence each others' execution times when accessing a shared memory. Therefore, the system needs an arbiter that is able to limit the worst-case execution time of a task running on a CPU, even though tasks executing simultaneously on other CPUs access the main memory. Our research shows that timing analysis is in fact possible for homogeneous multiprocessor systems with a shared memory. The timing analysis of tasks, executing on the CMP using time-sliced memory arbitration, leads to viable worst-case execution time bounds. The time-sliced arbiter divides the memory access time into equal time slots, one time slot for each CPU. This memory arbitration scheme allows for a calculation of upper bounds of Java application worst-case execution times, depending on the number of CPUs, the time slot size, and the memory access time. Examples of worst-case execution time calculation are presented, and the analyzed results of a real-world application task are compared to measured execution time results. Finally, we evaluate the tradeoffs when using a time-predictable solution compared to using average-case optimized chip-multiprocessors, applying three different benchmarks. These experiments are carried out by executing the programs on the CMP prototype.",
    "keywords": [
      "design",
      "experimentation",
      "measurement",
      "performance",
      "real-time system",
      "multiprocessor",
      "java processor",
      "shared memory",
      "worst-case execution time"
    ]
  },
  {
    "id": "150",
    "title": "A new finite element to represent prismatic joint constraints in mechanisms",
    "abstract": "Among existing kinematic analysis methods of mechanisms, the techniques based on finite elements represent a generally applicable alternative which enable a wide variety of problems to be solved, including linear (velocities, accelerations, jerk, ) and non-linear ones (position). To modelize a mechanism via these techniques, the link element may be used to introduce a distance constraint between two points. The stiffness matrix assembly of these link elements enables stiffness matrix construction from the model, from which the kinematic behaviour of the mechanism may be extracted. Normally kinematic link conditions introduced directly into the system stiffness matrix are used to introduce point to line constraints like those originated by prismatic joints. A new finite element is presented in this paper, defined by its stiffness or geometric matrix, capable of alternatively modelizing the constraints imposed by the prismatic joint. This new element offers numerous advantages against the procedure based on anterior link conditions, particularly in the case of non-linear problems.",
    "keywords": [
      "finite elements",
      "mechanism kinematics",
      "linear problems",
      "prismatic joint",
      "velocity",
      "acceleration"
    ]
  },
  {
    "id": "151",
    "title": "Image thresholding based on the EM algorithm and the generalized Gaussian distribution",
    "abstract": "In this paper, a novel parametric and global image histogram thresholding method is presented. It is based on the estimation of the statistical parameters of object and background classes by the expectationmaximization (EM) algorithm, under the assumption that these two classes follow a generalized Gaussian (GG) distribution. The adoption of such a statistical model as an alternative to the more common Gaussian model is motivated by its attractive capability to approximate a broad variety of statistical behaviors with a small number of parameters. Since the quality of the solution provided by the iterative EM algorithm is strongly affected by initial conditions (which, if inappropriately set, may lead to unreliable estimation), a robust initialization strategy based on genetic algorithms (GAs) is proposed. Experimental results obtained on simulated and real images confirm the effectiveness of the proposed method.",
    "keywords": [
      "image thresholding",
      "expectationmaximization algorithm",
      "generalized gaussian distribution",
      "genetic algorithms"
    ]
  },
  {
    "id": "152",
    "title": "Fast fractal image coding based on LMSE analysis and subblock feature",
    "abstract": "In this paper, we propose a fast fractal image coding based on LMSE (least mean square error) analysis and subblock feature. The proposed method focuses on efficient search of contrast scaling, position of its matched domain block, and isometric transform for a range block. The contrast scaling and the domain block position are searched using a cost function that comes from the LMSE analysis of the range block and its fractal-approximated block. The isometric transform is searched using 2 x 2 blocks formed with the averages of subblocks of range block and domain block. Experimental results show that the encoding time of a conventional fractal image coding with our search method is 25.6-39.7 times faster than that with full search method at the same bit rate while giving PSNR decrement of 0.2-0.7 dB with negligible deterioration in subjective quality. It is also shown that the encoding time of a conventional fractal image coding with our search method is 3.4-4.2 times faster than Jacquin's fractal image coding and is superior by maximum 0.8 dB in PSNR. It also yields reconstructed images of better quality.",
    "keywords": [
      "image coding",
      "fractal image coding",
      "lmse",
      "contractive mapping"
    ]
  },
  {
    "id": "153",
    "title": "Finite element solution to passive scalar transport behind line sources under neutral and unstable stratification",
    "abstract": "This study employed a direct numerical simulation (DNS) technique to contrast the plume behaviours and mixing of passive scalar emitted from line sources (aligned with the spanwise direction) in neutrally and unstably stratified open-channel flows. The DNS model was developed using the Galerkin finite element method (FEM) employing trilinear brick elements with equal-order interpolating polynomials that solved the momentum and continuity equations, together with conservation of energy and mass equations in incompressible flow. The second-order accurate fractional-step method was used to handle the implicit velocity-pressure Coupling in incompressible flow. It also segregated the solution to the advection and diffusion terms, which were then integrated in time, respectively, by the explicit third-order accurate Runge-Kutta method and the implicit second-order accurate Crank-Nicolson method. The buoyancy term under unstable stratification was integrated in time explicitly by the first-order accurate Euler method. The DNS FEM model calculated the scalar-plume development and the mean plume path. In particular, it calculated the plume meandering in the wall-normal direction under unstable stratification that agreed well with the laboratory and field measurements. as well as previous modelling results available in literature. ",
    "keywords": [
      "fluid turbulence",
      "direct numerical simulation ",
      "finite element method ",
      "open-channel flow",
      "passive scalar plume"
    ]
  },
  {
    "id": "154",
    "title": "The effect of radius/height ratio on truss optimization",
    "abstract": "The optimal topology of a Michell's truss is being considered as a benchmark problem. It has been observed that this optimal topology is only applicable up to a particular ratio of distance between the loading point to the line joining the supports and the span of the supports. Once the ratio exceeds this critical ratio, the optimum topology of the Michell's truss changes. It has been observed from the studies that it is possible to demarcate the region of two different types of optimum topologies by a linear relation. Extending this problem to a 3-D, similar type of observation of different optimum topologies has been observed above and below the critical ratio of height to radius ratio. This critical ratio, similar to a 2-D case, follows almost a linear relation.",
    "keywords": [
      "genetic algorithms",
      "optimization",
      "2-d truss",
      "3-d truss",
      "structure",
      "topology"
    ]
  },
  {
    "id": "155",
    "title": "exploratory evaluations of a computer game supporting cognitive behavioural therapy for adolescents",
    "abstract": "The need to provide effective mental health treatments for adolescents has been described as a global public health challenge [27]. In this paper we discuss the exploratory evaluations of the first adolescent intervention to fully integrate a computer game implementing Cognitive Behavioural Therapy. Three distinct studies are presented: a detailed evaluation in which therapists independent of the design team used the game with 6 adolescents experiencing clinical anxiety disorders; a study in which a member of the design team used the game with 15 adolescents; and finally a study assessing the acceptability of the game and intervention with 216 practicing therapists. Findings are presented within the context of a framework for the design and evaluation of complex health interventions. The paper provides an in-depth insight into the use of therapeutic games to support adolescent interventions and provides stronger evidence than previously available for both their effectiveness and acceptability to stakeholders.",
    "keywords": [
      "computer games",
      "evaluations",
      "cognitive behavioural therapy",
      "complex health interventions",
      "adolescent mental health"
    ]
  },
  {
    "id": "156",
    "title": "Efficient high order waveguide mode solvers based on boundary integral equations",
    "abstract": "For optical waveguides with high index contrast and sharp corners, high order full-vectorial mode solvers are difficult to develop, due to the field singularities at the corners. A recently developed method (the so-called BIE-NtD method) based on boundary integral equations (BIEs) and Neumann-to-Dirichlet (NtD) maps achieves high order of accuracy for dielectric waveguides. In this paper, we develop two new BIE mode solvers, including an improved version of the BIE-NtD method and a new BIE-DtN method based on Dirichlet-to-Neumann (DtN) maps. For homogeneous domains with sharp corners, we propose better BIEs to compute the DtN and NtD maps, and new kernel-splitting techniques to discretize hypersingular operators. Numerical results indicate that the new methods are more efficient and more accurate, and work very well for metallic waveguides and waveguides with extended mode profiles.",
    "keywords": [
      "optical waveguides",
      "boundary integral equations",
      "dirichlet-to-neumann map",
      "neumann-to-dirichlet map",
      "mode solvers",
      "hypersingular integral operators"
    ]
  },
  {
    "id": "157",
    "title": "Caches for Multimedia Workloads: Power and Energy Tradeoffs",
    "abstract": "One of the significant workloads in current generation desktop processors and mobile devices is multimedia processing. Large on-chip caches are common in modern processors, but large caches will result in increased power consumption and increased access delays. Regular data access patterns in streaming multimedia applications and video processing applications can provide high hit-rates, but due to issues associated with access time, power and energy, caches cannot be made very large. Characterizing and optimizing the memory system is conducive for designing power and performance efficient multimedia application processors. Performance tradeoffs for multimedia applications have been studied in the past, however, power and energy tradeoffs for caches for multimedia processing have not been adequately studied in the past. In this paper, we characterize multimedia applications for I-cache and D-cache power and energy using a multilevel cache hierarchy. Both dynamic and static power increase with increasing cache sizes, however, the increase in dynamic power is small. The increase in static power is significant, and becomes increasingly relevant for smaller feature sizes. There is significant static power dissipation, similar to 45%, in L1 & L2 caches at 70 ram technology sizes, emphasizing the fact that future multimedia systems must be designed by taking leakage power reduction techniques into account. The energy consumption of on-chip L2 caches is seen to be very sensitive to cache size variations. Sizes larger than 16 k for I-caches and 32 k for D-caches will not be efficient choices to maintain power and performance balance. Since multimedia applications spend significant amounts of time in integer operations, to improve the performance, we propose implementing low power full adders and hybrid multipliers in the data path, which results in 9% to 21% savings in the overall power consumption.",
    "keywords": [
      "cache",
      "leakage power",
      "low power",
      "multimedia workload characterization"
    ]
  },
  {
    "id": "158",
    "title": "Methodology for modeling and analysis of supply networks",
    "abstract": "The analysis and modeling of business processes are the basis on which management methodologies, simulation models and information systems are developed. The goal of this paper is to point out the possibility of establishing relationships between processes in supply networks and functioning of the whole system. In this integrated system, all relevant factors for supply network management, both at the global level and at the single process level, could be observed. The idea is to form a process library of the supply network, which would contain process description, inputs, outputs, and the way the process is realized. Every record in the library presents the single instance of that process. The relationships of one process with another depend on process structure and the way of its realization. Every instance of a process represents its realization. The assembly of mutual compatible instances of all processes represents one realization of the supply network. The key problem, triggering the process realization, is solved by specific production expert system. Process realization is very similar to a real system, because the environment influence, uncertainty, and available resources are taken into consideration. As the output, the aggregate of relevant parameters for the evaluation of model functioning are derived. This concept presents the basis of virtual framework for supply network simulation.",
    "keywords": [
      "supply network",
      "modeling",
      "simulation model",
      "analysis"
    ]
  },
  {
    "id": "159",
    "title": "quantifying information leaks in software",
    "abstract": "Leakage of confidential information represents a serious security risk. Despite a number of novel, theoretical advances, it has been unclear if and how quantitative approaches to measuring leakage of confidential information could be applied to substantial, real-world programs. This is mostly due to the high complexity of computing precise leakage quantities. In this paper, we introduce a technique which makes it possible to decide if a program conforms to a quantitative policy which scales to large state-spaces with the help of bounded model checking. Our technique is applied to a number of officially reported information leak vulnerabilities in the Linux Kernel. Additionally, we also analysed authentication routines in the Secure Remote Password suite and of a Internet Message Support Protocol implementation. Our technique shows when there is unacceptable leakage; the same technique is also used to verify, for the first time, that the applied software patches indeed plug the information leaks. This is the first demonstration of quantitative information flow addressing security concerns of real-world industrial programs.",
    "keywords": [
      "quantitative information flow",
      "linux kernel",
      "information leakage"
    ]
  },
  {
    "id": "160",
    "title": "Four-dimensional radiotherapeutic dose calculation using biomechanical respiratory motion description",
    "abstract": "Organ motion due to patient breathing introduces a technical challenge for dosimetry and lung tumor treatment by hadron therapy. Accurate dose distribution estimation requires patient-specific information on tumor position, size, and shape as well as information regarding the material density and stopping power of the media along the beam path. A new 4D dosimetry method was developed, which can be coupled to any motion estimation method. As an illustration, the new method was implemented and tested with a biomechanical model and clinical data.",
    "keywords": [
      "particle therapy",
      "moving organs",
      "dosimetry",
      "4d-ct"
    ]
  },
  {
    "id": "161",
    "title": "Fast dynamic organization without short-term synaptic plasticity: A new view on Hebb's dynamical assemblies",
    "abstract": "Hebb postulated cell assemblies as the basic computational elements for understanding cortical processing. He defined them as temporary associations of neurons that organize fast and flexibly into functional units, using correlation-based short-term synaptic plasticity. Based on the properties of spiking neurons, we implement dynamical assemblies that organize completely without synaptic plasticity. Instead, we find varying effective connection strengths that reflect the organizational process. We propose that this dynamic reorganization capabilities ocurring on a fast temporal scale may be a central element of cortical processing.",
    "keywords": [
      "hebb",
      "spiking neurons",
      "pools",
      "dynamical assemblies",
      "grouping",
      "correlations",
      "coherence",
      "oscillatory activity"
    ]
  },
  {
    "id": "162",
    "title": "State space analysis of Petri nets with relation-algebraic methods",
    "abstract": "A large variety of systems can be modelled by Petri nets. Their formal semantics are based on linear algebra which in particular allows the Calculation of a Petri net's state space. Since state space explosion is still a serious problem, efficiently calculating, representing, and analysing the state space is mandatory. We propose a formal semantics of Petri nets based on executable relation-algebraic specifications. Thereupon, we suggest how to calculate the markings reachable from a given one simultaneously. We provide an efficient representation of reachability graphs and show in a correct-by-construction approach how to efficiently analyse their properties. Therewith we cover two aspects: modelling and model checking systems by means of one and the same logic-based approach. On a practical side, we explore the power and limits of relation-algebraic concepts for concurrent system analysis.  ",
    "keywords": [
      "relation algebra",
      "petri nets",
      "reachability graph",
      "state space analysis",
      "systems analysis"
    ]
  },
  {
    "id": "163",
    "title": "Longitudinal Image Registration With Temporally-Dependent Image Similarity Measure",
    "abstract": "Longitudinal imaging studies are frequently used to investigate temporal changes in brain morphology and often require spatial correspondence between images achieved through image registration. Beside morphological changes, image intensity may also change over time, for example when studying brain maturation. However, such intensity changes are not accounted for in image similarity measures for standard image registration methods. Hence, 1) local similarity measures, 2) methods estimating intensity transformations between images, and 3) metamorphosis approaches have been developed to either achieve robustness with respect to intensity changes or to simultaneously capture spatial and intensity changes. For these methods, longitudinal intensity changes are not explicitly modeled and images are treated as independent static samples. Here, we propose a model-based image similarity measure for longitudinal image registration that estimates a temporal model of intensity change using all available images simultaneously.",
    "keywords": [
      "deformable registration",
      "longitudinal registration",
      "magnetic resonance imaging ",
      "nonuniform appearance change"
    ]
  },
  {
    "id": "164",
    "title": "Simultaneous optimization of phase balancing and reconfiguration in distribution networks using BFNM algorithm",
    "abstract": "Rephasing strategy is one of the main methods used for phase balancing and neutral current reduction in electrical distribution networks and the reconfiguration technique is an effective method for network loss reduction. In this paper, a new method for the simultaneous implementation of reconfiguration and phase balancing strategies is presented as a combinational strategy. In order to solve the proposed optimization problem, Nelder Mead algorithm combined with a bacterial foraging algorithm (BFNM) is used based on a fuzzy multi-objective function. The proposed method allows for the simultaneous execution of reconfiguration and phase balancing while minimizing the interruption cost of rephasing in addition to eliminating network unbalancing and reducing neutral current and network losses. To demonstrate the efficiency of the BFNM algorithm, its performance is compared with bacterial foraging (BF), particle swarm optimization (PSO), genetic and immune algorithms (GA and IA). The proposed method is applied to the IEEE 123-bus test network for evaluation. The simulation results confirm the efficiency of the method in reducing the system costs and network phase balancing.",
    "keywords": [
      "phase balancing",
      "rephasing strategy",
      "reconfiguration technique",
      "bfnm algorithm",
      "distribution networks"
    ]
  },
  {
    "id": "165",
    "title": "Vowel onset point detection for noisy speech using spectral energy at formant frequencies",
    "abstract": "In this paper, we propose a method for robust detection of the vowel onset points (VOPs) from noisy speech. The proposed VOP detection method exploits the spectral energy at formant frequencies of the speech segments present in glottal closure region. In this work, formants are extracted by using group delay function, and glottal closure instants are extracted by using zero frequency filter based method. Performance of the proposed VOP detection method is compared with the existing method, which uses the combination of evidence from excitation source, spectral peaks energy and modulation spectrum. Speech data from TIMIT database and noise samples from NOISEX database are used for analyzing the performance of the VOP detection methods. Significant improvement in the performance of VOP detection is observed by using proposed method compared to existing method.",
    "keywords": [
      "vowel onset point ",
      "formant frequencies",
      "glottal closure region",
      "excitation source",
      "spectral peaks",
      "modulation spectrum"
    ]
  },
  {
    "id": "166",
    "title": "Permutable fuzzy consequence and interior operators and their connection with fuzzy relations",
    "abstract": "Fuzzy operators are an essential tool in many fields and the operation of composition is often needed. In general, composition is not a commutative operation. However, it is very useful to have operators for which the order of composition does not affect the result. In this paper, we analyze when permutability appears. That is, when the order of application of the operators does not change the outcome. We characterize permutability in the case of the composition of fuzzy consequence operators and the dual case of fuzzy interior operators. We prove that for these cases, permutability is completely connected to the preservation of the operator type. We also study the particular case of fuzzy operators induced by fuzzy relations through Zadehs compositional rule and the inf-composition. For this cases, we connect permutability of the fuzzy relations (using the sup-? composition) with permutability of the induced operators. Special attention is paid to the cases of operators induced by fuzzy preorders and similarities. Finally, we use these results to relate the operator induced by the transitive closure of the composition of two reflexive fuzzy relations with the closure of the operator this composition induces.",
    "keywords": [
      "permutability",
      "fuzzy consequence operator",
      "fuzzy closure operator",
      "fuzzy interior operator",
      "fuzzy preorder",
      "indistinguishability relation"
    ]
  },
  {
    "id": "167",
    "title": "Text mining, names and security",
    "abstract": "A Process Query System, a new approach to representing and querying multiple hypotheses, is proposed for cross-document co-reference and linking based on existing entity extraction, co-reference and database name-matching technologies. A crucial component of linking entities across documents is the ability to recognize when different name strings are potential references to the same entity. Given the extraordinary range of variation international names can take when rendered in the Roman alphabet, this is a daunting task. The extension of name variant matching to free text will add important text mining functionality for intelligence and security informatics' toolkits.",
    "keywords": [
      "co-reference",
      "multiple hypothesis tracking",
      "name matching",
      "natural language processors"
    ]
  },
  {
    "id": "168",
    "title": "Carbenic vs. ionic mechanistic pathway in reaction of cyclohexanone with bromoform",
    "abstract": "The extensive computation study was done to elucidate the mechanism of formation dibromoepoxide from cyclohexanone and bromoform. In this reaction, the formation of dihaloepoxide 2 is postulated as a key step that determines the distribution and stereochemistry of products. Two mechanistic paths of reaction were investigated: the addition of dibromocarbene to carbonyl group of ketone, and the addition of tribromomethyl carbanion to the same (C=O) group. The mechanisms for the addition reactions of dibromocarbenes and tribromomethyl carbanions with cyclohexanone have been investigated using ab initio HF/6-311++G** and MP2/6-311+G* level of theory. Solvent effects on these reactions have been explored by calculations which included a continuum polarizable conductor model (CPCM) for the solvent (H2O). The calculations showed that both mechanisms are possible and are exothermic, but have markedly different activation energies.",
    "keywords": [
      "ab initio calculations",
      "dibromocarbene",
      "dihaloepoxides",
      "reaction mechanisms",
      "tribromomethyl carbanion"
    ]
  },
  {
    "id": "169",
    "title": "query operations for moving objects database systems",
    "abstract": "Geographical Information Systems were originally intended to deal with snapshots representing a single state of some reality but there are more and more applications requiring the representation and querying of time-varying information. This work addresses the representation of moving objects on GIS. The continuous nature of movement raises problems for representation in information systems due to the limited capacity of storage systems and the inherently discrete nature of measurement instruments. The stored information has therefore to be partial and does not allow an exact inference of the real-world object's behavior. To cope with this, query operations must take uncertainty into consideration in their semantics in order to give accurate answers to the users. The paper proposes a set of operations to be included in a GIS or a spatial database to make it able to answer queries on the spatio-temporal behavior of moving objects. The operations have been selected according to the requirements of real applications and their semantics with respect to uncertainty is specified. A collection of examples from a case study is included to illustrate the expressiveness of the proposed operations.",
    "keywords": [
      "moving objects",
      "movement operations",
      "spatio-temporal databases",
      "spatio-temporal uncertainty"
    ]
  },
  {
    "id": "170",
    "title": "ON THE RELATIONSHIP BETWEEN THE TRACEABILITY PROPERTIES OF REED-SOLOMON CODES",
    "abstract": "Fingerprinting codes are used to prevent dishonest users (traitors) from redistributing digital contents. In this context, codes with the traceability (TA) property and codes with the identifiable parent property (IPP) allow the unambiguous identification of traitors. The existence conditions for IPP codes are less strict than those for TA codes. In contrast, IPP codes do not have an efficient decoding algorithm in the general case. Other codes that have been widely studied but possess weaker identification capabilities are separating codes. It is a well-known result that a TA code is an IPP code, and an IPP code is a separating code. The converse is in general false. However, it has been conjectured that for Reed-Solomon codes all three properties are equivalent. In this paper we investigate this equivalence, providing a positive answer when the number of traitors divides the size of the ground field.",
    "keywords": [
      "fingerprinting and traitor tracing",
      "identifiable parent property",
      "separating codes",
      "mds codes",
      "reed-solomon codes"
    ]
  },
  {
    "id": "171",
    "title": "occlusion handling based on sub-blobbing in automated video surveillance system",
    "abstract": "Object tracking with occlusion handling is a challenging problem in automated video surveillance. In particular, occlusion handling and tracking have been often considered as separate modules. This paper proposes a tracking method in the context of video surveillance, where occlusions are automatically detected and handled to solve ambiguities. Hence, the tracking process can continue to track the different moving objects correctly. The proposed approach is based on sub-blobbing, that is, blobs representing moving objects are segmented into sections whenever occlusions occur. These sub-blobs are then treated as blobs with the occluded ones ignored. By doing so, the tracking of objects has become more accurate and less sensitive to occlusions. We have also used a feature-based framework for identifying the tracked objects, where several flexible attributes were involved. Experiments on several videos have clearly demonstrated the success of the proposed method.",
    "keywords": [
      "occlusion handling",
      "features",
      "video surveillance",
      "object tracking"
    ]
  },
  {
    "id": "172",
    "title": "towards the formalization of interaction semantics",
    "abstract": "With the advent of Web 2.0 and the emergence of improved technologies to enhance UI, the importance of user experience and intuitiveness of Web interfaces led to the growth and success of Interaction Design. Web designers often turn to pre-defined and well-founded design patterns and user interaction paradigms to build novel and more effective Web interfaces. The rational behind Interaction Design patterns is based on user behavior and Web navigation studies. The \"semantics\" of user interaction is therefore a rich and interesting area that is worth exploring in association with traditional Semantic Web approaches. In this paper, we present our first attempts of an ontological formalization of interaction patterns and its implications. To prove our concept, we illustrate the mapping approach we employed to put in relation that interaction formalization with data-specific ontologies, to create Web interfaces to browse and navigate that specialized kind of information; the aforementioned ontologies and mapping rules are the basis of the internal operation of a Semantic Web application framework called STAR:chart, leveraged to build the Service-Finder portal; finally, we present our evaluation results.",
    "keywords": [
      "web interfaces",
      "semantics",
      "semantic web",
      "interaction semantics"
    ]
  },
  {
    "id": "173",
    "title": "College students academic motivation, media engagement and fear of missing out",
    "abstract": "Possible links between FoMO, social media engagement, and three motivational constructs were examined. A new scale was designed to measure the extent to which students used social media tools in the classroom. The links between social media engagement and motivational factors were mediated by FoMO.",
    "keywords": [
      "fear of missing out",
      "social media engagement",
      "self-determination theory",
      "academic motivation",
      "higher education"
    ]
  },
  {
    "id": "174",
    "title": "A hybrid collaborative filtering method for multiple-interests and multiple-content recommendation in E-Commerce",
    "abstract": "Recommender systems apply knowledge discovery techniques to the problem of making personalized recommendations for products or services during a live interaction. These systems, especially collaborative filtering based on user, are achieving widespread success on the Web. The tremendous growth in the amount of available information and the kinds of commodity to Web sites in recent years poses some key challenges for recommender systems. One of these challenges is ability of recommender systems to be adaptive to environment where users have many completely different interests or items have completely different content (We called it as Multiple interests and Multiple-content problem). Unfortunately, the traditional collaborative filtering systems can not make accurate recommendation for the two cases because the predicted item for active user is not consist with the common interests of his neighbor users. To address this issue we have explored a hybrid collaborative filtering method, collaborative filtering based on item and user techniques, by combining collaborative filtering based on item and collaborative filtering based on user together. Collaborative filtering based on item and user analyze the user-item matrix to identify similarity of target item to other items, generate similar items of target item, and determine neighbor users of active user for target item according to similarity of other users to active user based on similar items of target item. In this paper we firstly analyze limitation of collaborative filtering based on user and collaborative filtering based on item algorithms respectively and emphatically make explain why collaborative filtering based on user is not adaptive to Multiple-interests and Multiplecontent recommendation. Based on analysis, we present collaborative filtering based on item and user for Multiple-interests and Multiple-content recommendation. Finally, we experimentally evaluate the results and compare them with collaborative filtering based on user and collaborative filtering based on item, respectively. The experiments suggest that collaborative filtering based on item and user provide better recommendation quality than collaborative filtering based on user and collaborative filtering based on item dramatically. ",
    "keywords": [
      "collaborative filtering",
      "recommender systems",
      "personalization",
      "e-commerce"
    ]
  },
  {
    "id": "175",
    "title": "new approaches to covering and packing problems",
    "abstract": "Covering and packing integer programs model a large family of combinatorial optimization problems. The current-best approximation algorithms for these are an instance of the basic probabilistic method: showing that a certain randomized approach produces a good approximation with positive probability. This approach seems inherently sequential; by employing the method of alteration we present the first RNC and NC approximation algorithms that match the best sequential guarantees. Extending our approach, we get the first RNC and NC approximation algorithms for certain multi-criteria versions of these problems. We also present the first NC algorithms for two packing and covering problems that are not subsumed by the above result: finding large independent sets in graphs, and rounding fractional Group Steiner solutions on trees.",
    "keywords": [
      "approximation algorithms",
      "method",
      "families",
      "approximation",
      "matching",
      "trees",
      "graph",
      "probability",
      "version",
      "group",
      "combinatorial optimization",
      "model",
      "algorithm",
      "randomization",
      "posit"
    ]
  },
  {
    "id": "176",
    "title": "Novel phenotype issues raised in cross-national epidemiological research on drug dependence",
    "abstract": "Stage-transition models based on the American Diagnostic and Statistical Manual (DSM) generally are applied in epidemiology and genetics research on drug dependence syndromes associated with cannabis, cocaine, and other internationally regulated drugs (IRDs). Difficulties with DSM stage-transition models have surfaced during cross-national research intended to provide a truly global perspective, such as the work of the World Mental Health Surveys Consortium. Alternative simpler dependence-related phenotypes are possible, including population-level count process models for steps early and before coalescence of clinical features into a coherent syndrome (e.g., zero-inflated Poisson [ZIP] regression). Selected findings are reviewed, based on ZIP modeling of alcohol, tobacco, and IRD count processes, with an illustration that may stimulate new research on genetic susceptibility traits. The annual National Surveys on Drug Use and Health (NSDUH) can be readily modified for this purpose, along the lines of a truly anonymous research approach that can help make NSDUH-type cross-national epidemiological surveys more useful in the context of subsequent genomewide association (GWAS) research and post-GWAS investigations with a truly global health perspective.",
    "keywords": [
      "alcohol",
      "tobacco",
      "dependence",
      "epidemiology",
      "phenotype"
    ]
  },
  {
    "id": "177",
    "title": "TCAD study on gate-all-around cylindrical (GAAC) transistor for CMOS scaling to the end of the roadmap",
    "abstract": "In this paper, we report TCAD study on gate-all-around cylindrical (GAAC) transistor for sub-10-nm scaling. The GAAC transistor device physics, TCAD simulation, and proposed fabrication procedure have been discussed. Among all other novel fin field effect transistor (FinFET) devices, the gate-all-around cylindrical device can be particularly used for reducing the problems of conventional multi-gate FinFET, improving device performance, and scaling-down capabilities. With gate-all-around cylindrical architecture, the transistor is controlled essentially by infinite number of gates surrounding the entire cylinder-shaped channel. Electrical integrity within the channel is improved by reducing the leakage current due to the non-symmetrical field accumulation such as the corner effect. Our proposed fabrication procedure for making devices having the gate-all-around cylindrical (GAAC) device architecture is also discussed.",
    "keywords": [
      "gate-all-around cylindrical  transistor",
      "device physics",
      "tcad simulation",
      "fabrication procedure"
    ]
  },
  {
    "id": "178",
    "title": "a rule engine to process acceleration data on small sensor nodes",
    "abstract": "In this paper, we propose a compact rule processing engine to process acceleration data on a small sensor device. Our proposed engine enables us to develop applications using acceleration data on the small device with a quite simple and short description. We describe the outline of both our proposed rule engine and an implementation on our developed sensor device called the Mo-Co-Mi chip.",
    "keywords": [
      "ubiquitous computing",
      "sensor node"
    ]
  },
  {
    "id": "179",
    "title": "mug1 - an incremental compiler-compiler",
    "abstract": "MUG1 is a compiler generating system developed and implemented at the Technical University of Munich. The structure of the system and the concepts used in the compiler description are presented. Special emphasis is laid on the use of MUG1 as a tool for the incremental design of programming languages and the construction of their compilers in parallel.",
    "keywords": [
      "concept",
      "tool",
      "compilation",
      "use",
      "structure",
      "systems",
      "parallel",
      "design",
      "programming language",
      "incremental"
    ]
  },
  {
    "id": "180",
    "title": "Parallel and distributed local search in COMET",
    "abstract": "The availability of commodity multiprocessors and high-speed networks of workstations offer significant opportunities for addressing the increasing computational requirements of optimization applications. To leverage these potential benefits, it is important, however, to make parallel and distributed processing easily accessible to a wide audience of optimization programmers. This paper addresses this challenge by proposing parallel and distributed programming abstractions that keep the distance from sequential local search algorithms as small as possible. The abstractions, including parallel loops, interruptions, thread pools, and shared objects, are compositional and cleanly separate the optimization program and the parallel instructions. They have been evaluated experimentally on a variety of applications, including warehouse location and coloring, for which they provide significant speedups.",
    "keywords": [
      "combinatorial optimization",
      "local search",
      "constraint programming",
      "parallel",
      "distributed",
      "language"
    ]
  },
  {
    "id": "181",
    "title": "Causal Relationship from Exposure to Chemicals in Oil Refining and Chemical Industries and Malignant Melanoma",
    "abstract": "Malignant melanoma has been thought to be related mainly to exposure to the sun or radiation. A review of the scientific literature reveals many significant correlations between benzene and benzene-containing solvents in the workplace and the occurrence of malignant melanoma, particularly in sites that have never been exposed to sunlight. A comparison of positive correlations between such exposure and malignant melanoma by independent investigators and negative findings by investigators with industry affiliations reveals that this difference, at least in part, may account for the discrepant findings. Based on independent studies, it is reasonable to conclude that malignant melanoma is causally related to employment-related chemical exposures in the petroleum refining industry",
    "keywords": [
      "benzene",
      "malignant melanoma",
      "industry studies",
      "independent investigators",
      "petroleum industry",
      "chemical carcinogenesis",
      "cutaneous malignancies"
    ]
  },
  {
    "id": "182",
    "title": "Control of a wind turbine cluster based on squirrel cage induction generators connected to a single VSC power converter",
    "abstract": "A control procedure for wind farms connected to a unique converter is presented. The control method is based on vector control, providing high performance. The system operates in the maximum efficiency area due to the use of a MPPT. A power reduction method used in case of an electrical contingency is described. The proposed wind farm layout claims to improve the efficiency and the reliability.",
    "keywords": [
      "wind power generation",
      "high voltage direct current ",
      "variable frequency wind farm",
      "offshore wind power",
      "wind turbine cluster"
    ]
  },
  {
    "id": "183",
    "title": "Semantic inference of user's reputation and expertise to improve collaborative recommendations",
    "abstract": "Collaborative recommender systems select potentially interesting items for each user based on the preferences of like-minded individuals. Particularly, e-commerce has become a major domain in these research field due to its business interest, since identifying the products the users may like or find useful can boost consumption. During the last years, a great number of works in the literature have focused in the improvement of these tools. Expertise, trust and reputation models are incorporated in collaborative recommender systems to increase their accuracy and reliability. However, current approaches require extra data from the users that is not often available. In this paper, we present two contributions that apply a semantic approach to improve recommendation results transparently to the users. On the one hand, we automatically build implicit trust networks in order to incorporate trust and reputation in the selection of the set of like-minded users that will drive the recommendation. On the other hand, we propose a measure of practical expertise by exploiting the data available in any e-commerce recommender system - the consumption histories of the users.  ",
    "keywords": [
      "personalized e-commerce",
      "semantic reasoning",
      "collaborative filtering",
      "trust",
      "reputation",
      "expertise"
    ]
  },
  {
    "id": "184",
    "title": "Two-tier image annotation model based on a multi-label classifier and fuzzy-knowledge representation scheme",
    "abstract": "Multi-label classification and knowledge-based approach to image annotation. The definition of the fuzzy knowledge representation scheme based on FPN. Novel data-driven algorithms for automatic acquisition of fuzzy knowledge. Novel inference based algorithms for annotation refinement and scene recognition. A comparison of inference-based scene classification with an ordinary approach.",
    "keywords": [
      "image annotation",
      "knowledge representation",
      "inference algorithms",
      "fuzzy petri net",
      "multi-label image classification"
    ]
  },
  {
    "id": "185",
    "title": "Betting system for formative code review in educational competitions",
    "abstract": "Grading systems based on competition ranking usually limit the grade distribution. We propose a methodology based on a betting system to relax the ranking restrictions. Betting assesses the skill to critically analyze source code. A case study in a video game development course validates our proposal.",
    "keywords": [
      "assessment",
      "gamification",
      "competition",
      "software development",
      "code review"
    ]
  },
  {
    "id": "186",
    "title": "COMBINATORIAL CONSTRUCTION OF LOCALLY TESTABLE CODES",
    "abstract": "An error correcting code is said to be locally testable if there is a test that checks whether a given string is a codeword, or rather far from the code, by reading only a constant number of symbols of the string. While the best known construction of locally testable codes (LTCs) by Ben-Sasson and Sudan [SIAM J. Comput., 38 (2008), pp. 551-607] and Dinur [J. ACM, 54 (2007), article 12] achieves very efficient parameters, it relies heavily on algebraic tools and on probabilistically checkable proof (PCP) machinery. In this work we present a new and arguably simpler construction of LTCs that is purely combinatorial, does not rely on PCP machinery, and matches the parameters of the best known construction. However, unlike the latter construction, our construction is not entirely explicit.",
    "keywords": [
      "locally testable codes ",
      "probabilistically checkable proofs ",
      "pcps of proximity "
    ]
  },
  {
    "id": "187",
    "title": "Deterministic and probabilistic multi-modal analysis of slope stability",
    "abstract": "Traditional slope stability analysis involves predicting the location of the critical slip surface for a given slope and computing a safety factor at that location. However, for some slopes with complicated stratigraphy several distinct critical slip surfaces can exist. Furthermore, the global minimum safety factor in some cases can be less important than potential failure zones when rehabilitating or reinforcing a slope. Existing search techniques used in slope stability analysis cannot find all areas of concern, but instead converge exclusively on the critical slip surface. This paper therefore proposes the use of a holistic multi modal optimisation technique which is able to locate and converge to multiple failure modes simultaneously. The search technique has been demonstrated on a number of benchmark examples using both deterministic and probabilistic analysis to find all possible failure mechanisms, and their respective factors of safety and reliability indices. The results from both the deterministic and probabilistic models show that the search technique is effective in locating the known critical slip surface while also establishing the locations of any other distinct critical slip surfaces within the slope. The approach is of particular relevance for investigating the stability of large slopes with complicated stratigraphy, as these slopes are likely to contain multiple failure mechanisms.",
    "keywords": [
      "multi-modal failure",
      "probabilistic analysis",
      "deterministic analysis",
      "slope stability",
      "multi-modal optimisation"
    ]
  },
  {
    "id": "188",
    "title": "Epithelial Tight Junctions in Intestinal Inflammation",
    "abstract": "The epithelium in inflamed intestinal segments of patients with Crohn's disease is characterized by a reduction of tight junction strands, strand breaks, and alterations of tight junction protein content and composition. In ulcerative colitis, epithelial leaks appear early due to micro-erosions resulting from upregulated epithelial apoptosis and in addition to a prominent increase of claudin-2. Th1-cytokine effects by interferon-? in combination with TNF? are important for epithelial damage in Crohn's disease, while interleukin-13 (IL-13) is the key effector cytokine in ulcerative colitis stimulating apoptosis and upregulation of claudin-2 expression. Focal lesions caused by apoptotic epithelial cells contribute to barrier disturbance in IBD by their own conductivity and by confluence toward apoptotic foci or erosions. Another type of intestinal barrier defect can arise from ?-hemolysin harboring E. coli strains among the physiological flora, which can gain pathologic relevance in combination with proinflammatory cytokines under inflammatory conditions. On the other hand, intestinal barrier impairment can also result from transcellular antigen translocation via an initial endocytotic uptake into early endosomes, and this is intensified by proinflammatory cytokines as interferon-? and may thus play a relevant role in the onset of IBD. Taken together, barrier defects contribute to diarrhea by a leak flux mechanism (e.g., in IBD) and can cause mucosal inflammation by luminal antigen uptake. Immune regulation of epithelial functions by cytokines may cause barrier dysfunction not only by tight junction impairments but also by apoptotic leaks, transcytotic mechanisms, and mucosal gross lesions.",
    "keywords": [
      "apoptosis",
      "barrier function",
      "claudins",
      "crohn's disease",
      "inflammatory bowel disease",
      "interleukin-13",
      "tight junction",
      "tumor necrosis factor-alpha",
      "ulcerative colitis"
    ]
  },
  {
    "id": "189",
    "title": "Fusion of perceptual cues for robust tracking of head pose and position",
    "abstract": "The paradigm of perceptual fusion provides robust solutions to computer vision problems. By combining the outputs of multiple vision modules, the assumptions and constraints of each module are factored out to result in a more robust system overall. The integration of different modules can be regarded as a form of data fusion. To this end, we propose a framework for fusing different information sources through estimation of covariance from observations. The framework is demonstrated in a face and 3D pose tracking system that fuses similarity-to-prototypes measures and skin colour to track head pose and face position. The use of data fusion through covariance introduces constraints that allow the tracker to robustly estimate head pose and track face position simultaneously.  ",
    "keywords": [
      "data fusion",
      "pose estimation",
      "similarity representation",
      "face recognition"
    ]
  },
  {
    "id": "190",
    "title": "Encoding multiple orientations in a recurrent network",
    "abstract": "Models containing recurrent connections amongst the cells within a population can account for a range of empirical data on orientation selectivity in striate cortex. However, existing recurrent models are unable to veridically encode more than one orientation at a time. Underlying this inability is an inherent limitation in the variety of activity profiles that can be stably maintained. We propose a new recurrent model that can form a broader range of stable population activity patterns. We demonstrate that these patterns preserve information about multiple orientations present in the population inputs. This preservation has significant computational consequences when information encoded in several populations must be integrated to perform behavioral tasks, such as visual discrimination.",
    "keywords": [
      "population codes",
      "orientation selectivity",
      "recurrent network models"
    ]
  },
  {
    "id": "191",
    "title": "An automated design and assembly of interference-free modular fixture setup",
    "abstract": "This paper describes an automated modular fixture design system developed using a CAD-based methodology and implemented on a 3-D CAD/CAM software package. The developed automated fixture design (AFD) system automates the fixturing points determination and is integrated on top of the previously developed interactive and semi-automated fixture design systems. The determination of fixturing points is implemented in compliance with the fixturing principles that are formulated as heuristics rules to generate candidate list of points and then select the exact points from the list. Apart from determining the fixturing points automatically, the system is capable of producing cutting tool collision-free fixture design using its machining interference detection sub-module. The machining interference detection is accomplished through the use of cutter swept solid based on cutter swept volume approach. Therefore, using the developed AFD, an interference-free fixture design and assembly can be achieved in the possible shortest design lead-time.",
    "keywords": [
      "modular fixture",
      "automated fixture design",
      "machining interference",
      "cutter swept volume approach"
    ]
  },
  {
    "id": "192",
    "title": "Pseudorandomness and average-case complexity via uniform reductions",
    "abstract": "Impagliazzo and Wigderson (1998) gave the first construction of pseudorandom generators from a uniform complexity assumption on EXP (namely EXP not equal BPP). Unlike results in the nonuniform setting, their result does not provide a continuous trade-off between worst-case hardness and pseudorandomness, nor does it explicitly establish an average-case hardness result. In this paper: We obtain an optimal worst-case to average-case connection for EXP: if EXP not subset of BPTIME(t(n)), then EXP has problems that cannot be solved on a fraction 1/2 + 1/t'(n) of the inputs by BPTIME(t'(n)) algorithms, for t' = t(Omega(1)). We exhibit a PSPACE-complete self-correctible and downward self-reducible problem. This slightly simplifies and strengthens the proof of Impagliazzo and Wigderson, which used a #P-complete problem with these properties. We argue that the results of lmpagliazzo and Wigderson, and the ones in this paper, cannot be proved via \"black-box\" uniform reductions.",
    "keywords": [
      "pseudorandomness",
      "average-case complexity",
      "derandomization",
      "instance checkers"
    ]
  },
  {
    "id": "193",
    "title": "Galectins in acute and chronic inflammation",
    "abstract": "Galectins are animal lectins that bind to ?-galactosides, such as lactose and N-acetyllactosamine, in free form or contained in glycoproteins or glycolipids. They are located intracellularly or extracellularly. In the latter they exhibit bivalent or multivalent interactions with glycans on cell surfaces and induce various cellular responses, including production of cytokines and other inflammatory mediators, cell adhesion, migration, and apoptosis. Furthermore, they can form lattices with membrane glycoprotein receptors and modulate receptor properties. Intracellular galectins can participate in signaling pathways and alter biological responses, including apoptosis, cell differentiation, and cell motility. Current evidence indicates that galectins play important roles in acute and chronic inflammatory responses, as well as other diverse pathological processes. Galectin involvement in some processes in vivo has been discovered, or confirmed, through studies of genetically engineered mouse strains, each deficient in a given galectin. Current evidence also suggests that galectins may be therapeutic targets or employed as therapeutic agents for these inflammatory responses.",
    "keywords": [
      "galectins",
      "inflammation",
      "allergic inflammation",
      "autoimmune disease",
      "atherosclerosis"
    ]
  },
  {
    "id": "194",
    "title": "Hierarchical analysis of power distribution networks",
    "abstract": "Careful design and verification of the power distribution network of a chip are of critical importance to ensure its reliable performance. With the increasing number of transistors on a chip, the size of the power network has grown so large as to make the verification task very challenging. The available computational power and memory resources impose limitations on the size of networks that can be analyzed using currently known techniques. Many of today's designs have power networks that are too large to be analyzed in the traditional way as flat networks. In this paper, we propose a hierarchical analysis technique to overcome the aforesaid capacity limitation. We present a new technique for analyzing a power grid using macromodels that are created for a set of partitions of the grid. Efficient numerical techniques for the computation and sparsification of the port admittance matrices of the macromodels are presented. A novel sparsification technique using a 0-1 integer linear programming formulation is proposed to achieve superior sparsification for a specified error. The run-time and memory efficiency of the proposed method are illustrated on industrial designs. It is shown that even for a 60 million-node power grid, our approach allows for an efficient analysis, whereas previous approaches have been unable to handle power grids of such size.",
    "keywords": [
      "circuit simulation",
      "ir drop",
      "matrix sparsification",
      "partitioning",
      "power distribution networks",
      "power grid",
      "signal integrity"
    ]
  },
  {
    "id": "195",
    "title": "On the interference of ultra wide band systems on point to point links and fixed wireless access systems",
    "abstract": "Ultra Wide Bandwidth (UWB) spread-spectrum techniques will play a key role in short range wireless connectivity supporting high bit rates availability and low power consumption. UWB can be used in the design of wireless local and personal area networks providing advanced integrated multimedia services to nomadic users within hot-spot areas. Thus the assessment of the possible interference caused by UWB devices on already existing narrowband and wideband systems is fundamental to ensure nonconflicting coexistence and, therefore, to guarantee acceptance of UWB technology worldwide. In this paper, we study the coexistence issues between an indoor UWB-based system (hot-spot) and outdoor point to point (PP) links and Fixed Wireless Access (FWA) systems operating in the 3.5 - 5.0 GHz frequency range. We consider a realistic UWB master/slave system architecture and we show through computer simulation, that in all practical cases UWB system can coexist with PP and FWA without causing any dangerous interference.",
    "keywords": [
      "4g communication systems",
      "spread spectrum",
      "ultra wide band"
    ]
  },
  {
    "id": "196",
    "title": "Scheduling Multimedia Services in a Low-Power MAC for Wireless and Mobile ATM Networks",
    "abstract": "This paper describes the design and analysis of the scheduling algorithm for energy conserving medium access control (EC-MAC), which is a low-power medium access control (MAC) protocol for wireless and mobile ATM networks. We evaluate the scheduling algorithms that have been proposed for traditional ATM networks. Based on the structure of EC-MAC and the characteristics of wireless channel, we propose a new algorithm that can deal with the burst errors and the location-dependent errors. Most scheduling algorithms proposed for either wired or wireless networks were analyzed with homogeneous traffic or multimedia services with simplified traffic models. We analyze our scheduling algorithm with more realistic multimedia traffic models based on H. 263 video traces and self-similar data traffic. One of the key goals of the scheduling algorithms is simplicity and fast implementation. Unlike the time-stamped based algorithms, our algorithm does not need to sort the virtual time, and thus, the complexity of the algorithm is reduced significantly.",
    "keywords": [
      "low-power operation",
      "multiple access methods",
      "queuing and scheduling algorithms",
      "wireless and mobile atm",
      "wireless multimedia communications"
    ]
  },
  {
    "id": "197",
    "title": "Solving capacitated arc routing problems using a transformation to the CVRP",
    "abstract": "A well-known transformation by Pearn, Assad and Golden reduces a capacitated arc routing problem (CARP) into an equivalent capacitated vehicle routing problem (CVRP). However, that transformation is regarded as unpractical, since an original instance with r  required edges is turned into a CVRP over a complete graph with 3r+1 3 r + 1 vertices. We propose a similar transformation that reduces this graph to 2r+1 2 r + 1 vertices, with the additional restriction that a previously known set of r pairwise disconnected edges must belong to every solution. Using a recent branch-and-cut-and-price algorithm for the CVRP, we observed that it yields an effective way of attacking the CARP, being significantly better than the exact methods created specifically for that problem. Computational experiments obtained improved lower bounds for almost all open instances from the literature. Several such instances could be solved to optimality. Scope and purpose The scope of this paper is transforming arc routing problems into node routing problems. The paper shows that this approach can be effective and, in particular, that the original instances may generate node routing instances that behave as if the size is not increased. This result is obtained by slightly modifying the well-known transformation by Pearn, Assad and Golden from capacitated arc routing problem (CARP) to the capacitated vehicle routing problem (CVRP), that is regarded as unpractical. The paper provides a computational experience using a recent branch-and-cut-and-price algorithm for the CVRP. The results are significantly better than the exact methods created specifically for that problem, improving lower bounds for almost all open instances from the literature. Several such instances could be solved to optimality.",
    "keywords": [
      "arc routing",
      "mixed-integer programming"
    ]
  },
  {
    "id": "198",
    "title": "Combining two pheromone structures for solving the car sequencing problem with Ant Colony Optimization",
    "abstract": "The car sequencing problem involves scheduling cars along an assembly line while satisfying capacity constraints. In this paper, we describe an Ant Colony Optimization (ACO) algorithm for solving this problem, and we introduce two different pheromone structures for this algorithm: the first pheromone structure aims at learning for good sequences of cars, whereas the second pheromone structure aims at learning for critical cars. We experimentally compare these two pheromone structures, that have complementary performances, and show that their combination allows ants to solve very quickly most instances.",
    "keywords": [
      "ant colony optimization",
      "car sequencing problem",
      "multiple pheromone structures"
    ]
  },
  {
    "id": "199",
    "title": "Calibrating information users' views on relevance: A social representations approach",
    "abstract": "The purpose of this study is to investigate how information users view the concept of relevance and make their judgement(s) on relevant information through the framework of social representations theory. More specifically, this study attempts to address the questions of what users view as the constituent concepts of relevance, what are core and peripheral concepts of relevance, and how these concepts are structured by applying a structural analysis approach of social representations theory. We employ a free word association method for data collection. Two hundred and forty four information users of public and academic libraries responded to questionnaires on their relevance judgement criteria. Collected data were content analysed and assessed using weighted frequency, similarity measure, and core/periphery measurements to identify key elements of relevance and to differentiate core and periphery elements of relevance. Results show that four out of 26 emerged elements (concepts) are core and 22 are periphery elements of the concept of relevance. The findings of this study provide a quantitative measure of weighing various elements of relevance and the internal structure of the concept of relevance from users' perspectives providing enhancements for search algorithms with quantitative metadata support.",
    "keywords": [
      "relevance",
      "relevance criteria",
      "social representations",
      "structural analysis",
      "core-periphery analysis"
    ]
  },
  {
    "id": "200",
    "title": "Constructing fault-tolerant communication trees in hypercubes",
    "abstract": "A communication tree is a binomial tree embedded in a hypercube, whose communication direction is from its leaves to its root. If a problem to be solved is first divided into independent subproblems, then each subproblem can be solved by one of the hypercube processors, and all the subresults can be merged into the final results through tree communication. This paper uses two random search techniques, the genetic algorithm (GA) and simulated annealing (SA), to construct fault-tolerant communication trees with the minimum data transmission time. Experimental evaluation shows that, with reasonably low search time, the proposed GA and SA approaches are able to find more desirable communication trees (i.e., trees with less data transmission time) than the minimal cost approach can. A distributed approach which applies parallel search to communication subtrees in disjoint subcubes is also provided to reduce the search time of the proposed approaches.",
    "keywords": [
      "fault-tolerant communication trees",
      "hypercubes",
      "genetic algorithms",
      "simulated annealing",
      "data transmission time",
      "search time",
      "maximal fault-free subcubes"
    ]
  },
  {
    "id": "201",
    "title": "The firekeepers: aging considered as a resource",
    "abstract": "Technology can improve the quality of life for elderly persons by supporting and facilitating the unique leadership roles that elderly play in groups, communities, and other organizations. Elderly people are often organizational firekeepers. They maintain community memory, pass on organizational practices, and ensure social continuity. This paper reports studies of several essential community roles played by elderly community membersincluding the role of volunteer community webmasterand describes two positive design projects that investigated how technology can support new kinds of social endeavors and contributions to society by elderly citizens. Finally, the paper speculates on the utility of intergenerational teams in strengthening societys workforce.",
    "keywords": [
      "aging",
      "elderly",
      "positive design",
      "non-profit community-based groups",
      "intergenerational teams"
    ]
  },
  {
    "id": "202",
    "title": "Limited error based event localizing temporal decomposition and its application to variable-rate speech coding",
    "abstract": "This paper proposes a novel algorithm for temporal decomposition (TD) of speech, called limited error based event localizing temporal decomposition (LEBEL-TD), and its application to variable-rate speech coding. In previous work with TD, TD analysis was usually performed on each speech segment of about 200300ms or more, making it impractical for online applications. In this present work, the event localization is determined based on a limited error criterion and a local optimization strategy, which results in an average algorithmic delay of 65ms. Simulation results show that an average log spectral distortion of about 1.5dB can be achievable at an event rate of 20events/s. Also, LEBEL-TD uses neither the computationally costly singular value decomposition routine nor the event refinement process, thus reducing significantly the computational cost of TD. Further, a method for variable-rate speech coding an average rate of around 1.8kbps based on STRAIGHT (Speech Transformation and Representation using Adaptive Interpolation of weiGHTed spectrum), which is a high-quality speech analysissynthesis framework, using LEBEL-TD is also realized. Subjective test results indicate that the performance of the proposed speech coding method is comparable to that of the 4.8kbps FS-1016 CELP coder.",
    "keywords": [
      "temporal decomposition",
      "event vector",
      "event function",
      "straight",
      "speech coding",
      "line spectral frequency"
    ]
  },
  {
    "id": "203",
    "title": "ONLINE AND OFFLINE SOCIAL TIES OF SOCIAL NETWORK WEBSITE USERS: AN EXPLORATORY STUDY IN ELEVEN SOCIETIES",
    "abstract": "This study presents results of a survey about social network website (SNW) usage that was administered to university students in China, Egypt, France, Israel, India, Korea, Macao, Sweden, Thailand, Turkey, and the United States. The offline and online social ties of SNW users were examined by nationality, levels of individualism-collectivism (I-C), gender, SNW usage, age, and access location. Contrary to existing literature, we found no differences in the number of offline friends between individualist and collectivist nations. Similarly, there was not a difference in the number of online social ties between individualist and collectivist nations. However, members of collectivist nations had significantly more online social ties never met in person. Heavy SNW users in individualist nations maintained significantly higher numbers of offline social ties; however, heavy SNW users in collectivist nations did not have higher numbers of offline social ties. Related implications and recommendations are provided.",
    "keywords": [
      "social ties",
      "online social ties",
      "individualism",
      "collectivism",
      "social networking websites"
    ]
  },
  {
    "id": "204",
    "title": "Pair-wise path key establishment in wireless sensor networks",
    "abstract": "When sensor networks deployed in unattended and hostile environments, for securing communication between sensors, secret keys must be established between them. Many key establishment schemes have been proposed for large scale sensor networks. In these schemes, each sensor shares a secret key with its neighbors via preinstalled keys. But it may occur that two end nodes which do not share a key with each other could use a secure path to share a secret key between them. However during the transmission of the secret key, the secret key will be revealed to each node along the secure path. Several researchers proposed a multi-path key establishment to prevent a few compromised sensors from knowing the secret key, but it is vulnerable to stop forwarding or Byzantine attacks. To counter these attacks, we propose a hop by hop authentication scheme for path key establishment to prevent Byzantine attacks. Compared to conventional protocols, our proposed scheme can mitigate the impact of malicious nodes from doing a Byzantine attack and sensor nodes can identify the malicious nodes. In addition, our scheme can save energy since it can detect and filter false data not beyond two hops.  ",
    "keywords": [
      "byzantine attacks",
      "path key establishment",
      "security",
      "wireless sensor networks"
    ]
  },
  {
    "id": "205",
    "title": "Affect-aware behaviour modelling and control inside an intelligent environment",
    "abstract": "The evidence suggests that human actions are supported by emotional elements that complement logic inference in our decision-making processes. In this paper an exploratory study is presented providing initial evidence of the positive effects of emotional information on the ability of intelligent agents to create better models of user actions inside smart-homes. Preliminary results suggest that an agent incorporating valence-based emotional data into its input array can model user behaviour in a more accurate way than agents using no emotion-based data or raw data based on physiological changes.",
    "keywords": [
      "emotion detection",
      "ambient intelligence",
      "artificial neural networks",
      "fuzzy controllers"
    ]
  },
  {
    "id": "206",
    "title": "A new DSmT combination rule in open frame of discernment and its application",
    "abstract": "A new combination rule based on Dezert-Smarandache theory (DSmT) is proposed to deal with the conflict evidence resulting from the non-exhaustivity of the discernment frame. A two-dimensional measure factor in Dempster-Shafer theory (DST) is extended to DSmT to judge the conflict degree between evidence. The original DSmT combination rule or new DSmT combination rule can be selected for fusion according to this degree. Finally, some examples in simultaneous fault diagnosis of motor rotor are given to illustrate the effectiveness of the proposed combination rule.",
    "keywords": [
      "dsmt rule of combination",
      "open frame of discernment",
      "evidence conflict",
      "simultaneous faults diagnosis",
      "generalized basic probability assignment"
    ]
  },
  {
    "id": "207",
    "title": "trading off computation for error in providing immersive voice communications for mobile gaming",
    "abstract": "The interactive experiences of players in networked games can be enhanced with the provision of an Immersive Voice Communication Service. Game players are immersed in their voice communication experience as they exchange live voice streams which are rendered in real-time with directional and distance cues corresponding to the users' positions in the virtual game world. In particular, we propose a Mobile Immersive Communication Environment (MICE) which targets mobile game players using platforms such as Sony PSP and Nintendo DS. A computation reduction scheme was proposed in our previous work for the scalable delivery of MICE from a central server. On the basis of that computation reduction scheme, this paper identifies what factors, and to what extent, affect the unacceptable voice rendering error incurred when providing MICE. In the first experimental scenario, we investigate the level of unacceptable voice rendering error incurred in MICE for different avatar densities or avatar population sizes, with a fixed level of processing limit. In the second experimental scenario, we studied the level of unacceptable voice rendering error incurred in MICE for different processing resource limits, with a fixed avatar population size or avatar density. Our findings provide important insights into the planning and dimensioning of processing resources for the support of MICE, with due considerations to the impact on the unacceptable voice rendering error incurred.",
    "keywords": [
      "computation cost reduction",
      "immersive voice communications",
      "voice over ip ",
      "mobile gaming"
    ]
  },
  {
    "id": "208",
    "title": "a scalable heuristic for evacuation planning in large road network",
    "abstract": "Evacuation planning is of critical importance for civil authorities to prepare for natural disasters, but efficient evacuation planning in large city is computationally challenging due to the large number of evacuees and the huge size of transportation networks. One recently proposed algorithm Capacity Constrained Route Planner (CCRP) can give sub-optimal solution with good accuracy in less time and use less memory compared to previous approaches. However, it still can not scale to large networks. In this paper, we analyze the overhead of CCRP and come to a new heuristic CCRP++ that scalable to large network. Our algorithm can reuse search results in previous iterations and avoid the repetitive global shortest path expansion in CCRP. We conducted extensive experiments with real world road networks and different evacuation parameter settings. The result shows it can gives great speed-up without loosing the optimality.",
    "keywords": [
      "evacuation planning",
      "ccrp",
      "shortest path"
    ]
  },
  {
    "id": "209",
    "title": "Battery energy storage system for frequency support in microgrids and with enhanced control features for uninterruptible supply of local loads",
    "abstract": "A battery energy storage system to support the frequency in autonomous microgrids. Original frequency controller to better damp the frequency oscillations. The frequency controller covers the main two control levels, namely primary and secondary. Enhanced control functions to ensure uninterruptible power supply to local sensitive loads. Simulations and experimental results validate the proposed control solution.",
    "keywords": [
      "battery energy storage system",
      "microgrid",
      "frequency control",
      "single-phase inverter"
    ]
  },
  {
    "id": "210",
    "title": "Extended Average Magnitude Difference Function Based Pitch Detection",
    "abstract": "This paper presents a new extended average magnitude difference function for noise robust pitch detection. Average magnitude difference function based algorithms are suitable for real time operations, but suffer from incorrect pitch detection in noisy conditions. The proposed new extended average magnitude difference function involves in sufficient number of averaging for all lag values compared to the original average magnitude difference function, and thereby eliminates the falling tendency of the average magnitude difference function without emphasizing pitch harmonics at higher lags, which is a severe limitation of other existing improvements of the average magnitude difference function. A noise robust post processing that explores the contribution of each frequency channel is also presented. Experimental results on Keele pitch database in different noise level, both with white and color noise, shows the superiority of the proposed extended average magnitude difference function based pitch detection method over other methods based on average magnitude difference function.",
    "keywords": [
      "pitch detection",
      "amdf",
      "eamdf",
      "noise robust"
    ]
  },
  {
    "id": "211",
    "title": "multi-core demands multi-interfaces",
    "abstract": "The challenge for the microarchitect has always been (with very few notable domain-specific exceptions) how to translate the continually increasing processing power provided by Moore's Law into increased performance, or more recently into similar performance at lower cost in energy. The mechanisms in the past (almost entirely) kept the interface intact and used the increase in transistor count to improve the performance of the microarchitecture of the uniprocessor. When that became too hard, we went to larger and larger on-chip caches. Both are consistent with the notion that \"abstractions are good.\" At some point, we got overwhelmed with too many transistors; predictably, multi-core was born. As the transistor count continues to skyrocket, we are faced with two questions: what should be on the chip, and how should the software interface to it. If we expect to continue to take advantage of what process technology is providing, I think we need to do several things, starting with rethinking the notion of abstraction and providing multiple interfaces for the programmer.",
    "keywords": [
      "multicore",
      "software interface",
      "design",
      "performance"
    ]
  },
  {
    "id": "212",
    "title": "Decisions, decisions, decisions: transfer and specificity of decision-making skill between sports",
    "abstract": "The concept of transfer of learning holds that previous practice or experience in one task or domain will enable successful performance in another related task or domain. In contrast, specificity of learning holds that previous practice or experience in one task or domain does not transfer to other related tasks or domains. The aim of the current study is to examine whether decision-making skill transfers between sports that share similar elements, or whether it is specific to a sport. Participants (n=205) completed a video-based temporal occlusion decision-making test in which they were required to decide on which action to execute across a series of 4 versus 4 soccer game situations. A sport engagement questionnaire was used to identify 106 soccer players, 43 other invasion sport players and 58 other sport players. Positive transfer of decision-making skill occurred between soccer and other invasion sports, which are related and have similar elements, but not from volleyball, supporting the concept of transfer of learning.",
    "keywords": [
      "cognitive processes",
      "knowledge",
      "skill acquisition",
      "perceptualcognitive skill"
    ]
  },
  {
    "id": "213",
    "title": "an approach of creative application evolution on cloud computing platform",
    "abstract": "Cloud computing is a paradigm that focuses on sharing data and computing resources over a scalable network of nodes, so it is becoming a preferred environment for those applications with large scalability, dynamic collaboration and elastic resource requirements. Creative computing is an emerging research field in these applications, which can be considered as the study of computer science and related technologies and how they are applied to support creativity, take part in creative processes, and solve creativity related problems. However, it is a very hard work to develop such applications from the very beginning under new environment, while it is a big waste for legacy systems under existing environment. Now software evolution plays an important role. In this paper, we introduced creative computing firstly, including definition, properties and requirements. Then the advantages of cloud computing platform for supporting creative computing were analysed. Next, a private cloud as experimental environment was built. Finally, the process of creative application evolution was illustrated. Our work is about research and application of software evolution methodology, also is an exploratory try to do creative computing research under cloud environment.",
    "keywords": [
      "creative application",
      "creative computing",
      "software evolution",
      "cloud computing"
    ]
  },
  {
    "id": "214",
    "title": "A sensitivity-based approach to analyzing signal delay uncertainty of coupled interconnects",
    "abstract": "Performance optimization is a critical step in the design of integrated circuits. Rapid advances in very large scale integration (VLSI) technology have enabled shrinking feature sizes, wire widths, and wire spacings, making the effects of coupling capacitance more apparent. As signals switch faster, noise due to coupling between neighboring wires becomes more pronounced. Changing the relative signal arrival times (RSATs) alters the victim line delay due to the varying coupling noise on the victim line. The authors propose a sensitivity-based method to analyze delay uncertainties of coupled interconnects due to uncertain signal arrival times at its inputs. Compared to existing methods of analyzing delay uncertainties of coupled interconnects, the simulation results show that the proposed method strikes a good balance between model accuracy and complexity compared to the existing approaches.",
    "keywords": [
      "coupled interconnects",
      "delay changes",
      "sensitivity",
      "signal arrival times",
      "statistical timing"
    ]
  },
  {
    "id": "215",
    "title": "Visual estimation of pointed targets for robot guidance via fusion of face pose and hand orientation",
    "abstract": "Problem formulation: given a number of possible pointed targets, compute the target that the user points to. Estimate head pose by visually tracking the off-plane rotations of the face. Recognize two different hand pointing gestures (point left and point right). Model the problem using the DempsterShafer theory of evidence. Use Demspsters rule of combination to fuse information and derive the pointed target.",
    "keywords": [
      "humanrobot interaction",
      "computer vision",
      "gesture recognition",
      "pointing gestures",
      "head pose estimation"
    ]
  },
  {
    "id": "216",
    "title": "RTN distribution comparison for bulk, FDSOI and FinFETs devices",
    "abstract": "In this paper we investigate the sensitivity of RTN noise spectra to statistical variability alone and in combination with variability in the traps properties, such as trap level and trap activation energy. By means of 3D statistical simulation, we demonstrate the latter to be mostly responsible for noise density spectra dispersion, due to its large impact on the RTN characteristic time. As a result FinFETs devices are shown to be slightly more sensitive to RTN than FDSOI devices. In comparison bulk MOSFETs are strongly disadvantaged by the statistical variability associated with high channel doping.",
    "keywords": [
      "random telegraph noise",
      "simulation",
      "finfet",
      "statistical variability",
      "reliability"
    ]
  },
  {
    "id": "217",
    "title": "Learning similarity matching in multimedia content-based retrieval",
    "abstract": "Many multimedia content-based retrieval systems allow query formulation with user setting of relative importance of features (e.g., color, texture, shape, etc) to mimic the user's perception of similarity. However, the systems do not modify their similarity matching functions, which are defined during the system development. In this paper, we present a neural network-based learning algorithm for adapting similarity matching function toward the user's query preference based on his/her relevance feedback. The relevance feedback is given as ranking errors (misranks) between the retrieved and desired lists of multimedia objects. The algorithm is demonstrated for facial image retrieval using the NIST Mugshot Identification Database with encouraging results.",
    "keywords": [
      "content-based retrieval",
      "image retrieval",
      "multimedia databases",
      "learning",
      "ranking",
      "similarity matching",
      "relevance feedback"
    ]
  },
  {
    "id": "218",
    "title": "Self-organization of decentralized swarm agents based on modified particle swarm algorithm",
    "abstract": "In this paper, an attempt has been made by incorporating some special features in the conventional particle swarm optimization (PSO) technique for decentralized swarm agents. The modified particle swarm algorithm ( MPSA) for the self-organization of decentralized swarm agents is proposed and studied. In the MPSA, the update rule of the best agent in swarm is based on a proportional control concept and the objective value of each agent is evaluated on-line. In this scheme, each agent self-organizes to flock to the best agent in swarm and migrate to a moving target while avoiding collision between the agent and the nearest obstacle/agent. To analyze the dynamics of the MPSA, stability analysis is carried out on the basis of the eigenvalue analysis for the time-varying discrete system. Moreover, a guideline about how to tune the MPSA's parameters is proposed. The simulation results have shown that the proposed scheme effectively constructs a self-organized swarm system in the capability of flocking and migration.",
    "keywords": [
      "decentralized swarm systems",
      "particle swarm optimization",
      "self-organization"
    ]
  },
  {
    "id": "219",
    "title": "Computing large deformation metric mappings via geodesic flows of diffeomorphisms",
    "abstract": "This paper examine the Euler-Lagrange equations for the solution of the large deformation diffeomorphic metric mapping problem studied in Dupuis et al. (1998) and Trouve (1995) in which two images 10, 1, are given and connected via the diffeomorphic change of coordinates I-0 o phi(-1) = I, where p = 01 is the end point at t = 1 of curve phi(t), t is an element of [0, 1] satisfying (phi)over dot(t) = v(t)(phi(t)), t is an element of [0, 1] with phi(0) = id. The variational problem takes the form [GRAPHICS] where parallel tov(t)parallel to(V) is an appropriate Sobolev norm on the velocity field v(t)(.), and the second term enforces matching of the images with parallel to.parallel to(L2) representing the squared-error norm. In this paper we derive the Euler-Lagrange equations characterizing the minimizing vector fields vt(,) t is an element of [0, 1] assuming sufficient smoothness of the norm to guarantee existence of solutions in the space of diffeomorphisms. We describe the implementation of the Euler equations using semi-lagrangian method of computing particle flows and show the solutions for various examples. As well, we compute the metric distance on several anatomical configurations as measured by integral(0)(1) parallel tov(t)parallel to(v)dt on the geodesic shortest paths.",
    "keywords": [
      "computational anatomy",
      "euler-lagrange equation",
      "variational optimization",
      "deformable template",
      "metrics"
    ]
  },
  {
    "id": "220",
    "title": "ARVO-CL: The OpenCL version of the ARVO package - An efficient tool for computing the accessible surface area and the excluded volume of proteins via analytical equations",
    "abstract": "Introduction of Graphical Processing Units (GPUs) and computing using GPUs in recent years opened possibilities for simple parallelization of programs. In this update, we present the modernized version of program ARVO [J. Busa, J. Dzurina, E. Hayryan, S. Hayryan, C-K. Hu, J. Plavka, I. Pokorny, J. Skivanek, M.-C. Wu, Comput. Phys. Comm. 165 (2005) 59]. The whole package has been rewritten in the C language and parallelized using OpenCL. Some new tricks have been added to the algorithm in order to save memory much needed for efficient usage of graphical cards. A new tool called 'input_structure' was added for conversion of pdb files into files suitable for work with the C and OpenCL version of ARVO. New version program summary Program title: ARVO-CL Catalog identifier: ADUL_v2_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/ADUL_v2_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 11834 No. of bytes in distributed program, including test data, etc.: 182528 Distribution format: tar.gz Programming language: C. OpenCL. Computer: PC Pentium; SPP'2000. Operating system: All OpenCL capable systems. Has the code been vectorized or parallelized?: Parallelized using GPUs. A serial version (non GPU) is also included in the package. Classification: 3. External routines: cl.hpp (http://www.khronos.org/registry/cl/api/1.1/cl.hpp) Catalog identifier of previous version: ADUL_v1_0 Journal reference of previous version: Comput. Phys. Comm. 165(2005)59 Does the new version supercede the previous version?: Yes",
    "keywords": [
      "arvo",
      "proteins",
      "solvent accessible area",
      "excluded volume",
      "stereographic projection",
      "opencl package"
    ]
  },
  {
    "id": "221",
    "title": "Three-Dimensional Near-Field MIMO Array Imaging Using Range Migration Techniques",
    "abstract": "This paper presents a 3-D near-field imaging algorithm that is formulated for 2-D wideband multiple-input-multiple-output (MIMO) imaging array topology. The proposed MIMO range migration technique performs the image reconstruction procedure in the frequency-wavenumber domain. The algorithm is able to completely compensate the curvature of the wavefront in the near-field through a specifically defined interpolation process and provides extremely high computational efficiency by the application of the fast Fourier transform. The implementation aspects of the algorithm and the sampling criteria of a MIMO aperture are discussed. The image reconstruction performance and computational efficiency of the algorithm are demonstrated both with numerical simulations and measurements using 2-D MIMO arrays. Real-time 3-D near-field imaging can be achieved with a real-aperture array by applying the proposed MIMO range migration techniques.",
    "keywords": [
      "multiple-input multiple-output ",
      "near-field imaging",
      "range migration",
      "sparse array"
    ]
  },
  {
    "id": "222",
    "title": "Imprint lithography for flexible transparent plastic substrates",
    "abstract": "A novel imprinting process has been developed for the use of resist pattern transfer on flexible transparent plastic substrates. The polymer resist was first spin-coated on the mold, which was treated with a release agent. After softbaking, the resist layer was attached to a plastic substrate coated with an adhesive. The patterns were completely transferred to the substrate after removing the mold. Using this method, we were able to obtain the desired patterns on the plastic substrate without heating the substrate, which could deform the substrate.",
    "keywords": [
      "imprint lithography",
      "flexible plastic substrate",
      "pmma"
    ]
  },
  {
    "id": "223",
    "title": "Efficient Routing of Subspace Skyline Queries over Highly Distributed Data",
    "abstract": "Data generation increases at highly dynamic rates, making its storage, processing, and update costs at one central location excessive. The P2P paradigm emerges as a powerful model for organizing and searching large data repositories distributed over independent sources. Advanced query operators, such as skyline queries, are necessary in order to help users handle the huge amount of available data. A skyline query retrieves the set of nondominated data points in a multidimensional data set. Skyline query processing in P2P networks poses inherent challenges and demands nontraditional techniques, due to the distribution of content and the lack of global knowledge. Relying on a superpeer architecture, we propose a threshold-based algorithm, called SKYPEER and its variants, for efficient computation of skyline points in arbitrary subspaces, while reducing both computational time and volume of transmitted data. Furthermore, we address the problem of routing skyline queries over the superpeer network and we propose an efficient routing mechanism, namely SKYPEER(+), which further improves the performance by reducing the number of contacted superpeers. Finally, we provide an extensive experimental evaluation showing that our approach performs efficiently and provides a viable solution when a large degree of distribution is required.",
    "keywords": [
      "skyline queries",
      "peer-to-peer systems",
      "routing indexes"
    ]
  },
  {
    "id": "224",
    "title": "Parallel processing in regional climatology: The parallel version of the \"Karlsruhe Atmospheric Mesoscale Model\" (KAMM)",
    "abstract": "Simultaneously to improvements of computer performance and of availability of memory not only the resolution of meteorological models of atmospheric currents has been refined but also the accuracy of the necessary physical approximations has been improved more and more. Now full elastic models are developed which describe also sound waves, although sound processes are not supposed to be relevant for atmospheric flow phenomena. But the full set of the elastic Navier-Stokes equations has a quite simple structure in comparison to sound proved systems like \"anelastically\" approximated models, so that the corresponding numerical models can be implemented on parallel computer systems without too much efforts. This has been considered by the redesign of the \"Karlsruhe Atmospheric Mesoscale Model\" (KAMM) for parallel processing. The new full elastic version of this model is written in FORTRAN-90. The necessary communication operations are gathered into few functions of a communication library, which is designed for different computer architectures, for massive parallel systems, for parallel vector computers requiring long vectors, but also for mono processors.  ",
    "keywords": [
      "navier-stokes equation",
      "elastic model",
      "regional climatology",
      "communication library",
      "massive parallel systems",
      "vector computers",
      "benchmark"
    ]
  },
  {
    "id": "225",
    "title": "X-ray scattering processes and chemometrics for differentiating complex samples using conventional EDXRF equipment",
    "abstract": "Mild variations in organic matrices, which are investigated in this work, are caused by alterations in X-ray Raman scattering. The multivariate approaches, principal component analysis (PCA) and hierarchical cluster analysis (HCA), are applied to visualize these effects. Conventional energy-dispersive X-ray fluorescence equipment is used, where organic compounds produce intense scattering of the X-ray source. X-ray Raman processes, before obtained only for solid samples using synchrotron radiation, are indirectly visualized here through PCA scores and HCA cluster analysis, since they alter the Compton and Rayleigh scattering. As a result, their influences can be seen in known sample characteristics, as those associated with gender and melanin in dog hairs, and the differentiation in coconut varieties. Chemometrics has shown that, despite their complexity, natural samples can be easily classified.  ",
    "keywords": [
      "principal component analysis ",
      "hierarchical cluster analysis ",
      "natural sample differentiation",
      "x-ray raman scatter spectrometry ",
      "complex organic mixtures"
    ]
  },
  {
    "id": "226",
    "title": "A Deterministic Time Algorithm for the Reeb Graph",
    "abstract": "We present a deterministic algorithm to compute the Reeb graph of a PL real-valued function on a simplicial complex in time, where is the size of the 2-skeleton. The problem can be solved using dynamic graph connectivity. We obtain the running time by using offline graph connectivity which assumes that the deletion time of every arc inserted is known at the time of insertion. The algorithm is implemented and experimental results are given. In addition, we reduce the offline graph connectivity problem to computing the Reeb graph.",
    "keywords": [
      "algorithms",
      "reeb graph",
      "pl topology",
      "graph connectivity"
    ]
  },
  {
    "id": "227",
    "title": "advances in pcb routing",
    "abstract": "Due to rapid increases in printed circuit board (PCB) complexity and lack of research progresses in PCB routing algorithms over the years, routing has become a bottleneck in overall circuit board design time. Today, a high-end PCB typically takes significant tedious manual efforts to complete the wiring and this problem will only get worse for future generations of PCBs. In this talk, we present some of our recent research results on this problem.",
    "keywords": [
      "routing",
      "pcb",
      "algorithms"
    ]
  },
  {
    "id": "228",
    "title": "A Test of Two Models of Value Creation in Virtual Communities",
    "abstract": "Does a firm get any extra value from investing resources in sponsoring its own virtual community above and beyond the value that could be created for the firm, indirectly, via customer-initiated communities? If so, what explains the extra value derived from a firm-sponsored virtual community and how might this understanding inform managers about appropriate strategies for leveraging virtual communities as part of a value-creating strategy for the firm? We test two models of virtual community to help shed light on the answers to these questions. We hypothesize that in customer-initiated virtual communities, three attributes of member-generated information (MGI) drive value, while in firm-sponsored virtual communities, a sponsoring firm's efforts, as well as MGI, drive value. Drawing on information search and processing theories, and developing new measures of three attributes of MGI (consensus, consistency, and distinctiveness), we surveyed 465 consumers across numerous communities. We find that value can emerge via both models, but that in a firm-sponsored model, a sponsor's efforts are more powerful than MGI and have a positive, direct effect on the trust-building process. Our results suggest a continuum of value creation whereby firms extract greater value as they migrate toward the firm-sponsored model.",
    "keywords": [
      "attribution theory",
      "co-creation",
      "online communities",
      "online trust",
      "user-generated content",
      "virtual communities"
    ]
  },
  {
    "id": "229",
    "title": "Performance optimization and modeling of blocked sparse kernels",
    "abstract": "We present a method for automatically selecting optimal implementations of sparse matrix-vector operations. Our software \"AcCELS\" (Accelerated Compress-storage Elements for Linear Solvers) involves a setup phase that probes machine characteristics, and a run-time phase where stored characteristics are combined with a measure of the actual sparse matrix to find the optimal kernel implementation. We present a performance model that is shown to be accurate over a large range of matrices.",
    "keywords": [
      "optimization",
      "sparse",
      "matrix-vector product",
      "blocking",
      "self-adaptivity"
    ]
  },
  {
    "id": "230",
    "title": "Parameter estimation of two-level nonlinear mixed effects models using first order conditional linearization and the EM algorithm",
    "abstract": "Multi-level nonlinear mixed effects (ML-NLME) models have received a great deal of attention in recent years because of the flexibility they offer in handling the repeated-measures data arising from various disciplines. In this study, we propose both maximum likelihood and restricted maximum likelihood estimations of ML-NLME models with two-level random effects, using first order conditional expansion (FOCE) and the expectationmaximization (EM) algorithm. The FOCEEM algorithm was compared with the most popular Lindstrom and Bates (LB) method in terms of computational and statistical properties. Basal area growth series data measured from Chinese fir (Cunninghamia lanceolata) experimental stands and simulated data were used for evaluation. The FOCEEM and LB algorithms given the same parameter estimates and fit statistics for models that converged by both. However, FOCEEM converged for all the models, while LB did not, especially for the models in which two-level random effects are simultaneously considered in several base parameters to account for between-group variation. We recommend the use of FOCEEM in ML-NLME models, particularly when convergence is a concern in model selection.",
    "keywords": [
      "cunninghamia lanceolata",
      "expectationmaximization algorithm",
      "first order conditional expansion",
      "lindstrom and bates algorithm",
      "simulated data",
      "two-level nonlinear mixed effects models"
    ]
  },
  {
    "id": "231",
    "title": "Multi-scale modelling of sandwich structures using the Arlequin method Part I: Linear modelling",
    "abstract": "The paper presents an Arlequin based multi-scale method for studying problems related to the mechanical behaviour of sandwich composite structures. Towards this end, different models are mixed and glued to each other. Several coupling operators are tested in order to assess the usefulness of the proposed approach. A new coupling operator is proposed and tested on the different glued Arlequin zones. A freeclamped sandwich beam with soft core undergoing a concentrated effort on the free edge is used as a typical example (benchmark) in the validation procedure. Numerical simulations were conducted as the preliminary evaluation of the various coupling operators and the discrepancies between local and global models in the gluing zone have been addressed with sufficient care.",
    "keywords": [
      "arlequin",
      "multi-scale",
      "sandwich",
      "local effects",
      "finite element"
    ]
  },
  {
    "id": "232",
    "title": "DETERMINATES OF EIS ACCEPTANCE",
    "abstract": "The large number of organizations developing executive information systems (EISs) highlights the importance of understanding why executives use these systems. This survey investigated how ease of use, the number of features, and support staff characteristics are related to EIS acceptance. Acceptance was measured by the percentage of the targeted users who incorporate the EIS into their daily routine. High usage was not associated with ease of use, a large number of features, or the staff being physically close to the users. However, rapid development time was positively correlated with acceptance. Higher numbers of available features were associated with larger support staffs and larger user groups. The number of users was positively correlated with both staff size and EIS age. Existing EISs place a stronger emphasis on reporting internal rather than external data.",
    "keywords": [
      "executive information systems",
      "information systems features",
      "information systems support"
    ]
  },
  {
    "id": "233",
    "title": "SIFT-Based Non-blind Watermarking Robust to Non-linear Geometrical Distortions",
    "abstract": "This paper presents a non-blind watermarking technique that is robust to non-linear geometric distortion attacks. This is one of the most challenging problems for copyright protection of digital content because it is difficult to estimate the distortion parameters for the embedded blocks. In our proposed scheme, the location of the blocks are recorded by the translation parameters from multiple Scale Invariant Feature Transform (SIFT) feature points. This method is based on two assumptions: SIFT features are robust to non-linear geometric distortion and even such non-linear distortion can be regarded as \"linear\" distortion in local regions. We conducted experiments using 149,800 images (7 standard images and 100 images downloaded from Flickr, 10 different messages, 10 different embedding block patterns, and 14 attacks). The results show that the watermark detection performance is drastically improved, while the baseline method can achieve only chance level accuracy.",
    "keywords": [
      "watermarking",
      "scale invariant feature transform ",
      "non-linear geometric distortion attacks"
    ]
  },
  {
    "id": "234",
    "title": "zoom navigation exploring large information and application spaces",
    "abstract": "We present the concept of ZOOM NAVIGATION, a new interaction paradigm to cope with visualization and navigation problems as found in large information and application spaces. It is based on the pluggable zoom , an object-oriented component derived from the variable zoom fisheye algorithm.Working with a limited screen space we apply a Degree-of-interest (DOI) function to guide the level of detail used in presenting information. Furthermore we determine the user's information and navigation needs by analysing the interaction history. This leads to the definition of the aspect-of-interest (AOI) function. The AOI is evaluated in order to choose one of the several information aspects , under which an item can be studied. This allows us to change navigational affordance and thereby enhance navigation.In this paper we describe the ideas behind the pluggable zoom and the definition of DOI and AOI functions. The application of these functions is demonstrated within two case studies, the ZOOM ILLUSTRATOR and the ZOOM NAVIGATOR. We discuss our experience with these implemented systems.",
    "keywords": [
      "zooming interfaces",
      "zoom navigation",
      "screen layout",
      "information navigation",
      "fisheye display",
      "human-computer interfaces",
      "detail + context technique"
    ]
  },
  {
    "id": "235",
    "title": "FITS: A Finite-Time Reputation System for Cooperation in Wireless Ad Hoc Networks",
    "abstract": "A wireless ad hoc network does not have an infrastructure, and thus, needs the cooperation of nodes in forwarding other nodes' packets. Reputation system is an effective approach to give nodes incentives to cooperate in packet forwarding. However, existing reputation systems either lack rigorous analysis, or have analysis in unrealistic models. In this paper, we propose FITS, the first reputation system that has rigorous analysis and guaranteed incentive compatibility in a practical model. FITS has two schemes: the first scheme is very simple, but needs a Perceived Probability Assumption (PPA); the second scheme uses more sophisticated techniques to remove the need for PPA. We show that both of these two FITS schemes have a subgame perfect Nash equilibrium in which the packet forwarding probability of every node is one. Experimental results verify that FITS provides strong incentives for nodes to cooperate.",
    "keywords": [
      "keywords ad hoc networks",
      "incentive compatibility",
      "routing",
      "packet forwarding"
    ]
  },
  {
    "id": "236",
    "title": "Developing a verbal protocol method for collecting and analysing reports of workers thoughts during manual handling tasks",
    "abstract": "Concurrent and retrospective verbal protocol methods were used to collect thoughts from 18 participants during a manual handling task involving the repeated transfer of loads between locations at two tables. The effectiveness of qualitative and quantitative methods of analysing the reported information was tested in the study. A simple taxonomy was developed to investigate the content of the reports (including reports on postures and loads) and determine how the participants approached the task (whether they made plans, described actions or evaluated their completion of the task). References to posture were obtained in the verbal protocol reports, indicating that the participants had some awareness of their postures during parts of the task. There were similarities in the content of the concurrent and retrospective reports, but there were differences in the amount of detail between the methods and differences in the way the reports were constructed. There could be some scope for developing the quantitative analysis of the frequencies of references to classes of information, though this can only be recommended for concurrent reports on tasks of short duration. The analyses of qualitative data gave a deeper insight into the reports, such as identifying factors that can be important when planning to handle a load, or illustrating how participants can change their focus of attention periodically throughout the task. The relative strengths of the concurrent and retrospective methods are described, along with ideas for improving the quality of information collected in future studies. A number of potential problems with the interpretation of the reported information are explained.",
    "keywords": [
      "self reports",
      "verbal protocol methods",
      "manual handling tasks"
    ]
  },
  {
    "id": "237",
    "title": "Fast and flexible instruction selection with on-demand tree-parsing automata",
    "abstract": "Tree parsing as supported by code generator generators like BEG, burg, iburg, lburg and ml-burg is a popular instruction selection method. There are two existing approaches for implementing tree parsing: dynamic programming, and tree-parsing automata; each approach has its advantages and disadvantages. We propose a new implementation approach that combines the advantages of both existing approaches: we start out with dynamic programming at compile time, but at every step we generate a state for a tree-parsing automaton, which is used the next time a tree matching the state is found, turning the instruction selector into a fast tree-parsing automaton. We have implemented this approach in the Gforth code generator. The implementation required little effort and reduced the startup time of Gforth by up to a factor of 2.5.",
    "keywords": [
      "algorithms",
      "performance",
      "instruction selection",
      "tree parsing",
      "dynamic programming",
      "automaton",
      "lazy"
    ]
  },
  {
    "id": "238",
    "title": "challenges on preserving scientific data with data grids",
    "abstract": "The emerging context of e-Science imposes new scenarios and requirements for digital preservation. In particular, the data must be reliably stored, for which redundancy is a key strategy. But managing redundancy must take into account the potential failure of component. Considering that correlated failures can affect multiple components and potentially cause a complete loss of data, we propose an innovative solution to manage redundancy strategies in heterogeneous environments such as data grids. This solution comprises a simulator that can be used to evaluate redundancy strategies according to preservation requirements and supports the process to design the best architecture to be deployed, which can latter be used as an observer of the deployed system, supporting its monitoring and management.",
    "keywords": [
      "e-science",
      "simulation",
      "digital libraries",
      "data grid",
      "digital preservation"
    ]
  },
  {
    "id": "239",
    "title": "automated test order generation for software component integration testing",
    "abstract": "The order in which software components are tested can have a significant impact on the number of stubs required during component integration testing. This paper presents an efficient approach that applies heuristics based on a given software component test dependency graph to automatically generate a test order that requires a (near) minimal number of test stubs. Thus, the approach reduces testing effort and cost. The paper describes the proposed approach, analyses its complexity and illustrates its use. Comparison with three well known graph-based approaches, for a real-world software application, shows that only the classic Le Traon et al.s approach and ours give an optimal number of stubs. However, experiments on randomly simulated dependency models with 100 to 10,000 components show that our approach has a significant performance advantage with a reduction in the average running time of 96.01%.",
    "keywords": [
      "heuristic algorithms",
      "software testing",
      "component integration",
      "directed feedback vertex-set problem"
    ]
  },
  {
    "id": "240",
    "title": "Inhomogeneous and self-organized temperature in Schelling-Ising model",
    "abstract": "The Schelling model of 1971 is a complicated version of a square-lattice Ising model at zero temperature, to explain urban segregation, based on the neighbor preferences of the residents, without external reasons. Various versions between Ising and Schelling models give about the same results. Inhomogeneous \"temperatures\" T do not change the results much, while a feedback between segregation and T leads to a self-organization of an average T.",
    "keywords": [
      "urban segregation",
      "feedback",
      "phase transition",
      "randomness"
    ]
  },
  {
    "id": "241",
    "title": "An exploratory study of architectural effects on requirements decisions",
    "abstract": "The question of the \"manner in which an existing software architecture affects requirements decision-making\" is considered important in the research community; however, to our knowledge, this issue has not been scientifically explored. We do not know, for example, the characteristics of such architectural effects. This paper describes an exploratory study on this question. Specific types of architectural effects on requirements decisions are identified, as are different aspects of the architecture together with the extent of their effects. This paper gives quantitative measures and qualitative interpretation of the findings. The understanding gained from this study has several implications in the areas of: project planning and risk management, requirements engineering (RE) and software architecture (SA) technology, architecture evolution, tighter integration of RE and SA processes, and middleware in architectures. Furthermore, we describe several new hypotheses that have emerged from this study, that provide grounds for future empirical work. This study involved six RE teams (of university students), whose task was to elicit new requirements for upgrading a pre-existing banking software infrastructure. The data collected was based on a new meta-model for requirements decisions, which is a bi-product of this study.  ",
    "keywords": [
      "software architecture",
      "requirements engineering",
      "empirical study",
      "software quality",
      "process improvement",
      "quantitative and qualitative research",
      "architecture and requirements technology"
    ]
  },
  {
    "id": "242",
    "title": "A direction of arrival estimation method for spatial optical beam-forming network",
    "abstract": "Spatial optical beam-forming network (OBFN) is a superior structure than traditional ones in bandwidth adaptability, system complexity, and so forth. Compared with conventional beam-forming network, the output signal model of OBFN is different, making the previous direction of arrival (DOA) estimation methods improper for this structure. At present, DOA estimation method for this structure has not been sufficiently explored and there is no efficient algorithm. In this paper, the observation model of the network is established first, and then a new DOA estimation method is proposed. The new method makes use of the amplitude distribution of the fiber array to achieve direction finding. Sufficient numerical simulations are carried out to demonstrate the feasibility and efficiency of the proposed algorithm.",
    "keywords": [
      "spatial optical beam-forming network",
      "doa estimation",
      "fiber array",
      "fourier transform"
    ]
  },
  {
    "id": "243",
    "title": "On central algorithms of approximation under fuzzy information",
    "abstract": "We consider the problem of approximation of an operator by information described by n real characteristics in the case when this information is fuzzy. We develop the well-known idea of an optimal error method of approximation for this case. It is a method whose error is the infimum of the errors of all methods for a given problem characterized by fuzzy numbers in this case. We generalize the concept of central algorithms, which are always optimal error algorithms and in the crisp case are useful both in practice and in theory. In order to do this we define the centre of an L-fuzzy subset of a normed space. The introduced concepts allow us to describe optimal methods of approximation for linear problems using balanced fuzzy information.  ",
    "keywords": [
      "l-fuzzy number",
      "fuzzy information",
      "central algorithm of approximation"
    ]
  },
  {
    "id": "244",
    "title": "Prediction of currents and sea surface elevation in the Gulf of California from tidal to seasonal scales",
    "abstract": "A web tool that provides currents and/or sea surface elevation in the Gulf of California is presented. The above variables are reconstructed from harmonic constants obtained from harmonic analyses of time series produced by a 3D baroclinic numerical model of the Gulf. The numerical model was forced (1) at the Gulf's mouth by the tides and the hydrographic variability of the Pacific Ocean (at semiannual and annual frequencies), and (2) at the Gulf's surface by winds, heat and fresh water fluxes (also at the semiannual and annual frequencies). The response to these forcings results in motions with time scales limited to semidiurnal and diurnal, fortnightly and monthly (due to nonlinear interactions of the tidal components), and semiannual and annual frequencies (due to the nontidal forcing).",
    "keywords": [
      "prediction",
      "sea level",
      "currents",
      "gulf of california",
      "tides",
      "hamsom"
    ]
  },
  {
    "id": "245",
    "title": "Surpassing the fractional derivative: Concept of the memory-dependent derivative",
    "abstract": "Enlightened by the Caputo type of fractional derivative, here we bring forth a concept of memory-dependent derivative, which is simply defined in an integral form of a common derivative with a kernel function on a slipping interval. In case the time delay tends to zero it tends to the common derivative. High order derivatives also accord with the first order one. Comparatively, the form of kernel function for the fractional type is fixed, yet that of the memory-dependent type can be chosen freely according to the necessity of applications. So this kind of definition is better than the fractional one for reflecting the memory effect (instantaneous change rate depends on the past state). Its definition is more intuitionistic for understanding the physical meaning and the corresponding memory-dependent differential equation has more expressive force.",
    "keywords": [
      "memory-dependent derivative ",
      "memory-dependent differential equation ",
      "fractional differential equation ",
      "caputo derivative",
      "time delay"
    ]
  },
  {
    "id": "246",
    "title": "Deriving robust counterparts of nonlinear uncertain inequalities",
    "abstract": "In this paper we provide a systematic way to construct the robust counterpart of a nonlinear uncertain inequality that is concave in the uncertain parameters. We use convex analysis (support functions, conjugate functions, Fenchel duality) and conic duality in order to convert the robust counterpart into an explicit and computationally tractable set of constraints. It turns out that to do so one has to calculate the support function of the uncertainty set and the concave conjugate of the nonlinear constraint function. Conveniently, these two computations are completely independent. This approach has several advantages. First, it provides an easy structured way to construct the robust counterpart both for linear and nonlinear inequalities. Second, it shows that for new classes of uncertainty regions and for new classes of nonlinear optimization problems tractable counterparts can be derived. We also study some cases where the inequality is nonconcave in the uncertain parameters.",
    "keywords": [
      "fenchel duality",
      "robust counterpart",
      "nonlinear inequality",
      "robust optimization",
      "support functions",
      ""
    ]
  },
  {
    "id": "247",
    "title": "On the weight distribution of terminated convolutional codes",
    "abstract": "In this correspondence, the low-weight terms of the weight distribution of the block code obtained by terminating a convolutional code after x information blocks are expressed as a function of x. It is shown that this function is linear in x for codes with noncatastrophic encoders, but quadratic in x for codes with catastrophic encoders, These results are useful to explain the poor performance of convolutional codes with a catastrophic encoder at low-to-medium signal-to-noise ratios.",
    "keywords": [
      "block codes",
      "convolutional codes",
      "soft-decision decoding",
      "viterbi decoding",
      "weight distribution"
    ]
  },
  {
    "id": "248",
    "title": "Common data model for natural language processing based on two existing standard information models: CDA+GrAF",
    "abstract": "An increasing need for collaboration and resources sharing in the Natural Language Processing (NLP) research and development community motivates efforts to create and share a common data model and a common terminology for all information annotated and extracted from clinical text. We have combined two existing standards: the HL7 Clinical Document Architecture (CDA), and the ISO Graph Annotation Format (GrAF; in development), to develop such a data model entitled CDA+GrAF. We experimented with several methods to combine these existing standards, and eventually selected a method wrapping separate CDA and GrAF parts in a common standoff annotation (i.e., separate from the annotated text) XML document. Two use cases, clinical document sections, and the 2010 i2b2/VA NLP Challenge (i.e., problems, tests, and treatments, with their assertions and relations), were used to create examples of such standoff annotation documents, and were successfully validated with the XML schemata provided with both standards. We developed a tool to automatically translate annotation documents from the 2010 i2b2/VA NLP Challenge format to GrAF, and automatically generated 50 annotation documents using this tool, all successfully validated. Finally, we adapted the XSL stylesheet provided with HL7 CDA to allow viewing annotation XML documents in a web browser, and plan to adapt existing tools for translating annotation documents between CDA+GrAF and the UIMA and GATE frameworks. This common data model may ease directly comparing NLP tools and applications, combining their output, transforming and translating annotations between different NLP applications, and eventually plug-and-play of different modules in NLP applications.",
    "keywords": [
      "natural language processing ",
      "medical informatics ",
      "data model",
      "information model",
      "hl7 clinical document architecture",
      "iso graph annotation format"
    ]
  },
  {
    "id": "249",
    "title": "Labelings and encoders with the uniform bit error property with applications to serially concatenated trellis codes",
    "abstract": "The well-known uniform error property for signal constellations and codes is extended to encompass information bits. We introduce a class of binary labelings for signal constellations, called bit geometrically uniform (BGU) labelings, for which the uniform bit error property holds, i.e., the bit error probability does not depend on the transmitted signal. Strong connections between the symmetries of constellations and binary Hamming spaces are involved. For block-coded modulation (BCM) and trellis-coded modulation (TCM) Euclidean-space codes, BGU encoders are introduced and studied. The properties of BGU encoders prove quite useful for the analysis and design of codes aimed at minimizing the bit, rather than symbol, error probability. Applications to the analysis and the design of serially concatenated trellis codes are presented, together with a case study which realizes a spectral efficiency of 2 b/s/Hz.",
    "keywords": [
      "coding",
      "concatenated codes",
      "constellations",
      "encoding",
      "labeling",
      "trellis codes",
      "uniform error property"
    ]
  },
  {
    "id": "250",
    "title": "Stochastic analysis of packet-pair probing for network bandwidth estimation",
    "abstract": "In this paper, we perform a stochastic analysis of the packet-pair technique, which is a widely used method for estimating the network bandwidth in an end-to-end manner. There has been no explicit delay model of the packet-pair technique primarily because the stochastic behavior of a packet pair has not been fully understood. Our analysis is based on a novel insight that the transient analysis of the G/D/1 system can accurately describe the behavior of a packet pair, providing an explicit stochastic model. We first investigate a single-hop case and derive an analytical relationship between the input and the output probing gaps of a packet pair. Using this single-hop model, we provide a multi-hop model under an assumption of a single tight link. Our model shows the following two important features of the packet-pair technique: (i) The difference between the proposed model and the previous fluid model becomes significant when the input probing gap is around the characteristic value. (ii) The available bandwidth of any link after the tight link is not observable. We verify our model via ns-2 simulations and empirical results. We give a discussion on recent packet-pair models in relation to the proposed model and show that most of them can be regarded as special cases of the proposed model.",
    "keywords": [
      "packet-pair technique",
      "bandwidth estimation",
      "m/d/1 queue",
      "transient analysis"
    ]
  },
  {
    "id": "251",
    "title": "Development of a teleoperation system for agricultural vehicles",
    "abstract": "A teleoperation system for a hydro-static transmission (HST) drive crawler-type robotic vehicle is described in this paper. The system was developed to satisfy the needs of various farm operations and teleoperation in unknown agricultural fields. The controller has a layered architecture and supports two degrees of cooperation between the operator and robot, direct and supervisory control. The vehicle can travel autonomously by using an RTK-GPS and a fiber-optic gyroscope during supervisory control, and the operator interface also provides a field navigator based on Google Map technology. The vehicle's position and heading direction was capable of 1Hz update using precise satellite image maps. The results of field tests using direct control showed that it is difficult for the operator to control the movement of the vehicle along the target lines. On the other hand, the vehicle could travel in a straight line with a maximum lateral error of 0.3m by using supervisory control.",
    "keywords": [
      "agriculture",
      "computer vision",
      "autonomous mobile robot",
      "communication",
      "vehicle"
    ]
  },
  {
    "id": "252",
    "title": "A general audio classifier based on human perception motivated model",
    "abstract": "The audio channel conveys rich clues for content-based multimedia indexing. Interesting audio analysis includes, besides widely known speech recognition and speaker identification problems, speech/music segmentation, speaker gender detection, special effect recognition such as gun shots or car pursuit, and so on. All these problems can be considered as an audio classification problem which needs to generate a label from low audio signal analysis. While most audio analysis techniques in the literature are problem specific, we propose in this paper a general framework for audio classification. The proposed technique uses a perceptually motivated model of the human perception of audio classes in the sense that it makes a judicious use of certain psychophysical results and relies on a neural network for classification. In order to assess the effectiveness of the proposed approach, large experiments on several audio classification problems have been carried out, including speech/music discrimination in Radio/TV programs, gender recognition on a subset of the switchboard database, highlights detection in sports videos, and musical genre recognition. The classification accuracies of the proposed technique are comparable to those obtained by problem specific techniques while offering the basis of a general approach for audio classification.",
    "keywords": [
      "audio classification",
      "gender identification",
      "music genre recognition",
      "highlights detection",
      "perceptually motivated features",
      "content-based audio indexing",
      "piecewise gaussian modelling"
    ]
  },
  {
    "id": "253",
    "title": "Stability routing with constrained path length for improved routability in dynamic MANETs",
    "abstract": "Quality of service (QoS) routing is known to be an NP-hard problem in case of two or more additive constraints, and several exact algorithms and heuristics have been proposed to address this issue. In this paper, we consider a particular two-constrained quality of service routing problem maximizing path stability with a limited path length in the quest of improving routability in dynamic multi-hop mobile wireless ad hoc networks. First, we propose a novel exact algorithm to solve the optimal weight-constrained path problem. We instantiate our algorithm to solve the most stable path not exceeding a certain number of hops, in polynomial time. This algorithm is then applied to the practical case of proactive routing in dynamic multi-hop wireless ad hoc networks. In these networks, an adequate compromise between route stability and its length in hops is essential for appropriately mitigating the impact of the network dynamics on the validity of established routes. Secondly, we set up a common framework for the comparison between three families of proactive routing: the shortest path-based routing, the most stable path-based routing and our proposed most stable constrained path routing. We show then through extensive simulations that routing based on our proposed algorithm selects appropriate stable paths yielding a very high routability with an average path length just above that of the shortest paths.",
    "keywords": [
      "manets",
      "constrained-based routing",
      "quality of service routing",
      "stability constraint",
      "np-hard problems",
      "polynomial algorithms"
    ]
  },
  {
    "id": "254",
    "title": "SAR complex image data compression based on quadtree and zerotree Coding in Discrete Wavelet Transform Domain: A Comparative Study",
    "abstract": "SAR complex image data compression based on wavelet-quadtree is proposed. QC-DWT has achieved the-state-of-the-art performance. QC-DWT has achieved higher performance compared with wavelet-zerotree.",
    "keywords": [
      "quadtree",
      "sar complex image data compression",
      "zerotree"
    ]
  },
  {
    "id": "255",
    "title": "On the power of tree-walking automata",
    "abstract": "Tree-walking automata (TWAs) recently received new attention in the fields of formal languages and databases. To achieve a better understanding of their expressiveness, we characterize them in terms of transitive closure logic formulas in normal form. It is conjectured by Engelfriet and Hoogeboom that TWAs cannot define all regular tree languages, or equivalently, all of monadic second-order logic. We prove this conjecture for a restricted, but powerful, class of TWAs. In particular, we show that 1-bounded TWAs, that is TWAs that are only allowed to traverse every edge of the input tree at most once in every direction, cannot define all regular languages. We then extend this result to a class of TWAs that can simulate first-order logic (FO) and is capable of expressing properties not definable in FO extended with regular path expressions; the latter logic being a valid abstraction of current query languages for XML and semistructured data.",
    "keywords": [
      "tree-walking automata",
      "regular tree languages",
      "logic",
      "formal languages."
    ]
  },
  {
    "id": "256",
    "title": "authenticity by tagging and typing",
    "abstract": "We propose a type and effect system for  authentication  protocols built upon a tagging scheme that formalizes the intended semantics of ciphertexts. The main result is that the validation of each component in isolation is provably sound and  fully compositional : if all the protocol participants are independently validated, then the protocol as a whole guarantees authentication in the presence of Dolev-Yao intruders. The highly compositional nature of the analysis makes it suitable for multi-protocol systems, where different protocols might be executed concurrently.",
    "keywords": [
      "authentication",
      "static analysis",
      "process calculi"
    ]
  },
  {
    "id": "257",
    "title": "The conformational behavior, geometry and energy parameters of Menshutkin-like reaction of O-isopropylidene-protected glycofuranoid mesylates in view of DFT calculations",
    "abstract": "The reaction of three mesylates of furanoderivatives in pyridine is presented at the DFT. All the structures were fully optimized in the gas phase, in chloroform and water. The calculations revealed the barrier height increasing order as follows: 1>2>3. MPW1K/6-31+G** level activation barriers are higher than those from B3LYP/6-31+G**. The furanoid ring conformations are close to E0 or 0E.",
    "keywords": [
      "furanoid ring",
      "conformation",
      "dft calculations",
      "nbo",
      "quaternary ammonium salts"
    ]
  },
  {
    "id": "258",
    "title": "Translating update operations from relational to object-oriented databases",
    "abstract": "In migrating a legacy relational database system to the object-oriented (OO) platform, when database migration completes, application modules are to be migrated, where embedded relational database operations are mapped into their OO correspondents. In this paper we study mapping relational update operations to their OO equivalents, which include UPDATE1, INSERT and DELETE operations. Relational update operation translation from relational to OO faces the touchy problem of transformation from a value-based relationship model to a reference-based model and maintaining the relational integrity constraints. Moreover, with a relational database where inheritance is expressed as attribute value subset relationship, changing of some attribute values may lead to the change of the position of an object in the class inheritance hierarchy, which we call object migration. Considering all these aspects, algorithms are given mapping relational UPDATE, INSERT and DELETE operations to their OO correspondents. Our work emphasize in examining the differences in the representation of the source schema's semantics resulting from the translation process, as well as differences in the inherent semantics of the two models.",
    "keywords": [
      "relational model",
      "object-oriented model",
      "query translation",
      "update translation"
    ]
  },
  {
    "id": "259",
    "title": "Secure threshold multi authority attribute based encryption without a central authority",
    "abstract": "An attribute based encryption scheme (ABE) is a cryptographic primitive in which every user is identified by a set of attributes, and some function of these attributes is used to determine the ability to decrypt each ciphertext. Chase proposed the first multi authority ABE scheme which requires a fully trusted central authority who has the ability to decrypt each ciphertext in the system. This central authority would endanger the whole system if it is corrupted. This paper provides a threshold multi authority fuzzy identity based encryption (MA-FIBE) scheme without a central authority for the first time. An encrypter can encrypt a message such that a user could only decrypt if he has at least d(k) of the given attributes about the message for at least t + 1, t <= n/2 honest authorities of all the n attribute authorities in the proposed scheme. This paper considers a stronger adversary model in the sense that the corrupted authorities are allowed to distribute incorrect secret keys to the users. The security proof is based on the secrecy of the underlying distributed key generation protocol and joint zero secret sharing protocol and the standard decisional bilinear Diffie-Hellman assumption. The proposed MA-FIBE could be extended to the threshold multi authority attribute based encryption (MA-ABE) scheme, and both key policy based and ciphertext policy based MA-ABE schemes without a central authority are presented in this paper. Moreover, several other extensions, such as a proactive large universe MA-ABE scheme, are also provided in this paper.  ",
    "keywords": [
      "threshold multi authority abe",
      "without a central authority"
    ]
  },
  {
    "id": "260",
    "title": "Segmentation According to Natural Examples: Learning Static Segmentation from Motion Segmentation",
    "abstract": "The Segmentation According to Natural Examples (SANE) algorithm learns to segment objects in static images from video training data. SANE uses background subtraction to find the segmentation of moving objects in videos. This provides object segmentation information for each video frame. The collection of frames and segmentations forms a training set that SANE uses to learn the image and shape properties of the observed motion boundaries. When presented with new static images, the trained model infers segmentations similar to the observed motion segmentations. SANE is a general method for learning environment-specific segmentation models. Because it can automatically generate training data from video, it can adapt to a new environment and new objects with relative ease, an advantage over untrained segmentation methods or those that require human-labeled training data. By using the local shape information in the training data, it outperforms a trained local boundary detector. Its performance is competitive with a trained top-down segmentation algorithm that uses global shape. The shape information it learns from one class of objects can assist the segmentation of other classes.",
    "keywords": [
      "segmentation",
      "machine learning",
      "motion",
      "computer vision",
      "markov random field"
    ]
  },
  {
    "id": "261",
    "title": "New delay-dependent conditions on robust stability and stabilisation for discrete-time systems with time-delay",
    "abstract": "This article is concerned with new robust stability conditions and robust stabilisation method for a discrete-time system with time-delay and time-varying structured uncertainties that come into state and input matrices. An improved approach to obtain new robust stability conditions is proposed. Our approach employs a generalised Lyapunov functional combined with the parameterised model transformation method and the generalised free weighting matrix method. These generalisations lead to generalised robust stability conditions that are given in terms of linear matrix inequalities. Moreover, based on new robust stability conditions, a robust stabilisation method for uncertain discrete-time systems with time-delay is given. Numerical examples compare our robust stability conditions with some existing conditions to show the effectiveness of our approach and also illustrate the improvement of our robust stabilisation method.",
    "keywords": [
      "robust stability",
      "time-delay systems",
      "discrete-time systems",
      "uncertain systems",
      "delay-dependent conditions",
      "linear matrix inequality"
    ]
  },
  {
    "id": "262",
    "title": "Link testA statistical method for finding prostate cancer biomarkers",
    "abstract": "We present a new method, link-test, to select prostate cancer biomarkers from SELDI mass spectrometry and microarray data sets. Biomarkers selected by link-test are supported by data sets from both mRNA and protein levels, and therefore results in improved robustness. Link-test determines the level of significance of the association between a microarray marker and a specific mass spectrum marker by constructing background mass spectra distributions estimated by all human protein sequences in the SWISS-PROT database. The data set consist of both microarray and mass spectrometry data from prostate cancer patients and healthy controls. A list of statistically justified prostate cancer biomarkers is reported by link-test. Cross-validation results show high prediction accuracy using the identified biomarker panel. We also employ a text-mining approach with OMIM database to validate the cancer biomarkers. The study with link-test represents one of the first cross-platform studies of cancer biomarkers.",
    "keywords": [
      "microarray",
      "mass spectrometry",
      "biomarker",
      "prostate cancer",
      "text mining"
    ]
  },
  {
    "id": "263",
    "title": "a low leakage 9t sram cell for ultra-low power operation",
    "abstract": "This paper presents the design and evaluation of a new SRAM cell made of nine transistors (9T). The proposed 9T cell utilizes a scheme with separate read and write wordlines; it is shown that the 9T cell achieves improvements in power dissipation, performance and stability compared with previous designs (that require 10T and 8T) for low-power operation. The 9T scheme is amenable to small feature sizes as encountered in the deep sub-micron/nano ranges of CMOS technology.",
    "keywords": [
      "static noise margin",
      "sram cell",
      "nanotechnology",
      "leakage power",
      "low power"
    ]
  },
  {
    "id": "264",
    "title": "Bacteria Hunt Evaluating multi-paradigm BCI interaction",
    "abstract": "The multimodal, multi-paradigm brain-computer interfacing (BCI) game Bacteria Hunt was used to evaluate two aspects of BCI interaction in a gaming context. One goal was to examine the effect of feedback on the ability of the user to manipulate his mental state of relaxation. This was done by having one condition in which the subject played the game with real feedback, and another with sham feedback. The feedback did not seem to affect the game experience (such as sense of control and tension) or the objective indicators of relaxation, alpha activity and heart rate. The results are discussed with regard to clinical neurofeedback studies. The second goal was to look into possible interactions between the two BCI paradigms used in the game: steady-state visually-evoked potentials (SSVEP) as an indicator of concentration, and alpha activity as a measure of relaxation. SSVEP stimulation activates the cortex and can thus block the alpha rhythm. Despite this effect, subjects were able to keep their alpha power up, in compliance with the instructed relaxation task. In addition to the main goals, a new SSVEP detection algorithm was developed and evaluated.",
    "keywords": [
      "brain-computer interfacing",
      "multimodal interaction",
      "steady-state visually-evoked potentials",
      "concentration",
      "neurofeedback relaxation",
      "game"
    ]
  },
  {
    "id": "265",
    "title": "Metamodels and emergent behaviour in models of conflict",
    "abstract": "In this paper, we develop a simplified mathematical model (a metamodel) of a simulation model of conflict, based on ideas drawn from the analysis of more general physical systems, such as found in fluid dynamics modelling. We show that there is evidence from the analysis of historical conflicts to support the kind of emergent behaviour implied by this approach. We then apply this approach to the development of a metamodel of a particular complexity based simulation model of conflict (ISAAC), developed for the US Marine Corps. The approach we have illustrated here is very generic, and is applicable to any simulation model which has complex interactions similar to those found in fluid dynamic modelling, or in simulating the emergent behaviour of large numbers of simple systems which interact with each other locally.",
    "keywords": [
      "conflict",
      "emergent behaviour",
      "metamodel",
      "simulation",
      "complexity"
    ]
  },
  {
    "id": "266",
    "title": "Topological Implications of Selfish Neighbor Selection in Unstructured Peer-to-Peer Networks",
    "abstract": "Current peer-to-peer (P2P) systems often suffer from a large fraction of freeriders not contributing any resources to the network. Various mechanisms have been designed to overcome this problem. However, the selfish behavior of peers has aspects which go beyond resource sharing. This paper studies the effects on the topology of a P2P network if peers selfishly select the peers to connect to. In our model, a peer exploits locality properties in order to minimize the latency (or response times) of its lookup operations. At the same time, the peer aims at not having to maintain links to too many other peers in the system. By giving tight bounds on the price of anarchy, we show that the resulting topologies can be much worse than if peers collaborated. Moreover, the network may never stabilize, even in the absence of churn. Finally, we establish the complexity of Nash equilibria in our game theoretic model of P2P networks. Specifically, we prove that it is NP-hard to decide whether our game has a Nash equilibrium and can stabilize.",
    "keywords": [
      "game theory",
      "peer-to-peer",
      "price of anarchy",
      "np-hardness",
      "metric spaces"
    ]
  },
  {
    "id": "267",
    "title": "a readable tcp in the prolac protocol language",
    "abstract": "Prolac is a new statically-typed, object-oriented language for network protocol implementation. It is designed for readability, extensibility, and real-world implementation; most previous protocol languages, in contrast, have been based on hard-to-implement theoretical models and have focused on verification. We present a working Prolac TCP implementation directly derived from 4.4BSD. Our implementation is modular---protocol processing is logically divided into minimally-interacting pieces; readable---Prolac encourages top-down structure and naming intermediate computations; and extensible---subclassing cleanly separates protocol extensions like delayed acknowledgements and slow start. The Prolac compiler uses simple global analysis to remove expensive language features like dynamic dispatch, resulting in end-to-end performance comparable to an unmodified Linux 2.0 TCP.",
    "keywords": [
      "network protocol",
      "structure",
      "analysis",
      "dynamic",
      "language",
      "performance",
      "implementation",
      "verification",
      "readability",
      "process",
      "modular",
      "compilation",
      "extensibility",
      "model",
      "feature",
      "global",
      "object oriented language"
    ]
  },
  {
    "id": "268",
    "title": "A WEARABLE DOCUMENT READER FOR THE VISUALLY IMPAIRED: DEWARPING AND SEGMENTATION",
    "abstract": "While reading devices for the visually impaired have been available for many years, they are often expensive and difficult to use. The image processing required to enable the reading task is a composition of several important sub-tasks, such as image capture, image stabilization, image enhancement and page-curl dewarping region segmentation, regions grouping, and word recognition In this paper we deal with some of these sub-tasks in an effort to prototype a device (Tyflos-reader) that will read a document for a person with a visual impairment and respond to voice commands for control. Initial experimental results on a set of textbook and newspaper pages are also presented.",
    "keywords": [
      "assistive devices",
      "image super-resolution",
      "perspective rectification",
      "page-curl dewarping",
      "document segmentation",
      "voice user-interface"
    ]
  },
  {
    "id": "269",
    "title": "Top-K structural diversity search in large networks",
    "abstract": "Social contagion depicts a process of information (e.g., fads, opinions, news) diffusion in the online social networks. A recent study reports that in a social contagion process, the probability of contagion is tightly controlled by the number of connected components in an individuals neighborhood. Such a number is termed structural diversity of an individual, and it is shown to be a key predictor in the social contagion process. Based on this, a fundamental issue in a social network is to find top-(k) users with the highest structural diversities. In this paper, we, for the first time, study the top-(k) structural diversity search problem in a large network. Specifically, we study two types of structural diversity measures, namely, component-based structural diversity measure and core-based structural diversity measure. For component-based structural diversity, we develop an effective upper bound of structural diversity for pruning the search space. The upper bound can be incrementally refined in the search process. Based on such upper bound, we propose an efficient framework for top-(k) structural diversity search. To further speed up the structural diversity evaluation in the search process, several carefully devised search strategies are proposed. We also design efficient techniques to handle frequent updates in dynamic networks and maintain the top-(k) results. We further show how the techniques proposed in component-based structural diversity measure can be extended to handle the core-based structural diversity measure. Extensive experimental studies are conducted in real-world large networks and synthetic graphs, and the results demonstrate the efficiency and effectiveness of the proposed methods.",
    "keywords": [
      "structural diversity",
      "disjoint-set forest",
      " search",
      "dynamic graph"
    ]
  },
  {
    "id": "270",
    "title": "A unified approach for detecting and eliminating selfish nodes in MANETs using TBUT",
    "abstract": "Recent years have witnessed the increasing efforts toward making architecture standardization for the secured wireless mobile ad hoc networks. In this scenario when a node actively utilizes the other node resources for communicating and refuses to help other nodes in their transmission or reception of data, it is called a selfish node. As the entire mobile ad hoc network (MANETs) depends on cooperation from neighboring nodes, it is very important to detect and eliminate selfish nodes from being part of the network. In this paper, token-based umpiring technique (TBUT) is proposed, where every node needs a token to participate in the network and the neighboring nodes act as umpire. This proposed TBUT is found to be very efficient with a reduced detection time and less overhead. The security analysis and experimental results have shown that TBUT is feasible for enhancing the security and network performance of real applications.",
    "keywords": [
      "manet",
      "selfish node",
      "performance and token-based umpiring technique "
    ]
  },
  {
    "id": "271",
    "title": "Multiversion join index for multiversion data warehouse",
    "abstract": "The data warehouse (DW) technology is developed in order to support the integration of external data sources (EDSs) for the purpose of advanced data analysis by On-Line Analytical Processing (OLAP) applications. Since contents and structures of integrated EDSs may evolve in time, the content and schema of a DW must evolve too in order to correctly reflect the evolution of EDSs. In order to manage a DW evolution, we developed the multiversion data warehouse (MVDW) approach. In this approach, different states of a DW are represented by the sequence of persistent DW versions that correspond either to the real world state or to a simulation scenario. Typically, OLAP applications execute star queries that join multiple fact and dimension tables. An important optimization technique for this kind of queries is based on join indexes. Since in the MVDW fact and dimension data are physically distributed among multiple DW versions, standard join indexes need extensions. In this paper we present the concept of a multiversion join index (MVJI) applicable to indexing dimension and fact tables in the MVDW. The MVJI has a two-level structure, where an upper level is used for indexing attributes and a lower level is used for indexing DW versions. The paper also presents the theoretical upper bound (pessimistic) analysis of the MVJI performance characteristic with respect to I/O operations. The analysis is followed by experimental evaluation. It shows that the MVJI increases a system performance for queries addressing multiple DW versions with exact match and range predicates.  ",
    "keywords": [
      "star query",
      "join index",
      "multiversion data warehouse",
      "multiversion query",
      "multiversion join index"
    ]
  },
  {
    "id": "272",
    "title": "Reactive column profile map topology: Continuous distillation column with non-reversible kinetics",
    "abstract": "In this paper we present a topologically based approach to the analysis and synthesis of reactive distillation columns. We extend the definition of Tapp et al. [Tapp, M., Holland, S., Glasser, D., & Hildebrandt, D. (2004). Column profile maps part A: Derivation and interpretation. Industrial and Engineering Chemistry Research, 43, 364-374] of a column section in non-reactive distillation column to a reactive column section (RCS) in a reactive distillation column. A RCS is defined as a section of a reactive distillation column in which there is no addition or removal of material or energy. We introduce the concept of a reactive column profile map (RCPM) in which the profiles in the RCPM correspond to the liquid composition profiles in the RCS. By looking at the singular points in the RCPM, it is demonstrated that for a single chemical reaction with no net change in the total number of moles, the bifurcation of the singular points depends on both the difference point as introduced by Hauan et al. [Hauan, S., Ciric, A. R., Westerberg, A. W., & Lien, K. M. (2000). Difference points in extractive and reactive cascades I-Basic properties and analysis. Chemical Engineering Science, 55, 3145-3159] as well as the direction of the stoichiometric vector. These two vectors combine to define what we call the reactive difference point composition. We show that there only certain feasible topologies of the RCPM and these depend only on the position of the reactive difference point composition. We look at a simple example where the vapour liquid equilibrium (VLE) is ideal and show that we can classify regions of reactive difference point compositions that result in similar topology of the RCPM. Thus, by understanding the feasible topologies of the RCPM, one is able to identify profiles in the RCPM that are desirable and hence one is able to synthesize a reactive distillation column by combining RCS that correspond to the desired profile in the RCPM. We believe that this tool will help understand how and when reaction could introduce unexpected behaviors and this can be used as a complementary tool to existing methods used for synthesis of reactive distillation columns.  ",
    "keywords": [
      "reactive column profile map",
      "difference point",
      "reactive column section"
    ]
  },
  {
    "id": "273",
    "title": "Fault-tolerant hamiltonian laceability of hypercubes",
    "abstract": "It is known that every hypercube Q(n) is a bipartite graph. Assume that n greater than or equal to 2 and F is a subset of edges with F less than or equal to n - 2. We prove that there exists a hamiltonian path in Q(n) - F between any two vertices of different partite sets. Moreover, there exists a path of length 2(n) - 2 between any two vertices of the same partite set. Assume that n greater than or equal to 3 and F is a subset of edges with F less than or equal to n - 3. We prove that there exists a hamiltonian path in Q(n) - {v} - F between any two vertices in the partite set without v. Furthermore, all bounds are tight.  ",
    "keywords": [
      "hamiltonian laceable",
      "hypercube",
      "fault tolerance"
    ]
  },
  {
    "id": "274",
    "title": "incorporating user control into recommender systems based on naive bayesian classification",
    "abstract": "Recommender systems are increasingly being employed to personalize services, such as on the web, but also in electronics devices, such as personal video recorders. These recommenders learn a user profile, based on rating feedback from the user on, e.g., books, songs, or TV programs, and use machine learning techniques to infer the ratings of new items. The techniques commonly used are collaborative filtering and naive Bayesian classification, and they are known to have several problems, in particular the cold-start problem and its slow adaptivity to changing user preferences. These problems can be mitigated by allowing the user to set up or manipulate his profile. In this paper, we propose an extension to the naive Bayesian classifier that enhances user control. We do this by maintaining and flexibly integrating two profiles for a user, one learned by rating feedback, and one created by the user. We in particular show how the cold-start problem is mitigated.",
    "keywords": [
      "multi-valued features",
      "naive bayes",
      "user profile",
      "machine learning",
      "classification",
      "recommender",
      "user control"
    ]
  },
  {
    "id": "275",
    "title": "Population variation in genetic programming",
    "abstract": "A new population variation approach is proposed, whereby the size of the population is systematically varied during the execution of the genetic programming process with the aim of reducing the computational effort compared with standard genetic programming (SGP). Various schemes for altering population size under this proposal are investigated using a comprehensive range of standard problems to determine whether the nature of the population variation, i.e. the way the population is varied during the search, has any significant impact on GP performance. The initial population size is varied in relation to the initial population size of the SGP such that the worst case computational effort is never greater than that of the SGP. It is subsequently shown that the proposed population variation schemes do have the capacity to provide solutions at a lower computational cost compared with the SGP.",
    "keywords": [
      "genetic programming",
      "computational effort",
      "average number of evaluations",
      "convergence",
      "population variation"
    ]
  },
  {
    "id": "276",
    "title": "The Need for Power Debugging in the Multi-Core Environment",
    "abstract": "Debugging an application for power has a wide array of benefits ranging from minimizing the thermal hotspots to reducing the likelihood of CPU malfunction. In this work, we justify the need for power debugging, and show that performance debugging of a parallel application does not automatically guarantee power balance across multiple cores. We perform experiments and show our results using two case study benchmarks, Volrend from Splash-2 and Bodytrack from Parsec-1.0.",
    "keywords": [
      "multi-cores",
      "power debugging",
      "power imbalance"
    ]
  },
  {
    "id": "277",
    "title": "Human resource assignment system for distribution centers",
    "abstract": "Information technology and its wide range of applications have begun to make their presence in a new generation of logistic and distribution service industry. A more flexible breed of application packages Is emerging by the application of fourth generation language (4GL) technologies, which are able to provide foundations for true enterprise resource planning (ERP). There are many good reasons for adopting enterprise-wide resource planning systems. This research, however, focuses on the development of a human resource assignment module (HR module), usually considered as an essential part of an ERP system. This module provides crucial human resource data and supports decisions in human resource utilization in distribution center operations. We detail the crucial algorithm for the HR module, which provides efficient and effective manpower management for key logistic/distribution center operations.",
    "keywords": [
      "human resource management",
      "decision-support systems",
      "order picking"
    ]
  },
  {
    "id": "278",
    "title": "Quantized circulation in dilute Bose-Einstein condensates",
    "abstract": "We compute using a microscopic mean-field theory the structure and the quasiparticle excitation spectrum of a dilute. trapped Bose-Einstein condensate penetrated by an axisymmetric vortex line. The Gross-Pitaevskii equation for the condensate and the coupled Hartree-Fock-Bogoliubov-Popov equations describing the elementary excitations are solved self-consistently using finite-difference methods. We find locally stable vortex configurations at all temperatures below T-c.  ",
    "keywords": [
      "bose-einstein condensation",
      "vortices",
      "finite-difference methods"
    ]
  },
  {
    "id": "279",
    "title": "the design and evaluation of a high-performance soft keyboard",
    "abstract": "The design and evaluation of a high performance soft keyboard for mobile systems are described. Using a model to predict the upper-bound text entry rate for soft keyboards, we designed a keyboard layout with a predicted upper-bound entry rate of 58.2 wpm. This is about 35% faster than the predicted rate for a QWERTY layout. We compared our design (OPTI) with a QWERTY layout in a longitudinal evaluation using five participants and 20 45-minute sessions of text entry. Average entry rates for OPT1 increased from 17.0 wpm initially to 44.3 wpm at session 20. The average rates exceeded those for the QWERTY layout after the 10 session (about 4 hours of practice). A regression equation (R = .997) in the form of the power-law of learning predicts that our upper-bound prediction would be reach at about session 50.",
    "keywords": [
      "pen input",
      "regression",
      "power law",
      "design",
      "high-performance",
      "layout",
      "linguistic models",
      "digraph probabilities",
      "learning",
      "text entry",
      "model",
      "soft keyboards",
      "practical",
      "fitts' law",
      "stylus input",
      "evaluation",
      "participant",
      "mobile systems",
      "predict"
    ]
  },
  {
    "id": "280",
    "title": "Stereoscopic video coding and disparity estimation for low bitrate applications based on MPEG-4 multiple auxiliary components",
    "abstract": "Using an MPEG-4 MAC (multiple auxiliary component) system is a good way to encode stereoscopic video with existing standard CODECs, especially when it comes to low bitrate applications. In this paper, we discuss the properties and problems of MAC systems when encoding stereoscopic video, and propose an MAC-based stereoscopic video coder and disparity estimation scheme to solve those problems. We used a reconstructed disparity map during the disparity compensation process and took that disparity map into account while estimating the base-view sequence motion vectors. Moreover, we proposed a search range finding and illumination imbalance decision system. We also proposed a block-based disparity map regularization process as well as block splitting in the object boundary and occlusion regions (to reduce the number of bits to encode in both the disparity map and the residual image). Last, we compensated for the imbalance between two cameras with a novel system that used MAC characteristics. Experimental results indicate that the proposed MAC system outperformed conventional stereo coding systems by a maximum of 3.5dB in terms of the PSNR and 1018% in terms of bitsaving, especially in low bitrate applications.",
    "keywords": [
      "mpeg-4 mac",
      "stereoscopic video coder",
      "disparity estimation"
    ]
  },
  {
    "id": "281",
    "title": "Robust Recovery of Subspace Structures by Low-Rank Representation",
    "abstract": "In this paper, we address the subspace clustering problem. Given a set of data samples (vectors) approximately drawn from a union of multiple subspaces, our goal is to cluster the samples into their respective subspaces and remove possible outliers as well. To this end, we propose a novel objective function named Low-Rank Representation (LRR), which seeks the lowest rank representation among all the candidates that can represent the data samples as linear combinations of the bases in a given dictionary. It is shown that the convex program associated with LRR solves the subspace clustering problem in the following sense: When the data is clean, we prove that LRR exactly recovers the true subspace structures; when the data are contaminated by outliers, we prove that under certain conditions LRR can exactly recover the row space of the original data and detect the outlier as well; for data corrupted by arbitrary sparse errors, LRR can also approximately recover the row space with theoretical guarantees. Since the subspace membership is provably determined by the row space, these further imply that LRR can perform robust subspace clustering and error correction in an efficient and effective way.",
    "keywords": [
      "low-rank representation",
      "subspace clustering",
      "segmentation",
      "outlier detection"
    ]
  },
  {
    "id": "282",
    "title": "On an efficient CAD implementation of the distance term in Pelgrom's mismatch model",
    "abstract": "In 1989, Pelgrom et al published a mismatch model for MOS transistors, where the variation of parameter mismatch between two identical transistors is given by two independent terms: a size-dependent term and a distance-dependent term. Some CAD tools based on a nonphysical interpretation of Pelgrom's distance term result in excessive computationally expensive algorithms, which become nonviable even for circuits with a reduced number of transistors. Furthermore, some researchers are reporting new variations on the original nonphysically interpreted algorithms, which may render false results. The purpose of this paper is to clarify the physical interpretation of the distance term of Pelgrom et al. and indicate how to model it efficiently in prospective CAD tools.",
    "keywords": [
      "analog design",
      "mismatch gradient planes",
      "mismatch modeling",
      "pelgrom model",
      "sigma-space analysis"
    ]
  },
  {
    "id": "283",
    "title": "Vertex Ordering Characterizations of Graphs of Bounded Asteroidal Number",
    "abstract": "Asteroidal Triple-free (AT-free) graphs have received considerable attention due to their inclusion of various important graphs families, such as interval and cocomparability graphs. The asteroidal number of a graph is the size of a largest subset of vertices such that the removal of the closed neighborhood of any vertex in the set leaves the remaining vertices of the set in the same connected component. (AT-free graphs have asteroidal number at most 2.) In this article, we characterize graphs of bounded asteroidal number by means of a vertex elimination ordering, thereby solving a long-standing open question in algorithmic graph theory. Similar characterizations are known for chordal, interval, and cocomparability graphs.",
    "keywords": [
      "asteroidal triple",
      "at-free",
      "vertex elimination",
      "asteroidal number",
      ""
    ]
  },
  {
    "id": "284",
    "title": "Clustering validity checking methods: Part II",
    "abstract": "Clustering results validation is an important topic in the context of pattern recognition. We review approaches and systems in this context. In the first part of this paper we presented clustering validity checking approaches based on internal and external criteria. In the second, current part, we present a review of clustering validity approaches based on relative criteria. Also we discuss the results of an experimental study based on widely known validity indices. Finally the paper illustrates the issues that are under-addressed by the recent approaches and proposes the research directions in the field.",
    "keywords": [
      "clustering validation",
      "pattern discovery",
      "unsupervised learning"
    ]
  },
  {
    "id": "285",
    "title": "a training software model of an interrupt system",
    "abstract": "The paper justifies the necessity to introduce the students from the 'Computer Systems and Technologies' degree course to the structuce and way of operation of the interrupt system - one of the important components of the processor. Analysis of the basic funcionality of an example interrupt system is presented, an existing interrupt system is selected as a prototype of the training model and the arguments for its selection are proposed. The paper also describes the implemented model and its features. The work with the model will enable students to comprehend the way of operation of the interrupt system and it will be also used to check and assess their knowledge.",
    "keywords": [
      "interrupt system",
      "training software model",
      "virtual laboratory",
      "simulation"
    ]
  },
  {
    "id": "286",
    "title": "Efficient network QoS provisioning based on per node traffic shaping",
    "abstract": "This paper addresses the problem of providing per-connection end-to-end delay guarantees in a high-speed network, We assume that the network is connection oriented and enforces some admission control which ensures that the source traffic conforms to specified traffic characteristics. We concentrate on the class of rate-controlled service (RCS) disciplines, in which traffic from each connection is reshaped at every hop, and develop end-to-end delay bounds for the general case where different reshapers are used at each hop. In addition, we establish that these bounds can also be achieved when the shapers at each hop have the same ''minimal'' envelope. The main disadvantage of this class of service discipline is that the end-to-end delay guarantees are obtained as the sum of the worst-case delays at each node, but we show that this problem can be alleviated through ''proper'' reshaping of the traffic, We illustrate the impact of this reshaping by demonstrating its use in designing RCS disciplines that outperform service disciplines that are based on generalized processor sharing (GPS), Furthermore, we show that we fan restrict the space of ''good'' shapers to a family which is characterized by only one parameter, We also describe extensions to the service discipline that make it work conserving and as a result reduce the average end-to-end delays.",
    "keywords": [
      "qos provisioning",
      "real-time traffic",
      "traffic shaping",
      "atm",
      "scheduling",
      "end-to-end delay guarantees"
    ]
  },
  {
    "id": "287",
    "title": "AS-level source routing for multi-provider connection-oriented services",
    "abstract": "In this paper, we study the inter-domain Autonomous System (AS)-level routing problem within an alliance of ASs. We first describe the framework of our work, based on the introduction of a service plane for automatic multi-domain service provisioning. We adopt an abstract representation of domain relationships by means of directional metrics which are applied to a triplet (ingress point, transit AS, egress point) where the ingress and egress points can be ASs or routers. Then, we focus on the point-to-point and multipoint AS-level routing problems that arise in such an architecture. We propose an original approach that reaches near optimal solutions with tractable computation times. A further contribution of this paper is that a heavy step in the proposed heuristic can be precomputed, independently of the service demands. Moreover, we describe how in this context AS-level path diversity can be considered, and present the related extension of our heuristic. By extensive tests on AS graphs derived from the Internet, we show that our heuristic is often equal or a few percent close to the optimal, and that, in the case of precomputation, its time consumption can be much lower than with other well-known algorithms.",
    "keywords": [
      "inter-domain routing",
      "inter-as mpls",
      "qos routing",
      "multipoint routing",
      "as-level routing"
    ]
  },
  {
    "id": "288",
    "title": "Parallel and distributed simulation of sediment dynamics in shallow water using particle decomposition approach",
    "abstract": "This paper describes the parallel simulation of sediment dynamics in shallow water. By using a Lagrangian model, the problem is transformed to one in which a large number of independent particles must be tracked. This results in a technique that can be parallelised with high efficiency. We have developed a sediment transport model using three different sediment suspension methods. The first method uses a modified mean for the Poisson distribution function to determine the expected number of the suspended particles in each particular grid cell of the domain over all available processors. The second method determines the number of particles to suspend with the aid of the Poisson distribution function only in those grid cells which are assigned to that processor. The third method is based on the technique of using a synchronised pseudo-random-number generator to generate identical numbers of suspended particles in all valid grid cells for each processor. Parallel simulation experiments are performed in order to investigate the efficiency of these three methods. Also the parallel performance of the implementations is analysed. We conclude that the second method is the best method on distributed computing systems (e.g., a Beowulf cluster), whereas the third maintains the best load distribution.",
    "keywords": [
      "lagrangian particle model",
      "stochastic differential equation",
      "sediment transport",
      "parallel processing",
      "speed up",
      "load balance",
      "efficiency"
    ]
  },
  {
    "id": "289",
    "title": "A numerical technique to predict periodic and quasi-periodic response of nonlinear dynamic systems",
    "abstract": "A frequency domain based algorithm using Fourier approximation and Galerkin error minimization has been used to obtain the periodic orbits of large order nonlinear dynamic systems. The stability of these periodic response is determined through a bifurcation analysis using Floquet theory. This technique is applicable to dynamic systems having both analytic and nonanalytic nonlinearities. This technique is compared with numerical time integration and is found to be much faster in predicting the steady state periodic response.",
    "keywords": [
      "fouriergalerkinnewton technique",
      "floquet analysis",
      "bifurcations"
    ]
  },
  {
    "id": "290",
    "title": "Rough fuzzy approximations on two universes of discourse",
    "abstract": "In rough set theory, the lower and upper approximation operators can be constructed via a variety of approaches. Various fuzzy generalizations of rough approximation operators have been made over the years. This paper presents a framework for the study of rough fuzzy sets on two universes of discourse. By means of a binary relation between two universes of discourse, a covering and three relations are induced to a single universe of discourse. Based on the induced notions, four pairs of rough fuzzy approximation operators are proposed. These models guarantee that the approximating sets and the approximated sets are on the same universes of discourse. Furthermore, the relationship between the new approximation operators and the existing rough fuzzy approximation operators on two universes of discourse are scrutinized, and some interesting properties are investigated. Finally, the connections of these approximation operators are made, and conditions under which some of these approximation operators are equivalent are obtained.",
    "keywords": [
      "binary relations",
      "coverings",
      "fuzzy sets",
      "rough fuzzy approximation operators",
      "two universes"
    ]
  },
  {
    "id": "291",
    "title": "Identification of different stages of diabetic retinopathy using retinal optical images",
    "abstract": "Diabetes is a disease which occurs when the pancreas does not secrete enough insulin or the body is unable to process it properly. This disease affects slowly the circulatory system including that of the retina. As diabetes progresses, the vision of a patient may start to deteriorate and lead to diabetic retinopathy. In this study on different stages of diabetic retinopathy, 124 retinal photographs were analyzed. As a result, four groups were identified, viz., normal retina, moderate non-proliferative diabetic retinopathy, severe non-proliferative diabetic retinopathy and proliferative diabetic retinopathy. Classification of the four eye diseases was achieved using a three-layer feedforward neural network. The features are extracted from the raw images using the image processing techniques and fed to the classifier for classification. We demonstrate a sensitivity of more than 90% for the classifier with the specificity of 100%.",
    "keywords": [
      "eye",
      "normal",
      "features",
      "retinopathy",
      "neural network",
      "image processing",
      "feedforward",
      "classification"
    ]
  },
  {
    "id": "292",
    "title": "Constrained ZIP code segmentation by a PCNN-based thinning algorithm",
    "abstract": "This paper proposes a novel thinning algorithm and applies it to automatic constrained ZIP code segmentation. The segmentation method consists of two main stages: removal of rectangle boxes and location of ZIP code digits. Both the two stages are implemented on the skeleton of boxes, which is extracted by the proposed pulse coupled neural network (PCNN) based thinning algorithm. This algorithm is specially designed to merely skeletonize the boxes. At the second stage, a projection method is employed to segment ZIP code image into its constituent digits. Experimental results show that the proposed method is very efficient in segmenting ZIP code images even with noise.",
    "keywords": [
      "constrained zip code segmentation",
      "pulse coupled neural network",
      "skeleton",
      "projection"
    ]
  },
  {
    "id": "293",
    "title": "multimodal authentication based on random projections and source coding",
    "abstract": "In this paper, we consider an authentication framework for independent modalities based on binary hypothesis testing using source coding jointly with the random projections. The source coding ensures the multimodal signals reconstruction at the decoder based on the authentication data. The random projections are used to cope with the security, privacy, robustness and complexity issues. Finally, the authentication performance is investigated for both direct and random projections domains. The asymptotic performance approximation is derived and compared with the exact solutions. The impact of modality fusion on the authentication system performance is demonstrated.",
    "keywords": [
      "dimensionality reduction",
      "fusion",
      "hypothesis testing",
      "random projections",
      "multimodal authentication"
    ]
  },
  {
    "id": "294",
    "title": "Effects of additive elements on the phase formation and morphological stability of nickel monosilicide films",
    "abstract": "Alloying elements can substantially affect the formation and morphological stability of nickel monosilicide. A comprehensive study of phase formation was performed on 24 Ni alloys with varying concentrations of alloying elements. Silicide films have been used for more than 15 years to contact the source, drain and gate of state-of-the-art complementary-metal-oxide-semiconductor (CMOS) devices. In the past, the addition of alloying elements was shown to improve the transformation from the high resistivity C49 to the low resistivity C54-TiSi2 phase and to allow for the control of surface and interface roughness of CoSi2 films as well as produce significant improvements with respect to agglomeration of the films. Using simultaneous time-resolved X-ray diffraction (XRD), resistance and light scattering measurements, we follow the formation of the silicide phases in real time during rapid thermal annealing. Additions to the NiSi system lead to modifications in the phase formation sequence at low temperatures (metal-rich phases), to variations in the formation temperatures of NiSi and NiSi2, and to changes in the agglomeration behavior of the films formed. Of the 24 elements studied, additions of Mo, Re, Ta and W are amongst the most efficient to retard agglomeration while elements such as Pd, Pt and Rh are most efficient to retard the formation of NiSi2.",
    "keywords": [
      "nickel silicides",
      "nisi",
      "alloying",
      "agglomeration",
      "nisi2"
    ]
  },
  {
    "id": "295",
    "title": "HIGH-ORDER MULTIPLE-MODE AND TRANSADMITTANCE-MODE OTA-C UNIVERSAL FILTERS",
    "abstract": "This paper presents two new high-order OTA-C universal filters. The first proposed filter structure employs n + 3 operational transconductance amplifiers (OTAs) and n grounded capacitors, which can realize nth-order multiple-mode (including voltage, current, transadmittance, and transimpedance modes) universal filtering responses (lowpass, highpass, bandpass, bandreject, and allpass) from the same topology. Since the OTA has high input and output impedances, it is very suitable for transadmittance-mode circuit applications. Therefore, a new high-order transadmittance-mode OTA-C universal filter structure using the minimum components is introduced. The second proposed filter structure uses only n + 1 OTAs and n grounded capacitors, which are the minimum components necessary for realizing nth-order transadmittance-mode universal filtering responses (lowpass, highpass, bandpass, bandreject, and allpass) from the same topology. This represents the attractive feature from chip area and power consumption point of view. Moreover, the two new OTA-C universal filters still enjoy many important advantages: no need of extra inverting or double-type amplifiers for special input signals, using only n grounded capacitors, no need of any resistors, cascadably connecting the former voltage-mode stage and the latter current-mode stage, and low sensitivity performance. H-Spice simulations with TSMC 0.35 mu m process and +/- 1.65V supply voltages are included and confirm the theoretical predictions.",
    "keywords": [
      "operational transconductance amplifiers",
      "multiple-mode",
      "universal high-order filter",
      "transimpedance-mode",
      "transadmittance-mode"
    ]
  },
  {
    "id": "296",
    "title": "Random fuzzy fractional integral equations  theoretical foundations",
    "abstract": "This paper presents mathematical foundations for studies of random fuzzy fractional integral equations which involve a fuzzy integral of fractional order. We consider two different kinds of such equations. Their solutions have different geometrical properties. The equations of the first kind possess solutions with trajectories of nondecreasing diameter of their consecutive values. On the other hand, the solutions to equations of the second kind have trajectories with nonincreasing diameter of their consecutive values. Firstly, the existence and uniqueness of solutions is investigated. This is showed by using a method of successive approximations. An estimation of error of nth approximation is given. Also a boundedness of the solution is indicated. To show well-posedness of the considered theory, we prove that solutions depend continuously on the data of the equations. Some concrete examples of random fuzzy fractional integral equations are solved explicitly.",
    "keywords": [
      "random fuzzy fractional integral equation",
      "existence and uniqueness of solution",
      "uncertainty",
      "fuzzy differential equation",
      "set differential equation",
      "mathematical foundations"
    ]
  },
  {
    "id": "297",
    "title": "Efficient Opportunistic Routing in Utility-Based Ad Hoc Networks",
    "abstract": "Due to resource scarcity, a paramount concern in ad hoc networks is utilizing limited resources efficiently. The self-organized nature of ad hoc networks makes the network utility-based approach an efficient way to allocate limited resources. However, the effect of link instability has not yet been adequately addressed in literature. To efficiently address the routing problem in ad hoc networks, we integrate the cost and stability into a network utility metric, and adopt the metric to evaluate the routing optimality in a unified, opportunistic routing model. Based on this model, an efficient algorithm is designed, both centralized and distributed implementations are presented, and extensive simulations on NS-2 are conducted to verify our results.",
    "keywords": [
      "ad hoc networks",
      "distributed algorithms",
      "network utility",
      "opportunistic routing",
      "stability"
    ]
  },
  {
    "id": "298",
    "title": "A Computation to Integrate the Analysis of Genetic Variations Occurring within Regulatory Elements and Their Possible Effects",
    "abstract": "Single nucleotide polymorphisms (SNPs) and short tandem repeats (STRs) are the most common genetic variations, are widespread within genomes, and form the diversity within species. These genetic variations affect many regulatory elements such as transcription factor binding sites (TFBSs), DNA methylation sites on CpG islands, and microRNA target sites; these elements have been found to play major as well as indirect roles in regulating gene expression. Currently, systems are available to display such genetic variation occurring within regulatory elements. To understand and display all the potential variation described above, we have developed a web-based system tool, the Regulatory Element and Genetic Variation Viewer (REGV Viewer [REGV]), which provides a friendly web interface for users and shows genetic variation information within regulatory elements by either inputting a gene list or selecting a chromosome by name. Moreover, our tool not only supports logic operation queries, but after a query is submitted, it also shows a high-throughput simulation, including combined data, statistical graphs, and graphical views of the genetic variants and regulatory elements. Additionally, when the SNP variation occurs within TFBSs and if the SNP allele frequency and TFBS position weight matrices (PWMs) are available, our system will show the new putative TFBSs resulting from the SNP variation.",
    "keywords": [
      "genetic variation",
      "snp",
      "tfbs"
    ]
  },
  {
    "id": "299",
    "title": "Programs as visual, interactive documents",
    "abstract": "We present a novel approach to combined textual and visual programming by allowing visual, interactive objects to be embedded within textual source code and segments of source code to be further embedded within those objects. We retain the strengths of text-based source code, while enabling visual programming where it is beneficial. Additionally, embedded objects and code provide a simple object-oriented approach to adding a visual form of LISP-style macros to a language. The ability to freely combine source code and visual, interactive objects with one another allows for the construction of interactive programming tools and experimentation with novel programming language extensions. Our visual programming system is supported by a type coercion-based presentation protocol that displays normal Java and Python objects in a visual, interactive form. We have implemented our system within a prototype interactive programming environment called The Larch Environment. ",
    "keywords": [
      "java",
      "python",
      "environment",
      "visual programming",
      "interactive",
      "visualization",
      "implementation"
    ]
  },
  {
    "id": "300",
    "title": "The equilibrium limit of a constitutive model for two-phase granular mixtures and its numerical approximation",
    "abstract": "In this paper we analyze the equilibrium limit of the constitutive model for two-phase granular mixtures introduced in Papalexandris (2004) [13], and develop an algorithm for its numerical approximation. At, equilibrium, the constitutive model reduces to a strongly coupled, overdetermined system of quasilinear elliptic partial differential equations with respect to the pressure and the volume fraction of the solid granular phase. First we carry a perturbation analysis based on standard hydrostatic-type scaling arguments which reduces the complexity of the coupling of the equations. The perturbed system is then supplemented by an appropriate compatibility condition which arises from the properties of the gradient operator. Further, based on the Helmholtz decomposition and Ladyzhenskayas decomposition theorem, we develop a projection-type, Successive-Over-Relaxation numerical method. This method is general enough and can be applied to a variety of continuum models of complex mixtures and mixtures with micro-structure. We also prove that this method is both stable and consistent hence, under standard assumptions, convergent. The paper concludes with the presentation of representative numerical results.",
    "keywords": [
      "granular mixtures",
      "complex fluids",
      "overdetermined elliptic systems",
      "ladyzhenskayas theorem",
      "successive-over-relaxation",
      "predictorcorrector methods"
    ]
  },
  {
    "id": "301",
    "title": "Analysis of sonar targets by teager-huang transform (THT)",
    "abstract": "In this paper, an approach for Sonar targets analysis based on a new energy-time-frequency representation, called Teager-Huang Transform (THT), is presented. The THT is the combination of the empirical mode decomposition of Huang and the Teager-Kaiser signal demodulation method. The THT is free of interferences and does not requires basis functions for signals decomposition. The analysis is carried out, in free field, from the impulse responses of Sonar targets. We compare the analysis results of impulse responses of spherical and cylindrical targets given by THT to those of the smoothed Wigner-Ville transformation.",
    "keywords": [
      "time frequency analysis",
      "empirical mode decomposition",
      "teager-kaiser energy operator",
      "teager huang transform ",
      "sonar echos."
    ]
  },
  {
    "id": "302",
    "title": "International Standard Development for Knowledge Based Engineering Services for Product Lifecycle Management",
    "abstract": "In September 2005, the international information technology standard body Object Management Group (OMG) published a Request for Proposal (RFP) for an international standard for Knowledge Based Engineering (KBE) Services for Product Lifecycle Management (PLM). The standard aims to facilitate the integration of KBE applications in a PLM environment. KBE has been used in key engineering industry to deliver significant business benefits and has been a catalyst for changes in engineering processes. In recent years, mainstream CAD vendors begin to incorporate KBE functionalities in their solutions. PLM is evolving from the platform to manage engineering data to the repository of complete enterprise knowledge. As CAD becomes more knowledge based, the convergence of KBE and PLM is expected to happen soon. The OMG standard RFP is an action to accelerate this convergence. The RFP is the result of an international effort with a team that includes engineering end users, software vendors and researchers. This paper presents the essence and the development process of the RFP to widen the engagement with the engineering research community.",
    "keywords": [
      "knowledge based engineering",
      "product lifecycle management",
      "engineering knowledge management"
    ]
  },
  {
    "id": "303",
    "title": "An image topic model for image denoising",
    "abstract": "Topic model is a powerful tool for the basic document or image processing tasks. In this study we introduce a novel image topic model, called Latent Patch Model (LPM), which is a generative Bayesian model and assumes that the image and pixels are connected by a latent patch layer. Based on the LPM, we further propose an image denoising algorithm namely multiple estimate LPM (MELPM). Unlike other works, the proposed denoising framework is totally implemented on the latent patch layer, and it is effective for both Gaussian white noises and impulse noises. Experimental results demonstrate that LPM performs well in representing images. And its application in image denoising achieves competitive PSNR and visual quality with conventional algorithms.",
    "keywords": [
      "topic model",
      "denoising",
      "patch clustering",
      "semantic learning"
    ]
  },
  {
    "id": "304",
    "title": "improving test case generation for web applications using automated interface discovery",
    "abstract": "With the growing complexity of web applications, identifying web interfaces that can be used for testing such applications has become increasingly challenging. Many techniques that work effectively when applied to simple web applications are insufficient when used on modern, dynamic web applications, and may ultimately result in inadequate testing of the applications' functionality. To address this issue, we present a technique for automatically discovering web application interfaces based on a novel static analysis algorithm. We also report the results of an empirical evaluation in which we compare our technique against a traditional approach. The results of the comparison show that our technique can (1) discover a higher number of interfaces and (2) help generate test inputs that achieve higher coverage.",
    "keywords": [
      "interface extraction",
      "help",
      "web-application testing",
      "applications",
      "test",
      "dynamic",
      "functional",
      "addressing",
      "web interface",
      "static analysis",
      "test case generation",
      "interfaces",
      "discoveries",
      "web application",
      "complexity",
      "algorithm",
      "coverage",
      "comparisons",
      "empirical evaluation",
      "automation"
    ]
  },
  {
    "id": "305",
    "title": "Numerical study of stream-function formulation governing flows in multiply-connected domains by integrated RBFs and Cartesian grids",
    "abstract": "This paper describes a new numerical procedure, based on point collocation, integrated multiquadric functions and Cartesian grids, for the discretisation of the stream-function formulation for flows of a Newtonian fluid in multiply-connected domains. Three particular issues, namely (i) the derivation of the stream-function values on separate boundaries, (ii) the implementation of cross derivatives in irregular regions, and (iii) the treatment of double boundary conditions, are studied in the context of Cartesian grids and approximants based on integrated multiquadric functions in one dimension. Several test problems, i.e. steady flows between a rotating circular cylinder and a fixed square cylinder and also between eccentric cylinders maintained at different temperatures, are investigated. Results obtained are compared well with numerical data available in the literature.  ",
    "keywords": [
      "stream-function formulation",
      "multiply-connected domain",
      "integrated radial-basis-function network",
      "cartesian grid"
    ]
  },
  {
    "id": "306",
    "title": "Linear techniques to correct for temperature-induced spectral variation in multivariate calibration",
    "abstract": "The influence of external physical variation such as temperature fluctuations on near-infrared (NIR) spectra and their effect on the predictive power of calibration models such as PLS have been studied. Different methods to correct for the temperature effect by explicitly including the temperature in a calibration model have been tested. The results are compared to the implicit inclusion, which takes the temperature into account only through the calibration design. Two data sets are used, one well-designed data set measured in the laboratory and one industrial data set consisting of measurements for process samples. For both data sets, the explicit inclusion of the temperature in the calibration models did not result in an improvement of the prediction accuracy compared to implicit inclusion.  ",
    "keywords": [
      "spectral variation",
      "temperature",
      "nir spectra"
    ]
  },
  {
    "id": "307",
    "title": "Overlapping B+-trees: An implementation of a transaction time access method",
    "abstract": "A new variation of Overlapping B+ -trees is presented, which provides efficient indexing of transaction time and keys in a two dimensional key-time space. Modification operations (i.e. insertions, deletions and updates) are allowed at the current version, whereas queries are allowed to any temporal version, i.e. either in the current or in past versions. Using this structure, snapshot and range-timeslice queries can be answered optimally. However, the fundamental objective of the proposed method is to deliver efficient performance in case of a general pure-key query (i.e. 'history of a key'). The trade-off is a small increase in time cost for version operations and storage requirements.  ",
    "keywords": [
      "temporal databases",
      "transaction time",
      "access methods",
      "indexing",
      "algorithms",
      "time and space performance"
    ]
  },
  {
    "id": "308",
    "title": "Using a strategy-aligned fuzzy competitive analysis approach for market segment evaluation and selection",
    "abstract": "This study applies Five Forces Analysis to evaluate and select market segments for international business using a strategy-aligned fuzzy approach. An illustration segment evaluation procedure is used to demonstrate that our procedure is an effective quantification approach for integrating five forces, generic strategies and marketing information in a group decision-making process. The final decision-maker (DM) synthesizes the total crisp scores of individual alternatives by choosing judgmental coefficients ? based on individual attitude towards core business competitiveness and market risks to accommodate differences among market segments to the specific environment with a better understanding of the decision problem and individual decision-making behavior. In the illustration presented here, the final solution is then obtained by identifying the best market segment for further development and negotiation.",
    "keywords": [
      "market segment evaluation",
      "market segment selection",
      "fuzzy factor rating system",
      "strategy alignment",
      "multiple attributes decision-making"
    ]
  },
  {
    "id": "309",
    "title": "Using AdaBoost classifiers in a hierarchical framework for classifying surface images of marble slabs",
    "abstract": "In this paper, a new hierarchical classification method based on the use of various types of AdaBoost classification algorithms is proposed for automatic classification of marble slab images according to their quality. At first, features are extracted using the sum and difference histograms method and, at the second stage, different versions of the AdaBoost algorithms are used as classifiers together with those extracted features in a proposed hierarchical fashion. Performance of the proposed method is compared against performances of different types of neural network classifiers and a support vector machine (SVM) classifier. Computational results show that the proposed hierarchical structure employing AdaBoost algorithms performs superior to neural networks and the SVM classifier for classifying marble slab images in our large and diversified data set.  ",
    "keywords": [
      "classification of marble slab images",
      "hierarchical classification",
      "adaboost classification algorithms"
    ]
  },
  {
    "id": "310",
    "title": "On bridging the gap between stochastic integer programming and MIP solver technologies",
    "abstract": "Stochastic integer programs (SIPs) represent a very difficult class of optimization problems arising from the presence of both uncertainty and discreteness in planning and decision problems. Although applications of SIPs are abundant, nothing is available by way of computational software. On the other hand, commercial software packages for solving deterministic integer programs have been around for quite a few years, and more recently, a package for solving stochastic linear programs has been released. In this paper, we describe how these software tools can be integrated and exploited for the effective solution of general-purpose SIPs. We demonstrate these ideas on four problem classes from the literature and show significant computational advantages.",
    "keywords": [
      "stochastic programming",
      "integer programming",
      "branch and bound",
      "software"
    ]
  },
  {
    "id": "311",
    "title": "Historiographic mapping of knowledge domains literature",
    "abstract": "To better understand the topic of this colloquium, we have created a series of databases related to knowledge domains (dynamic systems [small world/Milgram], information visualization [Tufte], co-citation [Small], bibliographic coupling [Kessler], and scientometrics [Scientometrics]). I have used a software package called HistCite(TM) which generates chronological maps of subject (topical) collections resulting from searches of the ISI Web of Science(R) or ISI citation indexes (SCI, SSCI, and/or AHCI) on CD-ROM. When a marked list is created on WoS, an export file is created which contains all cited references for each source document captured. These bibliographic collections, saved as ASCII files, are processed by HistCite in order to generate chronological and other tables as well as historiographs which highlight the most-cited works in and outside the collection. HistCite also includes a module for detecting and editing errors or variations in cited references as well as a vocabulary analyzer which generates both ranked word lists and word pairs used in the collection. Ideally the system will be used to help the searcher quickly identify the most significant work on a topic and trace its year-by-year historical development. In addition to the collections mentioned above, historiographs based on collections of papers that cite the Watson-Crick 1953 classic paper identifying the helical structure of DNA were created. Both year-by-year as well as month-by-month displays of papers from 1953 to 1958 were necessary to highlight the publication activity of those years.",
    "keywords": [
      "mapping",
      "knowledge domains",
      "small world concept",
      "dna structure",
      "citation analysis",
      "historiography",
      "information visualization",
      "software",
      "histcite"
    ]
  },
  {
    "id": "312",
    "title": "Bot detection evasion: a case study on local-host alert correlation bot detection methods",
    "abstract": "Botnets have continuously evolved since their inception as a malicious entity. Attackers come up with new botnet designs that exploit the weaknesses in existing defense mechanisms and continue to evade detection. It is necessary to analyze the weaknesses of existing defense mechanisms to find out the lacunae in them. This research exposes a weakness found in an existing bot detection method (BDM) by implementing a specialized P2P botnet model and carrying out experiments on it. Weaknesses that are found and validated can be used to predict the development path of botnets, and as a result, detection and mitigation measures can be implemented in a proactive fashion. The main contribution of this work is to demonstrate the exploitation pattern of an inherent weakness in local-host alert correlation (LHAC) based methods and to assert that current LHAC implementations could allow pockets of cooperative bots to hide in an enterprise size network. This work suggests that additional monitoring capabilities must be added to current LHAC-based methods in order for them to remain a viable bot detection mechanism. ",
    "keywords": [
      "botnet",
      "p2p",
      "security",
      "network",
      "covert"
    ]
  },
  {
    "id": "313",
    "title": "New plating bath for electroless copper deposition on sputtered barrier layers",
    "abstract": "A new copper plating bath for electroless deposition directly on conductive copper-diffusion barrier layers has been developed. This plating bath can be operated at temperatures between 20 and 50C and has good stability. High temperature processing allows for increased deposition rates and decreased specific resistivity values for the deposited copper films. Electroless Cu films deposited from this bath showed a conformal step coverage in high aspect ratio trenches and, therefore, are promising as seed layers for copper electroplating. The effect of the bath composition, activation procedure and processing temperature on the plating rate and morphology of the deposited copper has been studied and is presented here.",
    "keywords": [
      "interconnection",
      "copper",
      "electroless deposition"
    ]
  },
  {
    "id": "314",
    "title": "An adaptive finite element procedure for fully-coupled point contact elastohydrodynamic lubrication problems",
    "abstract": "This paper presents an automatic locally adaptive finite element solver for the fully-coupled EHL point contact problems. The proposed algorithm uses a posteriori error estimation in the stress in order to control adaptivity in both the elasticity and lubrication domains. The implementation is based on the fact that the solution of the linear elasticity equation exhibits large variations close to the fluid domain on which the Reynolds equation is solved. Thus the local refinement in such region not only improves the accuracy of the elastic deformation solution significantly but also yield an improved accuracy in the pressure profile due to increase in the spatial resolution of fluid domain. Thus, the improved traction boundary conditions lead to even better approximation of the elastic deformation. Hence, a simple and an effective way to develop an adaptive procedure for the fully-coupled EHL problem is to apply the local refinement to the linear elasticity mesh. The proposed algorithm also seeks to improve the quality of refined meshes to ensure the best overall accuracy. It is shown that the adaptive procedure effectively refines the elements in the region(s) showing the largest local error in their solution, and reduces the overall error with optimal computational cost for a variety of EHL cases. Specifically, the computational cost of proposed adaptive algorithm is shown to be linear with respect to problem size as the number of refinement levels grows.",
    "keywords": [
      "elastohydrodynamic lubrication",
      "finite element method",
      "linear elasticity",
      "fully coupled approach",
      "adaptive h-refinement",
      "optimization of meshes"
    ]
  },
  {
    "id": "315",
    "title": "Allowing each node to communicate only once in a distributed system: shared whiteboard models",
    "abstract": "In this paper we study distributed algorithms on massive graphs where links represent a particular relationship between nodes (for instance, nodes may represent phone numbers and links may indicate telephone calls). Since such graphs are massive they need to be processed in a distributed way. When computing graph-theoretic properties, nodes become natural units for distributed computation. Links do not necessarily represent communication channels between the computing units and therefore do not restrict the communication flow. Our goal is to model and analyze the computational power of such distributed systems where one computing unit is assigned to each node. Communication takes place on a whiteboard where each node is allowed to write at most one message. Every node can read the contents of the whiteboard and, when activated, can write one small message based on its local knowledge. When the protocol terminates its output is computed from the final contents of the whiteboard. We describe four synchronization models for accessing the whiteboard. We show that message size and synchronization power constitute two orthogonal hierarchies for these systems. We exhibit problems that separate these models, i.e., that can be solved in one model but not in a weaker one, even with increased message size. These problems are related to maximal independent set and connectivity. We also exhibit problems that require a given message size independently of the synchronization model.",
    "keywords": [
      "distributed computing",
      "local computation",
      "graph properties",
      "bounded communication"
    ]
  },
  {
    "id": "316",
    "title": "A dual spinal cord lesion paradigm to study spinal locomotor plasticity in the cat",
    "abstract": "After a complete spinal cord injury (SCI) at the lowest thoracic level (T13), adult cats trained to walk on a treadmill can recover hindlimb locomotion within 23 weeks, resulting from the activity of a spinal circuitry termed the central pattern generator (CPG). The role of this spinal circuitry in the recovery of locomotion after partial SCIs, when part of descending pathways can still access the CPG, is not yet fully understood. Using a dual spinal lesion paradigm (first hemisection at T10 followed three weeks after by a complete spinalization at T13), we showed that major changes occurred in this locomotor spinal circuitry. These plastic changes at the spinal cord level could participate in the recovery of locomotion after partial SCI. This short review describes the main findings of this paradigm in adult cats.",
    "keywords": [
      "central pattern generator",
      "plasticity",
      "training",
      "spinal cord injury",
      "locomotion"
    ]
  },
  {
    "id": "317",
    "title": "Analysis of a sparse hypermatrix Cholesky with fixed-sized blocking",
    "abstract": "We present the way in which we have constructed an implementation of a sparse Cholesky factorization based on a hypermatrix data structure. This data structure is a storage scheme which produces a recursive 2D partitioning of a sparse matrix. It can be useful on some large sparse matrices. Subblocks are stored as dense matrices. Thus, efficient BLAS3 routines can be used. However, since we are dealing with sparse matrices some zeros may be stored in those dense blocks. The overhead introduced by the operations on zeros can become large and considerably degrade performance. We present the ways in which we deal with this overhead. Using matrices from different areas (Interior Point Methods of linear programming and Finite Element Methods), we evaluate our sequential in-core hypermatrix sparse Cholesky implementation. We compare its performance with several other codes and analyze the results. In spite of using a simple fixed-size partitioning of the matrix our code obtains competitive performance.",
    "keywords": [
      "sparse cholesky",
      "hypermatrix structure",
      "2d partitioning",
      "windows in submatrices",
      "small matrix library"
    ]
  },
  {
    "id": "318",
    "title": "Atomic precision patterning on Si: An opportunity for a digitized process",
    "abstract": "H depassivation lithography is a process by which a monolayer of H absorbed on a Si(100) 21 surface may be patterned by the removal of H atoms using a scanning tunneling microscope. This process can achieve atomic resolution where individual atoms are targeted and removed. This paper suggests that such a patterning process can be carried out as a digital process, where the pixels of the pattern are the individual H atoms. The goal is digital fabrication rather than digital information processing. The margins for the read and write operators appear to be sufficient for a digital process, and the tolerance for physical addressing of the atoms is technologically feasible. A digital fabrication process would enjoy some of the same advantages of digital computation; namely high reliability, error checking and correction, and the creation of complex systems.",
    "keywords": [
      "lithography",
      "scanning tunneling microscope",
      "si",
      "hydrogen depassivation",
      "digital process"
    ]
  },
  {
    "id": "319",
    "title": "Automated Worst-Case Execution Time Analysis Based on Program Modes",
    "abstract": "A program mode is a regular trajectory of the execution of a program that is determined by the values of its input variables. By exploiting program modes, we may make worst-case execution time (WCET) analysis more precise. This paper presents a novel method to automatically find program modes and calculate the WCET estimates of programs. First, the modes of a program will be identified automatically by mode-relevant program slicing, and the precondition will be calculated for each mode using a path-wise test data generation method. Then, for each feasible mode, we show how to calculate its WCET estimate for modern reduced instruction set computer (RISC) processors with caches and pipelines and for traditional complex instruction set computer (CISC) processors. We also present a method to obtain the symbolic expression for each mode for CISC processors. The experimental results show the effectiveness of the method.",
    "keywords": [
      "real-time systems",
      "wcet analysis",
      "program mode",
      "program slicing",
      "iterative relaxation method"
    ]
  },
  {
    "id": "320",
    "title": "Non-agricultural databases and thesauri Retrieval of subject headings and non-controlled terms in relation to agriculture",
    "abstract": "Purpose - The paper aims to assess the utility of non-agriculture-specific information systems, databases, and respective controlled vocabularies (thesauri) in organising and retrieving agricultural information. The purpose is to identify thesaurus-linked tree structures, controlled subject headings/terms (heading words, descriptors), and principal database-dependent characteristics and assess how controlled terms improve retrieval results (recall) in relation to free-text/uncontrolled terms in abstracts and document titles. Design/methodology/approach - Several different hosts (interfaces, platforms, portals) and databases were used: CSA Illumina.(ERIC, LISA), Ebscohost (Academic Search Complete, Medline, Political Science Complete), Ei-Engineering Village (Compendex, Inspec), OVID (PsycINFO), ProQuest (ABI/Inform Global). The search-terms agriculture and agricultural and truncated word-stem agricultur- were employed. Permuted (rotated index) search fields were used to retrieve terms from thesauri. Subject-heading search was assessed in relation to free-text search, based on abstracts and document titles. Findings - All thesauri contain agriculture-based headings; however, associative, hierarchical and synonymous relationships show important inter-database differences. Using subject headings along with abstracts and titles in search syntax (query) sometimes improves retrieval by up to 60 per cent. Retrieval depends on search fields and database-specifics, such as autostemming (lemmatization), explode function, word-indexing, or phrase-indexing. Research limitations/implications - Inter-database and host comparison, on consistent principles, can be limited because of some particular host- and database-specifics. Practical implications - End-users may exploit databases more competently and thus achieve better retrieval results in searching for agriculture-related information. Originality/value - The function of as many as ten databases in different disciplines in providing information relevant to subject matter that is not a topical focus of databases is assessed.",
    "keywords": [
      "thesauri",
      "controlled vocabularies",
      "indexing",
      "subject headings",
      "databases",
      "agriculture"
    ]
  },
  {
    "id": "321",
    "title": "Efficient Packet Classification with a Hybrid Algorithm",
    "abstract": "Packet classification categorizes incoming packets into multiple forwarding classes based on pre-defined filters. This categorization makes information accessible for quality of service or security handling in the network. In this paper, we propose a scheme which combines the Aggregate Bit Vector algorithm and the pruned Tuple Space Search algorithm to improve the performance of packet classification in terms of speed and storage. We also present the procedures of incremental update. Our scheme is evaluated with filter databases of varying sizes and characteristics. The experimental results demonstrate that our scheme is feasible and scalable.",
    "keywords": [
      "packet classification",
      "network intrusion defection systems",
      "firewalls",
      "qos",
      "packet forwarding"
    ]
  },
  {
    "id": "322",
    "title": "The Intra-Americas Sea Low-level Jet",
    "abstract": "A relevant climate feature of the Intra-Americas Sea (IAS) is the low-level jet (IALLJ) dominating the IAS circulation, both in summer and winter; and yet it is practically unknown with regard to its nature, structure, interactions with mid-latitude and tropical phenomena, and its role in regional weather and climate. This paper updates IALLJ current knowledge and its contribution to IAS circulationprecipitation patterns and presents recent findings about the IALLJ based on first in situ observations during Phase 3 of the Experimento Climtico en las Albercas de Agua Clida (ECAC), an international field campaign to study IALLJ dynamics during July 2001. Nonhydrostatic fifth-generation Pennsylvania State University National Center for Atmospheric Research Mesoscale Model (MM5) simulations were compared with observations and reanalysis. Large-scale circulation patterns of the IALLJ northern hemisphere summer and winter components suggest that trades, and so the IALLJ, are responding to landocean thermal contrasts during the summer season of each continent. The IALLJ is a natural component of the American monsoons as a result of the continent's approximate northsouth land distribution. During warm (cold) El NioSouthern Oscillation phases, winds associated with the IALLJ core (IALLJC) are stronger (weaker) than normal, so precipitation anomalies are positive (negative) in the western Caribbean near Central America and negative (positive) in the central IAS. During the ECAC Phase 3, strong surface winds associated with the IALLJ induced upwelling, cooling down the sea surface temperature by 12 C. The atmospheric mixed layer height reached 1 km near the surface wind maximum below the IALLJC. Observations indicate that primary water vapor advection takes place in a shallow layer between the IALLJC and the ocean surface. Latent heat flux peaked below the IALLJC. Neither the reanalysis nor MM5 captured the observed thermodynamic and kinematic IALLJ structure. So far, IALLJ knowledge is based on either dynamically initialized data or simulations of global (regional) models, which implies that a more systematic and scientific approach is needed to improve it. The Intra-Americas Study of Climate Processes is a great regional opportunity to address trough field work, modeling, and process studies, many of the IALLJ unknown features.",
    "keywords": [
      "intra-americas low-level jet",
      "tropical climate variability",
      "mm5 modeling",
      "el niosouthern oscillation",
      "enso"
    ]
  },
  {
    "id": "323",
    "title": "Operational knowledge representation for practical decision-making",
    "abstract": "For the design of an \"intelligent\" assistant system aimed at supporting operators' decision in subway control, we modeled operators' activity and know-how. As a result, we introduce the notion of a contextual graph, which appears as a simple solution to describe and manage operational decision-making.",
    "keywords": [
      "context representation",
      "contextual graphs",
      "decision tree",
      "knowledge representation",
      "operational knowledge"
    ]
  },
  {
    "id": "324",
    "title": "ORGANIZATIONAL STRUCTURE-SATISFACTORY SOCIAL LAW DETERMINATION IN MULTIAGENT WORKFLOW SYSTEMS",
    "abstract": "The multiagent workflow systems can be formalized from an organizational structure viewpoint, which includes three parts: the interaction structure among agents, the temporal flow of activities, and the critical resource sharing relations among activities. While agents execute activities, they should decide their strategies to satisfy the constraints brought by the organizational structure of multiagent workflow system. To avoid collisions in the multiagent workflow system, this paper presents a method to determine social laws in the system to restrict the strategies of agents and activities; the determined social laws can satisfy the characteristics of organization structures so as to minimize the conflicts among agents and activities. Moreover, we also deal with the social law adjustment mechanism for the alternations of interaction relations, temporal flows, and critical resource sharing relations. It is proved that our model can produce useful social laws for organizational structure of multiagent workflow systems, i.e., the conflicts brought by the constraints of organization structure can be minimized.",
    "keywords": [
      "multiagents",
      ".workflows",
      "coordination",
      "social laws",
      "social strategies",
      "organizational structures"
    ]
  },
  {
    "id": "325",
    "title": "CONTENT-AWARE RETARGETING FOR SOCCER VIDEO ADAPTATION",
    "abstract": "A content-aware retargeting method is proposed for adapting soccer video to heterogeneous terminals. According to domain-specific knowledge, ball, player and player's face are defined as user interested objects (UIOs) in different view-types. The UIOs are extracted by semantic analysis on soccer video, and then a region of interest (ROI) of each shot is determined jointly by three factors: terminal size, scaling factor and aspect ratio. The proposed method optimizes the retargeted region to contain more semantic content while adapting the constraint of terminal screen. The simulation results prove that the proposed CAR system wins better viewing experiences than the traditional methods such as resizing in a \"Letter box\" mechanism or cropping directly.",
    "keywords": [
      "video retargarting",
      "user interested object",
      "region of interest",
      "video analysis",
      "view-type"
    ]
  },
  {
    "id": "326",
    "title": "Ribosome kinetics and aa-tRNA competition determine rate and fidelity of peptide synthesis",
    "abstract": "It is generally accepted that the translation rate depends on the availability of cognate aa-tRNAs. In this study it is shown that the key factor that determines translation rate is the competition between near-cognate and cognate aa-tRNAs. The transport mechanism in the cytoplasm is diffusion, thus the competition between cognate, near-cognate and non-cognate aa-tRNAs to bind to the ribosome is a stochastic process. Two competition measures are introduced; C(i) and R(i) (i=1, 64) are quotients of the arrival frequencies of near-cognates vs. cognates and non-cognates vs. cognates, respectively. Furthermore, the reaction rates of bound cognates differ from those of bound near-cognates. If a near-cognate aa-tRNA binds to the A site of the ribosome, it may be rejected at the anti-codon recognition step or proofreading step or it may be accepted. Regardless of its fate, the near-cognates and non-cognates have caused delays of varying duration to the observed rate of translation. Rate constants have been measured at a temperature of 20C by (Gromadski, K.B., Rodnina, M.V., 2004. Kinetic determinants of high-fidelity tRNA discrimination on the ribosome. Mol. Cell 13, 191200). These rate constants have been re-evaluated at 37C, using experimental data at 24.5C and 37C (Varenne, S., et al., 1984. Translation in a non-uniform process: effect of tRNA availability on the rate of elongation of nascent polypeptide chains. J. Mol. Biol. 180, 549576). The key results of the study are: (i) the average time (at 37C) to add an amino acid, as defined by the ith codon, to the nascent peptide chain is: ?(i)=9.06+1.445[10.48C(i)+0.5R(i)] (in ms); (ii) the misreading frequency is directly proportional to the near-cognate competition, E(i)=0.0009C(i); (iii) the competition from near-cognates, and not the availability of cognate aa-tRNAs, is the most important factor that determines the translation rate  the four codons with highest near-cognate competition (in the case of E. coli) are [GCC]>[CGG]>[AGG]>[GGA], which overlap only partially with the rarest codons: [AGG]<[CCA]<[GCC]<[CAC]; (iv) based on the kinetic rates at 37C, the average time to insert a cognate amino acid is 9.06ms and the average delay to process a near-cognate aa-tRNA is 10.45ms and (vii) the model also provides estimates of the vacancy times of the A site of the ribosome  an important factor in frameshifting.",
    "keywords": [
      "ribosome kinetics",
      "translation",
      "trna availability",
      "mistranslation frequencies"
    ]
  },
  {
    "id": "327",
    "title": "An efficient and spectrally accurate numerical method for computing dynamics of rotating Bose-Einstein condensates",
    "abstract": "In this paper, we propose an efficient and spectrally accurate numerical method for computing the dynamics of rotating Bose-Einstein condensates (BEC) in two dimensions (2D) and 3D based on the Gross-Pitaevskii equation (GPE) with an angular momentum rotation term. By applying a time-splitting technique for decoupling the nonlinearity and properly using the alternating direction implicit (ADI) technique for the coupling in the angular momentum rotation term in the GPE, at every time step, the GPE in rotational frame is decoupled into a nonlinear ordinary differential equation (ODE) and two partial differential equations with constant coefficients. This allows us to develop new time-splitting spectral methods for computing the dynamics of BEC in a rotational frame. The new numerical method is explicit, unconditionally stable, and of spectral accuracy in space and second-order accuracy in time. Moreover, it is time reversible and time transverse invariant, and conserves the position density in the discretized level if the GPE does. Extensive numerical results are presented to confirm the above properties of the new numerical method for rotating BEC in 2D and 3D.  ",
    "keywords": [
      "rotating bose-einstein condensates",
      "gross-pitaevskii equation",
      "angular momentum rotation",
      "time spitting"
    ]
  },
  {
    "id": "328",
    "title": "The entropy of traces in parallel computation",
    "abstract": "The following problem arises in thtr context of parallel computation: how many bits of information are required to specify any one element from an arbitrary (nonempty) X-subset of a set? We characterize optimal coding techniques for this problem. We calculate the asymptotic behavior of the amount of information necessary, and construct an algorithm that specifies an element from a subset in an optimal manner.",
    "keywords": [
      "coding",
      "energy",
      "parallel computation",
      "traces"
    ]
  },
  {
    "id": "329",
    "title": "Proteomic Technologies to Study Diseases of the Lymphatic Vascular System",
    "abstract": "Now that the human genome has been mapped, a new challenge has emerged",
    "keywords": [
      "proteomics",
      "cancer",
      "lymph",
      "lymphatic endothelium",
      "lymphedema",
      "laser capture microdissection",
      "protein microarrays",
      "seldi-tof-mass spectroscopy"
    ]
  },
  {
    "id": "330",
    "title": "leveraging eclipse for integrated model-based engineering of web service compositions",
    "abstract": "In this paper we detail the design and implementation of an Eclipse plug-in for an integrated, model-based approach, to the engineering of web service compositions. The plug-in allows a designer to specify a service's obligations for coordinated web service compositions in the form of Message Sequence Charts (MSCs) and then generate policies in the form of WS-CDL and services in the form of BPEL4WS. The approach uses finite state machine representations of web service compositions and service choreography rules, and assigns semantics to the distributed process interactions. The move towards implementing web service choreography requires design time verification of these service interactions to ensure that service implementations fulfill requirements for multiple interested partners before such compositions and choreographies are deployed. The plug-in provides a tool for integrated specification, formal modeling, animation and providing verification results from choreographed web service interactions. The LTSA-Eclipse (for Web Services) plug-in is publicly available, along with other plug-ins, at: http://www.doc.ic.ac.uk/ltsa.",
    "keywords": [
      "eclipse plug-in",
      "verification",
      "service design",
      "web service choreography",
      "standards",
      "validation",
      "implementation",
      "model checking"
    ]
  },
  {
    "id": "331",
    "title": "Targeted Recruitment of Histone Modifications in Humans Predicted by Genomic Sequences",
    "abstract": "Histone modifications are important epigenetic regulators and play a critical role in development. The targeting mechanism for histone modifications is complex and still incompletely understood. Here we applied a computational approach to predict genome-scale histone modification targets in humans by the genomic DNA sequences using a set of recent ChIP-seq data. We found that a number of histone modification marks could be predicted with high accuracy. On the other hand, the impact of DNA sequences for each mark is intrinsically different dependent upon the target-and tissue-specificity. Diverse patterns are associated with different repetitive elements. Unexpectedly, we found that non-overlapping, functionally opposite histone modification marks could share similar sequence features. We propose that these marks may target a common set of loci but are mutually exclusive and that the competition may be important for developmental control. Taken together, we show that our computational approach has provided new insights into the targeting mechanism of histone modifications.",
    "keywords": [
      "dna sequence",
      "histone modification",
      "human"
    ]
  },
  {
    "id": "332",
    "title": "Normal vector voting: Crease detection and curvature estimation on large, noisy meshes",
    "abstract": "This paper describes a robust method for crease detection and curvature estimation on large, noisy triangle meshes. We assume that these meshes are approximations of piecewise-smooth surfaces derived from range or medical imaging systems and thus may exhibit measurement or even registration noise. The proposed algorithm, which we call normal vector voting, uses an ensemble of triangles in the geodesic neighborhood of a vertex-instead of its simple umbrella neighborhood-to estimate the orientation and curvature of the original surface at that point. With the orientation information, we designate a vertex as either lying on a smooth surface, following a crease discontinuity, or having no preferred orientation. For vertices on a smooth surface, the curvature estimation yields both principal curvatures and principal directions while for vertices on a discontinuity we estimate only the curvature along the crease. The last case for no preferred orientation occurs when three or more surfaces meet to form a corner or when surface noise is too large and sampling density is insufficient to determine orientation accurately. To demonstrate the capabilities of the method, we present results for both synthetic and real data and compare these results to the G. Taubin (1995, in Proceedings of the Fifth International Conference on Computer Vision, pp. 902-907) algorithm. Additionally, we show practical results for several large mesh data sets that are the motivation for this algorithm. ",
    "keywords": [
      "curvature estimation",
      "normal vector estimation",
      "crease detection",
      "dense triangle meshes",
      "piecewise-smooth surfaces"
    ]
  },
  {
    "id": "333",
    "title": "acoustic super models for large scale video event detection",
    "abstract": "Given the exponential growth of videos published on the Internet, mechanisms for clustering, searching, and browsing large numbers of videos have become a major research area. More importantly, there is a demand for event detectors that go beyond the simple finding of objects but rather detect more abstract concepts, such as \"feeding an animal\" or a \"wedding ceremony\". This article presents an approach for event classification that enables searching for arbitrary events, including more abstract concepts, in found video collections based on the analysis of the audio track. The approach does not rely on speech processing, and is language-indepent, instead it generates models for a set of example query videos using a mixture of two types of audio features: Linear-Frequency Cepstral Coefficients and Modulation Spectrogram Features. This approach can be used in complement with video analysis and requires no domain specific tagging. Application of the approach to the TRECVid MED 2011 development set, which consists of more than 4000 random \"wild\" videos from the Internet, has shown a detection accuracy of 64% including those videos which do not contain an audio track.",
    "keywords": [
      "multimedia event detection",
      "trecvid",
      "audio processing"
    ]
  },
  {
    "id": "334",
    "title": "Morphological influence of the beam overlap in focused ion beam induced deposition using raster scan",
    "abstract": "Material addition using focused ion beam induced deposition (FIBID) is a well-established local deposition technology in microelectronic engineering. We investigated FIBID characteristics as a function of beam overlap using phenanthrene molecules. To initiate the localization of gas molecules, we irradiated the ion beams using a raster scan. We varied the beam overlap between ?900% and 50% by adjusting the pixel size from 300nm to 15nm. We discuss the changes in surface morphologies and deposition rates due to delocalization by the range effect of excited surface atoms, the divided structure by continuous effect from the raster scan, enhanced localization by discrete effect from replenished gas molecules, the competition between deposition and sputtering processes, and the change in processing time with scan speed (smaller overlap case).",
    "keywords": [
      "fibid",
      "raster scan",
      "phenanthrene gas",
      "carbon",
      "beam overlap"
    ]
  },
  {
    "id": "335",
    "title": "Content-aware rate allocation for efficient video streaming via dynamic network utility maximization",
    "abstract": "Nowadays it is vital to design robust mechanisms to provide QoS for multimedia applications as an integral part of the network traffic. The main goal of this paper is to provide an efficient rate control scheme to support content-aware video transmission mechanism with buffer underflow avoidance at the receiver in congested networks. Towards this, we introduce a content-aware time-varying utility function, in which the quality impact of video content is incorporated into its mathematical expression. Moreover, we analytically model the buffer requirements of video sources in two ways: first as constraints of the optimization problem to guarantee a minimum rate demand for each source, and second as a penalty function embedded as part of the objective function attempting to achieve the highest possible rate for each source. Then, using the proposed analytical model, we formulate a dynamic network utility maximization problem, which aims to maximize the aggregate hybrid objective function of sources subject to capacity and buffer constraints. Finally, using primaldual method, we solve DNUM problem and propose a distributed algorithm called CA-DNUM that optimally allocates the shared bandwidth to video streams. The experimental results demonstrate the efficacy and performance improvement of the proposed content-aware rate allocation algorithm for video sources in different scenarios.",
    "keywords": [
      "video streaming",
      "dynamic network utility maximization",
      "content-aware video utility model",
      "buffer underflow avoidance",
      "convex optimization"
    ]
  },
  {
    "id": "336",
    "title": "INVERSE PROBLEM FOR FRICATIVES",
    "abstract": "Articulatory parameters, vocal tract shape and cross-sectional area function were determined from fricative spectra. A model of fricative generation was used for providing acoustical constraints for an optimization procedure with muscles work as the criterion of optimality. A distance between spectra was measured with the use of the Cauchy-Bounjakovsky non-equality. A proper initial approximation of articulatory parameters is required to obtain an accurate and stable solution of the inverse problem.",
    "keywords": [
      "speech",
      "inverse problem",
      "vocal tract shape",
      "fricatives",
      "optimization"
    ]
  },
  {
    "id": "337",
    "title": "efficient web usage mining process for sequential patterns",
    "abstract": "The tremendous growth in volume of web usage data results in the boost of web mining research with focus on discovering potentially useful knowledge from web usage data. This paper presents a new web usage mining process for finding sequential patterns in web usage data which can be used for predicting the possible next move in browsing sessions for web personalization. This process consists of three main stages: preprocessing web access sequences from the web server log, mining preprocessed web log access sequences by a tree-based algorithm, and predicting web access sequences by using a dynamic clustering-based model. It is designed based on the integration of the dynamic clustering-based Markov model with the Pre-Order Linked WAP-Tree Mining (PLWAP) algorithm to enhance mining performance. The proposed mining process is verified by experiments with promising results.",
    "keywords": [
      "sequential patterns",
      "web usage mining ",
      "pre-order linked wap-tree ",
      "markov model",
      "web access patterns "
    ]
  },
  {
    "id": "338",
    "title": "experimenting with an organic metaphor and hypervisual links for the interface of a video collection",
    "abstract": "In this paper we describe the prototype of an archive of short movies. The project proposes two original solutions for implementing the interface of this archive: an organic metaphor and a hypervisual navigation mechanism.",
    "keywords": [
      "user interfaces",
      "metaphors",
      "hyperlinks",
      "hypervideo"
    ]
  },
  {
    "id": "339",
    "title": "A Randomized Exhaustive Propositionalization Approach for Molecule Classification",
    "abstract": "Drug discovery is the process of designing compounds that have desirable properties, such as activity and nontoxicity. Molecule classification techniques are used along with this process to predict the properties of the compounds to expedite their testing. Ideally, the classification rules found should be accurate and reveal novel chemical properties, but current molecule representation techniques lead to less-than-adequate accuracy and knowledge discovery. This work extends the propositionalization approach recently proposed for multirelational data mining in two ways: it generates expressive attributes exhaustively, and it uses randomization to sample a limited set of complex (\"deep\") attributes. Our experimental tests show that the procedure is able to generate meaningful and interpretable attributes from molecular structural data, and that these features are effective for classification purposes.",
    "keywords": [
      "relational learning",
      "propositionalization",
      "molecule classification",
      "drug discovery"
    ]
  },
  {
    "id": "340",
    "title": "Error propagation analysis for underwater cooperative multi-hop communications",
    "abstract": "The potential gains of cooperative Communication and multi-hopping in underwater acoustic communication channels is examined. In particular, performance of such systems is compared to a comparable single hop system (direct transmission) with a common transmission distance. The effects of error propagation with decode and forward at each relay are explicitly treated and it is shown that strong gains can be achieved by multi-hopping (an effective SNR gain) is well as cooperation, which contributes to a diversity gain. We observe that cooperative diversity gains are retained even when considering error propagation. The analysis is done via a Markov chain analysis for both regular linear and grid networks. Our initial analysis is for single path channels; the effects of inter symbol interference as well as multi.-user interference are examined. It is found that due to the strong decay of signal power as a function of transmission distance, multi-user interference is not as significant as inter-symbol interference. In both cases, cooperative and multi-hopping gains are observed. ",
    "keywords": [
      "underwater acoustic communications",
      "cooperative communications",
      "multi-hopped networks",
      "error propagation analysis",
      "fading multipath channels",
      "diversity",
      "sensor networks"
    ]
  },
  {
    "id": "341",
    "title": "evolutionary algorithms for reasoning in fuzzy description logics with fuzzy quantifiers",
    "abstract": "The task of reasoning with fuzzy description logics with fuzzy quantification is approached by means of an evolutionary algorithm. An essential ingredient of the proposed method is a heuristic, implemented as an intelligent mutation operator, which observes the evolutionary process and uses the information gathered to guess at the mutations most likely to bring about an improvement of the solutions. The viability of the method is demonstrated by applying it to reasoning on a resource sheduling problem.",
    "keywords": [
      "fuzzy quantification",
      "fuzzy logic",
      "evolutionary algorithms",
      "description logics"
    ]
  },
  {
    "id": "342",
    "title": "Deadline-based scheduling in support of real-time data delivery",
    "abstract": "The use of deadline-based scheduling in support of real-time delivery of application data units (ADUs) in a packet-switched network is investigated. Of interest is priority scheduling where a packet with a smaller ratio of T/H (time until delivery deadline over number of hops remaining) is given a higher priority. We refer to this scheduling algorithm as the T/H algorithm. T/H has time complexity of O(logN) for a backlog of N packets and was shown to achieve good performance in terms of the percentage of ADUs that are delivered on-time. We develop a new and efficient algorithm, called T/H?p, that has O(1) time complexity. The performance difference of T/H, T/H?p and FCFS are evaluated by simulation. Implementations of T/H and T/H?p in high-speed routers are also discussed. We show through simulation that T/H?p is superior to FCFS but not as good as T/H. In view of the constant time complexity, T/H?p is a good candidate for high-speed routers when both performance and implementation cost are taken into consideration.",
    "keywords": [
      "real-time data delivery",
      "deadline-based scheduling",
      "packet-switched networks",
      "performance evaluation"
    ]
  },
  {
    "id": "343",
    "title": "Space hierarchy theorem revised",
    "abstract": "We show that, for an arbitrary function h(n) and each recursive function e(n), that are separated by a nondeterministically fully space constructible g(n), such that h(n) E Q(g(n)) but l(n) Q(g(n)), there exists a unary language L in NSPACE(h(n)) that is not contained in NSPACE(l(n)). The same holds for the deterministic case. The main contribution to the well-known Space Hierarchy Theorem is that (i) the language L separating the two space classes is unary (tally), (ii) the hierarchy is independent of whether h(n) or l(n) are in Omega(log n) or in o(log n), (iii) the functions h(n) or l(n) themselves need not be space constructible nor monotone increasing, (iv) the hierarchy is established both for strong and weak space complexity classes. This allows us to present unary languages in such complexity classes as, for example, NSPACE(log log n (.) log* n)  NSPACE(log log n), using a plain diagonalization.  ",
    "keywords": [
      "computational complexity",
      "space complexity"
    ]
  },
  {
    "id": "344",
    "title": "Non-dominated sorting genetic algorithm with decomposition to solve constrained optimisation problems",
    "abstract": "Pareto-domination was adopted to handle not only trade-off between objective and constraints but also trade-off between convergence and diversity on solving a constrained optimisation problem (COP) in this paper like many other researchers. But there are some differences. This paper converts a COP into an equivalent dynamic constrained multi-objective optimisation problem (DCMOP) first, then dynamic version of non-dominated sorting genetic algorithm with decomposition (NSGA/D) is designed to solve the equivalent DCMOP, consequently solve the COP. A key issue for the NSGA/D working effectively is that the environmental change should not destroy the feasibility of the population. With a feasible population, the NSGA/D could solve well the DCMOP just as a MOEA usually can solve well an unconstrained MOP. Experimental results show that the NSGA/D outperforms or performs similarly to other state-of-the-art algorithms referred to in this paper, especially in global search.",
    "keywords": [
      "evolutionary algorithm",
      "constrained optimisation",
      "multi-objective optimisation",
      "dynamic multi-objective optimisation",
      "dynamic optimisation"
    ]
  },
  {
    "id": "345",
    "title": "selecting for evolvable representations",
    "abstract": "Evolutionary algorithms tend to produce solutions that are not evolvable: Although current fitness may be high, further search is impeded as the effects of mutation and crossover become increasingly detrimental. In nature, in addition to having high fitness, organisms have evolvable genomes: phenotypic variation resulting from random mutation is structured and robust. Evolvability is important because it allows the population to produce meaningful variation, leading to efficient search. However, because evolvability does not improve immediate fitness, it must be selected for indirectly. One way to establish such a selection pressure is to change the fitness function systematically. Under such conditions, evolvability emerges only if the representation allows manipulating how genotypic variation maps onto phenotypic variation and if such manipulations lead to detectable changes in fitness. This research forms a framework for understanding how fitness function and representation interact to produce evolvability. Ultimately evolvable encodings may lead to evolutionary algorithms that exhibit the structured complexity and robustness found in nature.",
    "keywords": [
      "modularity",
      "estimation-of-distribution",
      "representations",
      "genetic algorithms",
      "indirect encodings",
      "evolvability",
      "development"
    ]
  },
  {
    "id": "346",
    "title": "Modeling biocomplexity  actors, landscapes and alternative futures",
    "abstract": "Increasingly, models (and modelers) are being asked to address the interactions between human influences, ecological processes, and landscape dynamics that impact many diverse aspects of managing complex coupled human and natural systems. These systems may be profoundly influenced by human decisions at multiple spatial and temporal scales, and the limitations of traditional process-level ecosystems modeling approaches for representing the richness of factors shaping landscape dynamics in these coupled systems has resulted in the need for new analysis approaches. New tools in the areas of spatial data management and analysis, multicriteria decision-making, individual-based modeling, and complexity science have all begun to impact how we approach modeling these systems. The term biocomplexity has emerged as a descriptor of the rich patterns of interactions and behaviors in human and natural systems, and the challenges of analyzing biocomplex behavior is resulting in a convergence of approaches leading to new ways of understanding these systems. Important questions related to system vulnerability and resilience, adaptation, feedback processing, cycling, non-linearities and other complex behaviors are being addressed using models employing new representational approaches to analysis. The complexity inherent in these systems challenges the modeling community to provide tools that capture sufficiently the richness of human and ecosystem processes and interactions in ways that are computationally tractable and understandable. We examine one such tool, EvoLand, which uses an actor-based approach to conduct alternative futures analyses in the Willamette Basin, Oregon.",
    "keywords": [
      "complexity",
      "resilience",
      "adaptation",
      "simulation"
    ]
  },
  {
    "id": "347",
    "title": "Computerised video tracking, movement analysis and behaviour recognition in insects",
    "abstract": "The need for automating behavioural observations and the evolution of systems developed for that purpose are outlined. Automatic video tracking systems enable behaviour to be studied in a reliable and consistent way, and over longer time periods than if it is manually recorded. To overcome limitations of currently available systems and to meet researchers' needs as these have been identified, we have developed an integrated system (EthoVision) for automatic recording of activity, movement and interactions of insects. The system is described here, with special emphasis on file management, experiment design, arena and zone definition, object detection, experiment control, visualisation of tracks and calculation of analysis parameters. A review of studies using our system is presented, to demonstrate its use in a variety of entomological applications. This includes research on beetles, fruit flies, soil insects, parasitic wasps, predatory mites, ticks, and spiders. Finally, possible future directions for development are discussed.",
    "keywords": [
      "video tracking",
      "movement analysis",
      "behaviour recognition",
      "ethovision"
    ]
  },
  {
    "id": "348",
    "title": "A novel similarity measure on intuitionistic fuzzy sets with its applications",
    "abstract": "The intuitionistic fuzzy set, as a generation of Zadeh fuzzy set, can express and process uncertainty much better, by introducing hesitation degree. Similarity measures between intuitionistic fuzzy sets (IFSs) are used to indicate the similarity degree between the information carried by IFSs. Although several similarity measures for intuitionistic fuzzy sets have been proposed in previous studies, some of those cannot satisfy the axioms of similarity, or provide counter-intuitive cases. In this paper, we first review several widely used similarity measures and then propose new similarity measures. As the consistency of two IFSs, the proposed similarity measure is defined by the direct operation on the membership function, non-membership function, hesitation function and the upper bound of membership function of two IFS, rather than based on the distance measure or the relationship of membership and non-membership functions. It proves that the proposed similarity measures satisfy the properties of the axiomatic definition for similarity measures. Comparison between the previous similarity measures and the proposed similarity measure indicates that the proposed similarity measure does not provide any counter-intuitive cases. Moreover, it is demonstrated that the proposed similarity measure is capable of discriminating the difference between patterns.",
    "keywords": [
      "intuitionistic fuzzy set",
      "distance measure",
      "similarity measure",
      "pattern recognition"
    ]
  },
  {
    "id": "349",
    "title": "An object-oriented simulation framework for real-time control of automated flexible manufacturing systems",
    "abstract": "This paper describes an object-oriented simulation approach for the design of a flexable, manufacturing system that allows the implementation of control logic during the system design phase. The object-oriented design approach is built around the formal theory of supervisory control based on Finite Automata. The formalism is used to capture inter-object relationships that are difficult to identify in the object-oriented design approach. The system resources are modeled as object classes based on the events that have to be monitored for real-time control. Real-time control issues including deadlock resolution, resource failures in various modes of operation and recovery from failures while sustaining desirable logical system properties are integrated into the logical design for simulating the supervisory controller.  ",
    "keywords": [
      "object-oriented simulation",
      "flexible manufacturing systems",
      "real-time control"
    ]
  },
  {
    "id": "350",
    "title": "A fully distributed architecture for large scale workflow enactment",
    "abstract": "Standard client-server workflow management systems are usually designed as client-server systems. The central server is responsible for the coordination of the workflow execution and, in some cases, may manage the activities database. This centralized control architecture may represent a single point of failure, which compromises the availability of the system. We propose a fully distributed and configurable architecture for workflow management systems. It is based on the idea that the activities of a case (an instance of the process) migrate from host to host, executing the workflow tasks, following a process plan. This core architecture is improved with the addition of other distributed components so that other requirements for Workflow Management Systems, besides scalability, are also addressed. The components of the architecture were tested in different distributed and centralized configurations. The ability to configure the location of components and the use of dynamic allocation of tasks were effective for the implementation of load balancing policies.",
    "keywords": [
      "large-scale workflow management systems",
      "fully distributed workflow architectures",
      "corba workflow implementation",
      "and mobile agents"
    ]
  },
  {
    "id": "351",
    "title": "Finite-difference model for one-dimensional electro-osmotic consolidation",
    "abstract": "Small strain consolidation theories treat soil properties as being constant and uniform in the course of consolidation, which is not true in the case of electro-osmosis-induced consolidation practices. Electro-osmotic consolidation leads to large strain, which physically and electro-chemically affects to a non-negligible extent the nonlinear changes of the soil properties. For the nonlinear changes, iterative computations provide a mathematical approximation of the soil consolidation when the time steps and spatial geometry are intensively meshed. In this context, this paper presents a finite-difference model, EC1, for one-dimensional electro-osmotic consolidation, and this model is developed based on a fixed Eulerian co-ordinate system and uses a piecewise linear approximation. The model is able to account for the large-strain-induced nonlinear changes of the physical and electro-chemical properties in a compressible mass subjected to electro-osmotic consolidation and to predict the consolidation characteristics of the compressible mass. EC1 is verified against exact analytical solutions and test results obtained from an experimental program. Example problems are illustrated with respect to the numerical solutions of large-strain electro-osmotic consolidation.",
    "keywords": [
      "electro-osmosis",
      "consolidation",
      "large strain",
      "nonlinear",
      "electrical potential",
      "pore pressure"
    ]
  },
  {
    "id": "352",
    "title": "Nonlinear characteristics of on-chip spiral inductors under high RF power",
    "abstract": "This paper explores silicon CMOS on-chip spiral inductors performance degradation under high RF power. A novel methodology to calibrate and characterize on-chip spiral inductor with large signal inputs (high/medium power) is presented. Experiments showed 12% degradation of quality factor in a particular inductor design when 34dBm RF power was applied. The degradation of quality factor of inductor can be attributed to a local self heating effect. Thermal imaging of such an inductor under high RF power validates the hypothesis.",
    "keywords": [
      "high rf power",
      "on-chip inductor",
      "quality factor"
    ]
  },
  {
    "id": "353",
    "title": "On d-Multiplicative Secret Sharing",
    "abstract": "A multiplicative secret sharing scheme allows players to multiply two secret-shared field elements by locally converting their shares of the two secrets into an additive sharing of their product. Multiplicative secret sharing serves as a central building block in protocols for secure multiparty computation (MPC). Motivated by open problems in the area of MPC, we introduce the more general notion of d-multiplicative secret sharing, allowing to locally multiply d shared secrets, and study the type of access structures for which such secret sharing schemes exist. While it is easy to show that d-multiplicative schemes exist if no d unauthorized sets of players cover the whole set of players, the converse direction is less obvious for da parts per thousand yen3. Our main result is a proof of this converse direction, namely that d-multiplicative schemes do not exist if the set of players is covered by d unauthorized sets. In particular, t-private d-multiplicative secret sharing among k players is possible if and only if k > dt. Our negative result holds for arbitrary (possibly inefficient or even nonlinear) secret sharing schemes and implies a limitation on the usefulness of secret sharing in the context of MPC. Its proof relies on a quantitative argument inspired by communication complexity lower bounds.",
    "keywords": [
      "secret sharing",
      "secure multiparty computation",
      "secure multiplication"
    ]
  },
  {
    "id": "354",
    "title": "shortest path amidst disc obstacles is computable",
    "abstract": "An open question in Exact Geometric Computation is whether there re transcendental computations that can be made \"geometrically exact\".Perhaps the simplest such problem in computational geometry is that of computing the shortest obstacle-avoiding path between two points p, q in the plane, where the obstacles re collection of n discs.This problem can be solved in O (n 2 log n)time in the Real RAM model, but nothing was known about its computability in the standard (Turing) model of computation. We first show the Turing-computability of this problem,provided the radii of the discs are rationally related. We make the usual assumption that the numerical input data are real algebraic numbers. By appealing to effective bounds from transcendental number theory, we further show single-exponential time upper bound when the input numbers are rational.Our result ppears to be the first example of non-algebraic combinatorial problem which is shown computable. It is also rare example of transcendental number theory yielding positive computational results.",
    "keywords": [
      "guaranteed precision computation",
      "exponential complexity",
      "real ram model",
      "exact geometric computation",
      "robust numerical algorithms",
      "disc obstacles",
      "shortest path"
    ]
  },
  {
    "id": "355",
    "title": "A uniform approach to constraint-solving for lists, multisets, compact lists, and sets",
    "abstract": "Lists, multisets, and sets are well-known data structures whose usefulness is widely recognized in various areas of computer science. They have been analyzed from an axiomatic point of view with a parametric approach in Dovier et al. [1998], where the relevant unification algorithms have been developed. In this article, we extend these results considering more general constraints, namely, equality and membership constraints and their negative counterparts.",
    "keywords": [
      "theory",
      "algorithms",
      "membership and equality constraints",
      "lists",
      "multisets",
      "compact lists",
      "sets"
    ]
  },
  {
    "id": "356",
    "title": "a competency framework for the stakeholders of a software process improvement initiative",
    "abstract": "The competencies (a set of specific knowledges, skills, attitudes and behaviors; e.g. stress handling, commitment, collaboration and identification of conflicts) of the employees of software organizations are a fundamental element for the success of a Software Process Improvement (SPI) initiative. We performed three case studies to identify the competencies required for the stakeholders in an SPI initiative. To identify these competencies, we observed the activities that each stakeholder performs and the interactions among them. We also identified the competencies that are required to perform those activities. We performed a classification of the identified competencies and integrated them into a framework. This framework defines the competencies for seven roles involved in an SPI initiative and defines the level of expertise required by each role for each competency. To evaluate the framework, we performed ten interviews and two empirical tests. Preliminary results show that this framework is relevant in SPI initiatives, the use of this framework can raise the awareness about the competencies, and it can support some SPI activities.",
    "keywords": [
      "software process improvement",
      "skills",
      "stakeholders",
      "knowledge",
      "competency framework",
      "competencies",
      "spi",
      "behavior",
      "roles"
    ]
  },
  {
    "id": "357",
    "title": "Remote information concentration via (W) state: reverse of ancilla-free phase-covariant telecloning",
    "abstract": "In this paper, we investigate generalized remote information concentration as the reverse process of ancilla-free phase-covariant telecloning (AFPCT) which is different from the reverse process of optimal universal telecloning. It is shown that the quantum information via (1rightarrow 2) AEPCT procedure can be remotely concentrated back to a single qubit with a certain probability by utilizing (non-)maximally entangled (W) states as quantum channels. Our protocols are the generalization of Wangs scheme (Open J Microphys 3:1821. doi:10.?4236/?ojm.?2013.?31004, 2013). And von Neumann measure and positive operator-valued measurement are performed in the maximal and non-maximal cases respectively. Relatively the former, the dimension of measurement space in the latter is greatly reduced. It makes the physical realization easier and suitable.",
    "keywords": [
      "ancilla-free phase-covariant teleclong",
      "remote information concentration",
      "three-qubit asymmetric entangled ",
      "nonmaximally entangled  state",
      "projective measurement",
      "povm"
    ]
  },
  {
    "id": "358",
    "title": "Simulation of continuous fibre reinforced thermoplastic forming using a shell finite element with transverse stress",
    "abstract": "A shell finite element with transverse stress is presented in this paper in order to simulate the forming of thermoplastic composites reinforced with continuous fibres. It is shown by an experimental work that many porosities occurs through the thickness of the composite during the heating and the forming process. Consequently the reconsolidation i.e. the porosity removing by applying a compressive stress through the thickness is a main point of the process. The presented shell finite element keeps the five degrees of freedom of the standard shell elements and adds a sixth one which is the variation in thickness. A locking phenomenon is avoided by uncoupling bending and pinching in the material law. A set of classical validation tests will prove the efficiency of this approach. Finally a forming process is simulated. It shows that the computed transverse stresses are in good agreement with porosity removing in the experiments.",
    "keywords": [
      "composites",
      "forming",
      "porosities",
      "shell finite element",
      "transverse stress",
      "locking"
    ]
  },
  {
    "id": "359",
    "title": "twiki a collaboration tool for the lhc",
    "abstract": "At the European Laboratory for High Energy Physics, CERN[1], the Large Hadron Collider (LHC)[2] accelerator is colliding beams of protons at energies of 3.5 TeV, recreating conditions close to those at the origin of the Universe. The four main LHC experiments, Alice, Atlas, CMS and LHCb are complex detectors with millions of output channels. These experiment detectors, \"large as cathedrals\", have been designed, built and are now operated by collaborations of physicists from universities and research institutes spread across the world. Wikis are a perfect match to the collaborative nature of CERN experiments and since TWiki[3] was installed at CERN in 2003 it has grown in popularity and the statistics from April 2011 show nearly 10000 registered editors and about 110000 topics (Figure 1). Since the start-up of the LHC more and more users are accessing TWiki requiring better server performance as well as finer control for read and write access and more features. This paper discusses the evolution of the use of TWiki at CERN.",
    "keywords": [
      "twiki",
      "cern",
      "lhc"
    ]
  },
  {
    "id": "360",
    "title": "A glimpse of symbolic-statistical modeling by PRISM",
    "abstract": "We give a brief overview of a logic-based symbolic modeling language PRISM which provides a unified approach to generative probabilistic models including Bayesian networks, hidden Markov models and probabilistic context free grammars. We include some experimental result with a probabilistic context free grammar extracted from the Penn Treebank. We also show EM learning of a probabilistic context free graph grammar as an example of exploring a new area.",
    "keywords": [
      "symbolic-statistical modeling",
      "prism",
      "probabilistic context free grammar"
    ]
  },
  {
    "id": "361",
    "title": "Improving validity and reliability in longitudinal case study timelines",
    "abstract": "Management Information Systems researchers rely on longitudinal case studies to investigate a variety of phenomena such as systems development, system implementation, and information systems-related organizational change. However, insufficient attention has been spent on understanding the unique validity and reliability issues related to the timeline that is either explicitly or implicitly required in a longitudinal case study. In this paper, we address three forms of longitudinal timeline validity: time unit validity (which deals with the question of how to segment the timeline - weeks, months, years, etc.), time boundaries validity (which deals with the question of how long the timeline should be), and time period validity (which deals with the issue of which periods should be in the timeline). We also examine timeline reliability, which deals with the question of whether another judge would have assigned the same events to the same sequence, categories, and periods. Techniques to address these forms of longitudinal timeline validity include: matching the unit of time to the pace of change to address time unit validity, use of member checks and formal case study protocol to address time boundaries validity, analysis of archival data to address both time unit and time boundary validity, and the use of triangulation to address timeline reliability. The techniques should be used to design, conduct, and report longitudinal case studies that contain valid and reliable conclusions.",
    "keywords": [
      "qualitative methods",
      "longitudinal case study",
      "timeline validity",
      "timeline reliability"
    ]
  },
  {
    "id": "362",
    "title": "Information privacy: Measuring individuals' concerns about organizational practices",
    "abstract": "Information privacy has been called one of the most important ethical issues of the information age. Public opinion polls show rising levels of concern about privacy among Americans. Against this backdrop, research into issues associated with information privacy is increasing. Based on a number of preliminary studies, it has become apparent that organizational practices, individuals' perceptions of these practices, and societal responses are inextricably linked in many ways. Theories regarding these relationships are slowly emerging. Unfortunately, researchers attempting to examine such relationships through confirmatory empirical approaches may be impeded by the lack of validated instruments for measuring individuals' concerns about organizational information privacy practices. To enable future studies in the information privacy research stream, we developed and validated an instrument that identifies and measures the primary dimensions of individuals' concerns about organizational information privacy practices. The development process included examinations of privacy literature; experience surveys and focus groups; and the use of expert judges. The result was a parsimonious 15-item instrument with four subscales tapping into dimensions of individuals' concerns about organizational information privacy practices. The instrument was rigorously tested and validated across several heterogenous populations, providing a high degree of confidence in the scales' validity, reliability, and generalizability.",
    "keywords": [
      "privacy",
      "lisrel",
      "ethical issues",
      "measures",
      "reliability",
      "validity"
    ]
  },
  {
    "id": "363",
    "title": "Using genetic programming and simulation to learn how to dynamically adapt the number of cards in reactive pull systems",
    "abstract": "We show how to learn dynamically to adapt the number of cards in real time in token-based pull systems. We propose a Simulation-based Genetic Programming approach which does not need training sets. We illustrate how the approach can be implemented using Arena and ?GP. A reactive ConWIP example show the efficiency of the approach and of the knowledge extracted. The resulting decision tree can be used online by production managers or for self-adaptation issues.",
    "keywords": [
      "kanban",
      "conwip",
      "manufacturing systems",
      "reactive pull systems",
      "self-adaptive systems",
      "learning",
      "simulation",
      "genetic programming"
    ]
  },
  {
    "id": "364",
    "title": "A development and verification framework for the SegBus platform",
    "abstract": "We describe the creation of a development framework for a platform-based design approach, in the context of the SegBus platform. The work intends to provide automated procedures for platform build-up and application mapping. The solution is based on a model-based process and heavily employs the UML. We develop a Domain Specific Language to support the platform modeling. An emulator is consequently introduced to allow an as much as possible accurate performance estimation of the solution, at high abstraction levels. Automated execution schedule generation is also featured. The resulting framework is applied to build actual design solutions for a MP3-decoder application.",
    "keywords": [
      "model-based engineering",
      "domain-specific languages",
      "system emulation",
      "code generation",
      "system-on-chip"
    ]
  },
  {
    "id": "365",
    "title": "A neuro-fuzzy controller for speed control of a permanent magnet synchronous motor drive",
    "abstract": "This paper introduces a neuro-fuzzy controller (NFC) for the speed control of a PMSM. A four layer neural network (NN) is used to adjust input and output parameters of membership functions in a fuzzy logic controller (FLC). The back propagation learning algorithm is used for training this network. The performance of the proposed controller is verified by both simulations and experiments. The hardware implementation of the controllers is made using a TMS320F240 DSP. The results are compared with the results obtain from a Proportional+Integral (PI) controller. Simulation and experimental results indicate that the proposed NFC is reliable and effective for the speed control of the PMSM over a wide range of operations of the PMSM drive.",
    "keywords": [
      "fuzzy logic control",
      "neural networks",
      "permanent magnet synchronous motor drive"
    ]
  },
  {
    "id": "366",
    "title": "On the sorting-complexity of suffix tree construction",
    "abstract": "The suffix tree of a string is the fundamental data structure of combinatorial pattern matching. We present a recursive technique for building suffix trees that yields optimal algorithms in different computational models. Sorting is an inherent bottleneck in building suffix trees and our algorithms match the sorting lower bound. Specifically, we present the following results. (1) Weiner [1973], who introduced the data structure, gave an optimal O(n)-time algorithm for building the suffix tree of an n-character string drawn from a constant-size alphabet. In the comparison model, there is a trivial n(n log n)-time lower bound based on sorting, and Weiner's algorithm matches this bound. For integer alphabets, the fastest known algorithm is the O(n log n) time comparison-based algorithm, but no super-linear lower bound is known. Closing this gap is the main open question in stringology. We settle this open problem by giving a linear time reduction to sorting for building suffix trees. Since sorting is a lower-bound for building suffix trees, this algorithm is time-optimal in every alphabet model, in particular, for an alphabet consisting of integers in a polynomial range we get the first known linear-time algorithm. (2) All previously known algorithms for building suffix trees exhibit a marked absence of locality of reference, and thus they tend to elicit many page faults (I/Os) when indexing very long strings. They are therefore unsuitable for building suffix trees in secondary storage devices, where I/Os dominate the overall computational cost. We give a linear-I/O reduction to sorting for suffix tree construction. Since sorting is a trivial I/O-lower bound for building suffix trees, our algorithm is I/O-optimal.",
    "keywords": [
      "algorithms",
      "design",
      "theory",
      "dam model",
      "external-memory data structures",
      "ram model",
      "sorting complexity",
      "suffix array",
      "suffix tree"
    ]
  },
  {
    "id": "367",
    "title": "Quasi-BirthDeath Processes, Tree-Like QBDs, Probabilistic 1-Counter Automata, and Pushdown Systems",
    "abstract": "We begin by observing that (discrete-time) Quasi-BirthDeath Processes (QBDs) are equivalent, in a precise sense, to probabilistic 1-Counter Automata (p1CAs), and both Tree-Like QBDs (TL-QBDs) and Tree-Structured QBDs (TS-QBDs) are equivalent to both probabilistic Pushdown Systems (pPDSs) and Recursive Markov Chains (RMCs). We then proceed to exploit these connections to obtain a number of new algorithmic upper and lower bounds for central computational problems about these models. Our main result is this: for an arbitrary QBD, we can approximate its termination probabilities (i.e.,its G matrix) to within i bits of precision (i.e.,within additive error 1/2i 1 / 2 i ), in time polynomial in both the encoding size of the QBD and in i , in the unit-cost rational arithmetic RAM model of computation. Specifically, we show that a decomposed Newtons method can be used to achieve this. We emphasize that this bound is very different from the well-known linear/quadratic convergence of numerical analysis, known for QBDs and TL-QBDs, which typically gives no constructive bound in terms of the encoding size of the system being solved. In fact, we observe (based on recent results) that for the more general TL-QBDs such a polynomial upper bound on Newtons method fails badly. Our upper bound proof for QBDs combines several ingredients: a detailed analysis of the structure of 1-Counter Automata, an iterative application of a classic condition number bound for errors in linear systems, and a very recent constructive bound on the performance of Newtons method for strongly connected monotone systems of polynomial equations. We show that the quantitative termination decision problem for QBDs (namely, is Gu,v?1/2 G u , v ? 1 / 2 ?) is at least as hard as long-standing open problems in the complexity of exact numerical computation, specifically the square-root sum problem. On the other hand, it follows from our earlier results for RMCs that any non-trivial approximation of termination probabilities for TL-QBDs is sqrt-root-sum-hard.",
    "keywords": [
      "quasi-birthdeath processes",
      "tree-like qbds",
      "probabilistic 1-counter automata",
      "pushdown systems",
      "newtons method"
    ]
  },
  {
    "id": "368",
    "title": "Composite kernel learning",
    "abstract": "The Support Vector Machine is an acknowledged powerful tool for building classifiers, but it lacks flexibility, in the sense that the kernel is chosen prior to learning. Multiple Kernel Learning enables to learn the kernel, from an ensemble of basis kernels, whose combination is optimized in the learning process. Here, we propose Composite Kernel Learning to address the situation where distinct components give rise to a group structure among kernels. Our formulation of the learning problem encompasses several setups, putting more or less emphasis on the group structure. We characterize the convexity of the learning problem, and provide a general wrapper algorithm for computing solutions. Finally, we illustrate the behavior of our method on multi-channel data where groups correspond to channels.",
    "keywords": [
      "supervized learning",
      "support vector machine",
      "kernel learning",
      "structured kernels",
      "feature selection and sparsity"
    ]
  },
  {
    "id": "369",
    "title": "recent progress in linear algebra and lattice basis reduction",
    "abstract": "A general goal concerning fundamental linear algebra problems is to reduce the complexity estimates to essentially the same as that of multiplying two matrices (plus possibly a cost related to the input and output sizes). Among the bottlenecks one usually finds the questions of designing a recursive approach and mastering the sizes of the intermediately computed data. In this talk we are interested in two special cases of lattice basis reduction. We consider bases given by square matrices over K[x] or Z, with, respectively, the notion of reduced form and LLL reduction. Our purpose is to introduce basic tools for understanding how to generalize the Lehmer and Knuth-Schnhage gcd algorithms for basis reduction. Over K[x] this generalization is a key ingredient for giving a basis reduction algorithm whose complexity estimate is essentially that of multiplying two polynomial matrices. Such a problem relation between integer basis reduction and integer matrix multiplication is not known. The topic receives a lot of attention, and recent results on the subject show that there might be room for progressing on the question.",
    "keywords": [
      "polynomial matrix",
      "matrix reduction",
      "lll basis reduction",
      "euclidean lattice"
    ]
  },
  {
    "id": "370",
    "title": "quantifying content consistency improvements through opportunistic contacts",
    "abstract": "Contacts between mobile users provide opportunities for data updates that supplement infrastructure-based mechanisms. While the benefits of such opportunistic sharing are intuitive, quantifying the capacity increase they give rise to is challenging because both contact rates and contact graphs depend on the structure of the social networks users belong to. Furthermore, social connectivity influences not only users' interests, i.e., the content they own, but also their willingness to share data with others. All these factors can have a significant effect on the capacity gains achievable through opportunistic contacts. This paper's main contribution is in developing a tractable model for estimating such gains in a content update system, where content originates from a server along multiple channels, with blocks of information in each channel updated at a certain rate, and users differ in their contact graphs, interests, and willingness to share content, e.g., only to the members of their own social networks. We establish that the added capacity available to improve content consistency through opportunistic sharing can be obtained by solving a convex optimization problem. The resulting optimal policy is evaluated using traces reflecting contact graphs in different social settings and compared to heuristic policies. The evaluation demonstrates the capacity gains achievable through opportunistic sharing, and the impact on those gains of the structure of the underlying social network.",
    "keywords": [
      "consistency",
      "optimization",
      "dynamic content",
      "dissemination",
      "delay-tolerant networks",
      "social networks"
    ]
  },
  {
    "id": "371",
    "title": "Study of the communication distance of a MEMS Pressure Sensor Integrated in a RFID Passive Tag",
    "abstract": "The performance of a MEMS (Micro Electro-Mechanical Systems) Sensor in a RFID system has been calculated, simulated and analyzed. It documents the viability from the power consumption point of view- of integrating a MEMS sensor in a passive tag maintaining its long range. The wide variety of sensors let us specify as many applications as the imagination is able to create. The sensor tag works without battery, and it is remotely powered through a commercial reader accomplishing the EPC standard Class 1 Gen 2. The key point is the integration in the tag of a very low power consumption pressure MEMS sensor. The power consumption of the sensor is 12.5 mu W. The specifically developed RFID CMOS passive module, with an integrated temperature sensor, is able to communicate up to 2.4 meters. Adding the pressure MEMS sensor - an input capacity, a maximum range of 2 meters can be achieved between the RFID sensor tag and a commercial reader (typical reported range for passive pressure sensors are in the range of a few centimeters). The RFID module has been fabricated with a CMOS process compatible with a bulk micromachining MEMS process. So, the feasibility of a single chip is presented.",
    "keywords": [
      "radiofrequency identification",
      "sensor systems",
      "low power electronics",
      "wireless sensor networks"
    ]
  },
  {
    "id": "372",
    "title": "A hardware architecture for real-time image compression using a searchless fractal image coding method",
    "abstract": "In this paper we present a novel hardware architecture for real-time image compression implementing a fast, searchless iterated function system (SIFS) fractal coding method. In the proposed method and corresponding hardware architecture, domain blocks are fixed to a spatially neighboring area of range blocks in a manner similar to that given by Furao and Hasegawa. A quadtree structure, covering from 3232 blocks down to 22 blocks, and even to single pixels, is used for partitioning. Coding of 22 blocks and single pixels is unique among current fractal coders. The hardware architecture contains units for domain construction, zig-zag transforms, range and domain mean computation, and a parallel domain-range match capable of concurrently generating a fractal code for all quadtree levels. With this efficient, parallel hardware architecture, the fractal encoding speed is improved dramatically. Additionally, attained compression performance remains comparable to traditional search-based and other searchless methods. Experimental results, with the proposed hardware architecture implemented on an Altera APEX20K FPGA, show that the fractal encoder can encode a 5125128 image in approximately 8.36ms operating at 32.05MHz. Therefore, this architecture is seen as a feasible solution to real-time fractal image compression.",
    "keywords": [
      "fractal image encoding",
      "quadtree",
      "searchless",
      "real-time image compression"
    ]
  },
  {
    "id": "373",
    "title": "Distinguishing views in symmetric networks: A tight lower bound",
    "abstract": "The view of a node in a port-labeled network is an infinite tree encoding all walks in the network originating from this node. We prove that for any integers n?D?1 n ? D ? 1 , there exists a port-labeled network with at most n nodes and diameter at most D  which contains a pair of nodes whose (infinite) views are different, but whose views truncated to depth ?(Dlog?(n/D)) ? ( D log ? ( n / D ) ) are identical.",
    "keywords": [
      "anonymous network",
      "port-labeled network",
      "view",
      "quotient graph"
    ]
  },
  {
    "id": "374",
    "title": "cache injection for parallel applications",
    "abstract": "For two decades, the memory wall has affected many applications in their ability to benefit from improvements in processor speed. Cache injection addresses this disparity for I/O by writing data into a processor's cache directly from the I/O bus. This technique reduces data latency and, unlike data prefetching, improves memory bandwidth utilization. These improvements are significant for data-intensive applications whose performance is dominated by compulsory cache misses. We present an empirical evaluation of three injection policies and their effect on the performance of two parallel applications and several collective micro-benchmarks. We demonstrate that the effectiveness of cache injection on performance is a function of the communication characteristics of applications, the injection policy, the target cache, and the severity of the memory wall. For example, we show that injecting message payloads to the L3 cache can improve the performance of network-bandwidth limited applications. In addition, we show that cache injection improves the performance of several collective operations, but not all-to-all operations (implementation dependent). Our study shows negligible pollution to the target caches.",
    "keywords": [
      "memory wall",
      "cache injection"
    ]
  },
  {
    "id": "375",
    "title": "Cryptanalysis and improvement of an access control in user hierarchy based on elliptic curve cryptosystem",
    "abstract": "In a key management scheme for hierarchy based access control, each security class having higher clearance can derive the cryptographic secret keys of its other security classes having lower clearances. In 2008, Chung et al. proposed an efficient scheme on access control in user hierarchy based on elliptic curve cryptosystem [Information Sciences 178 (1) (2008) 230243]. Their scheme provides solution of key management efficiently for dynamic access problems. However, in this paper, we propose an attack on Chung et al.s scheme to show that Chung et al.s scheme is insecure against the exterior root finding attack. We show that under this attack, an attacker (adversary) who is not a user in any security class in a user hierarchy attempts to derive the secret key of a security class by using the root finding algorithm. In order to remedy this attack, we further propose a simple improvement on Chung et al.s scheme. Overall, the main theme of this paper is very simple: a security flaw is presented on Chung et al.s scheme and then a fix is provided in order to remedy the security flaw found in Chung et al.s scheme.",
    "keywords": [
      "key management",
      "elliptic curve",
      "hierarchical access control",
      "polynomial interpolation",
      "security",
      "exterior root finding attacks"
    ]
  },
  {
    "id": "376",
    "title": "Use of molecular modeling, docking, and 3D-QSAR studies for the determination of the binding mode of benzofuran-3-yl-(indol-3-yl)maleimides as GSK-3 beta inhibitors",
    "abstract": "Molecular modeling and docking studies along with three-dimensional quantitative structure relationships (3D-QSAR) studies have been used to determine the correct binding mode of glycogen synthase kinase 3 beta (GSK-3 beta) inhibitors. The approaches of comparative molecular field analysis (CoMFA) and comparative molecular similarity index analysis (CoMSIA) are used for the 3D-QSAR of 51 substituted benzofuran-3-yl-(indol-3-yl)maleimides as GSK-3 beta inhibitors. Two binding modes of the inhibitors to the binding site of GSK-3 beta are investigated. The binding mode 1 yielded better 3D-QSAR correlations using both CoMFA and CoMSIA methodologies. The three-component CoMFA model from the steric and electrostatic fields for the experimentally determined pIC(50) values has the following statistics: R(2)(cv) = 0.386 nd SE(cv) = 0.854 for the cross-validation, and R(2) = 0.811 and SE = 0.474 for the fitted correlation. F (3,47) = 67.034, and probability of R(2) = 0 (3,47) = 0.000. The binding mode suggested by the results of this study is consistent with the preliminary results of X-ray crystal structures of inhibitor-bound GSK-3 beta. The 3D-QSAR models were used for the estimation of the inhibitory potency of two additional compounds.",
    "keywords": [
      "benzofuran-3-yl- maleimides",
      "binding mode",
      "comfa",
      "comsia",
      "docking",
      "gsk-3beta inhibitors",
      "3d-qsar",
      "x-ray"
    ]
  },
  {
    "id": "377",
    "title": "On the numerical solution of some semilinear elliptic problems - II",
    "abstract": "In the earlier paper [6], a Galerkin method was proposed and analyzed for the numerical solution of a Dirichlet problem for a semi-linear elliptic boundary value problem of the form -Delta U = F((.),U). This was converted to a problem on a standard domain and then converted to an equivalent integral equation. Galerkina's method was used to solve the integral equation, with the eigenfunctions of the Laplacian operator on the standard domain D as the basis functions. In this paper we consider the implementing of this scheme, and we illustrate it for some standard domains D.",
    "keywords": [
      "elliptic",
      "nonlinear",
      "integral equation",
      "galerkin method"
    ]
  },
  {
    "id": "378",
    "title": "supporting mobile work processes in logistics with wearable computing",
    "abstract": "Logistics is a very dynamic and heterogeneous application area which generates complex requirements regarding the development of information and communication technologies (ICT). For this area, it is a challenge to support mobile workers on-site in an unobtrusive manner. In this contribution, wearable computing technologies are investigated as basis for a \"mobile worker supporting system\" for tasks at an automobile terminal. The features of wearable computing technologies are checked against the requirements of the application area to come to an usable and acceptable mobile solution in an user-centred design process.",
    "keywords": [
      "mobile usability",
      "logistics",
      "wearable computing",
      "mobile work processes",
      "autonomous control",
      "user-centred design"
    ]
  },
  {
    "id": "379",
    "title": "A nonlinear domain decomposition formulation with application to granular dynamics",
    "abstract": "Simulation of granular media undergoing dynamic evolution involves nonsmooth problems when grains are modeled as rigid bodies. With dense samples, this nonsmoothness occurs everywhere in the studied domain, and large sized systems lead to computationally intensive simulations. In this article, we combine domain decomposition approaches and nonsmooth contact dynamics. Unlike the smooth continuum media case, a coarse space problem does not trivially increase the convergence rate, as it is exemplified in this article, with semi-analytical examples and real size numerical simulations. Nevertheless, the description of an underlying force network in the samples may guide the analysis for new approximation schemes or algorithms.",
    "keywords": [
      "nonsmooth contact dynamics",
      "multicontact systems",
      "scalability",
      "multiscale",
      "asymptotic analysis"
    ]
  },
  {
    "id": "380",
    "title": "Minimum cut linear arrangement of p-q dags for VLSI layout of adder trees",
    "abstract": "Two algorithms for minimum cut linear arrangement of a class of graphs called p-q dags are proposed. A p-q dag represents the connection scheme of an adder tree, such as Wallace tree, and the VLSI layout problem of a bit slice of an adder tree is treated as the minimum cut linear arrangement problem of its corresponding p-q dag. One of the two algorithms is based on dynamic programming. It calculates an exact minimum solution within n(O(1)) time and space, where n is the size of a given graph. The other algorithm is an approximation algorithm which calculates a solution with O(log n) cutwidth. It requires O(n log n) time.",
    "keywords": [
      "graph algorithm",
      "minimum cut linear arrangement",
      "vlsi layout",
      "adder tree",
      "multiplier"
    ]
  },
  {
    "id": "381",
    "title": "OPTIMIZATION OF ILLUMINATION ENVIRONMENTAL FACTORS BASED ON ORTHOGONAL TEST",
    "abstract": "This paper presents a comparative research of nine different combinations of imaging environmental factors using orthogonal test approach to gain optimal illumination in an image acquisition device which was self-designed. The effect of four different environmental factors such as shoot distance, lamp number, lamp height, lamp side distance have been investigated on the key parameters. Experimental results based on L(9)(3)(4) orthogonal test design shows that under different combination of environmental factors, there are obvious differences between illumination intensity and illumination uniformity of images and which are mainly affected by the shoot distance and lamp number. Based on these experiments, we get two preferable combinations. Attention is concentrated on finding the best. Through further analysis and discussion, the best combination is identified. Our experimental results indicate that orthogonal test here is very suitable for gaining optimal environmental factors.",
    "keywords": [
      "image acquisition",
      "diffuse reflection",
      "orthogonal test",
      "illumination intensity",
      "illumination uniformity"
    ]
  },
  {
    "id": "382",
    "title": "Linear B cell epitope prediction for epitope vaccine design against meningococcal disease and their computational validations through physicochemical properties",
    "abstract": "Neisseria meningitidis serogroup B is predominantly known for its leading role in bacterial meningitis and septicemia worldwide. Although, polysaccharide conjugate vaccines have been developed and used successfully against many of the serogroups of N. meningitidis, such strategy has proved ineffective against group B meningococci. Here, we proposed to develop peptide epitope-based vaccine candidates from outer membrane (OM) protein contained in the outer membrane vesicles (OMV) based on our in silico analysis. In OMV, a total of 236 proteins were identified, only 15 (6.4%) of which were predicted to be located in the outer membrane. For the preparation of specific monoclonal antibodies against pathogenic bacterial protein, identification and selection of B cell epitopes that act as a vaccine target are required. We selected 13 outer membrane proteins from OMV proteins while taking into consideration the removal of cross-reactivity. Epitopia web server was used for the prediction of B cell epitopes. Epitopes are distinguished from non-epitopes by properties such as amino acid preference on the basis of amino acid composition, secondary structure composition, and evolutionary conservation. Predicted results were subject to verification with experimental data and we performed string-based search through IEDB. Our finding shows that epitopes have general preference for charged and polar amino acids; epitopes are enriched with loop as a secondary structure element that renders them flexible and also exposes another view of antibodyantigen interaction.",
    "keywords": [
      "b cell epitopes",
      "meningococcal",
      "neisseria meningitides",
      "structural features",
      "vaccine candidates"
    ]
  },
  {
    "id": "383",
    "title": "Object recognition using proportion-based prior information: Application to fisheries acoustics",
    "abstract": "This paper addresses the inference of probabilistic classification models using weakly supervised learning. The main contribution of this work is the development of learning methods for training datasets consisting of groups of objects with known relative class priors. This can be regarded as a generalization of the situation addressed by Bishop and Ulusoy (2005), where training information is given as the presence or absence of object classes in each set. Generative and discriminative classification methods are conceived and compared for weakly supervised learning, as well as a non-linear version of the probabilistic discriminative models. The considered models are evaluated on standard datasets and an application to fisheries acoustics is reported. The proposed proportion-based training is demonstrated to outperform model learning based on presence/absence information and the potential of the non-linear discriminative model is shown.",
    "keywords": [
      "weakly supervised learning",
      "generative classification model",
      "discriminative classification model"
    ]
  },
  {
    "id": "384",
    "title": "Hes frequencyamplitude formulation for nonlinear oscillators with an irrational force",
    "abstract": "In this paper, Hes frequencyamplitude formulation is applied to determine the periodic solution for a nonlinear oscillator system with an irrational force. Comparison with the exact solution shows that the result obtained is of high accuracy.",
    "keywords": [
      "nonlinear oscillators",
      "hes frequency formulation",
      "periodic solution"
    ]
  },
  {
    "id": "385",
    "title": "The effectiveness of a training method using self-modeling webcam photos for reducing musculoskeletal risk among office workers using computers",
    "abstract": "An intervention study was conducted to examine the effectiveness of an innovative self-modeling photo-training method for reducing musculoskeletal risk among office workers using computers. Sixty workers were randomly assigned to either: 1) a control group; 2) an office training group that received personal, ergonomic training and workstation adjustments or 3) a photo-training group that received both office training and an automatic frequent-feedback system that displayed on the computer screen a photo of the workers current sitting posture together with the correct posture photo taken earlier during office training. Musculoskeletal risk was evaluated using the Rapid Upper Limb Assessment (RULA) method before, during and after the six weeks intervention. Both training methods provided effective short-term posture improvement; however, sustained improvement was only attained with the photo-training method. Both interventions had a greater effect on older workers and on workers suffering more musculoskeletal pain. The photo-training method had a greater positive effect on women than on men.",
    "keywords": [
      "occupational exposure",
      "ergonomics",
      "telemedicine",
      "feedback",
      "task performance and analysis",
      "algorithm",
      "posture"
    ]
  },
  {
    "id": "386",
    "title": "the influence of feedback on egocentric distance judgments in real and virtual environments",
    "abstract": "A number of investigators have reported that distance judgments in virtual environments (VEs) are systematically smaller than distance judgments made in comparably-sized real environments. Many variables that may contribute to this difference have been investigated but none of them fully explain the distance compression. One approach to this problem that has implications for both VE applications and the study of perceptual mechanisms is to examine the influence of the feedback available to the user. Most generally, we asked whether feedback within a virtual environment would lead to more accurate estimations of distance. Next, given the prediction that some change in behavior would be observed, we asked whether specific adaptation effects would generalize to other indications of distance. Finally, we asked whether these effects would transfer from the VE to the real world. All distance judgments in the head-mounted display (HMD) became near accurate after three different forms of feedback were given within the HMD. However, not all feedback sessions within the HMD altered real world distance judgments. These results are discussed with respect to the perceptual and cognitive mechanisms that may be involved in the observed adaptation effects as well as the benefits of feedback for VE applications.",
    "keywords": [
      "adaptation",
      "virtual environments",
      "space perception",
      "feedback"
    ]
  },
  {
    "id": "387",
    "title": "synchronous versus asynchronous collaboration in situated multi-agent systems",
    "abstract": "According to the taxonomy for agent activity, proposed by V. Parunak, a collaboration is an interaction between agents of a multi-agent system (MAS) whereby the agents explicitly coordinate their actions before they cooperate. We discuss two sub-types of collaboration in the context of situated MASs, namely asynchronous and synchronous collaboration. After setting up collaboration, the interaction between the agents in an asynchronous collaboration happens indirectly through the environment. Agents direct their actions via the perceived state change of their environment. On the other hand, during a synchronous collaboration agents have to act simultaneously and this requires an additional agreement about which actions should be executed. Although they both fit the characteristics of collaboration, the requirements for their implementation is quite different. Whereas agents in an asynchronous collaboration can be implemented as separate processes that act directly into the environment, the implementation of synchronous collaboration is more complex since it requires support for simultaneous actions. In the paper we give examples of both kinds of collaborations and outline the necessary support for their implementation.",
    "keywords": [
      "synchronization",
      "collaboration",
      "interaction"
    ]
  },
  {
    "id": "388",
    "title": "Web Services Compositions Modelling and Choreographies Analysis",
    "abstract": "In Rouached et al. (2006) and Rouached and Godart (2007) the authors described the semantics of WSBPEL by way of mapping each of the WSBPEL (Arkin et al., 2004) constructs to the EC algebra and building a model of the process behaviour. With these mapping rules, the authors describe a modelling approach of a process defined for a single Web service composition. However, this modelling is limited to a local view and can only be used to model the behaviour of a single process. The authors further the semantic mapping to include Web service composition interactions through modelling Web service conversations and their choreography. This paper elaborates the models to support a view of interacting Web service compositions extending the mapping from WSBPEL to EC, and including Web service interfaces (WSDL) for use in modelling between services. The verification and validation techniques are also exposed while automated induction-based theorem prover is used as verification back-end.",
    "keywords": [
      "choreography",
      "orchestration",
      "semantic mapping",
      "verification and validation",
      "web service composition"
    ]
  },
  {
    "id": "389",
    "title": "PnP problem revisited",
    "abstract": "Perspective-n-Point camera pose determination, or the PnP problem, has attracted much attention in the literature. This paper gives a systematic investigation on the PnP problem from both geometric and algebraic standpoints, and has the following contributions: Firstly, we rigorously prove that the PnP problem under distance-based definition is equivalent to the PnP problem under orthogonal-transformation-based definition when n > 3, and equivalent to the PnP problem under rotation-transformation-based definition when n = 3. Secondly, we obtain the upper bounds of the number of solutions for the PnP problem under different definitions. In particular, we show that for any three non-collinear control points, we can always find out a location of optical center such that the P3P problem formed by these three control points and the optical center can have 4 solutions, its upper bound. Additionally a geometric way is provided to construct these 4 solutions. Thirdly, we introduce a depth-ratio based approach to represent the solutions of the whole PnP problem. This approach is shown to be advantageous over the traditional elimination techniques. Lastly, degenerated cases for coplanar or collinear control points are also discussed. Surprisingly enough, it is shown that if all the control points are collinear, the PnP problem under distance-based definition has a unique solution, but the PnP problem under transformation-based definition is only determined up to one free parameter.",
    "keywords": [
      "perspective-n-point camera pose determination",
      "distance-based definition",
      "transformation-based definition",
      "depth-ratio based equation",
      "upper bound of the number of solutions"
    ]
  },
  {
    "id": "390",
    "title": "Example-driven animation synthesis",
    "abstract": "We introduce an easy and intuitive approach to create animations by assembling existing animations. Using our system, the user needs only to simply scribble regions of interest and select the example animations that he/she wants to apply. Our system will then synthesize a transformation for each triangle and solve an optimization problem to compute the new animation for this target mesh. Like playing a jigsaw puzzle game, even a novice can explore his/her creativity by using our system without learning complicated routines, but just using a few simple operations to achieve the goal.",
    "keywords": [
      "animation synthesis",
      "warping",
      "intelligent scribbling"
    ]
  },
  {
    "id": "391",
    "title": "Macroblock and Motion Feature Analysis to H.264/AVC Fast Inter Mode Decision",
    "abstract": "One fast inter mode decision algorithm is proposed in this paper. The whole algorithm is divided into two stages. In the pre-stage, by exploiting spatial and temporal information of encoded macrobocks (MBs), a skip mode early detection scheme is proposed. The homogeneity of current MB is also analyzed to filter out small inter modes in this stage. Secondly, during the block matching stage, a motion feature based inter mode decision scheme is introduced by analyzing the motion vector predictor's accuracy, the block overlapping situation and the smoothness of SAD (sum of absolute difference) value. Moreover, the rate distortion cost is checked in an early stage and we set some constraints to speed up the whole decision flow. Experiments show that our algorithm can achieve a speed up factor of up to 53.4% for sequences with different motion type. The overall bit increment and quality degradation is negligible compared with existing works.",
    "keywords": [
      "mode decision",
      "h.264/avc",
      "feature analysis"
    ]
  },
  {
    "id": "392",
    "title": "the design of parsers for incremental language processors",
    "abstract": "An incremental language processor is one that accepts as input a sequence of substrings of the source language and maps them independently onto fragments in some object code. The ordered sequence of these object code fragments are then either compiled, in which case we have an incremental compiler, or interpreted. In the first case the advantage resulting is that subsequent changes in the source program entail only reprocessing the source fragments affected and recompiling the updated collection of object code fragments. In an environment where small changes are made frequently to large programs, e.g. debugging, the curtailment of reprocessing is attractive. In the second case the object code fragments are the actual run-time program representation, and hence inter-fragment relations are transiently evaluated as needed in the process of execution, with no long-term preservation of these relationships beyond the scope of their immediate need in execution time. This permits the possibility of program recomposition in the midst of execution, one of the principal characteristics of conversational computing. Many conversational language processors execute a program representation functionally analogous to parse trees, i.e. the syntax analysis of a fragment, insofar as it is possible, is done at fragment load time. This representation choice is popular because many of the expensive aspects of interpretation, including character string scanning, symbol table lookup, and parsing, are performed once only and do not contribute to the execution overhead. This paper is devoted to examining the question of the construction of such a parser in a general manner for an arbitrary source language.",
    "keywords": [
      "parser",
      "input",
      "analysis",
      "maps",
      "design",
      "collect",
      "computation",
      "object",
      "trees",
      "case",
      "general",
      "timing",
      "paper",
      "representation",
      "scan",
      "strings",
      "change",
      "incremental",
      "fragmentation",
      "processor",
      "program",
      "character",
      "aspect",
      "preservation",
      "relationships",
      "environments",
      "interpretation",
      "code",
      "language",
      "process",
      "compilation",
      "sequence",
      "debugging",
      "parsing"
    ]
  },
  {
    "id": "393",
    "title": "On the computation of all supported efficient solutions in multi-objective integer network flow problems",
    "abstract": "This paper presents a new algorithm for identifying all supported non-dominated vectors (or outcomes) in the objective space, as well as the corresponding efficient solutions in the decision space, for multi-objective integer network flow problems. Identifying the set of supported non-dominated vectors is of the utmost importance for obtaining a first approximation of the whole set of non-dominated vectors. This approximation is crucial, for example, in two-phase methods that first compute the supported non-dominated vectors and then the unsupported non-dominated ones. Our approach is based on a negative-cycle algorithm used in single objective minimum cost flow problems, applied to a sequence of parametric problems. The proposed approach uses the connectedness property of the set of supported non-dominated vectors/efficient solutions to find all integer solutions in maximal non-dominated/efficient facets.",
    "keywords": [
      "multi-objective linear and integer programming",
      "multi-objective network flows",
      "negative-cycle algorithms",
      "parametric programming"
    ]
  },
  {
    "id": "394",
    "title": "Two-objective method for crisp and fuzzy interval comparison in optimization",
    "abstract": "In real optimization we always meet two main groups of criteria: requirements of useful outcomes increasing or expenses decreasing and demands of lower uncertainty or, in other words, risk minimization. Therefore, it seems advisable to formulate optimization problem under conditions of uncertainty, at least, two-objective on the basis of local criteria of outcomes increasing or expenses reduction and risk minimization. Generally, risk may be treated as the uncertainty of obtained result. In the considered situation, the degree of risk (uncertainty) may be defined in a natural way through the width of final interval objective function at the point of optimum achieved. To solve the given problem, the two-objective interval comparison technique has been developed taking into account the probability of supremacy of one interval over the other one and relation of compared widths of intervals. To illustrate the efficiency of the proposed method, the simple examples of minimization of interval double-extreme discontinuous cost function and fuzzy extension of Rosenbrock's test function are presented.",
    "keywords": [
      "crisp interval",
      "fuzzy interval",
      "interval comparison",
      "probabilistic approach",
      "optimization"
    ]
  },
  {
    "id": "395",
    "title": "Development of head detection and tracking systems for visual surveillance",
    "abstract": "This paper proposes a technique for the detection of head nod and shake gestures based on eye tracking and head motion decision. The eye tracking step is divided into face detection and eye location. Here, we apply a motion segmentation algorithm that examines differences in moving peoples faces. This system utilizes a Hidden Markov Model-based head detection module that carries out complete detection in the input images, followed by the eye tracking module that refines the search based on a candidate list provided by the preprocessing module. The novelty of this paper is derived from differences in real-time input images, preprocessing to remove noises (morphological operators and so on), detecting edge lines and restoration, finding the face area, and cutting the head candidate. Moreover, we adopt a K-means algorithm for finding the head region. Real-time eye tracking extracts the location of eyes from the detected face region and is performed at close to a pair of eyes. After eye tracking, the coordinates of the detected eyes are transformed into a normalized vector of x-coordinate and y-coordinate. Head nod and shake detector uses three hidden Markov models (HMMs). HMM representation of the head detection can estimate the underlying HMM states from a sequence of face images. Head nod and shake can be detected by three HMMs that are adapted by a directional vector. The directional vector represents the direction of the head movement. The vector is HMMs for determining neutral as well as head nod and shake. These techniques are implemented on images, and notable success is notified.",
    "keywords": [
      "head detection",
      "head location",
      "eye location",
      "hidden markov models"
    ]
  },
  {
    "id": "396",
    "title": "Scheduling of printed circuit board (PCB) assembly systems with heterogeneous processors using simulation-based intelligent optimization methods",
    "abstract": "The complexity of printed circuit boards (PCBs), as an important sector of the electronics manufacturing industry, has increased over the last three decades. This paper focuses on a practical application observed at a PCB assembly line of electronics manufacturing facility. It is shown that this problem is equivalent to a flowshop scheduling with multiple heterogeneous batch processors where processors can perform multiple tasks as long as the sizes of jobs in a batch do not violate the processors capacity. The equivalent problem is mathematically formulated as a mixed integer programming model. Then, a Monte Carlo simulation is incorporated into high-level genetic algorithm-based intelligent optimization techniques to assess the performance of makespan-oriented system under uncertain processing times. At each iteration of algorithm, the output of simulator is used by optimizers to provide online feedbacks on the progress of the search and direct the search toward a promising solution zone. Furthermore, various parameters and operators of the algorithm are discussed and calibrated by means of Taguchi statistical technique. The result of extensive computational experiments shows that the solution approach gives high-quality solutions in reasonable computational time.",
    "keywords": [
      "electronics manufacturing",
      "pcb assembly",
      "monte carlo simulation",
      "genetic algorithms"
    ]
  },
  {
    "id": "397",
    "title": "Multi-agent systems with reinforcement hierarchical neuro-fuzzy models",
    "abstract": "This paper introduces a new multi-agent model for intelligent agents, called reinforcement learning hierarchical neuro-fuzzy multi-agent system. This class of model uses a hierarchical partitioning of the input space with a reinforcement learning algorithm to overcome limitations of previous RL methods. The main contribution of the new system is to provide a flexible and generic model for multi-agent environments. The proposed generic model can be used in several applications, including competitive and cooperative problems, with the autonomous capacity to create fuzzy rules and expand their own rule structures, extracting knowledge from the direct interaction between the agents and the environment, without any use of supervised algorithms. The proposed model was tested in three different case studies, with promising results. The tests demonstrated that the developed system attained good capacity of convergence and coordination among the autonomous intelligent agents.",
    "keywords": [
      "multi-agent systems ",
      "hierarchical neuro-fuzzy",
      "intelligent agents",
      "reinforcement learning"
    ]
  },
  {
    "id": "398",
    "title": "Possibilistic meanvariance models and efficient frontiers for portfolio selection problem",
    "abstract": "In this paper, it is assumed that the rates of return on assets can be expressed by possibility distributions rather than probability distributions. We propose two kinds of portfolio selection models based on lower and upper possibilistic means and possibilistic variances, respectively, and introduce the notions of lower and upper possibilistic efficient portfolios. We also present an algorithm which can derive the explicit expression of the possibilistic efficient frontier for the possibilistic mean-variance portfolio selection problem dealing with lower bounds on asset holdings.",
    "keywords": [
      "possibility theory",
      "possibilistic mean",
      "possibilistic variance",
      "portfolio selection",
      "optimization"
    ]
  },
  {
    "id": "399",
    "title": "Millennials among the Professional Workforce in Academic Libraries: Their Perspective on Leadership",
    "abstract": "This study explores possible leadership perceptions of Millennials working in academic libraries, specifically their definition, the attributes they associate with leadership, whether they want to assume formal leadership roles, whether they perceive themselves as leaders, and whether they perceive leadership opportunities within their organizations and LIS professional associations. An online survey was utilized to gather the responses and the study participants comprised of Millennials (born 1982 or after) currently working full-time in libraries that were a member of the Committee on Institutional Cooperation (CIC), a consortium of the Big Ten universities and the University of Chicago in 201112.",
    "keywords": [
      "leadership",
      "millennials",
      "academic",
      "leaders",
      "perceptions",
      "management"
    ]
  },
  {
    "id": "400",
    "title": "Mesh fusion using functional blending on topologically incompatible sections",
    "abstract": "Three-dimensional mesh fusion provides an easy and fast way to create new mesh models from existing ones. We introduce a novel approach of mesh fusion in this paper based on functional blending. Our method has no restriction of disk-like topology or one-ring opening on the meshes to be merged. First of all, sections with boundaries of the under-fusing meshes are converted into implicit representations. An implicit transition surface, which joins the sections together while keeping smoothness at the boundaries, is then created based on cubic Hermite functional blending. Finally, the implicit surface is tessellated to form the resultant mesh. Our scheme is both efficient and simple, and with it users can easily construct interesting, complex 3D models.",
    "keywords": [
      "mesh fusion",
      "functional blending",
      "interactive modeling tool"
    ]
  },
  {
    "id": "401",
    "title": "Imaging spectrometry and asphalt road surveys",
    "abstract": "This study integrates ground spectrometry, imaging spectrometry, and in situ pavement condition surveys for asphalt road assessment. Field spectra showed that asphalt aging and deterioration produce measurable changes in spectra as these surfaces undergo a transition from hydrocarbon dominated new roads to mineral dominated older roads. Several spectral measures derived from field and image spectra correlated well with pavement quality indicators. Spectral confusion between pavement material aging and asphalt mix erosion on the one hand, and structural road damages (e.g. cracking) on the other, poses some limits to remote sensing based mapping. Both the common practice methods (Pavement management system-PMS, in situ vehicle inspections), and analysis using imaging spectrometry are effective in identifying roads in good and very good condition. Variance and uncertainty in all survey data (PMS, in situ vehicle inspections, remote sensing) increases for road surfaces in poor condition and clear determination of specific (and expensive) surface treatment decisions remains problematic from these methods.",
    "keywords": [
      "asphalt road survey",
      "imaging spectrometry",
      "pavement management",
      "spectral library",
      "remote sensing",
      "hyperspectral"
    ]
  },
  {
    "id": "402",
    "title": "Conceptions of learning versus conceptions of web-based learning: The differences revealed by college students",
    "abstract": "Past research has shown the variations of students' conceptions of learning, but little has been especially undertaken to address students' conceptions of web-based learning and to make comparisons between students' conceptions of learning in general and their conceptions of web-based learning in particular. By interviewing 83 Taiwanese college students with some web-based learning experiences, this study attempted to investigate the students' conceptions of learning, conceptions of web-based learning, and the differences between these conceptions. Using the phenomenographic method of analyzing student interview transcripts, several categories of conceptions of learning and of web-based learning were revealed. The analyses of interview results suggested that the conceptions of web-based learning were often more sophisticated than those of learning. For example, much more students conceptualized learning in web-based context as pursuing real understanding and seeing in a new way than those for learning in general. This implies that the implementation of web-based instruction may be a potential avenue for promoting students' conceptions of learning. By gathering questionnaire responses from the students, this study further found that the sophistication of the conceptions toward web-based learning was associated with better searching strategies as well as higher self-efficacy for web-based learning.  ",
    "keywords": [
      "post-secondary education",
      "distance education and telelearning"
    ]
  },
  {
    "id": "403",
    "title": "on the behaviour of weighted multi-recombination evolution strategies optimising noisy cigar functions",
    "abstract": "Cigar functions are convex quadratic functions that are characterised by the presence of only two distinct eigenvalues of their Hessian, the smaller one of which occurs with multiplicity one. Their ridge-like topology makes them a useful test case for optimisation strategies. This paper extends previous work on modelling the behaviour of evolution strategies with isotropically distributed mutations optimising cigar functions by considering weighted recombination as well as the effects of noise on optimisation performance. It is found that the same weights that have previously been seen to be optimal for the sphere and parabolic ridge functions are optimal for cigar functions as well. The influence of the presence of noise on optimisation performance depends qualitatively on the trajectory of the search point, which in turn is determined by the strategy's mutation strength as well as its population size and recombination weights. Analytical results are obtained for the case of cumulative step length adaptation.",
    "keywords": [
      "evolution strategy",
      "weighted recombination",
      "cumulative step length adaptation",
      "cigar function",
      "noise"
    ]
  },
  {
    "id": "404",
    "title": "Human emotion recognition from videos using spatio-temporal and audio features",
    "abstract": "In this paper, we present human emotion recognition systems based on audio and spatio-temporal visual features. The proposed system has been tested on audio visual emotion data set with different subjects for both genders. The mel-frequency cepstral coefficient (MFCC) and prosodic features are first identified and then extracted from emotional speech. For facial expressions spatio-temporal features are extracted from visual streams. Principal component analysis (PCA) is applied for dimensionality reduction of the visual features and capturing 97% of variances. Codebook is constructed for both audio and visual features using Euclidean space. Then occurrences of the histograms are employed as input to the state-of-the-art SVM classifier to realize the judgment of each classifier. Moreover, the judgments from each classifier are combined using Bayes sum rule (BSR) as a final decision step. The proposed system is tested on public data set to recognize the human emotions. Experimental results and simulations proved that using visual features only yields on average 74.15% accuracy, while using audio features only gives recognition average accuracy of 67.39%. Whereas by combining both audio and visual features, the overall system accuracy has been significantly improved up to 80.27%.",
    "keywords": [
      "human computer interface ",
      "multimodal system",
      "human emotions",
      "support vector machines ",
      "spatio-temporal features"
    ]
  },
  {
    "id": "405",
    "title": "Fortification of rice: technologies and nutrients",
    "abstract": "This article provides a comprehensive review of the currently available technologies for vitamin and mineral rice fortification. It covers currently used technologies, such as coating, dusting, and the various extrusion technologies, with the main focus being on cold, warm, and hot extrusion technologies, including process flow, required facilities, and sizes of operation. The advantages and disadvantages of the various processing methods are covered, including a discussion on micronutrients with respect to their technical feasibility during processing, storage, washing, and various cooking methods and their physiological importance. The microstructure of fortified rice kernels and their properties, such as visual appearance, sensory perception, and the impact of different micronutrient formulations, are discussed. Finally, the article covers recommendations for quality control and provides a summary of clinical trials.",
    "keywords": [
      "rice fortification",
      "technologies",
      "nutrients",
      "vitamins",
      "minerals"
    ]
  },
  {
    "id": "406",
    "title": "A perceptual mapping procedure for analysis of proximity data to determine common and unique product-market structures",
    "abstract": "Traditional techniques of perceptual mapping hypothesize that products are differentiated in a common perceptual space of attributes. This paper suggests that each product is differentiated not only in a common perceptual space, but also a unique perceptual space consisting of as many dimensions as the number of products. It provides a model and estimation procedure based on alternating least squares for estimating the model parameters.",
    "keywords": [
      "product differentiation",
      "product uniqueness",
      "brand image",
      "three-way data",
      "multidimensional scaling",
      "proximities"
    ]
  },
  {
    "id": "407",
    "title": "On the Design and Prototype Implementation of a Multimodal Situation Aware System",
    "abstract": "In this paper we describe the design concepts and prototype implementation of a situation aware ubiquitous computing system using multiple modalities such as National Marine Electronics Association (NMEA) data from global positioning system (GPS) receivers, text, speech, environmental audio, and handwriting inputs. While most mobile and communication devices know where and who they are, by accessing context information primarily in the form of location, time stamps, and user identity, the concept of sharing of this information in a reliable and intelligent fashion is crucial in many scenarios. A framework which takes the concept of context aware computing to the level of situation aware computing by intelligent information exchange between context aware devices is designed and implemented in this work. Four sensual modes of contextual information like text, speech, environmental audio, and handwriting are augmented to conventional contextual information sources like location from GPS, user identity based on IP addresses (IPA), and time stamps. Each device derives its context not necessarily using the same criteria or parameters but by employing selective fusion and fission of multiple modalities. The processing of each individual modality takes place at the client device followed by the summarization of context as a text file. Exchange of dynamic context information between devices is enabled in real time to create multimodal situation aware devices. A central repository of all user context profiles is also created to enable self-learning devices in the future. Based on the results of simulated situations and real field deployments it is shown that the use of multiple modalities like speech, environmental audio, and handwriting inputs along with conventional modalities can create devices with enhanced situational awareness.",
    "keywords": [
      "bidirectional ftp synchronization",
      "environmental audio",
      "gps",
      "speech recognition",
      "ubiquitous computing"
    ]
  },
  {
    "id": "408",
    "title": "Neural-association of microcalcification patterns for their reliable classification in digital mammography",
    "abstract": "Breast cancer continues to be the most common cause of cancer deaths in women. Early detection of breast cancer is significant for better prognosis. Digital Mammography currently offers the best control strategy for the early detection of breast cancer. The research work in this paper investigates the significance of neural-association of microcalcification patterns for their reliable classification in digital mammograms. The proposed technique explores the auto-associative abilities of a neural network approach to regenerate the composite of its learned patterns most consistent with the new information, thus the regenerated patterns can uniquely signify each input class and improve the overall classification. Two types of features: computer extracted (gray level based statistical) features and human extracted (radiologists' interpretation) features are used for the classification of calcification type of breast abnormalities. The proposed technique attained the highest 90.5% classification rate on the calcification testing dataset.",
    "keywords": [
      "neural networks",
      "auto-associator",
      "classifier",
      "feature extraction",
      "digital mammography"
    ]
  },
  {
    "id": "409",
    "title": "Modeling hemodynamics in an unoccluded and partially occluded inferior vena cava under rest and exercise conditions",
    "abstract": "Pulmonary embolism is the third leading cause of death in hospitalized patients in the US. Vena cava filters are medical devices inserted into the inferior vena cava (IVC) and are designed to trap thrombi before they reach the lungs. Once trapped in a filter, however, thrombi disturb otherwise natural flow patterns, which may be clinically significant. The goal of this work is to use computational modeling to study the hemodynamics of an unoccluded and partially occluded IVC under rest and exercise conditions. A realistic, three-dimensional model of the IVC, iliac, and renal veins represents the vessel geometry and spherical clots represent thombi trapped by several conical filter designs. Inflow rates correspond to rest and exercise conditions, and a transitional turbulence model captures transitional flow features, if they are present. The flow equations are discretized and solved using a second-order finite-volume method. No significant regions of transitional flow are observed. Nonetheless, the volume of stagnant and recirculating flow increases with partial occlusion and exercise. For the partially occluded vessel, large wall shear stresses are observed on the IVC and on the model thrombus, especially under exercise conditions. These large wall shear stresses may have mixed clinical implications: thrombotic-like behavior may initiate on the vessel wall, which is undesirable; and thrombolysis may be accelerated, which is desirable.",
    "keywords": [
      "cfd",
      "filter",
      "thrombosis",
      "vena cava",
      "wall shear stress"
    ]
  },
  {
    "id": "410",
    "title": "Recent advances in direct methods for solving unsymmetric sparse systems of linear equations",
    "abstract": "During the past few years, algorithmic improvements alone have reduced the time required for the direct solution of unsymmetric sparse systems of linear equations by almost an order of magnitude. This paper compares the performance of some well-known software packages for solving general sparse systems. In particular, it demonstrates the consistently high level of performance achieved by WSMP-the most recent of such solvers. It compares the various algorithmic components of these solvers and discusses their impact on solver performance. Our experiments show that the algorithmic choices made in WSMP enable it to run more than twice as fast as the best among similar solvers and that WSMP can factor some of the largest sparse matrices available from real applications in only a few seconds on a 4-CPU workstation. Thus, the combination of advances in hardware and algorithms makes it possible to solve those general sparse linear systems quickly and easily that might have been considered too large until recently.",
    "keywords": [
      "algorithms",
      "performance",
      "sparse matrix factorization",
      "sparse lu decomposition",
      "multifrontal method",
      "parallel sparse solvers"
    ]
  },
  {
    "id": "411",
    "title": "A linear goal programming approach to determining the relative importance weights of customer requirements in quality function deployment",
    "abstract": "Quality function deployment (QFD) is a planning tool used in new product development and quality management. It aims at achieving maximum customer satisfaction by listening to the voice of customers. To implement QFD, customer requirements (CRs) should be identified and assessed first. The current paper proposes a linear goal programming (LGP) approach to assess the relative importance weights of CRs. The LGP approach enables customers to express their preferences on the relative importance weights of CRs in their preferred or familiar formats, which may differ from one customer to another but have no need to be transformed into the same format, thus avoiding information loss or distortion. A numerical example is tested with the LGP approach to demonstrate its validity, effectiveness and potential applications in QFD practice.  ",
    "keywords": [
      "quality function deployment",
      "customer requirement",
      "customer preference",
      "preference format",
      "goal programming",
      "group decision making"
    ]
  },
  {
    "id": "412",
    "title": "Local, deformable precomputed radiance transfer",
    "abstract": "Precomputed radiance transfer (PRT) captures realistic lighting effects from distant, low-frequency environmental lighting but has been limited to static models or precomputed sequences. We focus on PRT for local effects such as bumps, wrinkles, or other detailed features, but extend it to arbitrarily deformable models. Our approach applies zonal harmonics (ZH) which approximate spherical functions as sums of circularly symmetric Legendre polynomials around different axes. By spatially varying both the axes and coefficients of these basis functions, we can fit to spatially varying transfer signals. Compared to the spherical harmonic (SH) basis, the ZH basis yields a more compact approximation. More important, it can be trivially rotated whereas SH rotation is expensive and unsuited for dense per-vertex or per-pixel evaluation. This property allows, for the first time, PRT to be mapped onto deforming models which re-orient the local coordinate frame. We generate ZH transfer models by fitting to PRT signals simulated on meshes or simple parametric models for thin membranes and wrinkles. We show how shading with ZH transfer can be significantly accelerated by specializing to a given lighting environment. Finally, we demonstrate real-time rendering results with soft shadows, inter-reflections, and subsurface scatter on deforming models.",
    "keywords": [
      "lighting environments",
      "nonlinear optimization",
      "spherical",
      "harmonics",
      "soft shadows",
      "subsurface scattering",
      "texture maps",
      "zonal harmonics"
    ]
  },
  {
    "id": "413",
    "title": "A mimetic mass, momentum and energy conserving discretization for the shallow water equations",
    "abstract": "A spatial semi-discretization is developed for the two-dimensional depth-averaged shallow water equations on a non-equidistant structured and staggered grid. The vector identities required for energy conservation in the continuous case are identified. Discrete analogues are developed, which lead to a finite-volume semi-discretisation which conserves mass, momentum, and energy simultaneously. The key to discrete energy conservation for the shallow water equations is to numerically distinguish storage of momentum from advective transport of momentum. Simulation of a large-amplitude wave in a basin confirms the conservative properties of the new scheme, and demonstrates the enhanced robustness resulting from the compatibility of continuity and momentum equations. The scheme can be used as a building block for constructing fully conservative curvilinear, higher order, variable density, and non-hydrostatic discretizations.  ",
    "keywords": [
      "shallow water equations",
      "energy-conservation",
      "fully conservative",
      "mimetic",
      "finite-volume",
      "finite-difference",
      "symmetry preservation",
      "staggered",
      "c-grid"
    ]
  },
  {
    "id": "414",
    "title": "3d object search through semantic component",
    "abstract": "In this paper, we present a novel concept named semantic component for 3D object search which describes a key component that semantically defines a 3D object. In most cases, the semantic component is intra-category stable and therefore can be used to construct an efficient 3D object retrieval scheme. By segmenting an object into segments and learning the similar segments shared by all the objects in the same category, we can summarise what human uses for object recognition, from the analysis of which we develop a method to find the semantic component of an object. In our experiments, the proposed method is justified and the effectiveness of our algorithm is also demonstrated.",
    "keywords": [
      "semantic component",
      "3d object search"
    ]
  },
  {
    "id": "415",
    "title": "Formal semantics and efficient analysis of Timed Rebeca in Real-Time Maude",
    "abstract": "Provides an executable formal Real-Time Maude semantics for Timed Rebeca. Integrates Real-Time Maude analysis into the Rebeca toolchain. Provides an efficient semantics using partial-order-reduction-like techniques. Shows the performance gained by this optimization.",
    "keywords": [
      "real-time actors",
      "timed rebeca",
      "formal semantics",
      "model checking",
      "real-time maude"
    ]
  },
  {
    "id": "416",
    "title": "A new bonding-tool solution to improve stitch bondability",
    "abstract": "A new bonding-tool solution is proposed to improve stitch bondability by creating a new surface morphology on the tip surface of a wire-bonding tool (capillary). The surface has relatively deep lines with no fixed directions. This new capillary has less slipping between the wire and the capillary tip surface and provides better coupling effect between them. Experiments of wire bonding on unstable lead frames/substrates, alloyed wire (2N gold wire) bonding, and copper wire bonding were carried out to confirm the effect of the new capillary on the stitch bondability. The experimental results are promising and have proved that the use of the new capillary could improve the bondability of the stitch bond and minimize the occurrence of short tail defects and non-sticking on lead during bonding.",
    "keywords": [
      "microelectronics packaging",
      "wire bond",
      "stitch bondability",
      "capillary",
      "copper wire bonding"
    ]
  },
  {
    "id": "417",
    "title": "optimal instruction scheduling using integer programming",
    "abstract": "This paper presents a new approach to local instruction scheduling based on integer programming that produces optimal instruction schedules in a reasonable time, even for very large basic blocks. The new approach first uses a set of graph transformations to simplify the data-dependency graph while preserving the optimality of the final schedule. The simplified graph results in a simplified integer program which can be solved much faster. A new integer-programming formulation is then applied to the simplified graph. Various techniques are used to simplify the formulation, resulting in fewer integer-program variables, fewer integer-program constraints and fewer terms in some of the remaining constraints, thus reducing integer-program solution time. The new formulation also uses certain adaptively added constraints (cuts) to reduce solution time. The proposed optimal instruction scheduler is built within the Gnu Compiler Collection (GCC) and is evaluated experimentally using the SPEC95 floating point benchmarks. Although optimal scheduling for the target processor is considered intractable, all of the benchmarks' basic blocks are optimally scheduled, including blocks with up to 1000 instructions, while total compile time increases by only 14%.",
    "keywords": [
      "processor",
      "variability",
      "instruction scheduling",
      "scheduling",
      "optimality",
      "benchmark",
      "data dependence",
      "collect",
      "instruction",
      "compilation",
      "graph",
      "floating point",
      "timing",
      "graph transformation",
      "paper",
      "constraint",
      "integer programming",
      "locality"
    ]
  },
  {
    "id": "418",
    "title": "Web Services Discovery with Rough Sets",
    "abstract": "Web services are emerging as a major technology for building service-oriented distributed systems. Potentially, various resources on the Internet can be virtualized as Web services for a wider use by their communities. Service discovery becomes an issue of vital importance for Web services applications. This article presents ROSSE, a Rough Sets based Search Engine for Web service discovery. One salient feature of ROSSE lies in its capability to deal with uncertainty of service properties when matching services. A use case is presented to demonstrate the use of ROSSE for discovery of car services. ROSSE is evaluated in terms of its accuracy and efficiency in service discovery.",
    "keywords": [
      "owl-s",
      "rough sets",
      "service matchmaking",
      "web service discovery"
    ]
  },
  {
    "id": "419",
    "title": "Stability-Optimized Time Adjustment for a Networked Computer Clock",
    "abstract": "We propose an optimal time adjustment method from the viewpoint of frequency stability, which is defined as the Allan deviation. When time adjustment is needed for a clock in a networked computer, it is made over a period called a time adjustment period. The proposed method optimizes frequency stability for a given time adjustment period. This method has been evaluated and compared with the adjtime() system call in UNIX systems in terms of frequency stability and duration of time adjustment period needed for achieving particular values of frequency stability. For time intervals from 1 to 1; 000 s, the frequency stability achieved by the proposed method was about 0.01-0.5 of that achieved by the adjtime() system call. The evaluation also showed that the duration of a time adjustment period needed for achieving the frequency stability of 1.0 x 10(-10) in the proposed method was less than 1/12 (1/6) that of the period in the adjtime() system call when we optimized frequency stability for a 60 (3,600) s time interval under the condition that the duration of the time-adjustment period was 12 h.",
    "keywords": [
      "clock synchronization",
      "allan deviation",
      "allan variance",
      "frequency stability"
    ]
  },
  {
    "id": "420",
    "title": "Improving mobile services design: A QFD approach",
    "abstract": "The quality of mobile services in the mobile and wireless world is ultimately judged in terms of customer satisfaction. This is particularly true for the third generation (3G) and beyond multimedia mobile services which should meet or exceed customer expectations. In this study Quality Function Deployment (QFD) is used for the first time as a quality improvement approach for building customers' requirements into mobile services. Traditionally QFD approach is adopted in product and manufacturing industries. In this paper QFD is extended to mobile service industry which is such a promising industry in today's information society. This paper proposes a generic framework based on QFD concepts and practices to improve mobile service design and development. An example is presented to illustrate the use of QFD for mobile e-learning services for university students and lecturers. The data transmission speed is found to be the most critical requirement in mobile e-learning services. By the use of QFD the developed mobile services can best meet customers' requirements or even exceed their expectations. At the end of this paper some benefits as well as further improvements regarding QFD approach are discussed and concluded.",
    "keywords": [
      "mobile service",
      "e-learning",
      "quality function deployment",
      "customer requirement",
      "voice of customer",
      "house of quality"
    ]
  },
  {
    "id": "421",
    "title": "International research collaborations of ASEAN Nations in economics, 19792010",
    "abstract": "This study examines the research performance and international research collaborations (IRC) of ASEAN nations in the area of economics. Over the last 3 decades international collaborated papers have increased in the region, while locally-co-authored papers have declined. Singapore towered among ASEAN nations in research efficiency based on geographical area, population and GDP. Vietnam performed relatively better in research efficiency than research productivity (number of papers produced), while Indonesia performed poorly. Overall, internationally co-authored papers were cited twice as often as locally authored papers except that both The Philippines and Indonesia exhibited almost no difference in how their local and internationally co-authored papers were cited. The study also examined IRC from the perspective of social networks. Centrality had a strong correlation with research performance; however, vertex tie-strength (a result of repeat collaboration) showed maximum correlation with research performance. While Malaysia emerged as the nation with the highest betweenness centrality or bridging power, the US emerged as the most favoured international partner of ASEAN nations. However, collaboration between ASEAN countries accounted for just 4% of all international collaborations. Increased academic mobility and more joint scientific works are suggestions to consider to boost educational co-operation among the ASEAN nations.",
    "keywords": [
      "international research collaborations",
      "research efficiency",
      "social networks",
      "asean",
      "economics"
    ]
  },
  {
    "id": "422",
    "title": "Automated assistants for analyzing team behaviors",
    "abstract": "Multi-agent teamwork is critical in a large number of agent applications, including training, education, virtual enterprises and collective robotics. The complex interactions of agents in a team as well as with other agents make it extremely difficult for human developers to understand and analyze agent-teambehavior. It has thus become increasingly important to develop tools that can help humans analyze, evaluate, and understand team behaviors. However, the problem of automated team analysis is largely unaddressed in previous work. In this article, we identify several key constraints faced by team analysts. Most fundamentally, multiple types of models of team behavior are necessary to analyze different granularities of team events, including agent actions, interactions, and global performance. In addition, effective ways of presenting the analysis to humans is critical and the presentation techniques depend on the model being presented. Finally, analysis should be independent of underlying team architecture and implementation. We also demonstrate an approach to addressing these constraints by building an automated team analyst called ISAAC for post-hoc, off-line agent-team analysis. ISAAC acquires multiple, heterogeneous team models via machine learning over teams' external behavior traces, where the specific learning techniques are tailored to the particular model learned. Additionally, ISAAC employs multiple presentation techniques that can aid human understanding of the analyses. ISAAC also provides feedback on team improvement in two novel ways: (i) It supports principled \"what-if'' reasoning about possible agent improvements; (ii) It allows the user to compare different teams based on their patterns of interactions. This paper presents ISAAC's general conceptual framework, motivating its design, as well as its concrete application in two domains: ( i) RoboCup Soccer; ( ii) software agent teams participating in a simulated evacuation scenario. In the RoboCup domain, ISAAC was used prior to and during the RoboCup '99 tournament, and was awarded the RoboCup Scientific Challenge Award. In the evacuation domain, ISAAC was used to analyze patterns of message exchanges among software agents, illustrating the generality of ISAAC's techniques. We present detailed algorithms and experimental results from ISAAC's application.",
    "keywords": [
      "teamwork",
      "analysis",
      "multiagent systems"
    ]
  },
  {
    "id": "423",
    "title": "Adaptive clustering in wireless sensor networks by mining sensor energy data",
    "abstract": "Clustering has been well received as one of the effective solutions to enhance energy efficiency and scalability of large-scale wireless sensor networks. The goal of clustering is to identify a subset of nodes in a wireless sensor network, then all the other nodes communicate with the network sink via these selected nodes. However, many current clustering algorithms are tightly coupled with exact sensor locations derived through either triangulation methods or extra hardware such as GPS equipment. However, in practice, it is very difficult to know sensor location coordinates accurately due to various factors such as random deployment and low-power, low-cost sensing devices. Therefore, how to develop an adaptive clustering algorithm without relying on exact sensor location information is a very important yet challenging problem. In this paper, we try to address this problem by proposing a new adaptive clustering algorithm for energy efficiency of wireless sensor networks. Compared with other work having been done in this area, our proposed adaptive clustering algorithm is original because of its capability to infer the location information by mining wireless sensor energy data. Furthermore, based on the inferred location information and the remaining (residual) energy level of each node, the proposed clustering algorithm will dynamically change cluster heads for energy efficacy. Simulation results show that the proposed adaptive clustering algorithm is efficient and effective for energy saving in wireless sensor networks.  ",
    "keywords": [
      "adaptive clustering",
      "wireless sensor networks",
      "network management",
      "data mining"
    ]
  },
  {
    "id": "424",
    "title": "To feature space and back: Identifying top-weighted features in polynomial Support Vector Machine models",
    "abstract": "Polynomial Support Vector Machine models of degree d are linear functions in a feature space of monomials of at most degree d. However, the actual representation is stored in the form of support vectors and Lagrange multipliers that is unsuitable for human understanding. An efficient, heuristic method for searching the feature space of a polynomial Support Vector Machine model for those features with the largest absolute weights is presented. The time complexity of this method is Theta(dms(2) + sdp), where m is the number of variables, d the degree of the kernel, s the number of support vectors, and p the number of features the algorithm is allowed to search. In contrast, the brute force approach of constructing all weights and then selecting the largest weights has complexity Theta(sd((m+d)(d))). The method is shown to be effective in identifying the top-weighted features on several simulated data sets, where the true weight vector is known. Additionally, the method is run on several high-dimensional, real world data sets where the features returned may be used to construct classifiers with classification performances similar to models built with all or subsets of variables returned by variable selection methods. This algorithm provides a new ability to understand, conceptualize, visualize, and communicate polynomial SVM models and has implications for feature construction, dimensionality reduction, and variable selection.",
    "keywords": [
      "support vector machines",
      "classification",
      "variable selection"
    ]
  },
  {
    "id": "425",
    "title": "Sound direction estimation using an artificial ear for robots",
    "abstract": "We propose a novel design of an artificial robot ear for sound direction estimation using two measured outputs only. The spectral features in the interaural transfer functions (ITFs) of the proposed artificial ears are distinctive and move monotonically according to the sound direction. Thus, these features provide effective sound cues to estimate sound direction using the measured two output signals. Bilateral asymmetry of microphone positions can enhance the estimation performance even in the median plane where interaural differences vanish. We propose a localization method to estimate the lateral and vertical angles simultaneously. The lateral angle is estimated using interaural time difference and Woodworth and Schlosberg's formula, and the front-back discrimination is achieved by finding the spectral features in the ITF estimated from two measured outputs. The vertical angle of a sound source in the frontal region is estimated by comparing the spectral features in the estimated ITF with those in the database built in an anechoic chamber. The feasibility of the designed artificial ear and the estimation method were verified in a real environment. In the experiment, it was shown that both the front-back discrimination and the sound direction estimation in the frontal region can be achieved with reasonable accuracy. Thus, we expect that robots with the proposed artificial ear can estimate the direction of speaker from two output signals only.  ",
    "keywords": [
      "sound direction estimation",
      "artificial ear",
      "human-robot interaction",
      "head-related transfer function",
      "interaural transfer function"
    ]
  },
  {
    "id": "426",
    "title": "Alcohol, Type of Alcohol, and All-Cause and Coronary Heart Disease Mortality",
    "abstract": "Many studies from a variety of countries have shown a U- or J-shaped relation between alcohol intake and mortality from all causes. It is now quite well documented from epidemiologic as well as clinical and experimental studies that the descending leg of the curve results from a decreased risk of cardiovascular disease among those with light-to-moderate alcohol consumption. The findings that wine drinkers are at a decreased risk of mortality from cardiovascular disease compared to non-wine drinkers suggest that substances present in wine are responsible for a beneficial effect on the outcome, in addition to that from a light intake of ethanol. Several potential confounding factors still remain to be excluded, however.",
    "keywords": [
      "coronary heart disease",
      "alcohol intake",
      "flavonoids",
      "wine"
    ]
  },
  {
    "id": "427",
    "title": "Rigorous development of an embedded fault-tolerant system based on coordinated atomic actions",
    "abstract": "This paper describes our experience using coordinated atomic (CA) actions as a system structuring tool to design and validate a sophisticated and embedded control system for a complex industrial application that has high reliability and safety requirements. Our study is based on an extended production cell model, the specification and simulator for which were defined and developed by FZI (Forschungszentrum Informatik, Germany). This \"Fault-Tolerant Production Cell\" represents a manufacturing process involving redundant mechanical devices (provided in order to enable continued production in the presence of machine faults). The challenge posed by the model specification is to design a control system that maintains specified safety and liveness properties even in the presence of a large number and variety of device and sensor failures. Based on an analysis of such failures, we provide in this paper details of: 1) a design for a control program that uses CA actions to deal with both safety-related and fault tolerance concerns and 2) the formal verification of this design based on the use of model-checking. We found that CA action structuring facilitated both the design and verification tasks by enabling the various safety problems (involving possible clashes of moving machinery) to be treated independently. Even complex situations involving the concurrent occurrence of any pairs of the many possible mechanical and sensor failures can be handled simply yet appropriately. The formal verification activity was performed in parallel with the design activity and the interaction between them resulted in a combined exercise in \"design for validation\"; formal verification was very valuable in identifying some very subtle residual bugs in early versions of our design which would have been difficult to detect otherwise.",
    "keywords": [
      "concurrency",
      "coordinated atomic  actions",
      "embedded fault-tolerant systems",
      "exception handling",
      "object orientation",
      "formal verification",
      "model checking",
      "reliability",
      "safety"
    ]
  },
  {
    "id": "428",
    "title": "Ventricular shape visualization using selective volume rendering of cardiac datasets",
    "abstract": "In this paper, we present a novel technique of improving volume rendering quality and speed by integrating original volume data and global model information attained by segmentation. The segmentation information prevents object occlusions that may appear when volume rendering is based on local image features only. Thus the presented visualization technique provides meaningful visual results that enable a clear understanding of complex anatomical structures. In the first part, we describe a segmentation technique for extracting the region of interest based on an active contour model. In the second part, we propose a volume rendering method for visualizing the selected portions of fuzzy surfaces extracted by local image processing methods. We show the results of selective volume rendering of left and right ventricle based on cardiac datasets from clinical routines. Our method offers an accelerated technique to accurately visualize the surfaces of segmented objects.",
    "keywords": [
      "cardiology",
      "medical imaging",
      "selective volume rendering",
      "direct volume rendering",
      "distance transform",
      "segmentation"
    ]
  },
  {
    "id": "429",
    "title": "Mutual information for Lucas-Kanade tracking (MILK): An inverse compositional formulation",
    "abstract": "Mutual Information (MI) is popular for registration via function optimization. This work proposes an inverse compositional formulation of MI for Levenberg-Marquardt optimization. This yields a constant Hessian, which may be precomputed. Speed improvements of 15 percent were obtained, with convergence accuracies similar those of the standard formulation.",
    "keywords": [
      "mutual information",
      "registration",
      "newton optimization",
      "tracking"
    ]
  },
  {
    "id": "430",
    "title": "A Comparative Study of Software Model Checkers as Unit Testing Tools: An Industrial Case Study",
    "abstract": "Conventional testing methods often fail to detect hidden flaws in complex embedded software such as device drivers or file systems. This deficiency incurs significant development and support/maintenance cost for the manufacturers. Model checking techniques have been proposed to compensate for the weaknesses of conventional testing methods through exhaustive analyses. Whereas conventional model checkers require manual effort to create an abstract target model, modern software model checkers remove this overhead by directly analyzing a target C program, and can be utilized as unit testing tools. However, since software model checkers are not fully mature yet, they have limitations according to the underlying technologies and tool implementations, potentially critical issues when applied in industrial projects. This paper reports our experience in applying Blast and CBMC to testing the components of a storage platform software for flash memory. Through this project, we analyzed the strong and weak points of two different software model checking technologies in the viewpoint of real-world industrial application-counterexample-guided abstraction refinement with predicate abstraction and SAT-based bounded analysis.",
    "keywords": [
      "embedded software verification",
      "software model checking",
      "bounded model checking",
      "cegar-based model checking",
      "flash file systems"
    ]
  },
  {
    "id": "431",
    "title": "CMATERdb1: a database of unconstrained handwritten Bangla and Bangla-English mixed script document image",
    "abstract": "In this paper, we have described the preparation of a benchmark database for research on off-line Optical Character Recognition (OCR) of document images of handwritten Bangla text and Bangla text mixed with English words. This is the first handwritten database in this area, as mentioned above, available as an open source document. As India is a multi-lingual country and has a colonial past, so multi-script document pages are very much common. The database contains 150 handwritten document pages, among which 100 pages are written purely in Bangla script and rests of the 50 pages are written in Bangla text mixed with English words. This database for off-line-handwritten scripts is collected from different data sources. After collecting the document pages, all the documents have been preprocessed and distributed into two groups, i.e., CMATERdb1.1.1, containing document pages written in Bangla script only, and CMATERdb1.2.1, containing document pages written in Bangla text mixed with English words. Finally, we have also provided the useful ground truth images for the line segmentation purpose. To generate the ground truth images, we have first labeled each line in a document page automatically by applying one of our previously developed line extraction techniques [Khandelwal et al., PReMI 2009, pp. 369-374] and then corrected any possible error by using our developed tool GT Gen 1.1. Line extraction accuracies of 90.6 and 92.38% are achieved on the two databases, respectively, using our algorithm. Both the databases along with the ground truth annotations and the ground truth generating tool are available freely at http://code.google.com/p/cmaterdb.",
    "keywords": [
      "unconstrained handwritten document image database",
      "text line extraction",
      "ground truth preparation",
      "ocr of multi-script document"
    ]
  },
  {
    "id": "432",
    "title": "A time domain boundary element method for modeling the quasi-static viscoelastic behavior of asphalt pavements",
    "abstract": "A time domain boundary element method (BEM) is presented to model the quasi-static linear viscoelastic behavior of asphalt pavements. In the viscoelastic analysis, the fundamental solution is derived in terms of elemental displacement discontinuities (DDs) and a boundary integral equation is formulated in the time domain. The unknown DDs are assumed to vary quadratically in the spatial domain and to vary linearly in the time domain. The equation is then solved incrementally through the whole time history using an explicit time-marching approach. All the spatial and temporal integrations can be performed analytically, which guarantees the accuracy of the method and the stability of the numerical procedure. Several viscoelastic models such as Boltzmann, Burgers, and power-law models are considered to characterize the time-dependent behavior of linear viscoelastic materials. The numerical method is applied to study the load-induced stress redistribution and its effects on the cracking performance of asphalt pavements. Some benchmark problems are solved to verify the accuracy and efficiency of the approach. Numerical experiments are also carried out to demonstrate application of the method in pavement engineering.",
    "keywords": [
      "time domain bem",
      "viscoelasticity",
      "displacement discontinuities",
      "time marching",
      "viscoelastic models",
      "asphalt pavements",
      "stress redistribution"
    ]
  },
  {
    "id": "433",
    "title": "A decision support system for animated film selection based on a multi-criteria aggregation of referees' ordinal preferences",
    "abstract": "This paper presents a decision support system devoted to the selection of films for the International Animated Film Festival organized at Annecy, France, every year. It deals with the representation and aggregation of referees' preferences along predefined criteria in addition to their overall selection point of view. The practical requirements associated with this application (often encountered in social or cultural areas as well) are: a common ordinal scale for the criteria scores, a procedure to deal with inconsistencies between criteria and overall scores, explanation tools of each referee's preference model in order to facilitate the deliberation process and also to argument the selection decision. The processing of referees' preferences is achieved thanks to a recent method which consists in finding a generalized mean aggregation operator representing the preferences of a referee, in a finite ordinal scale context. The method allows to deal with consistency conditions on referees' behaviour in order to highlight the criteria or pair of criteria which are the most influential for each of the referees. All the functionalities have been implemented in an interactive decision software that facilitates a shared selection decision. Results issued from the 2007 selection are presented and analysed from the preference representation and processing point of view.  ",
    "keywords": [
      "animated film selection",
      "ordinal scale",
      "expert preference representation",
      "multi-criteria aggregation"
    ]
  },
  {
    "id": "434",
    "title": "Semantic Web search based on rough sets and Fuzzy Formal Concept Analysis",
    "abstract": "Fuzzy Formal Concept Analysis (FFCA) is a generalization of Formal Concept Analysis (FCA) for modeling uncertainty information. FFCA provides a mathematical framework which can support the construction of formal ontologies in the presence of uncertainty data for the development of the Semantic Web. In this paper, we show how rough set theory can be employed in combination with FFCA to perform Semantic Web search and discovery of information in the Web.",
    "keywords": [
      "semantic web",
      "formal concept analysis",
      "fuzzy information",
      "rough set theory",
      "formal concept"
    ]
  },
  {
    "id": "435",
    "title": "Constrained consensus of asynchronous discrete-time multi-agent systems with time-varying topology",
    "abstract": "The union graph is assumed to be strongly connected over each finite interval. An approach is proposed to transform the original network to a synchronous one. We show that the linear part converges and the projection error vanishes over time.",
    "keywords": [
      "constrained consensus",
      "multi-agent system",
      "asynchronous communication",
      "distributed control"
    ]
  },
  {
    "id": "436",
    "title": "Axiomatic theory of intuitionistic fuzzy sets",
    "abstract": "A Bernays-like axiomatic theory of intuitionistic fuzzy sets involving five primitives and seven axioms is presented.  ",
    "keywords": [
      "fuzzy sets",
      "intuitionistic fuzzy sets",
      "axiomatic theory"
    ]
  },
  {
    "id": "437",
    "title": "Top-down delayering to expose large inspection area on die side-edge with Platinum (Pt) deposition technique",
    "abstract": "Methodology to increase the flat inspection area Useful in exposition for die side edge. Platinum (Pt) deposition technique to form a protection mask Pt deposition to slow down the edging effect",
    "keywords": [
      "top-down polishing method",
      "large exposition",
      "edging effect",
      "platinum  deposition"
    ]
  },
  {
    "id": "438",
    "title": "Short-term robustness of production management systems: A case study",
    "abstract": "Whereas Operations Research concentrates on optimization, practitioners find the robustness of a proposed solution more important. Therefore this paper presents a practical methodology that is a stagewise combination of four proven techniques: (1) simulation, (2) optimization, (3) risk or uncertainty analysis, and (4) bootstrapping. This methodology is illustrated through a production-control study. That illustration defines robustness as the capability to maintain short-term service, in a variety of environments (scenarios); that is, the probability of the short-term fill-rate remains within a prespecified range. Besides satisfying this probabilistic constraint, the system minimizes expected long-term work-in-process. Actually, the example compares four systemsnamely, Kanban, Conwip, Hybrid, and Genericfor the well-known case of a production line with four stations and a single product. The conclusion is that in this particular example, Hybrid is best when risk is not ignored; otherwise Generic is best; that is, risk considerations do make a difference.",
    "keywords": [
      "risk analysis",
      "robustness and sensitivity analysis",
      "scenarios",
      "manufacturing",
      "inventory"
    ]
  },
  {
    "id": "439",
    "title": "The variety generated by semi-Heyting chains",
    "abstract": "The purpose of this paper was to investigate the structure of semi-Heyting chains and the variety ({{mathcal{CSH}}}) generated by them. We determine the number of non-isomorphic n-element semi-Heyting chains. As a contribution to the study of the lattice of subvarieties of ({{mathcal{CSH}}},) we investigate the inclusion relation between semi-Heyting chains. Finally, we provide equational bases for ({{mathcal{CSH}}}) and for the subvarieties of ({{mathcal{CSH}}}) introduced in [5].",
    "keywords": [
      "heyting algebras",
      "varieties",
      "semi-heyting algebras"
    ]
  },
  {
    "id": "440",
    "title": "Stochastic fault tree analysis with self-loop basic events",
    "abstract": "This paper presents an analytical approach for performing fault tree analysis (FTA) with stochastic self-loop events. The proposed approach uses the flow-graph concept, and moment generating function (MGF) to develop a new stochastic FTA model for computing the probability, mean time to occurrence, and standard deviation time to occurrence of the top event. The application of the method is demonstrated by solving one example.",
    "keywords": [
      "fault tree",
      "flow-graph",
      "reliability model",
      "stochastic failure analysis"
    ]
  },
  {
    "id": "441",
    "title": "global-view abstractions for user-defined reductions and scans",
    "abstract": "Since APL, reductions and scans have been recognized as powerful programming concepts. Abstracting an accumulation loop (reduction) and an update loop (scan), the concepts have efficient parallel implementations based on the parallel prefix algorithm. They are often included in high-level languages with a built-in set of operators such as sum, product, min, etc. MPI provides library routines for reductions that account for nearly nine percent of all MPI calls in the NAS Parallel Benchmarks (NPB) version 3.2. Some researchers have even advocated reductions and scans as the principal tool for parallel algorithm design.Also since APL, the idea of applying the reduction control structure to a user-defined operator has been proposed, and several implementations (some parallel) have been reported. This paper presents the first global-view formulation of user-defined scans and an improved global-view formulation of user-defined reductions, demonstrating them in the context of the Chapel programming language. Further, these formulations are extended to a message passing context (MPI), thus transferring global-view abstractions to local-view languages and perhaps signaling a way to enhance local-view languages incrementally. Finally, examples are presented showing global-view user-defined reductions \"cleaning up\" and/or \"speeding up\" portions of two NAS benchmarks, IS and MG. In consequence, these generalized reduction and scan abstractions make the full power of the parallel prefix technique available to both global- and local-view parallel programming.",
    "keywords": [
      "reductions",
      "mpi",
      "parallel programming",
      "scans",
      "parallel prefix",
      "chapel"
    ]
  },
  {
    "id": "442",
    "title": "Partial Derivative Guidance for Weak Classifier Mining in Pedestrian Detection",
    "abstract": "Boosting over weak classifiers is widely used in pedestrian detection. As the number of weak classifiers is large, researchers always use a sampling method over weak classifiers before training. The sampling makes the boosting process harder to reach the fixed target. In this paper, we propose a partial derivative guidance for weak classifier mining method which can be used in conjunction with a boosting algorithm. Using weak classifier mining method makes the sampling less degraded in the performance. It has the same effect as testing more weak classifiers while using acceptable time. Experiments demonstrate that our algorithm can process quicker than [1] algorithm in both training and testing, without any performance decrease. The proposed algorithms is easily extending to any other boosting algorithms using a window-scanning style and HOG-like features.",
    "keywords": [
      "pedestrian detection",
      "partial derivative",
      "classifier mining",
      "hog",
      "boosting"
    ]
  },
  {
    "id": "443",
    "title": "the role of operators in apl",
    "abstract": "Operators, which apply to functions to produce functions, are an important component of APL. Despite their importance, their role is not well understood, and they are often lumped with functions in expositions of the language. This paper attempts to clarify the role of operators in APL by tracing their development, outlining possible future directions, and commenting briefly on their roles in other languages, both natural and programming.",
    "keywords": [
      "role",
      "program",
      "direct",
      "operability",
      "traces",
      "paper",
      "future",
      "developer",
      "language",
      "roles",
      "component"
    ]
  },
  {
    "id": "444",
    "title": "Theoretical modeling of micro-scale biological phenomena in human coronary arteries",
    "abstract": "This paper presents a mathematical model of biological structures in relation to coronary arteries with atherosclerosis. A set of equations has been derived to compute blood flow through these transport vessels with variable axial and radial geometries. Three-dimensional reconstructions of diseased arteries from cadavers have shown that atherosclerotic lesions spiral through the artery. The theoretical framework is able to explain the phenomenon of lesion distribution in a helical pattern by examining the structural parameters that affect the flow resistance and wall shear stress. The study is useful for connecting the relationship between the arterial wall geometries and hemodynamics of blood. It provides a simple, elegant and non-invasive method to predict flow properties for geometrically complex pathology at micro-scale levels and with low computational cost.",
    "keywords": [
      "atherosclerosis",
      "axial and radial asymmetry",
      "spiraling lesion",
      "resistance to flow ratio",
      "wall shear stress"
    ]
  },
  {
    "id": "445",
    "title": "Unanticipated partial behavioral reflection: Adapting applications at runtime",
    "abstract": "Dynamic, unanticipated adaptation of running systems is of interest in a variety of situations, ranging from functional upgrades to on-the-fly debugging or monitoring of critical applications. In this paper we study a particular form of computational reflection, called unanticipated partial behavioral reflection (UPBR), which is particularly well suited for unanticipated adaptation of real-world systems. Our proposal combines the dynamicity of unanticipated reflection, i.e., reflection that does not require preparation of the code of any sort, and the selectivity and efficiency of partial behavioral reflection (PBR). First, we propose unanticipated partial behavioral reflection which enables the developer to precisely select the required reifications, to flexibly engineer the metalevel and to introduce the metabehavior dynamically. Second, we present a system supporting unanticipated partial behavioral reflection in Squeak Smalltalk, called GEPPETTO, and illustrate its use with a concrete example of a web application. Benchmarks validate the applicability of our proposal as an extension to the standard reflective abilities of Smalltalk.",
    "keywords": [
      "reflection",
      "metaprogramming",
      "metaobject protocol",
      "smalltalk"
    ]
  },
  {
    "id": "446",
    "title": "A weighted additive fuzzy programming approach for multi-criteria supplier selection",
    "abstract": "In supply chain management, to build strategic and strong relationships, firms should select best suppliers by applying appropriate method and selection criteria. In this paper, to handle ambiguity and fuzziness in supplier selection problem effectively, a new weighted additive fuzzy programming approach is developed. Firstly, linguistic values expressed as trapezoidal fuzzy numbers are used to assess the weights of the factors. By applying the distances of each factor between Fuzzy Positive Ideal Rating and Fuzzy Negative Ideal Rating, weights are obtained. Then applying suppliers' constraints, goals and weights of the factors, a fuzzy multi-objective linear model is developed to overcome the selection problem and assign optimum order quantities to each supplier. The proposed model is explained by a numerical example.  ",
    "keywords": [
      "supplier selection",
      "fuzzy multi-objective linear model",
      "linguistic variables",
      "multi-criteria decision making"
    ]
  },
  {
    "id": "447",
    "title": "FireGrid: An e-infrastructure for next-generation emergency response support",
    "abstract": "The FireGrid project aims to harness the potential of advanced forms of computation to support the response to large-scale emergencies (with an initial focus on the response to fires in the built environment). Computational models of physical phenomena are developed, and then deployed and computed on High Performance Computing resources to infer incident conditions by assimilating live sensor data from an emergency in real timeor, in the case of predictive models, faster-than-real time. The results of these models are then interpreted by a knowledge-based reasoning scheme to provide decision support information in appropriate terms for the emergency responder. These models are accessed over a Grid from an agent-based system, of which the human responders form an integral part. This paper proposes a novel FireGrid architecture, and describes the rationale behind this architecture and the research results of its application to a large-scale fire experiment.",
    "keywords": [
      "emergency response",
      "grid",
      "high performance computing",
      "multi-agent system",
      "knowledge-based reasoning",
      "fire simulation model"
    ]
  },
  {
    "id": "448",
    "title": "A branch and bound algorithm for minimizing total completion time on a single batch machine with incompatible job families and dynamic arrivals",
    "abstract": "In this paper, we consider a single batch machine scheduling problem with incompatible job families and dynamic job arrivals. The objective is to minimize the total completion time. This problem is known to be strongly NP-hard. We present several dominance properties and two types of lower bounds, which are incorporated to construct a basic branch and bound algorithm. Furthermore, according to the characteristics of dynamic job arrivals, a decomposed branch and bound algorithm is proposed to improve the efficiency. The proposed algorithms are tested on a large set of randomly generated problem instances.",
    "keywords": [
      "branch and bound algorithm",
      "dynamic arrivals",
      "batch scheduling",
      "incompatible job families"
    ]
  },
  {
    "id": "449",
    "title": "Predicting saturates of sour vacuum gas oil using artificial neural networks and genetic algorithms",
    "abstract": "Accurate predictions of chemical composition by physical properties of sour vaccum gas oil (VGO) fractions are important for the refinery. In this paper, a feed-forward type network based on genetic algorithm (GA), was developed and used for predicting saturates of sour vacuum gas oil. The number of neurons in the hidden layer, the momentum and the learning rates were determined by using the genetic algorithm. The five physical properties of sour VGO, namely, average boiling point, density at 20C, molecular weight, kinematic viscosity at 100C and refractive index at 70C were considered as input variables of the ANN and the saturates of sour VGO was used as output variable. The study shows that genetic algorithm could find the optimal networks architecture and parameters of the back-propagation algorithm. Further, the artificial neural network models based on genetic algorithm are tested and the results indicate that the adopted model is very suitable for the forecasting of saturates of sour VGO. Compared with other forecasting models, it can be found that this model can improve prediction accuracy.",
    "keywords": [
      "saturates",
      "sour",
      "vacuum gas oil",
      "prediction",
      "artificial neural networks",
      "genetic algorithm"
    ]
  },
  {
    "id": "450",
    "title": "Implementing a hardware-embedded reactive agents platform based on a service-oriented architecture over heterogeneous wireless sensor networks",
    "abstract": "Wireless Sensor Networks (WSNs) represent a key technology for collecting important information from different sources in context-aware environments. Unfortunately, integrating devices from different architectures or wireless technologies into a single sensor network is not an easy task for designers and developers. In this sense, distributed architectures, such as service-oriented architectures and multi-agent systems, can facilitate the integration of heterogeneous sensor networks. In addition, the sensors capabilities can be expanded by means of intelligent agents that change their behavior dynamically. This paper presents the Hardware-Embedded Reactive Agents (HERA) platform. HERA is based on Services laYers over Light PHysical devices (SYLPH), a distributed platform which integrates a service-oriented approach into heterogeneous WSNs. As SYLPH, HERA can be executed over multiple devices independently of their wireless technology, their architecture or the programming language they use. However, HERA goes one step ahead of SYLPH and adds reactive agents to the platform and also a reasoning mechanism that provides HERA Agents with Case-Based Planning features that allow solving problems considering past experiences. Unlike other approaches, HERA allows developing applications where reactive agents are directly embedded into heterogeneous wireless sensor nodes with reduced computational resources.",
    "keywords": [
      "distributed architectures",
      "multi-agent systems",
      "heterogeneous wireless sensor networks",
      "embedded agents",
      "case-based planning"
    ]
  },
  {
    "id": "451",
    "title": "efficient algorithms for stream mining of constrained frequent patterns in a limited memory environment",
    "abstract": "As technology advances, streams of data can be rapidly generated in many real-life applications. This calls for stream mining, which searches for implicit, previously unknown, and potentially useful information---such as frequent patterns---that might be embedded in continuous data streams. However, most of the existing algorithms do not allow users to express the patterns to be mined according to their intentions, via the use of constraints. As a result, these unconstrained mining algorithms can yield numerous patterns that are not interesting to the users. Moreover, many existing tree-based algorithms assume that all the trees constructed during the mining process can fit into memory. While this assumption holds for many situations, there are many other situations in which it does not hold. Hence, in this paper, we develop efficient algorithms for stream mining of constrained frequent patterns in a limited memory environment . Our algorithms allow users to impose a certain focus on the mining process, discover from data streams all those frequent patterns that satisfy the user constraints, and handle situations where the available memory space is limited.",
    "keywords": [
      "limited memory space",
      "frequent itemsets",
      "data streams",
      "constraints",
      "data mining"
    ]
  },
  {
    "id": "452",
    "title": "Synthesis of topology and sizing of analog electrical circuits by means of genetic programming",
    "abstract": "The design (synthesis) of an analog electrical circuit entails the creation of both the topology and sizing (numerical values) of all of the circuit's components. There has previously been no general automated technique for automatically creating the design for an analog electrical circuit from a high-level statement of the circuit's desired behavior. This paper shows how genetic programming can be used to automate the design of eight prototypical analog circuits, including a lowpass filter, a highpass filter, a bandstop filter, a tri-state frequency discriminator circuit, a frequency-measuring circuit, a 60 dB amplifier, a computational circuit for the square root function, and a time-optimal robot controller circuit.",
    "keywords": [
      "genetic programming",
      "genetic algorithms",
      "circuit synthesis",
      "electrical circuits",
      "design"
    ]
  },
  {
    "id": "453",
    "title": "A spectrum of compromise aggregation operators for multi-attribute decision making",
    "abstract": "In many decision making problems, a number of independent attributes or criteria are often used to individually rate an alternative from an agent's local perspective and then these individual ratings are combined to produce an overall assessment. Now, in cases where these individual ratings are not in complete agreement, the overall rating should be somewhere in between the extremes that have been suggested. However, there are many possibilities for the aggregated value. Given this, this paper systematically explores the space of possible compromise operators for such multi-attribute decision making problems. Specifically, we axiomatically identify the complete spectrum of such operators in terms of the properties they should satisfy, and show the main ones that are widely used-namely averaging operators, uninorms and nullnorms-represent only three of the nine types we identify. For each type, we then go onto analyse their properties and discuss how specific instances can actually be developed. Finally, to illustrate the richness of our framework, we show how a wide range of operators are needed to model the various attitudes that a user may have for aggregation in a given scenario (bidding in multi-attribute auctions).  ",
    "keywords": [
      "aggregation operator",
      "uninorm",
      "nullnorm risk",
      "multi-attribute decision making",
      "multi-attribute auction"
    ]
  },
  {
    "id": "454",
    "title": "Detecting categorical perception in continuous discrimination data",
    "abstract": "We present a method for assessing categorical perception from continuous discrimination data. Until recently, categorical perception of speech has exclusively been measured by discrimination and identification experiments with a small number of different stimuli, each of which is presented multiple times. Experiments by Rogers and Davis (2009), however, suggest that using non-repeating stimuli yields a more reliable measure of categorization. If this idea is applied to a single phonetic continuum, the continuum has to be densely sampled and the obtained discrimination data is nearly continuous. In the present study, we describe a maximum-likelihood method that is appropriate for analysing such continuous discrimination data.",
    "keywords": [
      "categorical perception",
      "dense sampling",
      "discrimination",
      "maximum likelihood"
    ]
  },
  {
    "id": "455",
    "title": "simplified similarity scoring using term ranks",
    "abstract": "We propose a method for document ranking that combines a simple document-centric view of text, and fast evaluation strategies that have been developed in connection with the vector space model. The new method defines the importance of a term within a document qualitatively rather than quantitatively, and in doing so reduces the need for tuning parameters. In addition, the method supports very fast query processing, with most of the computation carried out on small integers, and dynamic pruning an effective option. Experiments on a wide range of TREC data show that the new method provides retrieval effectiveness as good as or better than the Okapi BM25 formulation, and variants of language models.",
    "keywords": [
      "vector space model",
      "method",
      "strategies",
      "dynamic",
      "computation",
      "retrieval",
      "text",
      "language model",
      "data",
      "efficiency/scale: architectures",
      "efficient query evaluation",
      "experience",
      "similarity",
      "tuning",
      "documentation",
      "connection",
      "effect",
      "evaluation",
      "prune",
      "query processing",
      "text representation and indexing",
      "compression",
      "ranking"
    ]
  },
  {
    "id": "456",
    "title": "Midgar: Generation of heterogeneous objects interconnecting applications. A Domain Specific Language proposal for Internet of Things scenarios",
    "abstract": "Smart Objects and Internet of Things are two ideas that describe the future. The interconnection of objects can make them intelligent or expand their intelligence. This is achieved by a network that connects all the objects in the world. A network where most of the data traffic comes from objects instead of people. Cities, houses, cars or any other objects that come to life, respond, work and make their owners life easier. This is part of that future. But first, there are many basic problems that must be solved. In this paper we propose solutions for many of these problems: the interconnection of ubiquitous, heterogeneous objects and the generation of applications allow inexperienced people to interconnect them. For that purpose, we present three possible solutions: a Domain Specific Language capable of abstracting the application generation problem; a graphic editor that simplifies the creation of that DSL; and an IoT platform (Midgar) able to interconnect different objects between them. Through Midgar, you can register objects and create interconnection between ubiquitous and heterogeneous objects through a graphic editor that generates a model defined by the DSL. From this model, Midgar generates the interconnection defined by the user with the graphical editor.",
    "keywords": [
      "internet of things",
      "ubiquitous computing",
      "sensor network",
      "model driven engineering",
      "domain specific language",
      "smart objects"
    ]
  },
  {
    "id": "457",
    "title": "ROC curves in cost space",
    "abstract": "ROC curves and cost curves are two popular ways of visualising classifier performance, finding appropriate thresholds according to the operating condition, and deriving useful aggregated measures such as the area under the ROC curve (AUC) or the area under the optimal cost curve. In this paper we present new findings and connections between ROC space and cost space. In particular, we show that ROC curves can be transferred to cost space by means of a very natural threshold choice method, which sets the decision threshold such that the proportion of positive predictions equals the operating condition. We call these new curves rate-driven curves, and we demonstrate that the expected loss as measured by the area under these curves is linearly related to AUC. We show that the rate-driven curves are the genuine equivalent of ROC curves in cost space, establishing a point-point rather than a point-line correspondence. Furthermore, a decomposition of the rate-driven curves is introduced which separates the loss due to the threshold choice method from the ranking loss (Kendall tau distance). We also derive the corresponding curve to the ROC convex hull in cost space; this curve is different from the lower envelope of the cost lines, as the latter assumes only optimal thresholds are chosen.",
    "keywords": [
      "cost curves",
      "roc curves",
      "cost-sensitive evaluation",
      "ranking performance",
      "operating condition",
      "kendall tau distance",
      "area under the roc curve "
    ]
  },
  {
    "id": "458",
    "title": "On the characteristics of growing cell structures (GCS) neural network",
    "abstract": "In this paper, a self-developing neural network model, namely the Growing Cell Structures (GCS) is characterized. In GCS each node (or cell) is associated with a local resource counter tau (t). We show that GCS has the conservation property by which the summation of all resource counters always equals s(1 - alpha)/alpha, thereby s is the increment added to tau (t) of the wining node after each input presentation and alpha (0 < alpha < 1.0) is the forgetting (i.e., decay) factor applied to tau (t) of non-wining nodes. The conservation property provides an insight into how GCS can maximize information entropy. The property is also employed to unveil the chain-reaction effect and race-condition which can greatly influence the performance of GCS. We show that GCS can perform better in terms of equi-probable criterion if the resource counters are decayed by a smaller alpha.",
    "keywords": [
      "self-developing neural network",
      "competitive learning",
      "race-condition",
      "topology",
      "equi-probable criterion",
      "chain-reaction effect"
    ]
  },
  {
    "id": "459",
    "title": "limitations of multivariable controller tuning using genetic algorithms",
    "abstract": "In recent years Evolutionary Computation has come of age, with Genetic Algorithms (GA) being possibly the most popular technique. A study is presented revealing the performance of a GA in determining the PID tuning parameters for a multivariable process, including decoupling controllers. The process used for this investigation is a distillation column which is a MIMO high-order, nonlinear system. The results indicate some limitations of using GAs for controller tuning when MIMO systems are involved.",
    "keywords": [
      "genetic algorithm",
      "tuning",
      "pid",
      "multivariable"
    ]
  },
  {
    "id": "460",
    "title": "Collaborative real-time traffic information generation and sharing framework for the intelligent transportation system",
    "abstract": "Real-time traffic information collection and data fusion is one of the most important tasks in the advanced traffic management system (ATMS), and sharing traffic information to users is an essential part of the advance traveler information system (ATIS) among the intelligent transportation systems (ITS). Traditionally, sensor-based schemes or probing-vehicle based schemes have been used for collecting traffic information, but the coverage, cost, and real-time issues have remained unsolved. In this paper, a wiki-like collaborative real-time traffic information collection, fusion and sharing framework is proposed, which includes user-centric traffic event reacting mechanism, and automatic agent-centric traffic information aggregating scheme. Smart traffic agents (STA) developed for various front-end devices have the location-aware two-way real-time traffic exchange capability, and built-in event-reporting mechanism to allow users to report the real-time traffic events around their locations. In addition to collecting traffic information, the framework also integrates heterogeneous external real-time traffic information data sources and internal historical traffic information database to predict real-time traffic status by knowledge base system technique.",
    "keywords": [
      "collective intelligence",
      "traffic status prediction",
      "smart traffic agent",
      "intelligent transportation system ",
      "knowledge-based system"
    ]
  },
  {
    "id": "461",
    "title": "Perspectives on wellness self-monitoring tools for older adults",
    "abstract": "Compared older adults and healthcare providers perceptions on self-monitoring. Explored advantages in older adults voluntary use of self-monitoring. Identified challenges in older adults voluntary use of self-monitoring. Suggested design implications for older adults self-monitoring tools.",
    "keywords": [
      "consumer health information",
      "health communication",
      "self management",
      "independent living"
    ]
  },
  {
    "id": "462",
    "title": "Exploration of term relationship for Bayesian network based sentence retrieval",
    "abstract": "Sentence retrieval is to retrieve query-relevant sentences in response to user query. However, limited information contained in sentence always incurs a lot of uncertainties, which heavily influence the retrieval performance. To solve this problem, Bayesian network, which has been accepted as one of the most promising methodologies to deal with information uncertainty, is explored. Correspondingly, three sentence retrieval models based on Bayesian network are proposed, i.e. BNSR model, BNSR_TR model and BNSR_CR model. BNSR model assumes independency between terms and shows certain improvement in retrieval performance. BNSR_TR and BNSR_CR models relax the assumption of term independency but consider term relationships from two different points of view, namely term and term context. Experiments verify the performance improvements produced by these two models, but BNSR_CR shows more advantages than BNSR_TR model, because of its more accurate identification of term dependency.",
    "keywords": [
      "sentence retrieval",
      "bayesian network",
      "term relationship",
      "association rule mining"
    ]
  },
  {
    "id": "463",
    "title": "animation aerodynamics",
    "abstract": "Methods based on aerodynamics are developed to simulate and control the motion of objects in fluid flows. To simplify the physics for animation, the problem is broken down into two parts: a fluid flow regime and an object boundary regime. With this simplification one can approximate the realistic behaviour of objects moving in liquids or air. It also enables a simple way of designing and controlling animation sequences: from a set of flow primitives, an animator can design the spatial arrangement of flows, create flows around obstacles and direct flow timing. The approach is fast, simple, and is easily fitted into simulators that model objects governed by classical mechanics. The methods are applied to an animation that involves hundreds of flexible leaves being blown by wind currents.",
    "keywords": [
      "spatial",
      "simplification",
      "aerodynamics",
      "method",
      "simulation",
      "approximation",
      "arrangement",
      "design",
      "fluid mechanics",
      "leaves",
      "object",
      "direct",
      "flow primitives",
      "flow",
      "flexibility",
      "control motion design",
      "physical",
      "timing",
      "model",
      "animation",
      "sequence",
      "control",
      "motion"
    ]
  },
  {
    "id": "464",
    "title": "A Fourier-Analytic Approach to Reed-Muller Decoding",
    "abstract": "We present a Fourier-analytic approach to list-decoding Reed-Muller codes over arbitrary finite fields. We use this to show that quadratic forms over any field are locally list-decodable up to their minimum distance. The analogous statement for linear polynomials was proved in the celebrated works of Goldreich et al. Previously, tight bounds for quadratic polynomials were known only for q = 2 and 3; the best bound known for other fields was the Johnson radius. Departing from previous work on Reed-Muller decoding which relies on some form of self-corrector, our work applies ideas from Fourier analysis of Boolean functions to low-degree polynomials over finite fields, in conjunction with results about the weight-distribution. We believe that the techniques used here could find other applications, we present some applications to testing and learning.",
    "keywords": [
      "codes",
      "computational complexity",
      "fourier transforms",
      "polynomials"
    ]
  },
  {
    "id": "465",
    "title": "monotonic solution concepts in coevolution",
    "abstract": "Assume a coevolutionary algorithm capable of storing and utilizing all phenotypes discovered during its operation, for as long as it operates on a problem; that is, assume an algorithm with a monotonically increasing knowledge of the search space. We ask: If such an algorithm were to periodically report, over the course of its operation, the best solution found so far, would the quality of the solution reported by the algorithm improve monotonically over time? To answer this question, we construct a simple preference relation to reason about the goodness of different individual and composite phenotypic behaviors. We then show that whether the solutions reported by the coevolutionary algorithm improve monotonically with respect to this preference relation depends upon the solution concept implemented by the algorithm. We show that the solution concept implemented by the conventional coevolutionary algorithm does not guarantee monotonic improvement; in contrast, the game-theoretic solution concept of Nash equilibrium does guarantee monotonic improvement. Thus, this paper considers 1) whether global and objective metrics of goodness can be applied to coevolutionary problem domains (possibly with open-ended search spaces), and 2) whether coevolutionary algorithms can, in principle, optimize with respect to such metrics and find solutions to games of strategy.",
    "keywords": [
      "coevolution",
      "solution concepts",
      "monotonic progress"
    ]
  },
  {
    "id": "466",
    "title": "Building roadmaps of minima and transitions in visual models",
    "abstract": "Becoming trapped in suboptimal local minima is a perennial problem when optimizing visual models, particularly in applications like monocular human body tracking where complicated parametric models are repeatedly fitted to ambiguous image measurements. We show that trapping can be significantly reduced by building 'roadmaps' of nearby minima linked by transition pathways-paths leading over low 'mountain passes' in the cost surface-found by locating the transition state (codimension-1 saddle point) at the top of the pass and then sliding downhill to the next minimum. We present two families of transition-state-finding algorithms based on local optimization. In eigenvector tracking, unconstrained Newton minimization is modified to climb uphill towards a transition state, while in hypersurface sweeping, a moving hypersurface is swept through the space and moving local minima within it are tracked using a constrained Newton method. These widely applicable numerical methods, which appear not to be known in vision and optimization, generalize methods from computational chemistry where finding transition states is critical for predicting reaction parameters. Experiments on the challenging problem of estimating 3D human pose from monocular images show that our algorithms find nearby transition states and minima very efficiently, but also underline the disturbingly large numbers of minima that can exist in this and similar model based vision problems.",
    "keywords": [
      "model based vision",
      "global optimization",
      "saddle points",
      "3d human tracking"
    ]
  },
  {
    "id": "467",
    "title": "dynamic computational geometry on parallel computers",
    "abstract": "This paper surveys our parallel algorithms for determining geometric properties of systems of moving objects . The properties investigated include nearest (farthest) neighbor, closest (farthest) pair, collision, convex hull, diameter, and containment. The models of computation include the CREW PRAM, mesh, and hypercube.",
    "keywords": [
      "collision",
      "mesh",
      "systems",
      "survey",
      "parallel algorithm",
      "convex hull",
      "dynamic",
      "parallel computation",
      "hypercube",
      "paper",
      "model of computation",
      "computational geometry",
      "container"
    ]
  },
  {
    "id": "468",
    "title": "Why does the single neuron activity change from trial to trial during sensory-motor task",
    "abstract": "Single neuron activities from cortical areas of a monkey were recorded while performing a sensory-motor task (a choice reaction time task). Quantitative trial-by-trial analysis revealed that the timing of peak activity exhibited large variation from trial to trial, compared to the variation in the behavioral reaction time of the task. Therefore, we developed a multi-unit dynamic neural network model to investigate the effects of structure of neural connections on the variation of the timing of peak activity. Computer simulation of the model showed that, even though the units are connected in a cascade fashion, a wide variation exists in the timing of peak activity of neurons because of parallel organization of neural network within each unit.",
    "keywords": [
      "single neuron",
      "peak activity",
      "neural model",
      "simulation"
    ]
  },
  {
    "id": "469",
    "title": "Preventing design conflicts in distributed design systems composed of heterogeneous agents",
    "abstract": "We model the uncertainty in distributed design. We model the attitudes of design agents to develop novel collaboration indicators. Monte Carlo simulation is performed for heterogeneous design agents. Design conflicts of heterogeneous design agents are prevented. Design agent dominations are reduced to be coherent with agent characters.",
    "keywords": [
      "collaborative design",
      "distributed design",
      "set-based design",
      "conflict prevention",
      "constraint satisfaction problem",
      "agent attitude model"
    ]
  },
  {
    "id": "470",
    "title": "Survivability and performance optimization of mobile wireless communication networks in the event of base station failure",
    "abstract": "In this paper, we investigate the survivability of mobile wireless communication networks in the event of base station (BS) failure. A survivable network is modeled as a mathematical optimization problem in which the objective is to minimize the total amount of blocked traffic. We apply Lagrangean relaxation as a solution approach and analyze the experiment results in terms of the blocking rate, service rate, and CPU time. The results show that the total call blocking rate (CBR) is much less sensitive to the call blocking probability (CBP) threshold of each BS when the load is light, rather than heavy; therefore, the more traffic loaded, the less the service rate will vary. BS recovery is much more important when the network load is light. However, the BS recovery ratio (BSRR), which is a key factor in reducing the blocking rate for a small number of BSs, is more important when a system is heavily loaded. The proposed model provides network survivability subject to available resources. The model also fits capacity expansion requirements by locating mobile/portable BSs in the places they are most needed.",
    "keywords": [
      "base station recovery",
      "lagrangean relaxation",
      "mathematical modeling",
      "network survivability",
      "performance evaluation",
      "quality of service"
    ]
  },
  {
    "id": "471",
    "title": "2D shallow water flow model for the hydraulic jump",
    "abstract": "A flow model is presented for predicting a hydraulic jump in a straight open channel. The model is based on the general 2D shallow water equations in strong conservation form, without artificial viscosity, which is usually incorporated into the flow equations to capture a hydraulic jump. The equations are discretised using the finite volume method. The results are compared with experimental data and available numerical results, and have shown that the present model can provide good results. The model is simple and easy to implement. To demonstrate the potential application of the model, several hydraulic jumps occurring in different situations are simulated, and the predictions are in good agreement with standard solution for open channel hydraulics. ",
    "keywords": [
      "shallow water flow",
      "finite volume method",
      "hydraulic jump",
      "open channel flow"
    ]
  },
  {
    "id": "472",
    "title": "Using multiple query representations in patent prior-art search",
    "abstract": "Before a patent application is made, it is important to search the appropriate databases for prior-art (i.e., pre-existing patents that may affect the validity of the application). Previous work on prior-art search has concentrated on single query representations of the patent application. In the following paper, we describe an approach which uses multiple query representations. We evaluate our technique using a well-known test collection (CLEF-IP 2011). Our results suggest that multiple query representations significantly outperform single query representations.",
    "keywords": [
      "patent search",
      "prior-art",
      "collaborative filtering"
    ]
  },
  {
    "id": "473",
    "title": "A genetic algorithm with tabu search procedure for flexible job shop scheduling with transportation constraints and bounded processing times",
    "abstract": "In this paper, we propose a model for Flexible Job Shop Scheduling Problem (FJSSP) with transportation constraints and bounded processing times. This is a NP hard problem. Objectives are to minimize the makespan and the storage of solutions. A genetic algorithm with tabu search procedure is proposed to solve both assignment of resources and sequencing problems on each resource. In order to evaluate the proposed algorithm's efficiency, five types of instances are tested. Three of them consider sequencing problems with or without assignment of processing or/and transport resources. The fourth and fifth ones introduce bounded processing times which mainly characterize Surface Treatment Facilities (STFs). Computational results show that our model and method are efficient for solving both assignment and scheduling problems in various kinds of systems.",
    "keywords": [
      "flexible job shop scheduling problem with transportation",
      "bounded processing times",
      "genetic algorithm",
      "tabu search",
      "flexible manufacturing system",
      "robotic cell",
      "surface treatment facility",
      "disjunctive graph"
    ]
  },
  {
    "id": "474",
    "title": "Intrusion detection by integrating boosting genetic fuzzy classifier and data mining criteria for rule pre-screening",
    "abstract": "The purpose of the work described in this paper is to provide an intelligent intrusion detection system (IIDS) that uses two of the most popular data mining tasks, namely classification and association rules mining together for predicting different behaviors in networked computers. To achieve this, we propose a method based on iterative rule learning using a fuzzy rule-based genetic classifier. Our approach is mainly composed of two phases. First, a large number of candidate rules are generated for each class using fuzzy association rules mining, and they are pre-screened using two rule evaluation criteria in order to reduce the fuzzy rule search space. Candidate rules obtained after pre-screening are used in genetic fuzzy classifier to generate rules for the classes specified in IIDS: namely Normal, PRB-probe, DOS-denial of service, U2R-user to root and R2L-remote to local. During the next stage, boosting genetic algorithm is employed for each class to find its fuzzy rules required to classify data each time a fuzzy rule is extracted and included in the system. Boosting mechanism evaluates the weight of each data item to help the rule extraction mechanism focus more on data having relatively more weight, i.e., uncovered less by the rules extracted until the current iteration. Each extracted fuzzy rule is assigned a weight. Weighted fuzzy rules in each class are aggregated to find the vote of each class label for each data item.",
    "keywords": [
      "intrusion detection",
      "genetic classifier",
      "fuzziness",
      "data mining",
      "weighted fuzzy rules"
    ]
  },
  {
    "id": "475",
    "title": "An Objective Perceptual Quality-Based ADTE for Adapting Mobile SVC Video Content",
    "abstract": "In this paper, we propose an Adaptation Decision-Taking Engine (ADTE) that targets the delivery of scalable video content in mobile usage environments. Our ADTE design relies on an objective perceptual quality metric in order to achieve video adaptation according to human visual perception, thus allowing to maximize the Quality of Service (QoS). To describe the characteristics of a particular usage environment, as well as the properties of the scalable video content, MPEG-21 Digital Item Adaptation (DIA) is used. Our experimental results show that the proposed ADTE design provides video content with a higher subjective quality than an ADTE using the conventional maximum-bit-allocation method.",
    "keywords": [
      "adaptation",
      "adte",
      "quality metric",
      "subjective quality",
      "svc"
    ]
  },
  {
    "id": "476",
    "title": "A Memory-Efficient Pipelined Implementation of the Aho-Corasick String-Matching Algorithm",
    "abstract": "With rapid advancement in Internet technology and usages, some emerging applications in data communications and network security require matching of huge volume of data against large signature sets with thousands of strings in real time. In this article, we present a memory-efficient hardware implementation of the well-known Aho-Corasick (AC) string-matching algorithm using a pipelining approach called P-AC. An attractive feature of the AC algorithm is that it can solve the string-matching problem in time linearly proportional to the length of the input stream, and the computation time is independent of the number of strings in the signature set. A major disadvantage of the AC algorithm is the high memory cost required to store the transition rules of the underlying deterministic finite automaton. By incorporating pipelined processing, the state graph is reduced to a character trie that only contains forward edges. Together with an intelligent implementation of look-up tables, the memory cost of P-AC is only about 18 bits per character for a signature set containing 6,166 strings extracted from Snort. The control structure of P-AC is simple and elegant. The cost of the control logic is very low. With the availability of dual-port memories in FPGA devices, we can double the system throughput by duplicating the control logic such that the system can process two data streams concurrently. Since our method is memory-based, incremental changes to the signature set can be accommodated by updating the look-up tables without reconfiguring the FPGA circuitry.",
    "keywords": [
      "algorithms",
      "design",
      "performance",
      "security",
      "string-matching",
      "deterministic and nondeterministic finite automaton",
      "pipelined processing",
      "intrusion detection system"
    ]
  },
  {
    "id": "477",
    "title": "Development of online suites of social science-based resources for health researchers and practitioners",
    "abstract": "The burgeoning of the Internet has enormous potential for bringing scientific research into the hands of both health practitioners and health researchers to enhance their job performance. In this article, the authors give two examples of how carefully developed and organized online resources can leverage the engaging multimedia formats, ubiquitous access, and low cost of the Internet to address this goal. The article describes two new online suites of social and behavioral science-based resources designed for those in the HIV/AIDS and teen pregnancy prevention fields: HIV Research and Practice Resources and Teen Pregnancy Research and Practice Resources. Each online suite includes research data, survey instruments, prevention resources, and evaluation-related publications and tools that can enhance prevention research and practice. The article ends by peering into the future at how the field of health-related prevention and research might be further advanced using the Internet.",
    "keywords": [
      "science-based resources",
      "internet",
      "hiv/aids",
      "prevention",
      "health",
      "research",
      "practice"
    ]
  },
  {
    "id": "478",
    "title": "Deferring elimination of design alternatives in object-oriented methods",
    "abstract": "While developing systems, software engineers generally have to deal with a large number of design alternatives. Current object-oriented methods aim to eliminate design alternatives whenever they are generated. Alternatives, however, should be eliminated only when sufficient information to take such a decision is available. Otherwise, alternatives have to be preserved to allow further refinements along the development process. Too early elimination of alternatives results in loss of information and excessive restriction of the design space. This paper aims to enhance the current object-oriented methods by modeling and controlling the design alternatives through the application of fuzzy-logic-based techniques. By using an example method, it is shown that the proposed approach increases the adaptability and reusability of design models. The method has been implemented and tested in our experimental CASE environment. ",
    "keywords": [
      "design alternatives",
      "object-oriented methods",
      "fuzzy logic",
      "adaptable design models",
      "case environments",
      "software artifacts"
    ]
  },
  {
    "id": "479",
    "title": "Fiber reinforced concrete properties - a multiscale approach",
    "abstract": "This paper describes the development of a fiber reinforced concrete (FRC) unit cell for analyzing concrete structures by executing a multiscale analysis procedure using the theory of homogenization. This was achieved through solving a periodic unit cell problem of the material in order to evaluate its macroscopic properties. Our research describes the creation of an FRC unit cell through the use of concrete paste generic information e.g. the percentage of aggregates, their distribution, and the percentage of fibers in the concrete. The algorithm presented manipulates the percentage and distribution of these aggregates along with fiber weight to create a finite element unit cell model of the FRC which can be used in a multiscale analysis of concrete structures.",
    "keywords": [
      "frc-fibered reinforced concrete",
      "multiscale analysis",
      "concrete unit cell",
      "elastic properties",
      "mesoscale concrete finite element model"
    ]
  },
  {
    "id": "480",
    "title": "NEW SPLINE SPACES WITH GENERALIZED TENSION PROPERTIES",
    "abstract": "The paper describes a new space of variable degree polynomials. This space is isomorphic to P(6), possesses a Bernstein like basis and has generalized tension properties in the sense that, for limit values of the degrees, its functions approximate quadratic polynomials. The corresponding space of C(3), variable degree splines is also studied. This spline space can be profitably used in the construction of shape preserving curves or surfaces.",
    "keywords": [
      "variable degree polynomials",
      "bernstein basis",
      "b-splines",
      "shape preservation"
    ]
  },
  {
    "id": "481",
    "title": "ANTHEPROT: An integrated protein sequence analysis software with client/server capabilities",
    "abstract": "Programs devoted to the analysis of protein sequences exist either as stand-alone programs or as Web servers. However, stand-alone programs can hardly accommodate for the analysis that involves comparisons on databanks, which require regular updates. Moreover, Web servers cannot be as efficient as stand-alone programs when dealing with real-time graphic display. We describe here a stand-alone software program called ANTHEPROT, which is intended to perform protein sequence analysis with a high integration level and clients/server capabilities. It is an interactive program with a graphical user interface that allows handling of protein sequence and data in a very interactive and convenient manner. It provides many methods and tools, which are integrated into a graphical user interface. ANTHEPROT is available for Windows-based systems. It is able to connect to a Web server in order to perform large-scale sequence comparison on up-to-date databanks. ANTHEPROT is freely available to academic users and may be downloaded at http://pbil.ibcp.fr/ANTHEPROT.",
    "keywords": [
      "protein sequence analysis",
      "multiple alignment",
      "secondary structure prediction",
      "web server"
    ]
  },
  {
    "id": "482",
    "title": "Cardioids-based faster authentication and diagnosis of remote cardiovascular patients",
    "abstract": "In recent times, dealing with deaths associated with cardiovascular diseases (CVD) has been one of the most challenging issues. The usage of mobile phones and portable Electrocardiogram (ECG) acquisition devices can mitigate the risks associated with CVD by providing faster patient diagnosis and patient care. The existing technologies entail delay in patient authentication and diagnosis. However, for the cardiologists minimizing the delay between a possible CVD symptom and patient care is crucial, as this has a proven impact in the longevity of the patient. Therefore, every seconds counts in terms of patient authentication and diagnosis. In this paper, we introduce the concept of Cardioid based patient authentication and diagnosis. According to our experimentations, the authentication time can be reduced from 30.64 s (manual authentication in novice mobile user) to 0.4398 s (automated authentication). Our ECG based patient authentication mechanism is up to 4878 times faster than conventional biometrics like, face recognition. The diagnosis time could be improved from several minutes to less than 0.5 s (cardioid display on a single screen). Therefore, with our presented mission critical alerting mechanism on wireless devices, minute's worth of tasks can be reduced to second's, without compromising the accuracy of authentication and quality of diagnosis. ",
    "keywords": [
      "mission critical alerting",
      "cardiovascular disease detection",
      "remote monitoring",
      "wireless monitoring",
      "patient authentication",
      "cardioid"
    ]
  },
  {
    "id": "483",
    "title": "A multiprocessor system-on-chip for real-time biomedical monitoring and analysis: ECG prototype architectural design space exploration",
    "abstract": "In this article we focus on multiprocessor system-on- chip ( MPSoC) architectures for human heart electrocardiogram ( ECG) real time analysis as a hardware/ software ( HW/SW) platform offering an advance relative to state-of-the- art solutions. This is a relevant biomedical application with good potential market, since heart diseases are responsible for the largest number of yearly deaths. Hence, it is a good target for an application-specific system-on- chip (SoC) and HW/ SW codesign. We investigate a symmetric multiprocessor architecuture based on STMicroelecronics VLIW DSPs that process in real time 12-lead ECG signals. This architecture improves upon state-of-the-art SoC designs for ECG analysis in its ability to analyze the full 12 leads in real time, even with high sampling frequencies, and its ability to detect heart malfunction for the whole ECG signal interval. We explore the design space by considering a number of hardware and software architectural options. Comparing our design with present-day solutions from an SoC and application point-ofview shows that our platform can be used in real time and without failures.",
    "keywords": [
      "performance",
      "design",
      "experimentation"
    ]
  },
  {
    "id": "484",
    "title": "Accurate Approximation of the Earth Mover's Distance in Linear Time",
    "abstract": "Color descriptors are one of the important features used in content-based image retrieval. The dominant color descriptor (DCD) represents a few perceptually dominant colors in an image through color quantization. For image retrieval based on DCD, the earth mover's distance (EMD) and the optimal color composition distance were proposed to measure the dissimilarity between two images. Although providing good retrieval results, both methods are too time-consuming to be used in a large image database. To solve the problem, we propose a new distance function that calculates an approximate earth mover's distance in linear time. To calculate the dissimilarity in linear time, the proposed approach employs the space-filling curve for multidimensional color space. To improve the accuracy, the proposed approach uses multiple curves and adjusts the color positions. As a result, our approach achieves order-of-magnitude time improvement but incurs small errors. We have performed extensive experiments to show the effectiveness and efficiency of the proposed approach. The results reveal that our approach achieves almost the same results with the EMD in linear time.",
    "keywords": [
      "earth mover's distance",
      "approximation",
      "content-based image retrieval"
    ]
  },
  {
    "id": "485",
    "title": "Variable selection in linear regression: Several approaches based on normalized maximum likelihood",
    "abstract": "The use of the normalized maximum likelihood (NML) for model selection in Gaussian linear regression poses troubles because the normalization coefficient is not finite. The most elegant solution has been proposed by Rissanen and consists in applying a particular constraint for the data space. In this paper, we demonstrate that the methodology can be generalized, and we discuss two particular cases, namely the rhomboidal and the ellipsoidal constraints. The new findings are used to derive four NML-based criteria. For three of them which have been already introduced in the previous literature, we provide a rigorous analysis. We also compare them against five state-of-the-art selection rules by conducting Monte Carlo simulations for families of models commonly used in signal processing. Additionally, for the eight criteria which are tested, we report results on their predictive capabilities for real life data sets.",
    "keywords": [
      "gaussian linear regression",
      "model selection",
      "normalized maximum likelihood",
      "rhomboidal constraint",
      "ellipsoidal constraint"
    ]
  },
  {
    "id": "486",
    "title": "Spacetime adaptive finite difference method for European multi-asset options",
    "abstract": "The multi-dimensional BlackScholes equation is solved numerically for a European call basket option using a prioria posteriori error estimates. The equation is discretized by a finite difference method on a Cartesian grid. The grid is adjusted dynamically in space and time to satisfy a bound on the global error. The discretization errors in each time step are estimated and weighted by the solution of the adjoint problem. Bounds on the local errors and the adjoint solution are obtained by the maximum principle for parabolic equations. Comparisons are made with Monte Carlo and quasi-Monte Carlo methods in one dimension, and the performance of the method is illustrated by examples in one, two, and three dimensions.",
    "keywords": [
      "blackscholes equation",
      "finite difference method",
      "space adaptation",
      "time adaptation",
      "maximum principle"
    ]
  },
  {
    "id": "487",
    "title": "alpha-words and the radix order",
    "abstract": "Let alpha = (a(1), a(2),...) be a sequence (finite or infinite) of integers with a(1) >= 0 and a(n) >= 1, for all n >= 2. Let {a, b} be an alphabet. For n >= 1, and r = r(1)r(2)...r(n) is an element of N(n) with 0  = 2. Many interesting combinatorial properties of alpha-words have been studied by Chuan. In this paper, we obtain some new methods of generating the distinct alpha-words of the same order in lexicographic order. Among other results, we consider another function r bar right arrow w[r] from the set of labels of alpha-words to the set of alpha-words. The string r is called a new label of the alpha-word w[r]. Using any new label of an nth-order alpha-word w, we can compute the number of the nth-order alpha-words that are less than w in the lexicographic order. With the radix orders <(r) on N(n) (regarding N as an alphabet) and {a, b}(+) with a <(r) b, we prove that there exists a subset D of the set of all labels such that w[r] <(r) w[s] whenever r, s is an element of D and r <(r) S.  ",
    "keywords": [
      "alpha-word",
      "radix order",
      "lexicographic order"
    ]
  },
  {
    "id": "488",
    "title": "A buyerseller game model for selection and negotiation of purchasing bids: Extensions and new models",
    "abstract": "A number of efficiency-based vendor selection and negotiation models have been developed to deal with multiple attributes including price, quality and delivery performance. The efficiency is defined as the ratio of weighted outputs to weighted inputs. By minimizing the efficiency, Talluri [Eur. J. Operat. Res. 143(1) (2002) 171] proposes a buyerseller game model that evaluates the efficiency of alternative bids with respect to the ideal target set by the buyer. The current paper shows that this buyerseller game model is closely related to data envelopment analysis (DEA) and can be simplified. The current paper also shows that setting the (ideal) target actually incorporates implicit tradeoff information on the multiple attributes into efficiency evaluation. We develop a new buyerseller game model where the efficiency is maximized with respect to multiple targets set by the buyer. The new model allows the buyer to evaluate and select the vendors in the context of best-practice. By both minimizing and maximizing efficiency, the buyer can obtain an efficiency range within which the true efficiency lies given the implicit tradeoff information characterized by the targets. The current study establishes the linkage between buyerseller game models and DEA. Such a linkage can provide the buyer with correct evaluation methods based upon existing DEA models regarding the nature of bidding.",
    "keywords": [
      "game models",
      "linear programming",
      "efficiency",
      "data envelopment analysis"
    ]
  },
  {
    "id": "489",
    "title": "A Bayesian latent variable model with classification and regression tree approach for behavior and credit scoring",
    "abstract": "A Bayesian latent variable model with classification and regression tree approach is built to overcome three challenges encountered by a bank in credit-granting process. These three challenges include (1) the bank wants to predict the future performance of an applicant accurately; (2) given current information about cardholders credit usage and repayment behavior, financial institutions would like to determine the optimal credit limit and APR for an applicant; and (3) the bank would like to improve its efficiency by automating the process of credit-granting decisions. Data from a leading bank in Taiwan is used to illustrate the combined approach. The data set consists of each credit card holders credit usage and repayment data, demographic information, and credit report. Empirical study shows that the demographic variables used in most credit scoring models have little explanatory ability with regard to a cardholders credit usage and repayment behavior. A cardholders credit history provides the most important information in credit scoring. The continuous latent customer quality from the Bayesian latent variable model allows considerable latitude for producing finer rules for credit granting decisions. Compared to the performance of discriminant analysis, logistic regression, neural network, multivariate adaptive regression splines (MARS) and support vector machine (SVM), the proposed model has a 92.9% accuracy rate in predicting customer types, is less impacted by prior probabilities, and has a significantly low Type I errors in comparison with the other five approaches.",
    "keywords": [
      "behavior scoring",
      "credit scoring",
      "bayesian",
      "latent variable model",
      "classification and regression tree"
    ]
  },
  {
    "id": "490",
    "title": "The representation of manufacturing requirements in model-driven parts manufacturing",
    "abstract": "Today there is a need to make process and production planning more cost-effective while not compromising the quality of the product. Manufacturing requirements are used to ensure producibility in early development phases and also as a source for continuous improvement of the manufacturing system. To make this possible it is essential to have correct, updated information available and to be able to trace the relations between requirements and their origin and subjects. To trace requirements' origin in resources or processes is today very difficult owing to system integration problems. This article discusses the relations that need to be represented and proposes the use of model-based methods to enable traceability of requirements. Because requirements are a collaborative effort a standard for information exchange is needed. The ISO10303 STEP application protocol AP233 System Engineering is proposed for this purpose.",
    "keywords": [
      "information management",
      "manufacturing",
      "requirements"
    ]
  },
  {
    "id": "491",
    "title": "Baldwinian learning utilizing genetic and heuristic algorithms for logic synthesis and minimization of incompletely specified data with Generalized ReedMuller (ANDEXOR) forms",
    "abstract": "This research applies a new heuristic combined with a genetic algorithm (GA) to the task of logic minimization for incompletely specified data, with both single and multi-outputs, using the Generalized ReedMuller (GRM) equation form. The GRM equation type is a canonical expression of the Exclusive-Or Sum-of-Products (ESOPs) type, in which for every subset of input variables there exists not more than one term with arbitrary polarities of all variables. This ANDEXOR implementation has been shown to be economical, generally requiring fewer gates and connections than that of ANDOR logic. GRM logic is also highly testable, making it desirable for FPGA designs. The minimization results of this new algorithm tested on a number of binary benchmarks are given. This minimization algorithm utilizes a GA with a two-level fitness calculation, which combines human-designed heuristics with the evolutionary process, employing Baldwinian learning. In this algorithm, first a pure GA creates certain constraints for the selection of chromosomes, creating only genotypes (polarity vectors). The phenotypes (GRMs) are then learned in the environment and contribute to the GA fitness (which is the total number of terms of the best GRM for each output), providing indirect feedback as to the quality of the genotypes (polarity vectors) but the genotype chromosomes (polarity vectors) remain unchanged. In this process, the improvement in genotype chromosomes (polarity vectors) is the product of the evolutionary processes from the GA only. The environmental learning is achieved using a human-designed GRM minimization heuristic. As much previous research has presented the merit of ANDEXOR logic for its high density and testability, this research is the first application of the GRM (a canonical ANDEXOR form) to the minimization of incompletely specified data.",
    "keywords": [
      "incompletely specified generalized reedmuller forms",
      "andexor forms",
      "logic synthesis and minimization",
      "baldwinian learning",
      "genetic algorithms"
    ]
  },
  {
    "id": "492",
    "title": "Energy stable numerical methods for hyperbolic partial differential equations using overlapping domain decomposition",
    "abstract": "Overlapping domain decomposition methods, otherwise known as overset grid or chimera methods, are useful for simplifying the discretization of partial differential equations in or around complex geometries. Though in wide use, such methods are prone to numerical instability unless numerical diffusion or some other form of regularization is used, especially for higher-order methods. To address this shortcoming, high-order, provably energy stable, overlapping domain decomposition methods are derived for hyperbolic initial boundary value problems. The overlap is treated by splitting the domain into pieces and using generalized summation-by-parts derivative operators and polynomial interpolation. New implicit and explicit operators are derived that do not require regularization for stability in the linear limit. Applications to linear and nonlinear problems in one and two dimensions are presented, where it is found the explicit operators are preferred to the implicit ones.",
    "keywords": [
      "high order finite difference methods",
      "overlapping domain decomposition",
      "numerical stability",
      "generalized summation-by-parts"
    ]
  },
  {
    "id": "493",
    "title": "The Impact of Cluster Representatives on the Convergence of the K-Modes Type Clustering",
    "abstract": "As a leading partitional clustering technique, k-modes is one of the most computationally efficient clustering methods for categorical data. In the k-modes, a cluster is represented by a \"mode,\" which is composed of the attribute value that occurs most frequently in each attribute domain of the cluster, whereas, in real applications, using only one attribute value in each attribute to represent a cluster may not be adequate as it could in turn affect the accuracy of data analysis. To get rid of this deficiency, several modified clustering algorithms were developed by assigning appropriate weights to several attribute values in each attribute. Although these modified algorithms are quite effective, their convergence proofs are lacking. In this paper, we analyze their convergence property and prove that they cannot guarantee to converge under their optimization frameworks unless they degrade to the original k-modes type algorithms. Furthermore, we propose two different modified algorithms with weighted cluster prototypes to overcome the shortcomings of these existing algorithms. We rigorously derive updating formulas for the proposed algorithms and prove the convergence of the proposed algorithms. The experimental studies show that the proposed algorithms are effective and efficient for large categorical datasets.",
    "keywords": [
      "clustering",
      "k-modes type clustering algorithms",
      "categorical data",
      "weighted cluster prototype",
      "convergence"
    ]
  },
  {
    "id": "494",
    "title": "Service quality and ERP implementation: A conceptual and empirical study of semiconductor-related industries in Taiwan",
    "abstract": "This paper examines the effectiveness of the implementation of enterprise resource planning (ERP) in improving service quality in the Taiwanese semiconductor industry by assessing the expectations and the perceptions of service quality from the perspectives of both upstream manufacturers and downstream customers. The study first establishes a modified service quality gap model incorporating: (i) the downstream customers' expectations and perceptions, and (ii) the upstream manufacturers' perceptions of the customers' expectations and perceptions. An empirical study by questionnaire survey is then undertaken to investigate the gaps proposed in the research model. The results show that service quality gaps do exist in the Taiwanese semiconductor industry between upstream manufacturers that are implementing ERP and their downstream customers. The study shows that the proposed model provides valuable guidance to manufacturers with respect to the prevention, detection, and elimination of the demonstrated service quality gaps. The model thus helps manufacturers to evaluate the contribution of various ERP modules to improved customer satisfaction with service quality and also provides guidance on improvement strategies to enhance service quality by eliminating quality gaps.  ",
    "keywords": [
      "enterprise resource planning ",
      "semiconductor industry",
      "service quality gaps",
      "erp implementation"
    ]
  },
  {
    "id": "495",
    "title": "Largest inscribed rectangles in convex polygons",
    "abstract": "We consider approximation algorithms for the problem of computing an inscribed rectangle having largest area in a convex polygon on n  vertices. If the order of the vertices of the polygon is given, we present a randomized algorithm that computes an inscribed rectangle with area at least (1) ( 1) times the optimum with probability t  in time O ( 1 ? log n ) for any constant t<1 t < 1 . We further give a deterministic approximation algorithm that computes an inscribed rectangle of area at least (1) ( 1) times the optimum in running time O ( 1 ? 2 log n ) and show how this running time can be slightly improved.",
    "keywords": [
      "approximation algorithms",
      "geometric algorithms",
      "largest area rectangle",
      "inscribed rectangles in polygons"
    ]
  },
  {
    "id": "496",
    "title": "An effective node-selection scheme for the energy efficiency of solar-powered WSNs in a stream environment",
    "abstract": "We propose an effective node-selection scheme in the stream environment of solar-powered WSNs. We analyzed the stream environment including single stream and cross-stream cases. The deployment conditions are appropriate to each stream case. Based on the node selection scheme, the number of active nodes and transmitted packets is minimized. The proposed scheme prolongs the lifetime of the solar-powered WSN in a stream environment.",
    "keywords": [
      "sensor deployment",
      "node-selection",
      "stream environment",
      "solar-powered sensor",
      "wireless sensor network"
    ]
  },
  {
    "id": "497",
    "title": "The concept of a quasi-particle and the non-probabilistic interpretation of wave mechanics",
    "abstract": "In recent works of the author [found Phys 36 (2006) 1701-1717, Math Comput simul 74 (2007) 93-103], the argument has been made that Hertz's equations of electrodynamics reflect the material invariance (indifference) of the latter. Then the principle of material invariance was postulated in heu of Lorentz covariance. and the respective absolute medium wits named the metacontinuum Here. we go further to assume that the metacontinuum is a very thin but very stuff 3D hypershell in the 4D space The equation for the deflection of the shell along the fourth dimension is the \"master\" nonlinear dispersive equation of wave mechanics whose linear part (Euler-Bernoulli equation) is nothing else but the Schrodinger wave equation written for the real or the imaginary part of the wave function. The wave function has a clear non-probabilistic interpretation as the actual amplitude of the flexural deformation The \"master\" equation admits solitary-wave solutions/solutions that behave as quasi-particles (QPs). We stipulate that particles are our perception of the QPs (schaumkommen in Schrodinger's own words). We show the passage from the continuous Lagrangian of the field to the discrete Lagrangian of the centers of QPs and introduce the concept of (pseudo)mass. We interpret the membrane tension as all attractive (gravitational?) force acting between the QPs. Thus. it self-consistent unification of electrodynamics, wave mechanics, gravitation. and the wave-particle duality is achieved  ",
    "keywords": [
      "luminiferous metacontinuum",
      "maxwell-hertz electrodynamics",
      "schrodinger wave mechanics",
      "quasi-particles",
      "particle-wave duality"
    ]
  },
  {
    "id": "498",
    "title": "An empirical study of the expressiveness of the functional basis",
    "abstract": "Function models are frequently used in engineering design to describe the technical functions that a product performs. This paper investigates the use of the functional basis, a function vocabulary developed to aid in communication and archiving of product function information, in describing consumer products that have been decomposed, analyzed, modeled functionally, and stored in a Web-based design repository. The frequency of use of function terms and phrases in 11 graphical and 110 list-based representations in the repository is examined and used to analyze the organization and expressiveness of the functional basis and function models. Within the context of reverse engineering, we determined that the modeling resolution provided by the hierarchical levels, especially the tertiary level, is inadequate for function modeling; the tertiary terms are inappropriate for capturing sufficient details desired by modelers for archiving and reuse, and there is a need for a more expressive flow terms and flow qualifiers in the vocabulary. A critical comparison is also presented of two representations in the design repository: function structures and function lists. The conclusions are used to identify new research opportunities, including the extension of the vocabulary to incorporate flow qualifiers in addition to more expressive terms.",
    "keywords": [
      "functional basis",
      "function model",
      "function representation",
      "vocabulary"
    ]
  },
  {
    "id": "499",
    "title": "A 6.7 kbps vector sum excited linear prediction on TMS320C54X digital signal processor",
    "abstract": "In this paper, a 6.7-kbps vector sum excited linear prediction (VSELP) coder with less computational complexity is presented. A very efficient VSELP codebook with nine basis vectors and a heuristic K-selection method (to reduce the search space and complexity) is constructed to obtain the stochastic codebook vector. The nine basis vectors are obtained by optimizing a set of randomly generated basis vectors. During the optimization process, we have trained the basis vectors to give the system apriori knowledge of the characteristics of the input. The coder is implemented on a TMS320C541 digital signal processor. The performance is evaluated by testing the 6.7-kbps VSELP coder with different test speech data taken from different speakers. The quality of the coder is estimated by comparing the performance of the 6.7-kbps VSELP coder with an 8-kbps VSELP speech coder based on the IS-54 standards.  ",
    "keywords": [
      "vector sum excited linear prediction",
      "code excited linear prediction",
      "linear predictive coding",
      "digital signal processor"
    ]
  },
  {
    "id": "500",
    "title": "A design-for-digital-testability circuit structure for Sigma-Delta modulators",
    "abstract": "A design-for-digital-testability (DfDT) switched-capacitor circuit structure for testing Sigma-Delta modulators with digital stimuli is presented to reduce the overall testing cost. In the test mode, the DfDT circuits are reconfigured as a one-bit digital-to-charge converter to accept a repetitively applied Sigma-Delta modulated bit-stream as its stimulus. The single-bit characteristic ensures that the generated stimulus is nonlinearity free. In addition, the proposed DfDT structure reuses most of the analog components in the test mode and keeps the same loads for the operational amplifiers as if they were in the normal mode. It thereby achieves many advantages including lower cost, higher fault coverage, higher measurement accuracy, and the capability of performing at-speed tests. A second-order Sigma-Delta modulator was designed and fabricated to demonstrate the effectiveness of the DfDT structure. Our experimental results show that the digital test is able to measure a harmonic distortion lower than -106 dBFS. Meanwhile, the dynamic range measured with the digital stimulus is as high as 84.4 dB at an over-sampling ratio of 128. The proposed DfDT scheme can be easily applied to other types of Sigma-Delta modulators, making them also digitally testable.",
    "keywords": [
      "analog-to-digital converter ",
      "design-for-testability ",
      "digitally testable",
      "mixed-signal circuit testing",
      "sigma-delta modulator"
    ]
  },
  {
    "id": "501",
    "title": "Local Community Detection Using Link Similarity",
    "abstract": "Exploring local community structure is an appealing problem that has drawn much recent attention in the area of social network analysis. As the complete information of network is often difficult to obtain, such as networks of web pages, research papers and Facebook users, people can only detect community structure from a certain source vertex with limited knowledge of the entire graph. The existing approaches do well in measuring the community quality, but they are largely dependent on source vertex and putting too strict policy in agglomerating new vertices. Moreover, they have predefined parameters which are difficult to obtain. This paper proposes a method to find local community structure by analyzing link similarity between the community and the vertex. Inspired by the fact that elements in the same community are more likely to share common links, we explore community structure heuristically by giving priority to vertices which have a high link similarity with the community. A three-phase process is also used for the sake of improving quality of community structure. Experimental results prove that our method performs effectively not only in computer-generated graphs but also in real-world graphs.",
    "keywords": [
      "social network analysis",
      "community detection",
      "link similarity"
    ]
  },
  {
    "id": "502",
    "title": "Leveraging information technology towards enhancing patient care and a culture of safety in the US",
    "abstract": "Objectives: To heighten awareness about the critical issues currently affecting patient care and to propose solutions based on leveraging information technologies to enhance patient care and influence a culture of patient safety. Methods: Presentation and discussion of the issues affecting health care today, such as medical and medication-related errors and analysis of their root causes; proliferation of medical knowledge and medical technologies; initiatives to improve patient safety; steps necessary to develop a culture of safety; introduction of relevant enabling technologies; and evidence of results. Results and Conclusions: Medical errors affect not only mortality and morbidity, but they also create secondary costs leading to dissatisfaction by both provider and patient. Health care has been slow to acknowledge the benefits of enabling technologies to affect the quality of care. Evaluation of recent applications, such as the computerized patient record, physician order entry, and computerized alerting systems show tremendous potential to enhance patient care and influence the development of a culture focused on safety. They will also bring about changes in other areas, such as workflow and the creation of new partnerships among providers, patients, and payers.",
    "keywords": [
      "medical errors",
      "patient safety",
      "information technology",
      "computerized patient record",
      "physician order entry",
      "clinical decision support",
      "clinical outcomes",
      "evidence-based medicine"
    ]
  },
  {
    "id": "503",
    "title": "MMAPDNG: A new, fast code backed by a memory-mapped database for simulating delayed-ray emission with MCNPX package",
    "abstract": "The simulation of the emission of beta-delayed gamma rays following nuclear fission and the calculation of time-dependent energy spectra is a computational challenge. The widely used radiation transport code MCNPX includes a delayed gamma-ray routine that is inefficient and not suitable for simulating complex problems. This paper describes the code MMAPDNG (Memory-Mapped Delayed Neutron and Gamma), an optimized delayed gamma module written in C, discusses usage and merits of the code, and presents results. The approach is based on storing required Fission Product Yield (FPY) data, decay data, and delayed particle data in a memory-mapped file. When compared to the original delayed gamma-ray code in MCNPX, memory utilization is reduced by two orders of magnitude and the ray sampling is sped up by three orders of magnitude. Other delayed particles such as neutrons and electrons can be implemented in future versions of MMAPDNG code using its existing framework.",
    "keywords": [
      "delayed gamma",
      "fission products",
      "mcnpx",
      "mmap"
    ]
  },
  {
    "id": "504",
    "title": "Measurement of wireless pressure sensors fabricated in high temperature co-fired ceramic MEMS technology",
    "abstract": "High temperature co-fired ceramics (HTCCs) have wide applications with stable mechanical properties, but they have not yet been used to fabricate sensors. By introducing the wireless telemetric sensor system and ceramic structure embedding a pressure-deformable cavity, the designed sensors made from HTCC materials (zirconia and 96% alumina) are fabricated, and their capacities for the pressure measurement are tested using a wireless interrogation method. Using the fabricated sensor, a study is conducted to measure the atmospheric pressure in a sealed vessel. The experimental sensitivity of the device is 2 Hz/Pa of zirconia and 1.08 Hz/Pa of alumina below 0.5 MPa with a readout distance of 2.5 cm. The described sensor technology can be applied for monitoring of atmospheric pressure to evaluate important component parameters in harsh environments.",
    "keywords": [
      "high temperature co-fired ceramic ",
      "wireless",
      "micro-electro-mechanical systems "
    ]
  },
  {
    "id": "505",
    "title": "real time rectification using differentially encoded lookup table",
    "abstract": "In this paper, we propose a new real time rectification technique based on the compressed lookup table. To compress the lookup table we adopt a differential encoding. As a result, we successfully constructed the rectification with obtaining the compression ratio of 73% so as to fulfill real-time requirement (i.e., 40 fps at 74.25Mhz). Furthermore, our result on performance is comparable to the result of [17] that obtains 85fps at 90MHz for 640x512 images.",
    "keywords": [
      "data compression",
      "rectification",
      "differential encoding",
      "real time",
      "lookup table"
    ]
  },
  {
    "id": "506",
    "title": "Development of a maximum likelihood regression tree-based model for predicting subway incident delay",
    "abstract": "We develop a maximum likelihood regression tree-based model to predict subway incident delays. An AFT model is assigned to each terminal node in the maximum likelihood regression tree. Our tree-based model outperforms the traditional AFT models with fixed and random effects. Our tree-based model can account for the heterogeneity effect as well as avoid the over-fitting problem.",
    "keywords": [
      "subway incidents",
      "delay",
      "maximum likelihood regression tree",
      "accelerated failure time"
    ]
  },
  {
    "id": "507",
    "title": "Solving Inverse Frequent Itemset Mining with Infrequency Constraints via Large-Scale Linear Programs",
    "abstract": "Inverse frequent set mining (IFM) is the problem of computing a transaction database D satisfying given support constraints for some itemsets, which are typically the frequent ones. This article proposes a new formulation of IFM, called IFMI (IFM with infrequency constraints), where the itemsets that are not listed as frequent are constrained to be infrequent; that is, they must have a support less than or equal to a specified unique threshold. An instance of IFMI can be seen as an instance of the original IFM by making explicit the infrequency constraints for the minimal infrequent itemsets, corresponding to the so-called negative generator border defined in the literature. The complexity increase from PSPACE (complexity of IFM) to NEXP (complexity of IFMI) is caused by the cardinality of the negative generator border, which can be exponential in the original input size. Therefore, the article introduces a specific problem parameter. that computes an upper bound to this cardinality using a hypergraph interpretation for which minimal infrequent itemsets correspond to minimal transversals. By fixing a constant k, the article formulates a k-bounded definition of the problem, called k-IFMI, that collects all instances for which the value of the parameter. is less than or equal to k-its complexity is in PSPACE as for IFM. The bounded problem is encoded as an integer linear program with a large number of variables (actually exponential w.r.t. the number of constraints), which is thereafter approximated by relaxing integer constraints-the decision problem of solving the linear program is proven to be in NP. In order to solve the linear program, a column generation technique is used that is a variation of the simplex method designed to solve large-scale linear programs, in particular with a huge number of variables. The method at each step requires the solution of an auxiliary integer linear program, which is proven to be NP hard in this case and for which a greedy heuristic is presented. The resulting overall column generation solution algorithm enjoys very good scaling as evidenced by the intensive experimentation, thereby paving the way for its application in real-life scenarios.",
    "keywords": [
      "algorithms",
      "experimentation",
      "theory",
      "frequent itemset mining",
      "inverse problem",
      "minimal hypergraph transversals",
      "column generation simplex"
    ]
  },
  {
    "id": "508",
    "title": "An efficient optimization procedure for tetrahedral meshes by chaos search algorithm",
    "abstract": "A simple and efficient local optimization-based procedure for node repositioning/smoothing of three-dimensional tetrahedral meshes is presented. The initial tetrahedral mesh is optimized with respect, to a specified element shape measure by chaos search algorithm, which is very effective for the optimization problems with only a few design variables. Examples show that the presented smoothing procedure can provide favorable conditions for local transformation approach and the quality of mesh can be significantly improved by the combination of these two procedures with respect to a specified element shape measure. Meanwhile, several commonly used shape measures for tetrahedral element, which are considered to be equivalent in some weak sense over a long period of time, are briefly re-examined in this paper. Preliminary study indicates that using different measures to evaluate the change of element shape will probably lead to inconsistent result for both well shaped and poorly shaped elements. The proposed smoothing approach can be utilized as an appropriate and effective tool for evaluating element shape measures and their influence on mesh optimization process and optimal solution.",
    "keywords": [
      "mesh optimization",
      "smoothing",
      "chaos search algorithm",
      "element shape measure"
    ]
  },
  {
    "id": "509",
    "title": "Ethical Issues in Selecting Embryos",
    "abstract": "People involved in assisted reproduction frequently make decisions about which of several embryos to implant or which of several embryos to reduce from a multiple pregnancy. Yet, others have raised questions about the ethical acceptability of using sex or genetic characteristics as selection criteria. This paper reviews arguments for rejecting embryo selection and discusses the subject of choosing offspring in terms of the centrality of liberty and autonomous choice in ethics. It also presents a position on the acceptable scope of embryo selection and the professional responsibilities of those who practice reproductive medicine.",
    "keywords": [
      "ethics",
      "embryos",
      "sex selection",
      "reproductive choice",
      "liberty",
      "disabilities",
      "discrimination"
    ]
  },
  {
    "id": "510",
    "title": "Competitive equilibrium in e-commerce: Pricing and outsourcing",
    "abstract": "The success of firms engaged in e-commerce depends on their ability to understand and exploit the dynamics of the market. One component of this is the ability to extract maximum profit and minimize costs in the face of the harsh competition that the internet provides. We present a general framework for modeling the competitive equilibrium across two firms, or across a firm and the market as a whole. Within this framework, we study pricing choices and analyze the decision to outsource IT capability. Our framework is novel in that it allows for any number of distributions on usage levels, priceQoS tradeoffs, and price and cost structures.",
    "keywords": [
      "e-commerce",
      "non-cooperative nash equilibrium",
      "pricing",
      "qos",
      "outsourcing"
    ]
  },
  {
    "id": "511",
    "title": "Numerical simulation of 3D fluid-structure interaction flow using an immersed object method with overlapping grids",
    "abstract": "The newly developed immersed object method (IOM) [Tai CH, Zhao Y, Liew KM. Parallel computation of unsteady incompressible viscous flows around moving rigid bodies using an immersed object method with overlapping grids. J Comput Phys 2005; 207(l): 151-72] is extended for 3D unsteady flow simulation with fluid-structure interaction (FSI), which is made possible by combining it with a parallel unstructured multigrid Navier-Stokes solver using a matrix-free implicit dual time stepping and finite volume method [Tai CH, Zhao Y, Liew KM. Parallel computation of unsteady three-dimensional incompressible viscous flow using an unstructured multigrid method. In: The second M.I.T. conference on computational fluid and solid mechanics, June 17-20, MIT, Cambridge, MA 02139, USA, 2003; Tai CH, Zhao Y, Liew KM. Parallel computation of unsteady three-dimensional incompressible viscous flow using an unstructured multigrid method, Special issue on \"Preconditioning methods: algorithms, applications and software environments. Comput Struct 2004; 82(28): 2425-36]. This uniquely combined method is then employed to perform detailed study of 3D unsteady flows with complex FSI. In the IOM, a body force term F is introduced into the momentum equations during the artificial compressibility (AC) sub-iterations so that a desired velocity distribution V-0 can be obtained on and within the object boundary, which needs not coincide with the grid, by adopting the direct forcing method. An object mesh is immersed into the flow domain to define the boundary of the object. The advantage of this is that bodies of almost arbitrary shapes can be added without grid restructuring, a procedure which is often time-consuming and computationally expensive. It has enabled us to perform complex and detailed 3D unsteady blood flow and blood-leaflets interaction in a mechanical heart valve (MHV) under physiological conditions.  ",
    "keywords": [
      "fluid-structure interaction",
      "immersed object method",
      "overlapping grids",
      "unstructured parallel-multigrid computation",
      "matrix-free implicit method",
      "3d unsteady incompressible flows",
      "mechanical heart valves"
    ]
  },
  {
    "id": "512",
    "title": "Joint pricing and inventory control with a Markovian demand model",
    "abstract": "We consider the joint pricing and inventory control problem for a single product over a finite horizon and with periodic review. The demand distribution in each period is determined by an exogenous Markov chain. Pricing and ordering decisions are made at the beginning of each period and all shortages are backlogged. The surplus costs as well as fixed and variable costs are state dependent. We show the existence of an optimal (s,S,p)-type feedback policy for the additive demand model. We extend the model to the case of emergency orders. We compute the optimal policy for a class of Markovian demand and illustrate the benefits of dynamic pricing over fixed pricing through numerical examples. The results indicate that it is more beneficial to implement dynamic pricing in a Markovian demand environment with a high fixed ordering cost or with high demand variability.",
    "keywords": [
      "joint pricing and inventory control",
      "markovian demand",
      "optimal feedback policy"
    ]
  },
  {
    "id": "513",
    "title": "privacy preserving learning in negotiation",
    "abstract": "Machine learning techniques are widely used in negotiation systems. To get more accurate and satisfactory learning results, negotiation parties have the desire to employ learning techniques on the union of their past negotiation records. However, negotiation records are usually confidential and private, and owners may not want to reveal the details of these records. In this paper, we introduce a privacy preserving negotiation learning scheme that incorporate secure multiparty computation techniques into negotiation learning algorithms to allow negotiation parties to securely complete the learning process on a union of distributed data sets. As an example, a detailed solution for secure negotiation Q-learning is presented based on two secure multiparty computations: weighted mean and maximum. We also introduce a novel protocol for the secure maximum operation.",
    "keywords": [
      "negotiation",
      "secure maximum",
      "privacy",
      "q-learning"
    ]
  },
  {
    "id": "514",
    "title": "The relative utility of regression and artificial neural networks models for rapidly predicting the capacity of water supply reservoirs",
    "abstract": "Rapid prediction tools for reservoir over-year and within-year capacities that dispense with the sequential analysis of time-series runoff data are developed using multiple linear regression and multi-layer perceptron, artificial neural networks (MLP-ANNs). Linear regression was used to model the total (i.e. within-year+over-year) capacity using the over-year capacity as one of the inputs, while the ANNs were used to simultaneously model directly the over-year and total capacities. The inputs used for the ANNs were basic runoff and systems variables such as the coefficient of variation (Cv) of annual and monthly runoff, minimum monthly runoff, the demand ratio and reservoir reliability. The results showed that all the models performed well during their development and when they were tested with independent data sets. Both models offer faster prediction tools for reservoir capacity at gauged sites when compared with behaviour simulation. Additionally, when the predictor variables can be evaluated at un-gauged sites using e.g. catchment characteristics, they make capacity estimation at such un-gauged sites a feasible proposition.",
    "keywords": [
      "artificial neural networks",
      "storageyieldreliability",
      "sequent-peak algorithm",
      "over-year capacity",
      "within-year capacity",
      "multiple regression",
      "un-gauged sites"
    ]
  },
  {
    "id": "515",
    "title": "An Access Delay Model for IEEE 802.11e EDCA",
    "abstract": "We analyze the MAC access delay of the IEEE 802.11e enhanced distributed channel access (EDCA) mechanism under saturation. We develop a detailed analytical model to evaluate the influence of all EDCA differentiation parameters, namely AIFS, CWmin, CWmax, and TXOP limit, as well as the backoff multiplier beta. Explicit expressions for the mean, standard deviation, and generating function of the access delay distribution are derived. By applying numerical inversion on the generating function, we are able to efficiently compute values of the distribution. Comparison with simulation confirms the accuracy of our analytical model over a wide range of operating conditions. We derive simple asymptotics and approximations for the mean and standard deviation of the access delay, which reveal the salient model parameters for performance under different differentiation mechanisms. We also use the model to numerically study the differentiation performance and find that beta differentiation, though rejected during the standardization process, is an effective differentiation mechanism that has some advantages over the other mechanisms.",
    "keywords": [
      "medium access delay",
      "ieee 802.11e",
      "qos",
      "edca",
      "service differentiation",
      "generating function"
    ]
  },
  {
    "id": "516",
    "title": "Slide presentations as speech suppressors: When and why learners miss oral information",
    "abstract": "The objective of this study was to test whether information presented on slides during presentations is retained at the expense of information presented only orally, and to investigate part of the conditions under which this effect occurs, and how it can be avoided. Such an effect could be expected and explained either as a kind of redundancy effect due to excessive cognitive load caused by simultaneous presentation of oral and written information, or as a consequence of dysfunctional allocation of attention at the expense of oral information occurring in learners with a high subjective importance of slides. The hypothesized effect and these potential explanations were tested in an experimental study. In courses about literature search and access, 209 university students received a presentation accompanied either by no slides or by regular or concise PowerPoint slides. The retention of information presented orally and of information presented orally and on slides was measured separately in each condition and standardized for comparability. Cognitive load and subjective importance of slides were also measured. The results indicate a \"speech suppression effect\" of regular slides at the expense of oral information (within and across conditions), which cannot be explained by cognitive overload but rather by dysfunctional allocation of attention, and can be avoided by concise slides. It is concluded that theoretical approaches should account for the allocation of attention below the threshold of cognitive overload and its role for learning, and that a culture of presentations with concise slides should be established.  ",
    "keywords": [
      "improving classroom teaching",
      "media in education",
      "post-secondary education",
      "teaching/learning strategies"
    ]
  },
  {
    "id": "517",
    "title": "Development of a parallel Poisson's equation solver with adaptive mesh refinement and its application in field emission prediction",
    "abstract": "A parallel electrostatic Poisson's equation solver coupled with parallel adaptive mesh refinement (PAMR) is developed in this paper. The three-dimensional Poisson's equation is discretized using the Galerkin finite element method using a tetrahedral mesh. The resulting matrix equation is then solved through the parallel conjugate gradient method using the non-overlapping subdomain-by-subdomain scheme. A PAMR module is coupled with this parallel Poisson's equation solver to adaptively refine the mesh where the variation of potentials is large. The parallel performance of the parallel Poisson's equation is studied by simulating the potential distribution of a CNT-based triode-type field emitter. Results with ?100?000 nodes show that a parallel efficiency of 84.2% is achieved in 32 processors of a PC-cluster system. The field emission properties of a single CNT triode- and tetrode-type field emitter in a periodic cell are computed to demonstrate their potential application in field emission prediction.",
    "keywords": [
      "parallel poisson's equation",
      "galerkin finite element method",
      "parallel adaptive mesh refinement",
      "field emission"
    ]
  },
  {
    "id": "518",
    "title": "Isogeometric analysis of the advective CahnHilliard equation: Spinodal decomposition under shear flow",
    "abstract": "We present a numerical study of the spinodal decomposition of a binary fluid undergoing shear flow using the advective CahnHilliard equation, a stiff, nonlinear, parabolic equation characterized by the presence of fourth-order spatial derivatives. Our numerical solution procedure is based on isogeometric analysis, an approximation technique for which basis functions of high-order continuity are employed. These basis functions allow us to directly discretize the advective CahnHilliard equation without resorting to a mixed formulation. We present steady state solutions for rectangular domains in two-dimensions and, for the first time, in three-dimensions. We also present steady state solutions for the two-dimensional TaylorCouette cell. To enforce periodic boundary conditions in this curved domain, we derive and utilize a new periodic Bzier extraction operator. We present an extensive numerical study showing the effects of shear rate, surface tension, and the geometry of the domain on the phase evolution of the binary fluid. Theoretical and experimental results are compared with our simulations.",
    "keywords": [
      "cahnhilliard equation",
      "spinodal decomposition",
      "shear flow",
      "steady state",
      "isogeometric analysis",
      "bzier extraction"
    ]
  },
  {
    "id": "519",
    "title": "a vhdl compiler based on attribute grammar methodology",
    "abstract": "This paper presents aspects of a compiler for a new hardware description language (VHDL) written using attribute grammar techniques. VHDL is introduced, along with the new compiler challenges brought by a language that extends an Ada subset for the purpose of describing hardware. Attribute grammar programming solutions are presented for some of the language challenges. The organization of the compiler and of the target virtual machine represented by the simulation kernel are discussed, and performance and code-size figures are presented. The paper concludes that attribute grammars can be used for large commercial compilers with excellent results in terms of rapid development time and enhanced maintainability, and without paying any substantial penalty in terms of either the complexity of the language that can be handled or the resulting compilation speed.",
    "keywords": [
      "challenge",
      "program",
      "organization",
      "aspect",
      "simulation",
      "hardware description language",
      "methodology",
      "developer",
      "maintainability",
      "size",
      "code",
      "language",
      "performance",
      "compilation",
      "timing",
      "complexity",
      "hardware",
      "paper",
      "virtual machine",
      "attribute grammars"
    ]
  },
  {
    "id": "520",
    "title": "an agent based framework for virtual medical devices",
    "abstract": "In this paper we present the telemedical environment based on VMDs implemented with Java mobile agent technology, called aglets. The agent based VMD implementation provides ad-hoc agent interaction, support for mobile agents and different user interface components in the telemedical system. We have developed a VMD agent framework with four types of agents: data agents, processing agents, presentation agents, and monitoring agents. Data agents abstract data source, creating uniform view on different types of data, independent of data acquisition device. Processing agents produce derived data, such us FFT power spectrum, from raw data provided by the data agents. Presentation agents supply user interface components using a variety of user data views. User interface components are based on HTTP, SMS and WAP protocols. Monitoring agents collaborate with data and processing agents providing support for data mining operations, and search for relevant patterns. Typical example is monitoring for possible epileptic attacks. We have applied VMDs to facilitate distributed EEG analysis. We have found that the flexibility of distributed agent architecture is well suited for the telemedical application domain. This flexibility is particularly important in the case of an emergency, enabling swift system reconfiguration on the fly.",
    "keywords": [
      "distributed systems",
      "telemedicine",
      "software agents"
    ]
  },
  {
    "id": "521",
    "title": "Finite Block Method in elasticity",
    "abstract": "A new point collocation algorithm named Finite Block Method (FBM), which is based on the one-dimensional differential matrix is developed for 2D and 3D elasticity problems in this paper. The main idea is to construct the first order one-dimensional differential matrix for one block by using Lagrange series with uniformly distributed nodes. The higher order derivative matrix for one-dimensional problem is obtained. By introducing the mapping technique, a block of quadratic type is transformed from Cartesian coordinate(xyz) ( x y z ) to normalised coordinate () () with 8 seeds or 20 seeds for two or three dimensions. The differential matrices in physical domain are determined from that in the normalised transformed system. Several 2D and 3D examples are given and comparisons have been made with either analytical solutions or the boundary element method to demonstrate the accuracy and convergence of this method.",
    "keywords": [
      "finite block method",
      "1d mapping differential matrix",
      "lagrange series expansion",
      "elasticity",
      "functionally graded media",
      "anisotropy"
    ]
  },
  {
    "id": "522",
    "title": "New adaptive compressors for natural language text",
    "abstract": "Semistatic byte-oriented word-based compression codes have been shown to be an attractive alternative to compress natural language text databases, because of the combination of speed, effectiveness, and direct searchability they offer. In particular, our recently proposed family of dense compression codes has been shown to be superior to the more traditional byte-oriented word-based Huffman codes in most aspects. In this paper, we focus on the problem of transmitting texts among peers that do not share the vocabulary. This is the typical scenario for adaptive compression methods. We design adaptive variants of our semistatic dense codes, showing that they are much simpler and faster than dynamic Huffman codes and reach almost the same compression effectiveness. We show that our variants have a very compelling trade-off between compression/decompression speed, compression ratio, and search speed compared with most of the state-of-the-art general compressors. ",
    "keywords": [
      "text databases",
      "natural language text compression",
      "dynamic compression",
      "searching compressed text"
    ]
  },
  {
    "id": "523",
    "title": "Organisational computer supported collaborative learning: the affect of context",
    "abstract": "The purpose of this introduction is to provide a brief overview of the articles in this special issue and also a framework for understanding, designing and evaluating strategies for co-operative learning in the workplace and in educational environments. The special edition is divided into two partsIssue 1: Computer Supported Collaborative Learning in Formal Education, and Issue 2: Computer Supported Team and Organisational Learning in Workplaces. In general, Issue 1 focuses on collaborative learning in primary and secondary schools and in the University setting. Issue 2 is meant to focus on learning in complex and often highly stressful work situations which mostly require intensive communication in groups or teams and in each case allow for learning in the wider organisation. This introduction outlines a set of themes that can be found in the following papers and traces briefly how each paper fits within each discussion.",
    "keywords": [
      "computer supported collaborative learning",
      "schools",
      "workplace learning",
      "formal learning",
      "activity theory",
      "technology",
      "organisations"
    ]
  },
  {
    "id": "524",
    "title": "Estimation of the proportion ratio under a simple crossover trial",
    "abstract": "The proportion ratio (PR) of patient response is one of the most commonly used indices for measuring the relative treatment effect in a randomized clinical trial (RCT). Assuming a random effect multiplicative risk model, we develop two point estimators and three interval estimators in closed forms for the PR under a simple crossover RCT. On the basis of Monte Carlo simulation, we evaluate the performance of these estimators in a variety of situations. We note that the point estimator using a ratio of two arithmetic means of patient response probabilities over the two groups (distinguished by the order of treatment-received sequences) is generally preferable to the corresponding one using a ratio of two geometric means of patient response probabilities. We note that the three interval estimators developed in this paper can actually perform well with respect to the coverage probability when the number of patients per group is moderate or large. We further note that the interval estimator based on the ratio of two arithmetic means of patient response probabilities with the logarithmic transformation is probably the best among the three interval estimators discussed here. We use a simple crossover trial studying the suitability of two new inhalation devices for patients who were using a standard inhaler device delivering Salbutamol published elsewhere to illustrate the use of these estimators.",
    "keywords": [
      "binary data",
      "crossover trial",
      "proportion ratio",
      "bias",
      "precision",
      "coverage probability",
      "interval estimator",
      "point estimation"
    ]
  },
  {
    "id": "525",
    "title": "Pattern identification in biogeography",
    "abstract": "Identifying common patterns among area cladograms that arise in historical biogeography is an important tool for biogeographical inference. We develop the first rigorous formalization of these pattern-identification problems. We develop metrics to compare area cladograms. We define the maximum agreement area cladogram (MAAC) and we develop efficient algorithms for finding the MAAC of two area cladograms, while showing that it is NP-hard to find the MAAC of several binary area cladograms. We also describe a linear-time algorithm to identify if two area cladograms are identical.",
    "keywords": [
      "biogeography",
      "area cladograms",
      "distance metrics",
      "maximum agreement area cladogram",
      "maximum agreement subset"
    ]
  },
  {
    "id": "526",
    "title": "EAD and PEBD: Two Energy-Aware Duplication Scheduling Algorithms for Parallel Tasks on Homogeneous Clusters",
    "abstract": "High-performance clusters have been widely deployed to solve challenging and rigorous scientific and engineering tasks. On one hand, high performance is certainly an important consideration in designing clusters to run parallel applications. On the other hand, the ever increasing energy cost requires us to effectively conserve energy in clusters. To achieve the goal of optimizing both performance and energy efficiency in clusters, in this paper, we propose two energy-efficient duplication-based scheduling algorithms-Energy-Aware Duplication (EAD) scheduling and Performance-Energy Balanced Duplication (PEBD) scheduling. Existing duplication-based scheduling algorithms replicate all possible tasks to shorten schedule length without reducing energy consumption caused by duplication. Our algorithms, in contrast, strive to balance schedule lengths and energy savings by judiciously replicating predecessors of a task if the duplication can aid in performance without degrading energy efficiency. To illustrate the effectiveness of EAD and PEBD, we compare them with a nonduplication algorithm, a traditional duplication-based algorithm, and the dynamic voltage scaling (DVS) algorithm. Extensive experimental results using both synthetic benchmarks and real-world applications demonstrate that our algorithms can effectively save energy with marginal performance degradation.",
    "keywords": [
      "homogeneous clusters",
      "energy-aware scheduling",
      "duplication algorithms"
    ]
  },
  {
    "id": "527",
    "title": "Design of K-means clustering-based polynomial radial basis function neural networks (pRBF NNs) realized with the aid of particle swarm optimization and differential evolution",
    "abstract": "In this paper, we introduce an advanced architecture of K-means clustering-based polynomial Radial Basis Function Neural Networks (p-RBF NNs) designed with the aid of Particle Swarm Optimization (PSO) and Differential Evolution (DE) and develop a comprehensive design methodology supporting their construction. The architecture of the p-RBF NNs comes as a result of a synergistic usage of the evolutionary optimization-driven hybrid tools. The connections (weights) of the proposed p-RBF NNs being of a certain functional character and are realized by considering four types of polynomials. In order to design the optimized p-RBF NNs, a prototype (center value) of each receptive field is determined by running the K-means clustering algorithm and then a prototype and a spread of the corresponding receptive field are further optimized through running Particle Swarm Optimization (PSO) and Differential Evolution (DE). The Weighted Least Square Estimation (WLSE) is used to estimate the coefficients of the polynomials (which serve as functional connections of the network). The performance of the proposed model and the comparative analysis involving models designed with the aid of PSO and DE are presented in case of a nonlinear function and two Machine Learning (ML) datasets",
    "keywords": [
      "polynomial radial basis function neural networks ",
      "k-means clustering",
      "particle swarm optimization",
      "differential evolution algorithm",
      "weighted least square estimation ."
    ]
  },
  {
    "id": "528",
    "title": "Computers that recognise and respond to user emotion: theoretical and practical implications",
    "abstract": "Prototypes of interactive computer systems have been built that can begin to detect and label aspects of human emotional expression, and that respond to users experiencing frustration and other negative emotions with emotionally supportive interactions, demonstrating components of human skills such as active listening, empathy, and sympathy. These working systems support the prediction that a computer can begin to undo some of the negative feelings it causes by helping a user manage his or her emotional state. This paper clarifies the philosophy of this new approach to human computer interaction: deliberately recognising and responding to an individual user's emotions in ways, that help users meet their needs. We define user needs in a broader perspective than has been hitherto discussed in the HCI community, to include emotional and social needs, and examine technology's emerging capability to address and support such needs. We raise and discuss potential concerns and objections regarding this technology, and describe several opportunities for future work.  ",
    "keywords": [
      "user emotions",
      "affective computing",
      "social interface",
      "frustration",
      "human-centred designs",
      "empathetic interface",
      "emotional needs"
    ]
  },
  {
    "id": "529",
    "title": "On visualization techniques for solar data mining",
    "abstract": "Large-scale data mining is often aided with graphic visualizations to facilitate a better understanding of the data and results. This is especially true for visual data and highly detailed data too complex to be easily understood in raw forms. In this work, we present several of our recent interdisciplinary works in data mining solar image repositories and discuss the over-arching need for effective visualizations of data, metadata, and results along the way. First, we explain the complex characteristics and overwhelming abundance of image data being produced by NASAs Solar Dynamics Observatory (SDO). Then we discuss the wide scope of solar data mining and highlight visual results from work in data labeling, classification, and clustering. Lastly, we present an overview of the first-ever Content-Based Image Retrieval (CBIR) system for solar images, and conclude with a brief look at the direction of our future research.",
    "keywords": [
      "solar images",
      "visualization",
      "data mining",
      "cbir"
    ]
  },
  {
    "id": "530",
    "title": "Defeasible Contextual Reasoning with Arguments in Ambient Intelligence",
    "abstract": "The imperfect nature of context in Ambient Intelligence environments and the special characteristics of the entities that possess and share the available context information render contextual reasoning a very challenging task. The accomplishment of this task requires formal models that handle the involved entities as autonomous logic-based agents and provide methods for handling the imperfect and distributed nature of context. This paper proposes a solution based on the Multi-Context Systems paradigm in which local context knowledge of ambient agents is encoded in rule theories (contexts), and information flow between agents is achieved through mapping rules that associate concepts used by different contexts. To handle imperfect context, we extend Multi-Context Systems with nonmonotonic features: local defeasible theories, defeasible mapping rules, and a preference ordering on the system contexts. On top of this model, we have developed an argumentation framework that exploits context and preference information to resolve potential conflicts caused by the interaction of ambient agents through the mappings, and a distributed algorithm for query evaluation.",
    "keywords": [
      "ambient intelligence",
      "contextual reasoning",
      "defeasible reasoning",
      "argumentation systems"
    ]
  },
  {
    "id": "531",
    "title": "Nonextensive Information Theoretic Kernels on Measures",
    "abstract": "Positive definite kernels on probability measures have been recently applied to classification problems involving text, images, and other types of structured data. Some of these kernels are related to classic information theoretic quantities, such as (Shannon's) mutual information and the Jensen-Shannon (JS) divergence. Meanwhile, there have been recent advances in nonextensive generalizations of Shannon's information theory. This paper bridges these two trends by introducing nonextensive information theoretic kernels on probability measures, based on new JS-type divergences. These new divergences result from extending the the two building blocks of the classical JS divergence: convexity and Shannon's entropy. The notion of convexity is extended to the wider concept of q-convexity, for which we prove a Jensen q-inequality. Based on this inequality, we introduce Jensen-Tsallis (JT) q-differences, a nonextensive generalization of the JS divergence, and define a k-th order JT q-difference between stochastic processes. We then define a new family of nonextensive mutual information kernels, which allow weights to be assigned to their arguments, and which includes the Boolean, JS, and linear kernels as particular cases. Nonextensive string kernels are also defined that generalize the p-spectrum kernel. We illustrate the performance of these kernels on text categorization tasks, in which documents are modeled both as bags of words and as sequences of characters.",
    "keywords": [
      "positive definite kernels",
      "nonextensive information theory",
      "tsallis entropy",
      "jensen-shannon divergence",
      "string kernels"
    ]
  },
  {
    "id": "532",
    "title": "Point multiplication on ordinary elliptic curves over fields of characteristic three",
    "abstract": "In this paper we investigate the efficiency of cryptosystems based on ordinary elliptic curves over fields of characteristic three. We look at different representations for curves and consider some of the algorithms necessary to perform efficient point multiplication. We give example timings for our operations and compare them with timings for curves in characteristic two of a similar level of security. We show that using the Hessian form in characteristic three produces a point multiplication algorithm under 50 percent slower than the equivalent system in characteristic two. Thus it is conceivable that curves in characteristic three, could offer greater performance than currently perceived by the community.",
    "keywords": [
      "elliptic curve cryptography",
      "hessian form",
      "characteristic three"
    ]
  },
  {
    "id": "533",
    "title": "Determination of the heat-transfer coefficient during solidification of alloys",
    "abstract": "We consider a three-phase inverse Stefan problem. Such a problem consists in a reconstruction of the function describing the coefficient of heat-transfer, when the positions of the moving solid and liquid interfaces are well-known. We introduce three partial problems for each phase (liquid, solid and mushy) separately. The solutions of these problems are used for the determination of the unknown heat-transfer coefficient. The missing data for a mushy (solid) phase are computed from over-determined data at the moving liquid (solid) interface taking into account the transmission condition. At the end we present numerical calculations in one dimension using piecewise linear continuous finite elements in order to demonstrate the efficiency of the designed numerical algorithm.",
    "keywords": [
      "three-phase inverse stefan problem",
      "recovery of the heat-transfer coefficient"
    ]
  },
  {
    "id": "534",
    "title": "Simulation of land use spatial pattern of towns and villages based on CA-Markov model",
    "abstract": "Firstly, this paper analyzes the basic principles and processes of the spatial pattern changes of land use in towns and villages, and the result shows that the land resource demands of urban development and population growth lead to the spatial pattern changes. Secondly, in order to grip land use changes better, the paper proposes a method for the simulation of spatial patterns. The simulating method can be divided into two parts: one is a quantitative forecast by using the Markov model, and the other is simulating the spatial pattern changes by using the CA model. The above two models construct the simulative model of the spatial pattern of land use in towns and villages. Finally, selecting Fangshan which is a district of Beijing as the experimental area, both the quantity and spatial pattern changing characteristics are investigated through building a changing dataset of land use by using spatial analysis methods based on the land use data in 2001, 2006 and 2008; CA-Markov is used to simulate the spatial pattern of land use in Fangshan for 2015.  ",
    "keywords": [
      "land use change",
      "spatial pattern",
      "markov",
      "cellular automata",
      "fangshan district in beijing"
    ]
  },
  {
    "id": "535",
    "title": "Observations of IPv6 traffic on a 6to4 relay",
    "abstract": "FUNET has been operating a public, globally-used 6to4 (RFC 3056) relay router since November 2001. The traffic has been logged and is now analyzed to gather information of 6to4 and IPv6 deployment. Among other figures, we note that the number of 6to4 capable nodes has increased by an order of magnitude in half a year: in April 2004, there are records of about 2 million different 6to4 nodes using this particular relay. Vast majority of this is just testing the availability of the relay, done by the Microsoft Windows systems, but the real traffic has also increased over time. While the observed 6to4 traffic has typically consisted of relatively simple system-level applications, or applications by power users, the emergence of peer-to-peer applications such as BitTorrent was also observed.",
    "keywords": [
      "ipv6",
      "6to4",
      "ipv6 transition"
    ]
  },
  {
    "id": "536",
    "title": "Modelling and control of the motion of a riderless bicycle rolling on a moving plane",
    "abstract": "This work deals with the modelling and control of a riderless bicycle rolling on a moving plane. It is assumed here that the bicycle is controlled by a pedalling torque, a directional torque and by a rotor mounted on the crossbar that generates a tilting torque. In particular, a kinematic model of the bicycles motion is derived by using its dynamic model. Then, using this kinematic model, the expressions for the applied torques are obtained.",
    "keywords": [
      "riderless bicycle",
      "moving plane",
      "circular rotor plate",
      "nonholonomic constraints",
      "inverse dynamics control",
      "stabilization"
    ]
  },
  {
    "id": "537",
    "title": "Simplified models of neocortical pyramidal cells preserving somatodendritic voltage attenuation",
    "abstract": "Simplified models are needed for performing large-scale network simulations involving thousands of cells. Ideally, these models should be as simple as possible, but still capture important electrotonic properties, such as voltage attenuation. Here, we propose a method to design simplified models with correct voltage attenuation, based on camera-lucida reconstructions of neurons. The simplified model geometry is fit to the detailed model such that it preserves: (i) total membrane area, (ii) input resistance, (iii) time constant and (iv) voltage attenuation for current injection in the soma. Using the three dimensional reconstruction of a layer VI pyramidal cell, we show that this procedure leads to an efficient simplified model which preserves voltage attenuation for somatic current injection as well as for distributed synaptic inputs in dendrites. Attenuation was also correctly captured in the presence of synaptic background activity. These simplified models should be useful for performing network simulations of neurons with electrotonic properties consistent with detailed morphologies.",
    "keywords": [
      "cerebral cortex",
      "dendritic integration",
      "synaptic background activity",
      "computational models",
      "network"
    ]
  },
  {
    "id": "538",
    "title": "Hybrid force-assisted 3-D assembly of helical nanobelts",
    "abstract": "We present a new manipulation system for hybrid force assisted assembly. We propose a hybrid force assisted assembly process for 3-D helical nanobelts. Helical nanobelt tweezer and sensing probe have been assembled using the proposed method.",
    "keywords": [
      "hybrid force-assisted assembly",
      "ultra-flexible nanostructures",
      "thin-film nanostructures",
      "3-d helical nanobelts"
    ]
  },
  {
    "id": "539",
    "title": "A stochastic optimization approach for roundness measurements",
    "abstract": "In this paper, we develop a vision-based inspection system for roundness measurements. A stochastic optimization approach has been proposed to compute the reference circles of MIC (maximum inscribing circle), MCC (minimum circumscribing circle) and MZC (minimum zone circle) methods. The proposed algorithm is a hybrid optimization method based on simulated annealing and HookeJeeves pattern search. From the experimental results, it is noted that the algorithm can solve the roundness assessment problems effectively and efficiently. The developed vision-based inspection system can be an on-line tool for the measurement of circular components.",
    "keywords": [
      "measurement",
      "roundness error",
      "simulated annealing",
      "pattern search"
    ]
  },
  {
    "id": "540",
    "title": "Asymptotic solvers for second-order differential equation systems with multiple frequencies",
    "abstract": "In this paper, an asymptotic expansion is constructed to solve second-order differential equation systems with highly oscillatory forcing terms involving multiple frequencies. An asymptotic expansion is derived in inverse of powers of the oscillatory parameter and its truncation results in a very effective method of dicretizing the differential equation system in question. Numerical experiments illustrate the effectiveness of the asymptotic method in contrast to the standard RungeKutta method.",
    "keywords": [
      "highly oscillatory problems",
      "second-order differential equations",
      "modulated fourier expansions",
      "multiple frequencies",
      "numerical analysis",
      ""
    ]
  },
  {
    "id": "541",
    "title": "Power-Rate Allocation in DS-CDMA Systems Based on Discretized Verhulst Equilibrium",
    "abstract": "This paper proposes to extend the discrete Verhulst power equilibrium approach, previously suggested in [1], to the power-rate optimal allocation problem. Multirate users associated to different types of traffic are aggregated to distinct user classes, with the assurance of minimum rate allocation per user and QoS. Herein, Verhulst power allocation algorithm of [1] was adapted to the DS-CDMA jointly power-rate control problem. The analysis was carried out taking into account static and dynamic channels, as well as the convergence time (number of iterations), quality of solution, in terms of the normalized mean squared error (NSE), when compared to the analytical solution based on interference matrix inversion, and the solution given by classical Foschini algorithm [2] as well, besides the computational complexity analysis.",
    "keywords": [
      "resource allocation",
      "power-rate control",
      "siso multirate ds-cdma",
      "discrete verhulst equilibrium equation",
      "qos"
    ]
  },
  {
    "id": "542",
    "title": "A relational approach to probabilistic classification in a transductive setting",
    "abstract": "Transduction is an inference mechanism adopted from several classification algorithms capable of exploiting both labeled and unlabeled data and making the prediction for the given set of unlabeled data only. Several transductive learning methods have been proposed in the literature to learn transductive classifiers from examples represented as rows of a classical double-entry table (or relational table). In this work we consider the case of examples represented as a set of multiple tables of a relational database and we propose a new relational classification algorithm, named TRANSC, that works in a transductive setting and employs a probabilistic approach to classification. Knowledge on the data model, i.e., foreign keys, is used to guide the search process. The transductive learning strategy iterates on a k-NN based re-classification of labeled and unlabeled examples, in order to identify borderline examples, and uses the relational probabilistic classifier Mr-SBC to bootstrap the transductive algorithm. Experimental results confirm that TRANSC outperforms its inductive counterpart (Mr-SBC).",
    "keywords": [
      "multi-relational data mining",
      "transductive classification",
      "relational probabilistic classification",
      "relational learning",
      "transduction"
    ]
  },
  {
    "id": "543",
    "title": "Exploring the risk factors of preterm birth using data mining",
    "abstract": "Preterm birth is the leading cause of perinatal morbidity and mortality, but a precise mechanism is still unknown. Hence, the goal of this study is to explore the risk factors of preterm using data mining with neural network and decision tree C5.0. The original medical data were collected from a prospective pregnancy cohort by a professional research group in National Taiwan University. Using the nest case-control study design, a total of 910 motherchild dyads were recruited from 14,551 in the original data. Thousands of variables are examined in this data including basic characteristics, medical history, environment, and occupation factors of parents, and variables related to infants. The results indicate that multiple birth, hemorrhage during pregnancy, age, disease, previous preterm history, body weight before pregnancy and height of pregnant women, and paternal life style risk factors related to drinking and smoking are the important risk factors of preterm birth. Hence, the findings of our study will be useful for parents, medical staff, and public health workers in attempting to detect high risk pregnant women and provide intervention early to reduce and prevent preterm birth.",
    "keywords": [
      "preterm birth",
      "data mining",
      "neural network",
      "decision tree"
    ]
  },
  {
    "id": "544",
    "title": "Interval-based analysis in embedded system design",
    "abstract": "Complex multi-processor systems-on-chip and distributed embedded systems exhibit a confusing variety of run time interdependencies. For reliable timing validation, not only application, but also architecture, scheduling and communication properties have to be considered. This is very different from functional validation, where architecture, scheduling and communication can be idealized. To avoid unknown corner-case coverage in simulation-based validation on one had, and the state-space explosion or over-simplification of unified formal performance models on the other, we take a compositional approach and combine different efficient models and methods for timing analysis of single processes, real-time operating system (RTOS) overhead, single processors and communication components, and finally multiple connected components. As a result, timing analysis of complex, heterogeneous embedded systems becomes feasible.  on behalf of IMACS.",
    "keywords": [
      "real-time embedded systems",
      "performance verification",
      "interval analysis"
    ]
  },
  {
    "id": "545",
    "title": "sequential verification of serializability",
    "abstract": "Serializability is a commonly used correctness condition in concurrent programming. When a concurrent module is serializable, certain other properties of the module can be verified by considering only its sequential executions. In many cases, concurrent modules guarantee serializability by using standard locking protocols, such as tree locking or two-phase locking. Unfortunately, according to the existing literature, verifying that a concurrent module adheres to these protocols requires considering concurrent interleavings. In this paper, we show that adherence to a large class of locking protocols (including tree locking and two-phase locking) can be verified by considering only sequential executions. The main consequence of our results is that in many cases, the (manual or automatic) verification of serializability can itself be done using sequential reasoning .",
    "keywords": [
      "concurrency",
      "verification",
      "serializability",
      "reduction"
    ]
  },
  {
    "id": "546",
    "title": "diversifying search results of controversial queries",
    "abstract": "Diversifying search results of queries seeking for different view points about controversial topics is key to improving satisfaction of users. The challenge for finding different opinions is how to maximize the number of discussed arguments without being biased against specific sentiments. This paper addresses the issue by first introducing a new model that represents the patterns occurring in documents about controversial topics. Second, proposing an opinion diversification model that uses (1) relevance of documents, (2) semantic diversification to capture different arguments and (3) sentiment diversification to identify positive, negative and neutral sentiments about the query topic. We have conducted our experiments using queries on various controversial topics and applied our diversification model on the set of documents returned by Google search engine. The results show that our model outperforms the native ranking of Web pages about controversial topics by a significant margin.",
    "keywords": [
      "ranking",
      "opinion diversification"
    ]
  },
  {
    "id": "547",
    "title": "Memetic computation based on regulation between neural and immune systems: the framework and a case study",
    "abstract": "Lamarckian learning has been introduced into evolutionary computation to enhance the ability of local search. The relevant research topic, memetic computation, has received significant amount of interest. In this study, a novel memetic computational framework is proposed by simulating the integrated regulation between neural and immune systems. The Lamarckian learning strategy of simulating the unidirectional regulation of neural system on immune system is designed. Consequently, an immune memetic algorithm based on the Lamarckian learning is proposed for numerical optimization. The proposed algorithm combines the advantages of immune algorithms and mathematical programming, and performs well in both global and local search. The simulation results based on ten low-dimensional and ten high-dimensional benchmark problems show that the immune memetic algorithm outperforms the basic genetic algorithm-based memetic algorithm in solving most of the test problems.",
    "keywords": [
      "memetic computation",
      "artificial immune system",
      "lamarckian learning",
      "genetic algorithm",
      "numerical optimization"
    ]
  },
  {
    "id": "548",
    "title": "Optimal design of flywheels using an injection island genetic algorithm",
    "abstract": "This paper presents an approach to optimal design of elastic flywheels using an Injection Island Genetic Algorithm (iiGA), summarizing a sequence of results reported in earlier publications. An iiGA in combination with a structural finite element code is used to search for shape variations and material placement to optimize the Specific Energy Density (SED, rotational energy per unit weight) of elastic flywheels while controlling the failure angular velocity. iiGAs seek solutions simultaneously at different levels of refinement of the problem representation (and correspondingly different definitions of the fitness function) in separate subpopulations (islands). Solutions are sought first at low levels of refinement with an axi-symmetric plane stress finite element code for high-speed exploration of the coarse design space. Next, individuals are injected into populations with a higher level of resolution that use an axi-symmetric three-dimensional finite element code to \"fine-tune\" the structures. A greatly simplified design space (containing two million possible solutions) was enumerated for comparison with various approaches that include: simple GAs, threshold accepting (TA), iiGAs and hybrid iiGAs. For all approaches compared for this simplified problem, all variations of the iiGA were found to be the most efficient. This paper will summarize results obtained studying a constrained optimization problem with a huge design space approached with parallel GAs that had various topological structures and several different types of iiGA, to compare efficiency. For this problem, all variations of the iiGA were found to be extremely efficient in terms of computational time required to final solution of similar fitness when compared to the parallel GAs.",
    "keywords": [
      "optimization",
      "automated design",
      "flywheel",
      "genetic algorithm and fem"
    ]
  },
  {
    "id": "549",
    "title": "Paving the way for CRM success: The mediating role of knowledge management and organizational commitment",
    "abstract": "Data from an international sample of 312 hotels (from UK and Spain) is analyzed. The process through which CRM technology translates into organizational performance is described. A CRM technology, when properly implemented, shows a positive effect on performance. Knowledge management and organizational commitment acted as relevant mediators. Organizational commitment probed to be the main determinant of CRM success.",
    "keywords": [
      "customer relationship management ",
      "crm success",
      "crm technology infrastructure",
      "organizational commitment",
      "knowledge management"
    ]
  },
  {
    "id": "550",
    "title": "SUSY_FLAVORv2: A computational tool for FCNC and CP-violating processes in the MSSM",
    "abstract": "We present SUSY_FLAVOR version 2  a Fortran 77 program that calculates low-energy flavor observables in the general R-parity conserving MSSM. For a set of MSSM parameters as input, the code gives predictions for: Electric dipole moments of the leptons and the neutron. Anomalous magnetic moments (i.e.g?2 g ? 2 ) of the leptons. Radiative lepton decays (ee ? and,e, e ? ). Rare Kaon decays ( K L 00and K ++). Leptonic B decays (Bs,d?l+l? B s , d ? l + l ? , BBand B?DB ? D). Radiative B decays ( B ? X ? s ? ). ?F=2 ? F = 2 processes ( K ? 0 K0 K 0 , D ? D D , B ? d Bd B d and B ? s Bs B s mixing). Program title:SUSY_FLAVORv2 Catalogue identifier: AEGV_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEGV_v2_0.html Program obtainable from: CPC Program Library, Queens University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 15683 No. of bytes in distributed program, including test data, etc.: 89130 Distribution format: tar.gz Programming language: Fortran 77. Computer: Any. Operating system: Any, tested on Linux. Classification: 11.6. Does the new version supersede the previous version?: Yes Catalogue identifier of previous version: AEGV_v1_0 Journal reference of previous version: Comput. Phys. Comm. 181 (2010) 2180 Nature of problem: Predicting CP-violating observables, meson mixing parameters and branching ratios for set of rare processes in the general R-parity conserving MSSM. Solution method: We use standard quantum theoretical methods to calculate Wilson coefficients in MSSM and at one loop including QCD corrections at higher orders when this is necessary and possible. The input parameters can be read from an external file in SLHA format. Reasons for new version: A major rewrite of the internal code structure to accommodate higher order corrections; new observables added. Summary of revisions: SUSY_FLAVOR v2.0 is able to perform resummation of chirally enhanced corrections to all orders of perturbation expansion (v1.0 included 1-loop terms only). Routines calculating new observables are added: g-2 lepton magnetic moment anomaly,to e? e ? andto e?,e ? ,decays, B B to DDdecays, B B to ?e,?e,e , ? e ,decays. Parameter initialization in the sfermion sector is simplified and follows, by default, the SLHA2 conventions. Running time: For a single parameter set approximately 1s in double precision on a PowerBook Mac G4.",
    "keywords": [
      "rare decays",
      "flavor and cp violation",
      "supersymmetry",
      "general mssm",
      "higher order resummation",
      "fortran 77 code"
    ]
  },
  {
    "id": "551",
    "title": "Super-Resolution Iris Image Restoration Based on Multiple MLPs and CLS Filter",
    "abstract": "Iris recognition is a biometric technology which shows a very high level of recognition accuracy, but low resolution (LR) iris images cause the degradation of the recognition performance. Therefore, a zoom lens with a long focal length is used in an iris camera. However, a bulky and costly zoom lens whose focal length is longer than 150 mm is required for capturing the iris image at a distance, which can increase the size and cost of the system. In order to overcome this problem, we propose a new super-resolution method which restores a single LR iris image into a high resolution (HR) iris image. Our research is novel in the following three ways compared to previous works. First, in order to prevent the loss of the middle and high frequency components of the iris patterns in the original image, the LR iris image is up-sampled using multiple multi-layered perceptrons (MLPs). Second, a point spread function (PSF) and a constrained least square (CLS) filter are used to remove sensor blurring in the up-sampled image. Third, the optimal parameters of the CLS filter and PSF in terms of the recognition accuracy are determined according to the zoom factor of the LR image. The experimental results show that the accuracy of iris recognition with the HR images restored by the proposed method is much enhanced compared to the three previous methods.",
    "keywords": [
      "super-resolution restoration",
      "mlp",
      "iris recognition",
      "cls filter"
    ]
  },
  {
    "id": "552",
    "title": "A language-independent and formal approach to pattern-based modelling with support for composition and analysis",
    "abstract": "Patterns are used in different disciplines as a way to record expert knowledge for problem solving in specific areas. Their systematic use in Software Engineering promotes quality, standardization, reusability and maintainability of software artefacts. The full realisation of their power is however hindered by the lack of a standard formalization of the notion of pattern. Our goal is to provide a language-independent formalization of the notion of pattern, so that it allows its application to different modelling languages and tools, as well as generic methods to enable pattern discovery, instantiation, composition, and conflict analysis. For this purpose, we present a new visual and formal, language-independent approach to the specification of patterns. The approach is formulated in a general way, based on graphs and category theory, and allows the specification of patterns in terms of (nested) variable submodels, constraints on their allowed variance, and inter-pattern synchronization across several diagrams (e.g. class and sequence diagrams for UML design patterns). We provide a formal notion of pattern satisfaction by models and propose mechanisms to suggest model transformations so that models become consistent with the patterns. We define methods for pattern composition, and conflict analysis. We illustrate our proposal on UML design patterns, and discuss its generality and applicability on different types of patterns, e.g. workflow patterns, enterprise integration patterns and interaction patterns. The approach has proven to be powerful enough to formalize patterns from different domains, providing methods to analyse conflicts and dependencies that usually are expressed only in textual form. Its language independence makes it suitable for integration in meta-modelling tools and for use in Model-Driven Engineering.",
    "keywords": [
      "pattern formalization",
      "pattern-based modelling",
      "pattern composition",
      "pattern conflicts"
    ]
  },
  {
    "id": "553",
    "title": "Early mover advantage in e-commerce platforms with low entry barriers: The role of customer relationship management capabilities",
    "abstract": "This research investigates whether early mover advantage (EMA) exists among entrepreneurial e-tailers operating on third-party e-commerce platforms. Contrary to traditional wisdom, the current research hypothesizes that e-tailers may enjoy early mover advantages because of the consumer demand inertia amplified by the nature of the Internet and the system design characteristics of e-commerce platforms. We also argue that customer relationship management capabilities help enhance early mover advantages in an online setting. We employ panel data on 7309 e-tailers to perform analyses and find empirical evidence that strongly supports the abovementioned hypotheses.",
    "keywords": [
      "e-tailer",
      "e-commerce platform",
      "early mover advantage",
      "customer relationship management  capability",
      "market performance"
    ]
  },
  {
    "id": "554",
    "title": "Contextualized Monitoring and Root Cause Discovery in IPTV Systems Using Data Visualization",
    "abstract": "This article describes the architecture and design of an IPTV network monitoring system and some of the use cases it enables. The system is based on distributed agents within IPTV terminal equipment (set-top box), which collect and send the data to a server where it is analyzed and visualized. In the article we explore how large amounts of collected data can be utilized for monitoring the quality of service and user experience in real time, as well as for discovering trends and anomalies over longer periods of time. Furthermore, the data can be enriched using external data sources, providing a deeper understanding of the system by discovering correlations with events outside of the monitored domain. Four supported use cases are described, among them using weather information for explaining away the IPTV quality degradation. The system has been successfully deployed and is in operation at the Slovenian IPTV provider Telekom Slovenije.",
    "keywords": [
      "data visualization",
      "network security",
      "intrusion detection"
    ]
  },
  {
    "id": "555",
    "title": "Mean field and propagation of chaos in multi-class heterogeneous loss models",
    "abstract": "We consider a system consisting of N parallel servers, where jobs with different resource requirements arrive and are assigned to the servers for processing. Each server has a finite resource capacity and therefore can serve only a finite number of jobs at a time. We assume that different servers have different resource capacities. A job is accepted for processing only if the resource requested by the job is available at the server to which it is assigned. Otherwise, the job is discarded or blocked. We consider randomized schemes to assign jobs to servers with the aim of reducing the average blocking probability of jobs in the system. In particular, we consider a scheme that assigns an incoming job to the server having maximum available vacancy or unused resource among d randomly sampled servers. We consider the system in the limit where both the number of servers and the arrival rates of jobs are scaled by a large factor. This gives rise to a mean field analysis. We show that in the limiting system the servers behave independentlya property termed as propagation of chaos. Stationary tail probabilities of server occupancies are obtained from the stationary solution of the mean field which is shown to be unique and globally attractive. We further characterize the rate of decay of the stationary tail probabilities. Numerical results suggest that the proposed scheme significantly reduces the average blocking probability of jobs as compared to static schemes that probabilistically route jobs to servers independently of their states.",
    "keywords": [
      "mean field",
      "propagation of chaos",
      "loss models",
      "cloud computing",
      "power-of-d d"
    ]
  },
  {
    "id": "556",
    "title": "Conceptual indexing and active retrieval of video for interactive learning environments",
    "abstract": "Selecting an instructive story from a video case base is an information retrieval problem, but standard indexing and retrieval techniques [1]were not developed with such applications in mind. The classical model assumes a passive retrieval system queried by interested and well-informed users. In educational situations, students cannot be expected to form appropriate queries or to identify their own ignorance. Systems that teach must, therefore, be active retrievers that formulate their own retrieval cues and reason about the appropriateness of intervention. The Story Producer for InteractivE Learning (SPIEL) is an active retrieval system for recalling stories to tell to students who are learning social skills in a simulated environment 2and3. SPIEL is a component of the Guided Social Simulation (GuSS) architecture [4]used to build YELLO, a program that teaches account executives the fine points of selling Yellow Pages advertising. SPIEL uses structured, conceptual indices derived from research in case-based reasoning 5and6. SPIEL's manually-created indices are detailed representations of what stories are about, and they are needed to make precise assessments of stories' relevance. SPIEL's opportunistic retrieval architecture operates in two phases. During the storage phase, the system uses its educational knowledge encapsulated in a library of storytelling strategies to determine, for each story, what an opportunity to tell that story would look like. During the retrieval phase, the system tries to recognize those opportunities while the student interacts with the simulation. This design is similar to opportunistic memory architectures proposed for opportunistic planning 7and8.",
    "keywords": [
      "indexing",
      "multimedia",
      "intelligent tutoring systems",
      "care-based reasoning"
    ]
  },
  {
    "id": "557",
    "title": "When agile meets the enterprise",
    "abstract": "We present challenges of using agile practices in traditional enterprise environments. We organize the challenges under two factors. For both factors, we identify successful mitigation strategies.",
    "keywords": [
      "agile development",
      "enterprise environment",
      "grounded theory"
    ]
  },
  {
    "id": "558",
    "title": "Presenting a new multiclass classifier based on learning automata",
    "abstract": "Among the various traditional approaches of pattern recognition, the statistical approach has been most intensively studied and used in practice. This paper presents a new classifier called MLAC for multiclass classification based on the learning automata. The proposed classifier using a soft decision method could find the optimal hyperplanes in solution space and separate available classes from each other well. We have tested the MLAC classifier on some multiclass datasets including IRIS, WINE and GLASS.1 The results show a significant improvement in comparison with the previous learning automata based classifiers as it has more accuracy and lower running time. Also, in order to evaluate performance of the proposed MLAC classifier, it has been compared with conventional classifiers such as K-Nearest Neighbor, Multilayer Perceptron, Genetic classifier and Particle Swarm classifier on these datasets in terms of accuracy. The obtained results show that the proposed MLAC classifier not only improves the classification's accuracy, but also reduces time complexity.",
    "keywords": [
      "pattern classification",
      "learning automata ",
      "soft decision",
      "multiclass classifier"
    ]
  },
  {
    "id": "559",
    "title": "Secured communication protocol for internetworking ZigBee cluster networks",
    "abstract": "ZigBee uses the network security and application profile layers of the IEEE 802.15.4 and ZigBee Alliance standards for reliable, low-powered, wireless data communications. However, the ZigBee has problems of being less secure, and has a difficulty in distributing shared symmetric keys between each pair of nodes. In addition, the ZigBee protocol is inadequate for large sensor networks, which may consist of several very large scale clusters. In this paper, we first construct a secure ZigBee scheme for realistic scenarios consisting of a large network with several clusters containing coordinators and numerous devices. We present a new key management protocol for ZigEee networks, which can be used among participants of different clusters and analyze its performance.  ",
    "keywords": [
      "ieee 802.15.4",
      "zigbee cluster",
      "key management",
      "message complexity"
    ]
  },
  {
    "id": "560",
    "title": "hp-finite element simulations for Stokes flow  stable and stabilized",
    "abstract": "The stable Galerkin formulation and a stabilized Galerkin least squares formulation for the Stokes problem are analyzed in the context of the hp-version of the finite element method. Theoretical results for both formulations establish exponential rates of convergence under realistic assumptions on the input data. We confirm these results by a series of numerical experiments on an L-shaped domain where the solution exhibits corner singularities.",
    "keywords": [
      "hp-FEM",
      "Stokes problem",
      "Galerkin formulation",
      "Galerkin least squares formulation"
    ]
  },
  {
    "id": "561",
    "title": "Joint Optimization of Complexity and Overhead for the Routing in Hierarchical Networks",
    "abstract": "The hierarchical network structure was proposed in the early 80s and becomes popular nowadays. The routing complexity and the routing table size are the two primary performance measures in a dynamic route guidance system. Although various algorithms exist for finding the best routing policy in a hierarchical network, hardly exists any work in studying and evaluating the aforementioned measures for a hierarchical network. In this paper, a new mathematical framework to carry out the averages of the routing complexity and the routing table size is proposed to express the routing complexity and the routing table size as the functions of the hierarchical network parameters such as the number of the hierarchical levels and the subscriber density (cluster-population) for each hierarchical level.",
    "keywords": [
      "joint optimization",
      "hierarchical networks",
      "routing",
      "complexity"
    ]
  },
  {
    "id": "562",
    "title": "The solitary wave solution of coupled KleinGordonZakharov equations via two different numerical methods",
    "abstract": "In this research, we propose two different methods to solve the coupled KleinGordonZakharov (KGZ) equations: the Differential Quadrature (DQ) and Globally Radial Basis Functions (GRBFs) methods. In the DQ method, the derivative value of a function with respect to a point is directly approximated by a linear combination of all functional values in the global domain. The principal work in this method is the determination of weight coefficients. We use two ways for obtaining these coefficients: cosine expansion (CDQ) and radial basis functions (RBFs-DQ), the former is a mesh-based method and the latter categorizes in the set of meshless methods. Unlike the DQ method, the GRBF method directly substitutes the expression of the function approximation by RBFs into the partial differential equation. The main problem in the GRBFs method is ill-conditioning of the interpolation matrix. Avoiding this problem, we study the bases introduced in Pazouki and Schaback (2011) [44]. Some examples are presented to compare the accuracy and easy implementation of the proposed methods. In numerical examples, we concentrate on Inverse Multiquadric (IMQ) and second-order Thin Plate Spline (TPS) radial basis functions. The variable shape parameter (exponentially and random) strategies are applied in the IMQ function and the results are compared with the constant shape parameter.",
    "keywords": [
      "differential quadrature method",
      "kleingordonzakharov  equations",
      "radial basis functions ",
      "inverse multiquadric ",
      "thin plate spline "
    ]
  },
  {
    "id": "563",
    "title": "On tests for global maximum of the log-likelihood function",
    "abstract": "Given the location of a relative maximum of the log-likelihood function, how to assess whether it is the global maximum? This paper investigates an existing statistical tool, which, based on asymptotic analysis, answers this question by posing it as a hypothesis testing problem. A general framework for constructing tests for global maximum is given. The characteristics of the tests are investigated for two cases: correctly specified model and model mismatch. A finite sample approximation to the power is given, which gives a tool for performance prediction and a measure for comparison between tests. The sensitivity of the tests to model mismatch is analyzed in terms of the Renyi divergence and the Kullback-Leibler divergence between the true underlying distribution and the assumed parametric class and tests that are insensitive to small deviations from the model are derived thereby overcoming a fundamental weakness of existing tests. The tests are illustrated for three applications: passive localization or direction finding using an array of sensors, estimating the parameters of a Gaussian mixture model, and estimation of superimposed exponentials in noise-problems that are known to suffer from local maxima.",
    "keywords": [
      "array processing",
      "gaussian mixtures",
      "global optimization",
      "local maxima",
      "maximum likelihood ",
      "parameter estimation",
      "superimposed exponentials in noise"
    ]
  },
  {
    "id": "564",
    "title": "Using state equation to prove non-reachability in timed petrinets",
    "abstract": "Non-reachability proofs in Timed Petrinets were usually done by proving the non-reachability within the underlying timeless net. However, in many cases this approach fails. In this paper, we present an approach to prove non-reach ability within the actual Timed Petrinet. For this purpose, we introduce a state equation for Timed Petrinets in analogy to timeless nets. Using this state equation, we can express reachability as a system of equations and inequations, which is solvable in polynomial time.",
    "keywords": [
      "timed petrinet",
      "duration net",
      "state equation",
      "non-reachability"
    ]
  },
  {
    "id": "565",
    "title": "Microglial NADPH Oxidase Mediates Leucine Enkephalin Dopaminergic Neuroprotection",
    "abstract": "Abstract: Here, we report that leucine enkephalin (LE) is neuroprotective to dopaminergic (DA) neurons at femtomolar concentrations through anti-inflammatory properties. Mesencephalic neuron-glia cultures pretreated with femtomolar concentrations of LE (10?15-10?13 M) protected DA neurons from lipopolysaccharide (LPS)-induced DA neurotoxicity, as determined by DA uptake assay and tyrosine hydroxylase (TH) immunocytochemistry (ICC). However, des-tyrosine leucine enkephalin (DTLE), an LE analogue that is missing the tyrosine residue required for binding to the kappa opioid receptor, was also neuroprotective (10?15-10?13 M), as determined by DA uptake assay and TH ICC. Both LE and DTLE (10?15-10?13 M) reduced LPS-induced superoxide production from microglia-enriched cultures. Further, both LE and DTLE (10?14, 10?13 M) reduced the LPS-induced tumor necrosis factor-alpha (TNF?) mRNA and TNF? protein from PHOX+/+ microglia, as determined by quantitative real-time RT-PCR and ELISA analysis in mesencephalic neuron-glia cultures, respectively. However, both peptides failed to inhibit TNF? expression in PHOX?/? cultures, which are unable to produce extracellular superoxide in response to LPS. Additionally, LE and DTLE (10?14, 10?13 M) failed to show any neuroprotection against LPS in PHOX?/? cultures. Together, these data indicate that LE and DTLE are neuroprotective at femtomolar concentrations through the inhibition of oxidative insult associated with microglial NADPH oxidase and the attenuation of the ROS-mediated amplification of TNF? gene expression in microglia.",
    "keywords": [
      "microglial nadph oxidase",
      "leucine enkephalin ",
      "des-tyrosine leucine enkephalin ",
      "lipopolysaccharide ",
      "tumor necrosis factor-alpha ",
      "neuroprotection",
      "neurotoxicity"
    ]
  },
  {
    "id": "566",
    "title": "Query rewriting for SWIFT (First) answers",
    "abstract": "Traditionally, the answer to a database query is construed as the set of all tuples that meet the criteria stated. Strict adherence to this notion in query evaluation is, however, increasingly unsatisfactory because decision makers are more prone to adopting an exploratory strategy for information search which we call \"getting some answers quickly, and perhaps more later.\" From a decision-maker's perspective, such a strategy is optimal for coping with information overload and makes economic sense (when used in conjunction with a micropayment mechanism). These new requirements present new opportunities for database query optimization. In this paper, we propose a progressive query processing strategy that exploits this behavior to conserve system resources and to minimize query response time and user waiting time. This is accomplished by the heuristic decomposition of user queries into subqueries that can be evaluated on demand. To illustrate the practicality of the proposed methods, we describe the architecture of a prototype system that provides a nonintrusive implementation of our approach. Finally, we present experimental results obtained from an empirical study conducted using an Oracle Server that demonstrate the benefits of the progressive query processing strategy.",
    "keywords": [
      "www",
      "internet",
      "progressive query evaluation",
      "query optimization",
      "query rewrite"
    ]
  },
  {
    "id": "567",
    "title": "Ramsey Games With Giants",
    "abstract": "The classical result in the theory of random graphs, proved by Erd. os and Renyi in 1960, concerns the threshold for the appearance of the giant component in the random graph process. We consider a variant of this problem, with a Ramsey flavor. Now, each random edge that arrives in a sequence of rounds must be colored with one of r colors. The goal can be either to create a giant component in every color class, or alternatively, to avoid it in every color. One can analyze the offline or online setting for this problem. In this paper, we consider all these variants and provide nontrivial upper and lower bounds; in certain cases (like online avoidance) the obtained bounds are asymptotically tight. ",
    "keywords": [
      "random graphs",
      "giant component",
      "ramsey game"
    ]
  },
  {
    "id": "568",
    "title": "An Adaptive Speed Sensorless Induction Motor Drive With Artificial Neural Network for Stability Enhancement",
    "abstract": "An artificial neural network (ANN) based adaptive estimator is presented in this paper for the estimation of rotor speed in a sensorless vector-controlled induction motor (IM) drive. The model reference adaptive system (MRAS) is formed with instantaneous and steady state reactive power. Selection of reactive power as the functional candidate in MRAS automatically makes the system immune to the variation of stator resistance. Such adaptive system performs satisfactorily at very low speed. However, it is observed that an unstable region exists in the speed-torque domain during regeneration. In this work, ANN is applied to overcome such stability related problem. The proposed method is validated through computer simulation using MATLAB/SIMULINK. Sample results from a laboratory prototype (using dSPACE-1104) have confirmed the usefulness of the proposed estimator.",
    "keywords": [
      "artificial neural network ",
      "induction motor ",
      "model reference adaptive system ",
      "reactive power",
      "sensorless",
      "stability",
      "vector control"
    ]
  },
  {
    "id": "569",
    "title": "Adaptive anisotropic meshing for steady convection-dominated problems",
    "abstract": "Obtaining accurate solutions for convectiondiffusion equations is challenging due to the presence of layers when convection dominates the diffusion. To solve this problem, we design an adaptive meshing algorithm which optimizes the alignment of anisotropic meshes with the numerical solution. Three main ingredients are used. First, the streamline upwind PetrovGalerkin method is used to produce a stabilized solution. Second, an adapted metric tensor is computed from the approximate solution. Third, optimized anisotropic meshes are generated from the computed metric tensor by an anisotropic centroidal Voronoi tessellation algorithm. Our algorithm is tested on a variety of two-dimensional examples and the results shows that the algorithm is robust in detecting layers and efficient in avoiding non-physical oscillations in the numerical approximation.",
    "keywords": [
      "anisotropic mesh generation",
      "metric tensor",
      "convection-dominated problem",
      "stabilized method"
    ]
  },
  {
    "id": "570",
    "title": "Cell communication in tissue P systems: Universality results",
    "abstract": "We introduce an evolution-communication model for tissue P systems where communication rules are inspired by the general mechanism of cell communication based on signals and receptors: a multiset can enter a cell only in the presence of another multiset. Some basic variants of this model are also considered where communication is restricted either to be unidirectional or to use special multisets of objects called receptors. The universality for all these variants of tissue P systems is then proved by using two cells (three cells in the case of unidirectional communication) and rules of a minimal size.",
    "keywords": [
      "membrane computing",
      "turing computability",
      "tissue"
    ]
  },
  {
    "id": "571",
    "title": "activity rhythm detection and modeling",
    "abstract": "We present an algorithm for detecting and modeling rhythmic temporal patterns in the record of an individual's computer activity, or online \"presence.\" The model is both predictive and descriptive of temporal features and is constructed with minimal a priori knowledge.",
    "keywords": [
      "awareness",
      "rhythms",
      "user modeling",
      "cscw",
      "statistics"
    ]
  },
  {
    "id": "572",
    "title": "Energy-Efficient Paths in Radio Networks",
    "abstract": "We consider a radio network consisting of n stations represented as the complete graph on a set of n points in the Euclidean plane with edge weights omega(p,q)=|pq| (delta) +C (p) , for some constant delta > 1 and nonnegative offset costs C (p) . Our goal is to find paths of minimal energy cost between any pair of points that do not use more than some given number k of hops. We present an exact algorithm for the important case when delta=2, which requires O(kn log n) time per query pair (p, q). For the case of an unrestricted number of hops we describe a family of algorithms with query time O(n(1+alpha)), where alpha > 0 can be chosen arbitrarily. If we relax the exactness requirement, we can find an approximate (1 + epsilon) solution in constant time by querying a data structure which has linear size and which can be build in O(n log n) time. The dependence on epsilon is polynomial in 1/epsilon. One tool we employ might be of independent interest: For any pair of points (p, q) epsilon (P x P) we can report in constant time the cluster pair (A, B) representing (p, q) in a well-separated pair decomposition of P.",
    "keywords": [
      "computational geometry",
      "communication networks"
    ]
  },
  {
    "id": "573",
    "title": "Novelty detection for the inspection of light-emitting diodes",
    "abstract": "We propose novel feature-extraction and classification methods for the automatic visual inspection of manufactured LEDs. The defects are located at the area of the p-electrodes and lead to a malfunction of the LED. Besides the complexity of the defects, low contrast and strong image noise make this problem very challenging. For the extraction of image characteristic we compute radially-encoded features that measure discontinuities along the p-electrode. Therefore, we propose two different methods: the first method divides the object into several radial segments for which mean and standard deviation are computed and the second method computes mean and standard deviation along different orientations. For both methods we combine the features over several segments or orientations by computing simple measures such as the ratio between maximum and mean or standard deviation. Since defect-free LEDs are frequent and defective LEDs are rare, we apply and evaluate different novelty-detection methods for classification. Therefore, we use a kernel density estimator, kernel principal component analysis, and a one-class support vector machine. We further compare our results to Pearson's correlation coefficient, which is evaluated using an artificial reference image. The combination of one-class support vector machine and radially-encoded segment features yields the best overall performance by far, with a false alarm rate of only 0.13% at a 100% defect detection rate, which means that every defect is detected and only very few defect-free p-electrodes are rejected. Our inspection system does not only show superior performance, but is also computationally efficient and can therefore be applied to further real-time applications, for example solder joint inspection. Moreover, we believe that novelty detection as used here can be applied to various expert-system applications.  ",
    "keywords": [
      "defect detection",
      "novelty detection",
      "light emitting diodes",
      "feature extraction",
      "one-class svm",
      "kernel pca",
      "kernel density estimation"
    ]
  },
  {
    "id": "574",
    "title": "Finding a tree structure in a resolution proof is NP-complete",
    "abstract": "The resolution tree problem consists of deciding whether a given sequence-like resolution refutation admits a tree structure. This paper shows the NP-completeness of both the resolution tree problem and a natural generalization of the resolution tree problem that does not involve resolution.  ",
    "keywords": [
      "resolution",
      "tree-like resolution",
      "np-completeness"
    ]
  },
  {
    "id": "575",
    "title": "a real-time simulator for interventional radiology",
    "abstract": "Interventional radiologists manipulate guidewires and catheters and steer stents through the patient's vascular system under X-ray imaging for treatment of vascular diseases. The complexity of these procedures makes training mandatory in order to master hand-eye coordination, instrument manipulation and procedure protocols for each radiologist. In this paper we present a simulator for interventional radiology, which deploys a model of guidewire/catheter based on the Cosserat theory applied to one-dimensional structures. This model starts from the energetic formulation of the flament considering the Hook laws of continuum mechanics. The Lagrange formulations are used to describe the model deformation. This model takes (self-) collisions into account and it is revealed to be very efficient for interactive applications. The simulation environment allows to carry out the most common procedures: guidewire and catheter navigation, contrast dye injection to visualize the vessels, balloon angioplasty and stent placement. Moreover, heartbeat as well as breathing are also simulated visually.",
    "keywords": [
      "real-time simulation",
      "cosserat rod theory",
      "x-ray",
      "interventional radiology",
      "minimally invasive surgery"
    ]
  },
  {
    "id": "576",
    "title": "A semantic-based probabilistic approach for real-time video event recognition",
    "abstract": "This paper presents an approach for real-time video event recognition that combines the accuracy and descriptive capabilities of, respectively, probabilistic and semantic approaches. Based on a state-of-art knowledge representation, we define a methodology for building recognition strategies from event descriptions that consider the uncertainty of the low-level analysis. Then, we efficiently organize such strategies for performing the recognition according to the temporal characteristics of events. In particular, we use Bayesian Networks and probabilistically-extended Petri Nets for recognizing, respectively, simple and complex events. For demonstrating the proposed approach, a framework has been implemented for recognizing humanobject interactions in the video monitoring domain. The experimental results show that our approach improves the event recognition performance as compared to the widely used deterministic approach.",
    "keywords": [
      "video event detection",
      "semantic video analysis",
      "bayes network",
      "petri net",
      "low-level uncertainty"
    ]
  },
  {
    "id": "577",
    "title": "Reproductive weak solutions of magneto-micropolar fluid equations in exterior domains",
    "abstract": "We establish the existence of a reproductive weak solution, a so-called periodic weak solution, for the equations of motion of magneto-micropolar fluids in exterior domains in R-3.  ",
    "keywords": [
      "navier-stokes equations",
      "magneto-micropolar fluid",
      "reproductive solution",
      "exterior domain"
    ]
  },
  {
    "id": "578",
    "title": "Beam Bounding Box  a novel approach for beam concept modeling and optimization handling",
    "abstract": "Simplified models of the vehicle structure are often used during the concept phase of vehicle development to improve the Noise, Vibration and Harshness (NVH) performance. Together with the structural joints and panels, beams are one of the constituent parts of these models. There are different approaches for their modeling and optimization handling, which however are either not able to maintain the similarity with the detailed Finite Element (FE) model (reference and/or optimized) or suffering some flexibility and performance issues. The objective of the current work is to develop and validate a new method which is an improved alternative of the existing approaches. It keeps the reference cross-sectional shapes of all 1D beams, but when each of them is rescaled during optimization, the beam is represented by means of generic cross-sectional properties. Thus a lighter and simpler representation of the concept beams is created and at the same time the connection with the detailed FE model is not broken. The feasibility of the approach is successfully verified for a set of representative beam cross-sections and then for an industrial case study. Its benefit in terms of computational time is also demonstrated. The proposed method can be easily implemented and then applied to make concept modeling for the vehicle structure faster and more flexible.",
    "keywords": [
      "beam",
      "vehicle structure",
      "concept fe model",
      "optimization",
      "response surface"
    ]
  },
  {
    "id": "579",
    "title": "exploring the use of large displays in american megachurches",
    "abstract": "Within the HCI community, there is a growing interest in how technology is used and appropriated outside the workplace. In this paper, we present preliminary findings of how large displays, projection systems, and presentation software are used in American megachurches to support religious practice. These findings are based on ten visits to church services by the study.s authors. We describe how large display technology augments and replaces certain church traditions, and finish by discussing issues related to the design for church environments that are highlighted by this use of technology.",
    "keywords": [
      "large displays",
      "religious technologies",
      "field studies"
    ]
  },
  {
    "id": "580",
    "title": "AT-HOME 2.0-An Educational Framework for Home-based Healthcare",
    "abstract": "This paper intends to describe some of the primary social and cultural dynamics in South African home-based healthcare, using ethnographic case study material and design methodology. This constitutes a detailed narrative of the \"care experience\" in a poor community, emphasising the needs of and barriers to educational information, particularly concerning caregivers. In reaction to this context, a collaborative training model - AT-HOME 2.0 - emerges through design intervention. This is foreseen as a basic framework whereby caregivers (are encouraged to) develop educational content via information and communication technologies (e. g. mobile phones and social media). Such content can and may include experiences, suggestions, and guidelines that are relevant to the practice of caregiving. AT-HOME 2.0 will conceptually - and in some cases practically - demonstrate how educational content may be generated, published, and disseminated in the sphere of home-based healthcare.",
    "keywords": [
      "home-based healthcare",
      "informal learning",
      "digital technologies",
      "socio-cultural dynamics"
    ]
  },
  {
    "id": "581",
    "title": "overcoming limitations of prefetching in multiprocessors by compiler-initiated coherence action",
    "abstract": "In this paper we first identify limitations of compiler-controlled prefetching in a CC-NUMA multiprocessor with a write-invalidate cache coherence protocol. Compiler-controlled prefetch techniques for CC-NUMAs often are focused only, on stride-accesses, and this introduces a major limitation. We consider combining prefetch with two other compiler-controlled techniques to partly remedy the situation: (1) load-exclusive to reduce write-latency and (2) store-update to reduce read-latency. The purpose of each of these techniques in a machine with prefetch is to let them reduce latency for accesses which the prefetch technique could not handle. We evaluate two different scenarios, firstly with a hybrid compiler/hardware prefetch technique and secondly with an optimal stride-prefetcher. We find that the combined gains under the hybrid prefetch technique are significant for six applications we have studied: in average, 71% of the original write-stall time remains after using the hybrid prefetcher, and of these ownership-requests, 60% would be eliminated using load-exclusive; in average, 68% of the read-stall time remains after using the hybrid prefetcher and of these read-misses, 34% were serviced by remote caches and would be converted by store-update into misses serviced by a clean copy in memory which reduces the read-latency. With an optimal stride-prefetcher our results show that it beneficient to complement prefetch, with the two techniques here as well.",
    "keywords": [
      "memory access latency reduction",
      "compiler-initiated coherence",
      "read-latency",
      "migratory sharing",
      "cc-numa multiprocessor",
      "compiler-controlled prefetching",
      "prefetch",
      "compiler-analysis",
      "prefetching",
      "read-stall time",
      "write-latency",
      "multiprocessors",
      "parallel architectures"
    ]
  },
  {
    "id": "582",
    "title": "modeling and optimization of non-blocking checkpointing for optimistic simulation on myrinet clusters",
    "abstract": "Checkpointing and Communication Library (CCL) is a recently developed software implementing CPU offloaded checkpointing functionalities in support of optimistic parallel simulation on myrinet clusters. Specifically, CCL implements a non-blocking execution mode of memory-to-memory data copy associated with checkpoint operations, based on data transfer capabilities provided by a programmable DMA engine on board of myrinet network cards. Re-synchronization between CPU and DMA activities must sometimes be employed for several reasons, such as maintenance of data consistency, thus adding some overhead to (otherwise CPU cost-free) non-blocking checkpoint operations. In this paper we present a cost model for non-blocking checkpointing and derive a performance effective re-synchronization semantic which we call minimum cost re-synchronization   MC  . With this semantic, an occurrence of re-synchronization either commits an on-going DMA based checkpoint operation (causing suspension of CPU activities) or aborts the operation (with possible increase in the expected rollback cost due to a reduced amount of committed checkpoints) on the basis of a minimum overhead expectation evaluated through the cost model. We have implemented   MC   within CCL, and we also report experimental results demonstrating the performance benefits from this optimized re-synchronization semantic, in terms of increase in the execution speed, for a Personal Communication System (PCS) simulation application.",
    "keywords": [
      "optimistic simulation",
      "performance optimization",
      "checkpointing",
      "dma"
    ]
  },
  {
    "id": "583",
    "title": "Efficiency of IP Packets Pre-marking for H264 Video Quality Guarantees in Streaming Applications",
    "abstract": "For the last few years, many classifications and marking strategies have been proposed with the consideration of video streaming applications. According to IETF recommendation, two groups of solutions have been proposed. The firts one assumes that applications or IF end points pre-mark their packets. The second solution applies the router which is topologically closest to video source. It should perform Multifield Classification and mark all incomming packets. This paper investigates the most popular marking strategies belonging to both mentioned above groups of solutions. The pre-marking strategies based on H264 coder extensions are simulated based on NS-2 network simulator and Evalvid-RA framework. The results are compared with the IETF recommendations for video traffic shaping in the IP networks and marking algorithms proposed by other researchers.",
    "keywords": [
      "video streaming",
      "ip packet",
      "marking",
      "h264 video coding",
      "diffserv architecture"
    ]
  },
  {
    "id": "584",
    "title": "On sets of vectors of a finite vector space in which every subset of basis size is a basis II",
    "abstract": "This article contains a proof of the MDS conjecture for k a parts per thousand currency sign 2p - 2. That is, that if S is a set of vectors of in which every subset of S of size k is a basis, where q = p (h) , p is prime and q is not and k a parts per thousand currency sign 2p - 2, then |S| a parts per thousand currency sign q + 1. It also contains a short proof of the same fact for k a parts per thousand currency sign p, for all q.",
    "keywords": [
      "mds conjecture",
      "linear codes",
      "singleton bound"
    ]
  },
  {
    "id": "585",
    "title": "Group topic model: organizing topics into groups",
    "abstract": "Latent Dirichlet allocation defines hidden topics to capture latent semantics in text documents. However, it assumes that all the documents are represented by the same topics, resulting in the forced topic problem. To solve this problem, we developed a group latent Dirichlet allocation (GLDA). GLDA uses two kinds of topics: local topics and global topics. The highly related local topics are organized into groups to describe the local semantics, whereas the global topics are shared by all the documents to describe the background semantics. GLDA uses variational inference algorithms for both offline and online data. We evaluated the proposed model for topic modeling and document clustering. Our experimental results indicated that GLDA can achieve a competitive performance when compared with state-of-the-art approaches.",
    "keywords": [
      "topic modeling",
      "latent dirichlet allocation",
      "group",
      "variational inference",
      "online learning",
      "document clustering"
    ]
  },
  {
    "id": "586",
    "title": "Co-authorship networks in the digital library research community",
    "abstract": "The field of digital libraries (DLs) coalesced in 1994: the first digital library conferences were held that year, awareness of the World Wide Web was accelerating, and the National Science Foundation awarded $24 Million (US) for the Digital Library Initiative (DLI). In this paper we examine the state of the DL domain after a decade of activity by applying social network analysis to the co-authorship network of the past ACM, IEEE, and joint ACM/IEEE digital library conferences. We base our analysis on a common binary undirectional network model to represent the co-authorship network, and from it we extract several established network measures. We also introduce a weighted directional network model to represent the co-authorship network, for which we define AuthorRank as an indicator of the impact of an individual author in the network. The results are validated against conference program committee members in the same period. The results show clear advantages of PageRank and AuthorRank over degree, closeness and betweenness centrality metrics. We also investigate the amount and nature of international participation in Joint Conference on Digital Libraries (JCDL).",
    "keywords": [
      "digital library",
      "authorrank",
      "social network analysis",
      "co-authorship"
    ]
  },
  {
    "id": "587",
    "title": "Bipartite Matching in the Semi-streaming Model",
    "abstract": "We present the first deterministic 1+epsilon approximation algorithm for finding a large matching in a bipartite graph in the semi-streaming model which requires only O((1/epsilon)(5)) passes over the input stream. In this model, the input graph G = (V, E) is given as a stream of its edges in some arbitrary order, and storage of the algorithm is bounded by O(npolylog n) bits, where n = | V |. The only previously known arbitrarily good approximation for general graphs is achieved by the randomized algorithm of McGregor (Proceedings of the InternationalWorkshop on Approximation Algorithms for Combinatorial Optimization Problems and Randomization and Computation, Berkeley, CA, USA, pp. 170-181, 2005), which uses Omega((1/epsilon) 1/epsilon) passes. We show that even for bipartite graphs, McGregor's algorithm needs Omega(1/epsilon)(Omega(1/epsilon)) passes, thus it is necessarily exponential in the approximation parameter. The design as well as the analysis of our algorithm require the introduction of some new techniques. A novelty of our algorithm is a new deterministic assignment of matching edges to augmenting paths which is responsible for the complexity reduction, and gets rid of randomization. We repeatedly grow an initial matching using augmenting paths up to a length of 2k + 1 for k = [2/epsilon]. We terminate when the number of augmenting paths found in one iteration falls below a certain threshold also depending on k, that guarantees a 1 + epsilon approximation. The main challenge is to find those augmenting paths without requiring an excessive number of passes. In each iteration, using multiple passes, we grow a set of alternating paths in parallel, considering each edge as a possible extension as it comes along in the stream. Backtracking is used on paths that fail to grow any further. Crucial are the so-called position limits: when a matching edge is the ith matching edge in a path and it is then removed by backtracking, it will only be inserted into a path again at a position strictly lesser than i. This rule strikes a balance between terminating quickly on the one hand and giving the procedure enough freedom on the other hand.",
    "keywords": [
      "bipartite graph matching",
      "streaming algorithms",
      "approximation schemes",
      "approximation algorithms"
    ]
  },
  {
    "id": "588",
    "title": "Correlation analysis of principal components from two populations",
    "abstract": "We investigate a correlation coefficient of principal components from two sets of variables. Using perturbation expansion, we get a limiting distribution of the correlation. In addition, we obtain a limiting distribution of the Fisher's z transformation of the above correlation. Additionally, we verify the accuracy of the limiting distributions using Monte Carlo simulations. Finally in this study, we present two examples and a bootstrap estimation.",
    "keywords": [
      "principal component",
      "perturbation method",
      "canonical correlation analysis",
      "fisher's z transformation",
      "bootstrap"
    ]
  },
  {
    "id": "589",
    "title": "Modeling by singular value decomposition and the elimination of statistically insignificant coefficients",
    "abstract": "Numerical advantages of singular value decomposition over other least squares techniques. Elimination of statistically insignificant coefficients. Benefit of a statistical rejection procedure.",
    "keywords": [
      "svd",
      "stepwise regression",
      "numerical analysis"
    ]
  },
  {
    "id": "590",
    "title": "Linguistic majorities with difference in support",
    "abstract": "A new linguistic aggregation rule that extends numerical majorities based on difference in support is introduced. Linguistic majorities with difference in support are formalised for fuzzy set and 2-tuples. Both representations are proved to be mathematically isomorphic. A set of normative properties have been demonstrated to hold for the new linguistic majorities.",
    "keywords": [
      "social choice",
      "aggregation rule",
      "linguistic preferences",
      "linguistic majorities",
      "fuzzy sets",
      "2-tuples",
      "difference in support"
    ]
  },
  {
    "id": "591",
    "title": "An underfrequency load shedding scheme for islanded microgrids",
    "abstract": "In this paper an UFLS scheme for implementing in MG was proposed. This load shedding method estimates the power deficit based on the frequency first derivative. It considers power generation variations during the load shedding process. The proposed load shedding scheme is independent from MG parameters. A microgrid with several DER is adopted to demonstrate the effectiveness of the proposed method.",
    "keywords": [
      "microgrid",
      "minimum frequency",
      "power deficit",
      "underfrequency load shedding"
    ]
  },
  {
    "id": "592",
    "title": "Some new fuzzy entropy formulas",
    "abstract": "The purpose of this paper is twofold. Firstly, a general conclusion about fuzzy entropy induced by distance measure is presented based on the axiom definitions of fuzzy entropy and distance measure. Secondly, some fuzzy entropy formulas which relate to the fuzzy entropy formula defined by De Luca and Termini (Inform. Control 20 (1972) 301) are given.  ",
    "keywords": [
      "measures of information",
      "fuzzy entropy",
      "sigma-fuzzy entropy",
      "distance measure",
      "sigma-distance measure"
    ]
  },
  {
    "id": "593",
    "title": "Adhesion properties of polymethylsilsesquioxane based low dielectric constant materials by the modified edge lift-off test",
    "abstract": "Delamination occurring during the chemical and mechanical planarization process or wire bonding steps in packaging is a fundamental issue in integrating of low dielectric constant (low-k) materials into the multilayer structures of semiconductor chips. Since it is known that low adhesion strength is mainly attributed to the failure phenomenon, the measurement of interfacial fracture toughness is critical to provide a quantitative basis in the choice of the materials. In this study, a modified edge lift-off test was adopted to measure the fracture toughness of polymethylsilsesquioxane based low-k materials with various chemical and physical structures. Interfacial fracture toughness was improved by adding multi-functional monomers to methylsilsesquioxane monomers or by increasing the percentage of functional end groups inside the prepolymers. In addition, the change in curing conditions and thickness influenced the adhesion performance presumably by changing the morphology of low-k materials.",
    "keywords": [
      "adhesion",
      "thin film",
      "low-k dielectrics",
      "xps"
    ]
  },
  {
    "id": "594",
    "title": "When consensus meets self-stabilization",
    "abstract": "This paper presents a shared-memory self-stabilizing failure detector, asynchronous consensus and replicated state-machine algorithm suite, the components of which can be started in an arbitrary state and converge to act as a virtual state-machine. Self-stabilizing algorithms can cope with transient faults. Transient faults can alter the system state to an arbitrary state and hence, cause a temporary violation of the safety property of the consensus. Started in an arbitrary state, the long lived, memory bounded and self-stabilizing failure detector, asynchronous consensus, and replicated state-machine suite, presented in the paper, recovers to satisfy eventual safety and eventual liveness requirements. Several new techniques and paradigms are introduced. The bounded memory failure detector abstracts away synchronization assumptions using bounded heartbeat counters combined with a balance-unbalance mechanism. The practically infinite paradigm is introduced in the scope of self-stabilization, where an execution of, say, 2(64) sequential steps is regarded as (practically) infinite. Finally, we present the first self-stabilizing wait-free reset mechanism that ensures eventual safety and can be used to implement efficient self-stabilizing timestamps that are of independent interest.  ",
    "keywords": [
      "failure detector",
      "consensus",
      "state-machine",
      "wait-free",
      "distributed reset",
      "self-stabilization"
    ]
  },
  {
    "id": "595",
    "title": "Hybrid perturbation-Polynomial Chaos approaches to the random algebraic eigenvalue problem",
    "abstract": "The analysis of structures is affected by uncertainty in the structures material properties, geometric parameters, boundary conditions and applied loads. These uncertainties can be modelled by random variables and random fields. Amongst the various problems affected by uncertainty, the random eigenvalue problem is specially important when analyzing the dynamic behavior or the buckling of a structure. The methods that stand out in dealing with the random eigenvalue problem are the perturbation method and methods based on Monte Carlo Simulation. In the past few years, methods based on Polynomial Chaos (PC) have been developed for this problem, where each eigenvalue and eigenvector are represented by a PC expansion. In this paper four variants of a method hybridizing perturbation and PC expansion approaches are proposed and compared. The methods use Rayleigh quotient, the power method, the inverse power method and the eigenvalue equation. PC expansions of eigenvalues and eigenvectors are obtained with the proposed methods. The new methods are applied to the problem of an Euler Bernoulli beam and a thin plate with stochastic properties.",
    "keywords": [
      "stochastic finite element method",
      "random eigenvalue problem",
      "perturbation",
      "polynomial chaos",
      "iterative methods"
    ]
  },
  {
    "id": "596",
    "title": "A conservative box-scheme for the Euler equations",
    "abstract": "The work presented in this paper shows that the mixed-type scheme of Murman and Cole, originally developed for a scalar equation, can be extended to systems of conservation laws. A characteristic scheme for the equations of gas dynamics is introduced that has a close connection to a four operator scheme for the Burgers-Hopf equation. The results indicate that the scheme performs well on the classical test cases. The scheme has no tuning parameters and can be interpreted as the projection of an L-infinity-stable scheme. At steady state second order accuracy is obtained as a by-product of the box-scheme feature. ",
    "keywords": [
      "conservative box-scheme",
      "euler equations",
      "gas dynamics"
    ]
  },
  {
    "id": "597",
    "title": "Using a new protocol to enhance path reliability and realize load balancing in mobile ad hoc networks",
    "abstract": "In this paper we introduce a novel end-to-end approach for achieving the dual goal of enhanced reliability under path failures, and multi-path load balancing in mobile ad hoc networks (MANETs). These goals are achieved by fully exploiting the presence of multiple paths in mobile ad hoc networks in order to jointly attack the problems of frequent route failures and load balancing. More specifically, we built a disjoint-path identification mechanism for maintaining multiple routes between two endpoints on top of the Stream Control Transmission Protocol (SCTP), and the Dynamic Source Routing (DSR) protocol. A number of additional modifications are incorporated to the SCTP protocol in order to allow its smooth operation. The proposed approach differs from previously related work since it consists of an entirely end-to-end scheme built on top of a transport layer protocol. We provide both analytical and simulation results that prove the efficiency of our approach over a wide range of mobility scenarios.",
    "keywords": [
      "mobile ad hoc networks",
      "sctp",
      "dsr",
      "reliability",
      "load balancing"
    ]
  },
  {
    "id": "598",
    "title": "Situative Managementuntersttzungssysteme",
    "abstract": "Sogenannte digital natives sind mittlerweile auch auf den obersten Fhrungsebenen von Organisationen zu finden. Diese neue Managergeneration betrachtet Managementuntersttzungssysteme (MUS) mittlerweile als eine Selbstverstndlichkeit, hat aber auch zunehmend hohe Erwartungen, dass diese ihre individuellen Nutzungsprferenzen erfllen. Dementsprechend hinterfragen sie MUS, die keine Anpassungsmechanismen fr ihren jeweiligen Arbeitsstil, die verschiedenen relevanten MUS-Nutzungsflle und die unterschiedlichen MUS-Zugangsmglichkeiten vorsehen. Dieser Beitrag zeigt verschiedene Nutzungssituationen von Fhrungskrften auf, definiert als Klassen hnlicher Nutzergruppenprferenzen und schlgt Stellhebel vor, um die MUS-Gestaltung konzeptionell daran anzupassen. Basierend auf den Ergebnissen einer Literaturrecherche werden zunchst Nutzergruppenprferenzen in Form von 36 Nutzungssituationen klassifiziert. Hierauf aufbauend machen wir Vorschlge zur Endgerteauswahl. Wir vervollstndigen das Konfigurationsmodell, indem wir auch die Gestaltung der MUS-Benutzerschnittstelle einbeziehen. Schlielich zeigen wir die Ntzlichkeit unseres Vorschlags mithilfe einer Pilotumsetzung auf und evaluieren diese.",
    "keywords": [
      "managementuntersttzungssysteme ",
      "is-analyse und -gestaltung",
      "nutzergruppenprferenzen",
      "nutzungsfaktoren in der mus-gestaltung",
      "mus-konfiguration",
      "management support systems ",
      "is analysis and design",
      "user-group preferences",
      "use factors in mss design",
      "mss configuration"
    ]
  },
  {
    "id": "599",
    "title": "Regular Polytopes of Nearly Full Rank",
    "abstract": "An abstract regular polytope P of rank n can only be realized faithfully in Euclidean space E(d) of dimension d if d >= n when P is finite, or d >= n - 1 when P is infinite (that is, P is an apeirotope). In case of equality, the realization P of P is said to be of full rank. If there is a faithful realization P of P of dimension d = n + 1 or d = n ( as P is finite or not), then P is said to be of nearly full rank. In previous papers, all the at most four-dimensional regular polytopes and apeirotopes of nearly full rank have been classified. This paper classifies the regular polytopes and apeirotopes of nearly full rank in all higher dimensions.",
    "keywords": [
      "abstract regular polytope",
      "realization",
      "faithful",
      "nearly full rank",
      "fine schlafli symbol"
    ]
  },
  {
    "id": "600",
    "title": "Acceptable consistency analysis of interval reciprocal comparison matrices",
    "abstract": "When a decision maker expresses his/her opinions by means of an interval reciprocal comparison matrix, the study of consistency becomes a very important aspect in decision making in order to avoid a misleading solution. In the present paper. an acceptably consistent interval reciprocal comparison matrix is defined, which can be reduced to an acceptably consistent crisp reciprocal comparison matrix when the intervals become exact numbers. An interval reciprocal comparison matrix with unacceptable consistency can be easily adjusted such that the revised matrix possesses acceptable consistency. Utilizing a convex combination method, a family of crisp reciprocal comparison matrices with acceptable consistency can be obtained, whose weights are further found to exhibit a style of convex combination, and aggregated to obtain interval weights from an acceptably consistent interval reciprocal comparison matrix. A novel, simple yet effective formula of possibility degree is presented to rank interval weights. Numerical results are calculated to show the quality and quantity of the proposed approaches and compare with other existing procedures.  ",
    "keywords": [
      "interval reciprocal comparison matrix",
      "acceptable consistency",
      "convex combination",
      "interval weight",
      "possibility degree formula"
    ]
  },
  {
    "id": "601",
    "title": "Wireless Internet access: 3G vs. WiFi?",
    "abstract": "This article compares and contrasts two technologies for delivering broadband wireless Internet access services: 3G vs. WiFi. The former, 3G, refers to the collection of third-generation mobile technologies that are designed to allow mobile operators to offer integrated data and voice services over mobile networks. The latter, WiFi, refers to the 802.11b wireless Ethernet standard that was designed to support wireless LANs. Although the two technologies reflect fundamentally different service, industry, and architectural design goals, origins, and philosophies, each has recently attracted a lot of attention as candidates for the dominant platform for providing broadband wireless access to the Internet. It remains an open question as to the extent to which these two technologies are in competition or, perhaps, may be complementary. If they are viewed as in competition, then the triumph of one at the expense of the other would be likely to have profound implications for the evolution of the wireless Internet and structure of the service-provider industry.",
    "keywords": [
      "internet",
      "broadband",
      "wireless",
      "3g",
      "wlan",
      "ethernet",
      "access",
      "spectrum",
      "economics",
      "industry structure"
    ]
  },
  {
    "id": "602",
    "title": "Decentralized fuzzy control of multiple nonholonomic vehicles",
    "abstract": "This work considers the problem of controlling multiple nonholonomic vehicles so that they converge to a scent source without colliding with each other. Since the control is to be implemented on a simple 8-bit microcontroller, fuzzy control rules are used to simplify a linear quadratic regulator control design. The inputs to the fuzzy controllers for each vehicle are the noisy direction to the source, the distance to the closest neighbor vehicle, and the direction to the closest vehicle. These directions are discretized into four values: forward, behind, left, and right; and the distance into three values: near, far, and gone. The values of the control at these discrete values are obtained based on the collision-avoidance repulsive forces and an attractive force towards the goal. A fuzzy inference system is used to obtain control values from a small number of discrete input values. Simulation results are provided which demonstrate that the fuzzy control law performs well compared to the exact controller. In fact, the fuzzy controller demonstrates improved robustness to noise.",
    "keywords": [
      "fuzzy logic",
      "mobile robots",
      "decentralized",
      "linear quadratic",
      "group behavior",
      "cooperative robotics",
      "kalman estimation"
    ]
  },
  {
    "id": "603",
    "title": "On optimal p-cycle-based protection in WDM optical networks with sparse-partial wavelength conversion",
    "abstract": "We study the optimal configuration of p-cycles in survivable wavelength division multiplexing (WDM) optical mesh networks with sparse-partial wavelength conversion while 100% restorability is guaranteed against any single failures. We formulate the problem as two integer linear programs (Optimization Models I, and II) which have the same constraints, but different objective functions. p-cycles and wavelength converters are optimally determined subject to the constraint that only a given number of nodes have wavelength conversion capability, and the maximum number of wavelength converters that can be placed at such nodes is limited. Optimization Model I has a composite sequential objective function that first (G1) minimizes the cost of link capacity used by all p-cycles in order to accommodate a set of traffic demands; and then (G2) minimizes the total number of wavelength converters used in the entire network. In Optimization Model II, the cost of one wavelength. converter is measured as the cost of a deployed wavelength link with a length of a units; and the objective is to minimize the total cost of link capacity & wavelength converters required by p-cycle configuration. During p-cycle configuration, our schemes fully takes into account wavelength converter sharing, which reduces the number of converters required while attaining a satisfactory level of performance. Our simulation results indicate that the proposed schemes significantly outperform existing approaches in terms of protection cost, number of wavelength conversion sites, and number of wavelength converters needed.",
    "keywords": [
      "converter sharing",
      "integer linear programming",
      "optimal p-cycle configuration",
      "sparse-partial wavelength conversion",
      "wdm optical networks"
    ]
  },
  {
    "id": "604",
    "title": "Biomechanical simulation of the fetal descent without imposed theoretical trajectory",
    "abstract": "The medical training concerning childbirth for young obstetricians involves performing real deliveries, under supervision. This medical procedure becomes more complicated when instrumented deliveries requiring the use of forceps or suction cups become necessary. For this reason, the use of a versatile, configurable childbirth simulator, taking into account different anatomical and pathological cases, would provide an important benefit in the training of obstetricians, and improve medical procedures. The production of this type of simulator should be generally based on a computerized birth simulation, enabling the computation of the reproductive organs deformation of the parturient woman and fetal interactions as well as the calculation of efforts produced during the second stage of labor. In this paper, we present a geometrical and biomechanical modeling of the main parturient's organs involved in the birth process, interacting with the fetus. Instead of searching for absolute precision, we search to find a good compromise between accuracy and model complexity. At this stage, to verify the correctness of our hypothesis, we use finite element analysis because of its reliability, precision and stability. Moreover, our study improves the previous work carried out on childbirth simulators because: (a) our childbirth model takes into account all the major organs involved in birth process, thus potentially enabling different childbirth scenarios; (b) fetal head is not treated as a rigid body and its motion is computed by taking into account realistic boundary conditions, i.e. we do not impose a pre-computed fetal trajectory; (c) we take into account the cyclic uterine contractions as well as voluntary efforts produced by the muscles of the abdomen; (d) a slight pressure is added inside the abdomen, representing the residual muscle tone. The next stage of our work will concern the optimization of our numerical resolution approach to obtain interactive time simulation, enabling it to be coupled to our haptic device.",
    "keywords": [
      "biomechanical modeling of organs",
      "fetal descent",
      "finite element model",
      "medical training"
    ]
  },
  {
    "id": "605",
    "title": "Complex patterns in networks of hyperexcitable neurons",
    "abstract": "Complex patterns in neuronal networks emerge from the cooperative activity of the participating neurons, synaptic connectivity and network topology. Several neuron types exhibit complex intrinsic dynamics due to the presence of nonlinearities and multiple time scales. In this paper we extend previous work on hyperexcitability of neuronal networks, a hallmark of epileptic brain seizure generation, which results from the net imbalance between excitation and inhibition and the ability of certain neuron types to exhibit abrupt transitions between low and high firing frequency regimes as the levels of recurrent AMPA excitation change. We examine the effect of different topologies and connection delays on the hyperexcitability phenomenon in networks having recurrent synaptic AMPA (fast) excitation (in the absence of synaptic inhibition) and demonstrate the emergence of additional time scales.",
    "keywords": [
      "neuronal networks",
      "synchronization"
    ]
  },
  {
    "id": "606",
    "title": "Decentralized Management of Building Indoors through Embedded Software Agents",
    "abstract": "In order to support personalized people comfort and building energy efficiency as well as safety, emergency, and context-aware information exchange scenarios, next-generation buildings will be smart. In this paper we propose an agent-oriented decentralized and embedded architecture based on wireless sensor and actuator networks (WSANs) for enabling efficient and effective management of buildings. The main objective of the proposed architecture is to fully support distributed and coordinated sensing and actuation operations. The building management architecture is implemented at the WSAN side through MAPS (Mobile Agent Platform for Sun SPOTs), an agent-based framework for programming WSN applications based on the Sun SPOT sensor platform, and at the base station side through an OSGi-based application. The proposed agent-oriented architecture is demonstrated in a simple yet effective operating scenario related to monitoring workstation usage in computer laboratories/offices. The high modularity of the proposed architecture allows for easy adaptation of higher-level application-specific agents that can therefore exploit the architecture to implement intelligent building management policies.",
    "keywords": [
      "smart buildings",
      "multi-agent systems",
      "wireless sensor and actuator networks",
      "building management systems"
    ]
  },
  {
    "id": "607",
    "title": "Electronic Governance for Sustainable Development  Conceptual framework and state of research",
    "abstract": "Electronic Governance (EGOV) research studies the use of Information and Communication Technologies to improve governance processes. Sustainable Development (SD) research studies possible development routes that satisfy the needs of the present generation without compromising the ability of the future generations to meet their own needs. Despite substantial progress in advancing both domains independently, little research exists at their intersection  how to utilize EGOV in support of SD. We call this intersection Electronic Governance for Sustainable Development (EGOV4SD). This paper: 1) proposes a conceptual framework for EGOV4SD, 2) proposes EGOV4SD research assessment framework and 3) applies both frameworks to determine the state of EGOV4SD research. The main contribution of the paper is establishing a foundation for EGOV4SD research.",
    "keywords": [
      "electronic governance",
      "sustainable development",
      "electronic governance for sustainable development",
      "meta-research"
    ]
  },
  {
    "id": "608",
    "title": "SelGen: System to dynamically evaluate and compare the agronomic behavior of genotypes that participate in networks of comparative yield trials",
    "abstract": "One of the decisions of major impact on crop profitability is the selection of the genotype to sow. SelGen is a software that makes it possible to evaluate and dynamically compare genotypes; this allows a faster selection of the suitable genotype. The software processes, according to the options selected by the user, the information contained in a database regarding the genotype yield in different environments. The results are shown in graphs and tables. The database included in the program is updated twice a year. Users can analyze their own databases.",
    "keywords": [
      "genotype selection",
      "crop yield",
      "evaluation system"
    ]
  },
  {
    "id": "609",
    "title": "Control of mineral wool thickness using predictive functional control",
    "abstract": "The production process of mineral wool is affected by several constantly changing factors. The ingredients for the mineral wool are melted in a furnace. The molten mineral charge exits the bottom of the furnace in a water-cooled trough and falls into a fiberization device (the centrifuge). The centrifuge forms the fibers. At this stage binders are injected to bind the fibers together. To ensure the quality of the end product (the consistent thickness) the flow of the bounded fibers must be as constant as possible. One way to ensure that is to control the speed of the conveyor belt that transports the bounded fibers from the centrifuge to the curing process. Predictive functional controller and PID controller are considered to replace an existing algorithm. Both can easily replace an existing one as they do not require any new sensor installation. All three algorithms are presented and tested on a developed plant model. The study showed that the predictive control gives better results than the existing and PID controller.",
    "keywords": [
      "stone wool process",
      "predictive functional control",
      "thickness control",
      "conveyor belt control"
    ]
  },
  {
    "id": "610",
    "title": "Bioinformatics resource manager v2.3: an integrated software environment for systems biology with microRNA and cross-species analysis tools",
    "abstract": "MicroRNAs (miRNAs) are noncoding RNAs that direct post-transcriptional regulation of protein coding genes. Recent studies have shown miRNAs are important for controlling many biological processes, including nervous system development, and are highly conserved across species. Given their importance, computational tools are necessary for analysis, interpretation and integration of high-throughput (HTP) miRNA data in an increasing number of model species. The Bioinformatics Resource Manager (BRM) v2.3 is a software environment for data management, mining, integration and functional annotation of HTP biological data. In this study, we report recent updates to BRM for miRNA data analysis and cross-species comparisons across datasets.",
    "keywords": [
      "systems biology",
      "genomics",
      "microrna",
      "bioinformatics",
      "zebrafish"
    ]
  },
  {
    "id": "611",
    "title": "Integrated process planning using tool/process capabilities and heuristic search",
    "abstract": "CAD-CAM integration has involved either design with standard manufacturing features (feature-based design), or interpretation of a solid model based on a set of predetermined feature patterns (automatic feature recognition). Thus existing approaches are limited in application to predefined features, and also disregard the dynamic nature of the process and tool availability in the manufacturing shop floor. To overcome this problem, we develop a process oriented approach to design interpretation, and model the shape producing capabilities of the tools into tool classes. We then interpret the part by matching regions of it with the tool classes directly. In addition, there could be more than one way in which a part can be interpreted, and to obtain an optimal plan, it is necessary for an integrated computer aided process planning system to examine these alternatives. We develop a systematic search algorithm to generate the different interpretations, and a heuristic approach to sequence operations (set-ups/tools) for the features of the interpretations generated. The heuristic operation sequencing algorithm considers features and their manufacturing constraints (precedences) simultaneously, to optimally allocate set-ups and tools for the various features. The modules within the design interpretation and process planner are linked through an abstracted qualitative model of feature interactions. Such an abstract representation is convenient for geometric reasoning tasks associated with planning and design interpretation.",
    "keywords": [
      "computer-aided process planning",
      "qualitative spatial reasoning",
      "design interpretation",
      "feature recognition",
      "feature-based manufacturing",
      "tool/process capability matching"
    ]
  },
  {
    "id": "612",
    "title": "Reliable approaches of variational iteration method for nonlinear operators",
    "abstract": "In this paper, new approaches of the variational iteration method are developed to handle nonlinear problems. The proposed approaches are capable of reducing the size of calculations and easily overcome the difficulty arising in calculating complicated integrals. Numerical examples are examined to show the efficiency of the techniques. The modified approaches show improvements over the existing numerical schemes.  ",
    "keywords": [
      "variational iteration method",
      "lagrange multiplier",
      "klein-gordon equation",
      "sine-gordon equation"
    ]
  },
  {
    "id": "613",
    "title": "Program termination and well partial orderings",
    "abstract": "The following known observation is useful in establishing program termination: if a transitive relation R is covered by finitely many well-founded relations U(1),..., U(n) then R is well-founded. A question arises how to bound the ordinal height vertical bar R vertical bar of the relation R in terms of the ordinals alpha(i) = vertical bar U(i) vertical bar. We introduce the notion of the stature parallel to P parallel to of a well partial ordering P and show that vertical bar R vertical bar <= parallel to alpha(1) x ... x alpha(n) parallel to and that this bound is tight. The notion of stature is of considerable independent interest. We define parallel to P parallel to as the ordinal height of the forest of nonempty bad sequences of P, but it has many other natural and equivalent definitions. In particular, parallel to P parallel to is the supremum, and in fact the maximum, of the lengths of linearizations of P. And parallel to alpha(1) x ... x alpha(n) parallel to is equal to the natural product alpha(1) circle times ... circle times alpha(n).",
    "keywords": [
      "algorithms",
      "theory",
      "program termination",
      "well partial orderings",
      "covering observation",
      "game criterion"
    ]
  },
  {
    "id": "614",
    "title": "linear predictive coding and cepstrum coefficients for mining time variant information from software repositories",
    "abstract": "This paper presents an approach to recover time variant information from software repositories. It is widely accepted that software evolves due to factors such as defect removal, market opportunity or adding new features. Software evolution details are stored in software repositories which often contain the changes history. On the other hand there is a lack of approaches, technologies and methods to efficiently extract and represent time dependent information. Disciplines such as signal and image processing or speech recognition adopt frequency domain representations to mitigate differences of signals evolving in time. Inspired by time-frequency duality, this paper proposes the use of Linear Predictive Coding (LPC) and Cepstrum coefficients to model time varying software artifact histories. LPC or Cepstrum allow obtaining very compact representations with linear complexity. These representations can be used to highlight components and artifacts evolved in the same way or with very similar evolution patterns. To assess the proposed approach we applied LPC and Cepstral analysis to 211 Linux kernel releases (i.e., from 1.0 to 1.3.100), to identify files with very similar size histories. The approach, the preliminary results and the lesson learned are presented in this paper.",
    "keywords": [
      "software evolution",
      "data mining"
    ]
  },
  {
    "id": "615",
    "title": "Outlier detection for patient monitoring and alerting",
    "abstract": "We develop and evaluate a data-driven approach for detecting unusual (anomalous) patient-management decisions using past patient cases stored in electronic health records (EHRs). Our hypothesis is that a patient-management decision that is unusual with respect to past patient care may be due to an error and that it is worthwhile to generate an alert if such a decision is encountered. We evaluate this hypothesis using data obtained from EHRs of 4486 post-cardiac surgical patients and a subset of 222 alerts generated from the data. We base the evaluation on the opinions of a panel of experts. The results of the study support our hypothesis that the outlier-based alerting can lead to promising true alert rates. We observed true alert rates that ranged from 25% to 66% for a variety of patient-management actions, with 66% corresponding to the strongest outliers.",
    "keywords": [
      "machine learning",
      "clinical alerting",
      "conditional outlier detection",
      "medical errors"
    ]
  },
  {
    "id": "616",
    "title": "Maplets for correspondence-based object recognition",
    "abstract": "We present a correspondence-based system for visual object recognition with invariance to position, orientation, scale and deformation. The system is intermediate between high- and low-dimensional representations of correspondences. The essence of the approach is based on higher-order links, called here maplets, which are specific to narrow ranges of mapping parameters (position, scale and orientation), which interact cooperatively with each other, and which are assumed to be formed by learning. While being based on dynamic links, the system overcomes previous problems with that formulation in terms of speed of convergence and range of allowed variation. We perform face recognition experiments, comparing ours to other published systems. We see our work as a step towards a reformulation of neural dynamics that includes rapid network self-organization as essential aspect of brain state organization.",
    "keywords": [
      "object recognition",
      "correspondence",
      "dynamic link",
      "map formation",
      "self-organization",
      "maplet"
    ]
  },
  {
    "id": "617",
    "title": "Self-interruptions in discretionary multitasking",
    "abstract": "Human multitasking is often the result of self-initiated interruptions in the performance of an ongoing task. These self-interruptions occur in the absence of external triggers such as electronic alerts or email notifications. Compared to externally induced interruptions, self-interruptions have not received enough research attention. To address this gap, this paper develops a typology of self-interruptions based on the integration of Flow Theory and Self-regulation Theory. In this new typology, the two major categories stem from positive and negative feelings of task progress and prospects of goal attainment. The proposed classification is validated in an experimental multitasking environment with pre-defined tasks. Empirical findings indicate that negative feelings trigger more self-interruptions than positive feelings. In general, more self-interruptions result in lower accuracy in all tasks. The results suggest that negative internal triggers of self-interruptions unleash a downward spiral that may degrade performance.",
    "keywords": [
      "multitasking",
      "interruptions",
      "self-interruptions",
      "performance",
      "flow"
    ]
  },
  {
    "id": "618",
    "title": "PID-Controlled Particle Swarm Optimization",
    "abstract": "Premature convergence is a major challenge for particle swarm optimization algorithm (PSO) when dealing with multi-modal problems. The reason is partly due to the insufficient exploration capability because of the fast convergent speed especially in the final stage. In this paper, the PSO is regarded as a two-inputs one-output feedback system, and two PID controllers are incorporated into the methodology of PSO to improve the population diversity. Different from the integral controller, PID controller has three independent parameters and adjusts them dynamically. Theoretical results with support set theory and stability analysis both demonstrate that PID controller provides more chances to escaping from a local optimum. To validate the efficiency of this new variant, four other famous variants are used to compare including the comprehensive leaning PSO, modified time-varying accelerator coefficients PSO, integral-controlled PSO and the standard version, the test suit consists five unconstrained numerical benchmarks with dimensionality 30 and 100, respectively. Simulation results show PID-controlled PSO is suitable for high-dimensional multi-modal problems due to the large exploration capability in the final stage.",
    "keywords": [
      "particle swarm optimization",
      "pid controller",
      "support set theory",
      "stability analysis"
    ]
  },
  {
    "id": "619",
    "title": "Small k-pyramids and the complexity of determining k",
    "abstract": "Motivated by the computational complexity of determining whether a graph is hamiltonian, we study under algorithmic aspects a class of polyhedra called k-pyramids, introduced in [31], and discuss related applications. We prove that determining whether a given graph is the 1-skeleton of a k-pyramid, and if so whether it is belted or not, can be done in polynomial time for k?3 k ? 3 . The impact on hamiltonicity follows from the traceability of all 2-pyramids and non-belted 3-pyramids, and from the hamiltonicity of all non-belted 2-pyramids. The algorithm can also be used to determine the outcome for larger values of k, but the complexity increases exponentially with k. Lastly, we present applications of the algorithm, and improve the known bounds for the minimal cardinality of systems of bases called foundations in graph families with interesting properties concerning traceability and hamiltonicity.",
    "keywords": [
      "pyramid",
      "prism",
      "halin graph",
      "hamiltonian"
    ]
  },
  {
    "id": "620",
    "title": "Performance of MRC combining multi-antenna cooperative relay network",
    "abstract": "Performance of cooperative relaying employing infrastructure based fixed relays having multiple antennas has been investigated. Employing MGF based approach, closed form expression for outage probability and bit error rate performance of BPSK signal have been derived, when relay and destination are assumed to perform MRC combining of the signals. The effect of relay placement on the system performance has also been studied under different path loss conditions.",
    "keywords": [
      "outage probability",
      "bit error rate for bpsk",
      "mrc combining",
      "multi-antenna relay",
      "decode and forward mode"
    ]
  },
  {
    "id": "621",
    "title": "Novel Tight Closed-Form Bounds for the Symbol Error Rate of EGC and MRC Diversity Receivers Employing Linear Modulations Over (alpha -mu ) Fading",
    "abstract": "In this paper, we propose novel lower and upper bounds on the average symbol error rate (SER) of the dual-branch maximal-ratio combining and equal-gain combining diversity receivers assuming independent branches. (M)-ary pulse amplitude modulation and (M)-ary phase shift keying schemes are employed and operation over the (alpha -mu ) fading channel is assumed. The proposed bounds are given in closed form and are very simple to calculate as they are composed of a double finite summation of basic functions that are readily available in the commercial software packages. Furthermore, the proposed bounds are valid for any combination of the parameters (alpha ) and (mu ) as well as (M). Numerical results presented show that the proposed bounds are very tight when compared to the exact SER obtained via performing the exact integrations numerically making them an attractive much simpler alternative for SER evaluation studies.",
    "keywords": [
      " fading",
      "maximal-ratio combining",
      "equal-gain combining",
      "symbol error rate",
      "approximation",
      "bounds"
    ]
  },
  {
    "id": "622",
    "title": "Future directions in evaluation research: People, organizational, and social issues",
    "abstract": "Objective: To review evaluation literature concerning people, organizational, and social issues and provide recommendations for future research. Method: Analyze this research and make recommendations. Results and Conclusions: Evaluation research is key in identifying how people, organizational, and social issues - all crucial to system design, development, implementation, and use - interplay with informatics projects. Building on a long history of contributions and using a variety of methods, researchers continue developing evaluation theories and methods while producing significant interesting studies. We recommend that future research: I) Address concerns of the many individuals involved in or affected by informatics applications. 2) Conduct studies in different type and size sites, and with different scopes of systems and different groups of users. Do multi site or multi-system comparative studies. 3) Incorporate evaluation into all phases of a project. 4) Study failures, partial successes, and changes in project definition or outcome. 5) Employ evaluation approaches that take account of the shifting nature of health care and project environments, and do formative evaluations. 6) Incorporate people, social, organizational, cultural, and concomitant ethical issues into the mainstream of medical informatics. 7) Diversify research approaches and continue to develop new approaches. 8) Conduct investigations at different levels of analysis. 9) Integrate findings from different applications and contextual settings, different areas of health care, studies in other disciplines, and also work that is not published in traditional research outlets. 10) Develop and test theory to inform both further evaluation research and informatics practice.",
    "keywords": [
      "evaluation",
      "technology assessment",
      "medical informatics",
      "telemedicine",
      "organizational culture",
      "attitudes towards computers",
      "implementation",
      "barriers",
      "people, organizational, social issues",
      "sociotechnical",
      "ethical issues",
      "qualitative methods",
      "ethnographic methods",
      "multi-method",
      "human-computer interaction"
    ]
  },
  {
    "id": "623",
    "title": "An intervention method for occupational safety in farming  evaluation of the effect and process",
    "abstract": "In order to increase safety in Swedish farming an intervention methodology to influence attitudes and behaviour was tested. Eightyeight farmers and farm workers in nine groups gathered on seven occasions during 1 year. The basic concept was to create socially supportive networks and encourage discussions and reflection, focusing on risk manageability. Six of the groups made structured incident/accident analyses. Three of the latter groups also received information on risks and accident consequences. Effects were evaluated in a pre-post questionnaire using six-graded scales. A significant increase in safety activity and significant reduction in stress and risk acceptance was observed in the total sample. Risk perception and perceived risk manageability did not change. Analysing incidents/accidents, but not receiving information, showed a more positive outcome. Qualitative data indicated good feasibility and that the long duration of the intervention was perceived as necessary. The socially supportive network was reported as beneficial for the change process.",
    "keywords": [
      "long-term safety intervention",
      "attitude and behavioural change",
      "farming"
    ]
  },
  {
    "id": "624",
    "title": "A performance model of the Parallel Ocean Program",
    "abstract": "In this paper we describe a performance model of the Parallel Ocean Program (POP). In particular, the latest version of POP (v2.0) is considered, which has similarities and differences to the earlier version (v1.4.3) as commonly used in climate simulations. The performance model encapsulates an understanding of POP's data decomposition, processing flow, and scaling characteristics. The model is parametrized in many of the main input parameters to POP as well as characteristics of a processing system such as network latency and bandwidth. The performance model has been validated to date on a medium-sized (128 processor) AlphaServer ES40 system with the QsNet-1 interconnection network, and also on a larger scale (2048 processor) Blue Gene/Light system. The accuracy of the performance model is high when using two standard benchmark configurations, one of which represents a realistic configuration similar to that used in Community Climate System Model coupled climate simulations. The performance model is also used to explore the performance of POP after possible optimizations to the code, and different task to processor assignment strategies, whose performance cannot be currently measured.",
    "keywords": [
      "performance modeling",
      "large-scale systems",
      "performance analysis",
      "ocean modeling"
    ]
  },
  {
    "id": "625",
    "title": "Biosignals Modulated by Tumor-Associated Carbohydrate Antigens",
    "abstract": "Based on the remodeling of glycosphingolipids on the human tumor cell lines with manipulation of glycosyltransferase genes, roles of sugar moieties in tumor-associated carbohydrate antigens have been analyzed. Two main topics, that is, the roles of ganglioside GD3 in human malignant melanomas and those of GD2 in small cell lung cancer (SCLC) were reported. GD3 enhances tyrosine phosphorylation of two adaptor molecules, p130Cas and paxillin, resulting in the increased cell growth and invasion in melanoma cells. GD2 also enhances the proliferation and invasion of SCLC cells. GD2 also mediates apoptosis with anti-GD2 monoclonal antibodies (mAbs) via dephosphorylation of the focal adhesion kinase. These approaches have promoted further understanding of mechanisms by which gangliosides modulate malignant properties of human cancer, and the results obtained here propose novel targets for cancer therapy",
    "keywords": [
      "glycolipids",
      "gd3",
      "gd2",
      "melanoma",
      "lung cancer",
      "proliferation",
      "invasion"
    ]
  },
  {
    "id": "626",
    "title": "A novel hybrid intelligent system for multi-objective machine parameter optimization",
    "abstract": "This multidisciplinary research presents a novel hybrid intelligent system to perform a multi-objective industrial parameter optimization process. The intelligent system is based on the application of evolutionary and neural computation in conjunction with identification systems, which makes it possible to optimize the implementation conditions in the manufacturing process of high precision parts, including finishing precision, while saving time, financial costs and/or energy. Empirical verification of the proposed hybrid intelligent system is performed in a real industrial domain, where a case study is defined and analyzed. The experiments are carried out based on real dental milling processes using a high precision machining centre with five axes, requiring high finishing precision of measures in micrometers with a large number of process factors to analyze. The results of the experiments which validate the performance of the proposed approach are presented in this study.",
    "keywords": [
      "hybrid intelligent system",
      "dental milling process",
      "optimization",
      "unsupervised learning",
      "identification systems",
      "multi-objective optimization"
    ]
  },
  {
    "id": "627",
    "title": "A general approach for modeling the motion of rigid and deformable ellipsoids in ductile flows",
    "abstract": "A general approach for modeling the motion of rigid or deformable objects in viscous flows is presented. It is shown that the rotation of a 3D object in a viscous fluid, regardless of the mechanical property and shape of the object, is defined by a common and simple differential equation, d Q / d t = Q , where Q is a matrix defined by the orientation of the object and ?  is the angular velocity tensor of the object. The difference between individual cases lies only in the formulation for the angular velocity. Thus the above equation, together with Jeffery's theory for the angular velocity of rigid ellipsoids, describes the motion of rigid ellipsoids in viscous flows. The same equation, together with Eshelby's theory for the angular velocity of deformable ellipsoids, describes the motion of deformable ellipsoids in viscous flows. Both problems are solved here numerically by a general approach that is much simpler conceptually and more economic computationally, compared to previous approaches that consider the problems separately and require numerical solutions to coupled differential equations about Euler angles or spherical (polar coordinate) angles. A RungeKutta approximation is constructed for solving the above general differential equation. Singular cases of Eshelby's equations when the object is spheroidal or spherical are handled in this paper in a much simpler way than in previous work. The computational procedure can be readily implemented in any modern mathematics application that handles matrix operations. Four MathCad Worksheets are provided for modeling the motion of a single rigid or deformable ellipsoid immersed in viscous fluids, as well as the evolution of a system of noninteracting rigid or deformable ellipsoids embedded in viscous flows.",
    "keywords": [
      "jeffery's theory",
      "eshelby's theory",
      "clast rotation",
      "preferred orientation",
      "viscous flow",
      "numerical modeling"
    ]
  },
  {
    "id": "628",
    "title": "An MDD-based generalized arc consistency algorithm for positive and negative table constraints and some global constraints",
    "abstract": "A table constraint is explicitly represented as its set of solutions or non-solutions. This ad hoc (or extensional) representation may require space exponential to the arity of the constraint, making enforcing GAC expensive. In this paper, we address the space and time inefficiencies simultaneously by presenting the mddc constraint. mddc is a global constraint that represents its (non-)solutions with a multi-valued decision diagram (MDD). The MDD-based representation has the advantage that it can be exponentially smaller than a table. The associated GAC algorithm (called mddc) has time complexity linear to the size of the MDD, and achieves full incrementality in constant time. In addition, we show how to convert a positive or negative table constraint into an mddc constraint in time linear to the size of the table. Our experiments on structured problems, car sequencing and still-life, show that mddc is also a fast GAC algorithm for some global constraints such as sequence and regular. We also show that mddc is faster than the state-of-the-art generic GAC algorithms in Gent et al. (2007), Lecoutre and Szymanek (2006), Lhomme and R,gin (2005) for table constraint.",
    "keywords": [
      "ad hoc constraint",
      "global constraint",
      "table constraint",
      "positive constraint",
      "negative constraint",
      "multi-valued decision diagram",
      "generalized arc consistency"
    ]
  },
  {
    "id": "629",
    "title": "Opinion dynamics driven by leaders, media, viruses and worms",
    "abstract": "A model on the effects of leader, media, viruses, worms, and other agents on the opinion of individuals is developed and utilized to simulate the formation of consensus in society and price in market via excess between supply and demand. The effects of some time varying drives (harmonic and hyperbolic) are also investigated.",
    "keywords": [
      "opinion",
      "leader",
      "media",
      "market",
      "buyers",
      "sellers",
      "excess"
    ]
  },
  {
    "id": "630",
    "title": "ON THE LEARNING POTENTIAL OF THE APPROXIMATED QUANTRON",
    "abstract": "The quantron is a hybrid neuron model related to perceptrons and spiking neurons. The activation of the quantron is determined by the maximum of a sum of input signals, which is difficult to use in classical learning algorithms. Thus, training the quantron to solve classification problems requires heuristic methods such as direct search. In this paper, we present an approximation of the quantron trainable by gradient search. We show this approximation improves the classification performance of direct search solutions. We also compare the quantron and the perceptron's performance in solving the IRIS classification problem.",
    "keywords": [
      "quantron",
      "spiking neuron",
      "learning algorithm",
      "gradient search",
      "iris classification problem"
    ]
  },
  {
    "id": "631",
    "title": "trust management services in relational databases",
    "abstract": "Trust management represents today a promising approach for supporting access control in open environments. While several approaches have been proposed for trust management and significant steps have been made in this direction, a major obstacle that still exists in the realization of the benefits of this paradigm is represented by the lack of adequate support in the DBMS.In this paper, we present a design that can be used to implement trust management within current relational DBMSs. We propose a trust model with a SQL syntax and illustrate the main issues arising in the implementation of the model in a relational DBMS. Specific attention is paid to the efficient verification of a delegation path for certificates. This effort permits a relatively inexpensive realization of the services of an advanced trust management model within current relational DBMSs.",
    "keywords": [
      "access control",
      "relational dbms",
      "credentials",
      "trust"
    ]
  },
  {
    "id": "632",
    "title": "Note on algebraic solutions of differential equations with known finite Galois group",
    "abstract": "Given a linear differential equation with known finite differential Galois group, we discuss methods to construct the minimal polynomial of a solution. We first outline a well known general method involving a basis transformation of the basis of formal solutions at a singular point. In the second part we construct directly the minimal polynomial of an eigenvector of the monodromy matrix at a singular point. The method is very efficient for irreducible second and third order linear differential equations where a one dimensional eigenspace of some monodromy matrix always exists.",
    "keywords": [
      "linear differential equations",
      "differential galois theory",
      "algebraic solutions",
      "liouvillian solutions"
    ]
  },
  {
    "id": "633",
    "title": "Security of Kuwakado-Tanaka transitive signature scheme for directed trees",
    "abstract": "Recently, Kuwakado and Tanaka proposed a transitive signature scheme for directed trees. In this letter, we show that Kuwakado-Tanaka scheme is insecure against a forgery attack, in which an attacker is able to forge edge signatures by composing edge signatures provided by a signer.",
    "keywords": [
      "cryptography",
      "transitive signature",
      "forgery attack"
    ]
  },
  {
    "id": "634",
    "title": "An improved virtualization layer to support distribution of multimedia contents in pervasive social applications",
    "abstract": "Pervasive social computing is a new paradigm of computer science that aims to facilitate the realization of activities in whichever context, with the aid of information devices and considering social relations between users. This vision requires means to support the shared experiences by harnessing the communication and computing capabilities of the connected devices, relying on direct or hop-by-hop communications among people who happen to be close to each other. In this paper, we present an approach to turn mobile ad-hoc networks (MANETs) into stable communication environments for pervasive social applications. The proposal is based on an evolution of the VNLayer, a virtualization layer that defined procedures for mobile devices to collaboratively emulate an infrastructure of stationary virtual nodes. We refine the VNLayer procedures and introduce new ones to increase the reliability and the responsiveness of the virtual nodes, which serves to boost the performance of routing with a virtualized version of the well-known AODV algorithm. We prove the advantages of the resulting routing scheme by means of simulation experiments and measurements on a real deployment of an application for immersive and collective learning about History in museums and their surroundings.",
    "keywords": [
      "pervasive social computing",
      "mobile ad-hoc networks",
      "virtualization"
    ]
  },
  {
    "id": "635",
    "title": "A repartitioning hypergraph model for dynamic load balancing",
    "abstract": "In parallel adaptive applications, the computational structure of the applications changes over time, leading to load imbalances even though the initial load distributions were balanced. To restore balance and to keep communication volume low in further iterations of the applications, dynamic load balancing (repartitioning) of the changed computational structure is required. Repartitioning differs from static load balancing (partitioning) due to the additional requirement of minimizing migration cost to move data from an existing partition to a new partition. In this paper, we present a novel repartitioning hypergraph model for dynamic load balancing that accounts for both communication volume in the application and migration cost to move data, in order to minimize the overall cost. The use of a hypergraph-based model allows us to accurately model communication costs rather than approximate them with graph-based models. We show that the new model can be realized using hypergraph partitioning with fixed vertices and describe our parallel multilevel implementation within the Zoltan load balancing toolkit. To the best of our knowledge, this is the first implementation for dynamic load balancing based on hypergraph partitioning. To demonstrate the effectiveness of our approach, we conducted experiments on a Linux cluster with 1024 processors. The results show that, in terms of reducing total cost, our new model compares favorably to the graph-based dynamic load balancing approaches, and multilevel approaches improve the repartitioning quality significantly.",
    "keywords": [
      "dynamic load balancing",
      "hypergraph partitioning",
      "parallel algorithms",
      "scientific computing",
      "distributed memory computers"
    ]
  },
  {
    "id": "636",
    "title": "assigning location information to display individuals on a map for web people search results",
    "abstract": "Distinguishing people with identical names is becoming more and more important in Web search. This research aims to display person icons on a map to help users select person clusters that are separated into different people from the result of person searches on the Web. We propose a method to assign person clusters with one piece of location information. Our method is comprised of two processes: (a) extracting location candidates from Web pages and (b) assigning location information using a local search engine. Our main idea exploits search engine rankings and character distance to obtain good location information among location candidates. Experimental results revealed the usefulness of our proposed method. We also show a developed prototype system.",
    "keywords": [
      "information extraction",
      "location information",
      "character distance",
      "web people search",
      "map interface"
    ]
  },
  {
    "id": "637",
    "title": "A declarative approach to procedural modeling of virtual worlds",
    "abstract": "With the ever increasing costs of manual content creation for virtual worlds, the potential of creating it automatically becomes too attractive to ignore. However, for most designers, traditional procedural content generation methods are complex and unintuitive to use, hard to control, and generated results are not easily integrated into a complete and consistent virtual world. We introduce a novel declarative modeling approach that enables designers to concentrate on stating what they want to create instead of on describing how they should model it. It aims at reducing the complexity of virtual world modeling by combining the strengths of semantics-based modeling with manual and procedural approaches. This article describes two of its main contributions to procedural modeling of virtual worlds: interactive procedural sketching and virtual world consistency maintenance. We discuss how these techniques, integrated in our modeling framework SketchaWorld, build up to enable designers to create a complete 3D virtual world in minutes. Procedural sketching provides a fast and more intuitive way to model virtual worlds, by letting designers interactively sketch their virtual world using high-level terrain features, which are then procedurally expanded using a variety of integrated procedural methods. Consistency maintenance guarantees that the semantics of all terrain features is preserved throughout the modeling process. In particular, it automatically solves conflicts possibly emerging from interactions between terrain features. We believe that these contributions together represent a significant step towards providing more user control and flexibility in procedural modeling of virtual worlds. It can therefore be expected that by further reducing its complexity, virtual world modeling will become accessible to an increasingly broad group of users.",
    "keywords": [
      "virtual worlds",
      "declarative modeling",
      "semantic modeling",
      "consistency maintenance",
      "procedural methods",
      "procedural sketching"
    ]
  },
  {
    "id": "638",
    "title": "Lagrangian finite element modelling of dam-fluid interaction: Accurate absorbing boundary conditions",
    "abstract": "The dynamic dam-fluid interaction is considered via a Lagrangian approach, based on a fluid finite element 'FE, model under the assumption of small displacement and inviscid fluid. The fluid domain is discretized by enhanced displacement-based finite elements, which can be considered an evolution of those derived from the pioneering works of Bathe and Hahn [Bathe KJ, Hahn WF. On transient analysis of fluid-structure system. Comp Struct 1979;10:383-93] and of Wilson and Khalvati [Wilson EL, Khalvati M. Finite element for the dynamic analysis of fluid-solid system. Int. J Numer Methods Eng 1983;19:1657-68]. The irrotational condition for inviscid fluids is imposed by the penalty method and consequentially leads to a type of micropolar media. The model is implemented using a FE code, and the numerical results of a rectangular bidimensional basin (subjected to horizontal sinusoidal acceleration) are compared with the analytical solution. It is demonstrated that the Lagrangian model is able to perform pressure and gravity wave propagation analysis, even if the gravity (or surface) waves are dispersive. The dispersion nature of surface waves indicates that the wave propagation velocity is dependent on the wave frequency. For the practical analysis of the coupled dam-fluid problem the analysed region of the basin must be reduced and the use of suitable asymptotic boundary conditions must be investigated. The classical Sommerfeld condition is implemented by means of a boundary layer of dampers and the analysis results are shown for the cases of sinusoidal forcing. The classical Sommerfeld condition is highly efficient for pressure-based FE modelling, but may not be considered fully adequate for the displacement-based FE approach. In the present paper a high-order boundary condition proposed by Higdom [Higdom RL. Radiation boundary condition for dispersive waves. SIAM J Numer Anal 1994;31:64-100] is considered. Its implementation requires the resolution of a multifreedom constraint problem, defined in terms of incremental displacements, in the ambit of dynamic time integration problems. The first- and second-order Higdon conditions are developed and implemented. The results are compared with the Sommerfeld condition results, and with the analytical unbounded problem results. Finally, a number of finite element results are presented and their related features are discussed and critically compared.  ",
    "keywords": [
      "absorbing boundary",
      "dam-fluid interaction",
      "lagrangian finite element",
      "dynamic analysis"
    ]
  },
  {
    "id": "639",
    "title": "Optimal Independent Spanning Trees on Odd Graphs",
    "abstract": "The use of multiple independent spanning trees (ISTs) for data broadcasting in networks provides a number of advantages, including the increase of fault-tolerance and bandwidth. The designs of multiple ISTs on several classes of networks have been widely investigated. In this paper we show a construction algorithm of ISTs on odd graphs, and we analyze that all the lengths of the paths in the ISTs are less than or equal to the length of the shortest path+4, which is optimal. We also prove that the heights of the ISTs we constructed are d+1, which again is optimal, since the fault diameter of an odd graph is d+1.",
    "keywords": [
      "optimal independent spanning trees",
      "odd graphs",
      "internally disjoint paths",
      "algorithms"
    ]
  },
  {
    "id": "640",
    "title": "Reliability Evaluation for Single Event Transients on Digital Circuits",
    "abstract": "The effect of single event transient (SET) on reliability has become a significant concern for digital circuits. This paper proposed an algorithm for evaluating the reliability for SET on digital circuits, based on signal probability, universal generating function technique, and generalized reliability block diagrams. The algorithm provides an expression for the reliability of SET under consideration for the effects of logic masking, error attenuation of gates, and crosstalk effects among interconnect wires. We perform simulations of ISCAS85 circuits. The results indicate that the proposed algorithm can effectively evaluate the reliability for SET on circuits. The error attenuation of gates can increase the reliability by more than 41.6%, and the masking and crosstalk effects will improve the reliability by more than 43%.",
    "keywords": [
      "crosstalk effects",
      "masking",
      "reliability evaluation",
      "single event transient"
    ]
  },
  {
    "id": "641",
    "title": "Identifying Shapes Using Self-assembly",
    "abstract": "In this paper, we introduce the following problem in the theory of algorithmic self-assembly: given an input shape as the seed of a tile-based self-assembly system, design a finite tile set that can, in some sense, uniquely identify whether or not the given input shape-drawn from a very general class of shapes-matches a particular target shape. We first study the complexity of correctly identifying squares. Then we investigate the complexity associated with the identification of a considerably more general class of non-square, hole-free shapes.",
    "keywords": [
      "algorithmic self-assembly",
      "kolmogorov complexity",
      "rnase assembly model",
      "shape identification problem"
    ]
  },
  {
    "id": "642",
    "title": "Advanced multimodal visualisation of clinical gait and fluoroscopy analyses in the assessment of total knee replacement",
    "abstract": "Traditional gait and fluoroscopy analysis of human movement are largely utilised but are still limited in registration, integration, synchronisation and visualisation capabilities. The present work exploits the features of a recently developed software tool based on multimodal display (Data Manager developed within the EU-funded project Multimod) in an exemplary clinical case. Standard lower limb gait analysis, comprising segment position, ground reaction force and EMG data collection, and three-dimensional fluoroscopy analysis at the replaced joint were performed in a total knee replacement patient while ascending stairs. Clinical information such as X-rays and standard scores were also available. Data Manager was able to import all this variety of data and to structure these in an original hierarchical tree. Bone and prosthesis component models were registered to corresponding marker position data for effective three-dimensional animations. These were also synchronised with corresponding standard video sequences. Animations, video, time-histories of collected and also processed data were shown in various combinations, according to specific interests of the bioengineering and medical professionals expected to observe and to interpret this large amount of data. This software tool demonstrated to be a valuable means to enhance representation and interpretation of measurements coming from human motion analysis. In a single software, a thorough and effective clinical and biomechanical analysis of human motion was performed.",
    "keywords": [
      "gait analysis",
      "stereophotogrammetry",
      "fluoroscopy",
      "total knee replacement",
      "graphical representation",
      "multimodal display",
      "synchronisation",
      "registration"
    ]
  },
  {
    "id": "643",
    "title": "A high-performance angular speed measurement method based on adaptive hysteresis switching techniques",
    "abstract": "A new adaptive angular speed (AS) estimate method minimizes AS errors. Hysteresis switching technology is applied to avoid speed fluctuations. Errors caused by the mechanical errors of optical encoder are removed. Merits of high accuracy, fast response, and less fluctuation are achieved. Two-layer hysteresis switches system is realized in DSC.",
    "keywords": [
      "hysteresis switch",
      "adaptive angular speed measurement",
      "digital signal controller ",
      "optical encoder"
    ]
  },
  {
    "id": "644",
    "title": "A multidimensional segmentation evaluation for medical image data",
    "abstract": "Evaluation of segmentation methods is a crucial aspect in image processing, especially in the medical imaging field, where small differences between segmented regions in the anatomy can be of paramount importance. Usually, segmentation evaluation is based on a measure that depends on the number of segmented voxels inside and outside of some reference regions that are called gold standards. Although some other measures have been also used, in this work we propose a set of new similarity measures, based on different features, such as the location and intensity values of the misclassified voxels, and the connectivity and the boundaries of the segmented data. Using the multidimensional information provided by these measures, we propose a new evaluation method whose results are visualized applying a Principal Component Analysis of the data, obtaining a simplified graphical method to compare different segmentation results. We have carried out an intensive study using several classic segmentation methods applied to a set of MRI simulated data of the brain with several noise and RF inhomogeneity levels, and also to real data, showing that the new measures proposed here and the results that we have obtained from the multidimensional evaluation, improve the robustness of the evaluation and provides better understanding about the difference between segmentation methods.",
    "keywords": [
      "segmentation evaluation",
      "principal component analysis",
      "multidimensional visualization",
      "image segmentation",
      "mri segmentation",
      "similarity measure"
    ]
  },
  {
    "id": "645",
    "title": "PROBABILISTIC QUERYING OVER UNCERTAIN DATA STREAMS",
    "abstract": "Inherent imprecision of data in many applications motivates us to support uncertainty as a first-class concept. Data stream and probabilistic data have been recently considered noticeably in isolation. However, there are many applications including sensor data management systems and object monitoring systems which need both issues in tandem. Our main contribution is designing a probabilistic data stream management system, called Sarcheshmeh,(a) for continuous querying over probabilistic data streams. Sarcheshmeh supports uncertainty from input data to final query results. In this paper, after reviewing requirements and applications of probabilistic data streams, we present our new data model for probabilistic data streams and define our main logical operators formally. Then, we present query language and physical operators. In addition, we introduce the architecture of Sarcheshmeh and also describe some major challenges like memory management and our floating precision mechanism toward designing a more robust system. Finally, we report evaluation of our system and the effect of floating precision on the tradeoff between accuracy and efficiency.",
    "keywords": [
      "probabilistic data stream",
      "probabilistic queries",
      "uncertain sensors"
    ]
  },
  {
    "id": "646",
    "title": "Numerical study of the 3D failure envelope of a single pile in sand",
    "abstract": "The paper presents a comprehensive study of the failure envelope (or capacity diagram) of a single elastic pile in sand. The behavior of a pile subjected to different load combinations is simulated using a large number of finite element numerical calculations. The sand is modeled using a constitutive law based on hypoplasticity. In order to find the failure envelope in the three-dimensional space (i.e. horizontal force H, bending moment M and vertical force V), the radial displacement method and swipe tests are numerically performed. It is found that with increasing vertical load the horizontal bearing capacity of the pile decreases. Furthermore, the presence of bending moment on the pile head significantly influences the horizontal bearing capacity and the capacity diagram in the HM plane manifests an inclined elliptical shape. An analytical equation providing good agreement with the 3D numerical results is finally proposed. The formula is useful for design purposes and the development of simplified modeling numerical strategies such as macro-element.",
    "keywords": [
      "3d failure envelope",
      "capacity diagram",
      "pile",
      "hypoplasticity",
      "radial displacement tests",
      "swipe tests",
      "analytical equation",
      "sand"
    ]
  },
  {
    "id": "647",
    "title": "Semantic linkages in research information systems as a new data source for scientometric studies",
    "abstract": "A growing number of research information systems use a semantic linkage technique to represent in explicit mode information about relationships between elements of its content. This practice is coming nowadays to a maturity when already existed data on semantically linked research objects and expressed by this scientific relationships can be recognized as a new data source for scientometric studies. Recent activities to provide scientists with tools for expressing in a form of semantic linkages their knowledge, hypotheses and opinions about relationships between available information objects also support this trend. The study presents one of such activities performed within the Socionet research information system with a special focus on (a) taxonomy of scientific relationships, which can exist between research objects, especially between research outputs; and (b) a semantic segment of a research e-infrastructure that includes a semantic interoperability support, a monitoring of changes in linkages and linked objects, notifications and a new model of scientific communication, and at lastscientometric indicators built by processing of semantic linkages data. Based on knowledge what is a semantic linkage data and how it is stored in a research information system we propose an abstract computing model of a new data source. This model helps with better understanding what new indicators can be designed for scientometric studies. Using current semantic linkages data collected in Socionet we present some statistical experiments, including examples of indicators based on two data sets: (a) what objects are linked and (b) what scientific relationships (semantics) are expressed by the linkages.",
    "keywords": [
      "research information system",
      "scientific information objects",
      "semantic linkages",
      "new data source",
      "scientometric studies",
      ""
    ]
  },
  {
    "id": "648",
    "title": "Flash memory efficient LTL model checking",
    "abstract": "As the capacity and speed of flash memories in form of solid state disks grow, they are becoming a practical alternative for standard magnetic drives. Currently, most solid-state disks are based on NAND technology and much faster than magnetic disks in random reads, while in random writes they are generally not. So far, large-scale LTL model checking algorithms have been designed to employ external memory optimized for magnetic disks. We propose algorithms optimized for flash memory access. In contrast to approaches relying on the delayed detection of duplicate states, in this work, we design and exploit appropriate hash functions to re-invent immediate duplicate detection. For flash memory efficient on-the-fly LTL model checking, which aims at finding any counter-example to the specified LTL property, we study hash functions adapted to the two-level hierarchy of RAM and flash memory. For flash memory efficient off-line LTL model checking, which aims at generating a minimal counterexample and scans the entire state space at least once, we analyze the effect of outsourcing a memory-based perfect hash function from RAM to flash memory. Since the characteristics of flash memories are different to magnetic hard disks, the existing I/O complexity model is no longer sufficient. Therefore, we provide an extended model for the computation of the I/O complexity adapted to flash memories that has a better fit to the observed behavior of our algorithms.  ",
    "keywords": [
      "model checking",
      "external memory algorithms",
      "algorithm engineering"
    ]
  },
  {
    "id": "649",
    "title": "Data fusion trees for detection: Does architecture matter",
    "abstract": "We consider the problem of decentralized detection in a network consisting of a large number of nodes arranged as a tree of bounded height, under the assumption of conditionally independent and identically distributed (i.i.d.) observations. We characterize the optimal error exponent under a Neyman-Pearson formulation. We show that the Type II error probability decays exponentially fast with the number of nodes, and the optimal error exponent is often the same as that corresponding to a parallel configuration. We provide sufficient, as well as necessary, conditions for this to happen. For those networks satisfying the sufficient conditions, we propose a simple strategy that nearly achieves the optimal error exponent, and in which all non-leaf nodes need only send 1-bit messages.",
    "keywords": [
      "decentralized detection",
      "error exponent",
      "sensor networks"
    ]
  },
  {
    "id": "650",
    "title": "conversion of generalization hierarchies and union types from extended entity-relationship model to an xml logical model",
    "abstract": "This short paper proposes alternative rules for converting generalization/specialization hierarchies and union types, defined in the Extended Entity-Relationship model, to an XML logical model. Our approach considers all the possible constraints and constructs for generalization and union types, generating abstract schemas for the logical design of XML documents.",
    "keywords": [
      "union types",
      "generalization/specialization",
      "eer",
      "xml schemas"
    ]
  },
  {
    "id": "651",
    "title": "A qualitative process system for modeling NF-?B and AP-1 gene regulation in immune cell biology research",
    "abstract": "An experiment-oriented integrated model of the regulation of the biologically ubiquitous NF-?B and AP-1 gene transcription promoters was built by extending a previously developed qualitative process system for simulating cell behavior in the immune system. The core knowledge base (KB) implemented a deep biological ontology including molecular, ultrastructural, cytological, histological, and organismic definitions. KB states, relationships, predicates, and heuristics also represented process interactions between reactive oxygen species, growth factors, and a variety of kinases phosphorylating intermediate molecules in the NF-?B and AP-1 regulatory signaling pathways. The system successfully simulated the molecular process steps underlying outcomes of eight different molecular genetics laboratory experiments, including those dealing with NF-?B and AP-1 regulation in immunodeficiency virus infection and tumor necrosis factor responses.",
    "keywords": [
      "qualitative modeling",
      "gene regulation",
      "immunology research"
    ]
  },
  {
    "id": "652",
    "title": "Universality in two-dimensional enhancement percolation",
    "abstract": "We consider a type of dependent percolation introduced in [2], where it is shown that certain \"enhancements\" of independent (Bernoulli) percolation, called essential, make the percolation critical probability strictly smaller. In this study we first prove that, for two-dimensional enhancements with a natural monotonicity property, being essential is also a necessary condition to shift the critical point. We then show that (some) critical exponents and the scaling limit of crossing probabilities of a two-dimensional percolation process are unchanged if the process is subjected to a monotonic enhancement that is not essential. This proves a form of universality for all dependent percolation models obtained via a monotonic enhancement (of Bernoulli percolation) that does not shift the critical point. For the case of site percolation on the triangular lattice, we also prove a stronger form of universality by showing that the full scaling limit [12, 13] is not affected by any monotonic enhancement that does not shift the critical point. ",
    "keywords": [
      "enhancement percolation",
      "scaling limit",
      "critical exponents",
      "universality"
    ]
  },
  {
    "id": "653",
    "title": "proactive information caching for efficient resource discovery in a self-structured grid",
    "abstract": "The cornerstone of successful deployment of large scale grid systems depends on efficient resource discovery mechanisms. In this respect, this paper presents a grid information system supported by a self-structured overlay topology and proactive information caching. The proposed approach features an ant-inspired self-organized overlay construction that maintains a bounded diameter overlay, and a selective flooding based discovery algorithm that exploit local caches to reduce the number of visited nodes. The caches are periodically exchanged between neighboring nodes using an epidemic replication mechanism that is based on a gossiping algorithm, thus allowing nodes to have a more general view of the network and its resources. We conducted extensive experimentation that provides evidence that the average number of hops required to efficiently locate resources is limited and that our framework performs well with respect to hit rate and network overhead.",
    "keywords": [
      "grid computing",
      "overlay networks",
      "collaborative ant algorithms",
      "resource discovery"
    ]
  },
  {
    "id": "654",
    "title": "robust id-based threshold signcryption scheme from pairings",
    "abstract": "Recently bilinear pairings on elliptic curves have raised great interest in cryptographic community. Based on their good properties, many excellent ID-based cryptographic schemes have been proposed. However, in these proposed schemes, the private key generator should be assumed trusted, while in real environment, this assumption does not always hold. To overcome this weakness, in this paper, we will use the threshold technology to devise a secure ID-based signcryption scheme. Since the threshold technology is adopted not only in the master key management but also in the group signature, our scheme can achieve high security and resist some malicious attacks under a certain threshold.",
    "keywords": [
      "bilinear pairings",
      "identity-based cryptography",
      "threshold scheme",
      "signcryption"
    ]
  },
  {
    "id": "655",
    "title": "Non-convex fuzzy data and fuzzy statistics: a first descriptive approach to data analysis",
    "abstract": "LR-fuzzy numbers are widely used in Fuzzy Set Theory applications based on the standard definition of convex fuzzy sets. However, in some empirical contexts such as, for example, human decision making and ratings, convex representations might not be capable to capture more complex structures in the data. Moreover, non-convexity seems to arise as a natural property in many applications based on fuzzy systems (e.g., fuzzy scales of measurement). In these contexts, the usage of standard fuzzy statistical techniques could be questionable. A possible way out consists in adopting ad-hoc data manipulation procedures to transform non-convex data into standard convex representations. However, these procedures can artificially mask relevant information carried out by the non-convexity property. To overcome this problem, in this article we introduce a novel computational definition of non-convex fuzzy number which extends the traditional definition of LR-fuzzy number. Moreover, we also present a new fuzzy regression model for crisp input/non-convex fuzzy output data based on the fuzzy least squares approach. In order to better highlight some important characteristics of the model, we applied the fuzzy regression model to some datasets characterized by convex as well as non-convex features. Finally, some critical points are outlined in the final section of the article together with suggestions about future extensions of this work.",
    "keywords": [
      "non-convex fuzzy data",
      "fuzzy linear regression",
      "fuzzy least squares approach",
      "fuzzy rating scales",
      "fuzzy measurement tools"
    ]
  },
  {
    "id": "656",
    "title": "Training a neural network to select dispatching rules in real time",
    "abstract": "Dispatching rules are often Suggested to schedule manufacturing systems in real-time. Numerous dispatching rules exist. Unfortunately no dispatching rule (DR) is known to be globally better than any other. Their efficiency depends on the characteristics of the system, operating condition parameters and the production objectives, Several authors have demonstrated the benefits of changing dynamically these rules, so as to take into account the changes that can occur in the system state, A new approach based on neural networks (NN) is proposed here to select in real time, each time a resource becomes available, the most suited DR. The selection is made in accordance with the current system state and the workshop Operating condition parameters. Contrarily to the few learning approaches presented in the literature to select scheduling heuristics, no training set is needed. The NN parameters are determined through simulation optimization. The benefits of the proposed approach are illustrated through the example of a simplified flow-shop already published. It is shown that the NN can automatically select efficient DRs dynamically: the knowledge is only generated from Simulation experiments, Which are driven by the optimization method. Once trained offline, the resulting NN can be used online, in connection with the monitoring system of a flexible manufacturing system.  ",
    "keywords": [
      "simulation",
      "optimization",
      "neural network",
      "dynamic scheduling",
      "learning",
      "flow-shop"
    ]
  },
  {
    "id": "657",
    "title": "The optimisation of block layout and aisle structure by a genetic algorithm",
    "abstract": "The design of a manufacturing layout is incomplete without consideration of aisle structure for material handling. This paper presents a method to solve the layout and aisle structure problems simultaneously by a slicing floorplan. In this representation, the slicing lines are utilised as the aisles for a material handling system. The method decomposes the problem into two stages. The first stage minimises the material handling cost with aisle distance, and the second stage optimises the aisles in the aisle structure. A representation of slicing floorplan is introduced for the optimisation by genetic algorithms (GAs). The corresponding operators of the GA are also developed. Computational tests demonstrate the goodness of the method. A comparison study of the GA and the random search (RS) for the problem was performed. It showed that the GA has a much higher efficiency than a RS, though further study is still needed to improve the efficiency of the GA.  ",
    "keywords": [
      "layout",
      "slicing floorplan",
      "material handling",
      "genetic algorithms"
    ]
  },
  {
    "id": "658",
    "title": "Constrained High Accuracy Stereo Reconstruction Method for Surgical Instruments Positioning",
    "abstract": "In this paper, a high accuracy stereo reconstruction method for surgery instruments positioning is proposed. Usually, the problem of surgical instruments reconstruction is considered as a basic task in computer vision to estimate the 3-D position of each marker on a surgery instrument from three pairs of image points. However, the existing methods considered the 3-D reconstruction of the points separately thus ignore the structure information. Meanwhile, the errors from light variation, imaging noise and quantization still affect the reconstruction accuracy. This paper proposes a method which takes the structure information of surgical instruments as constraints, and reconstructs the whole markers on one surgical instrument together. Firstly, we calibrate the instruments before navigation to get the structure parameters. The structure parameters consist of markers' number, distances between each markers and a linearity sign of each instrument. Then, the structure constraints are added to stereo reconstruction. Finally, weighted filter is used to reduce the jitter. Experiments conducted on surgery navigation system showed that our method not only improve accuracy effectively but also reduce the jitter of surgical instrument greatly.",
    "keywords": [
      "high accuracy reconstruction",
      "computer vision",
      "surgical instruments positioning"
    ]
  },
  {
    "id": "659",
    "title": "Plastic collapse of circular conical shells under uniform external pressure",
    "abstract": "The article reports on two theoretical investigations and an experimental investigation into the collapse of six circular conical shells under uniform external pressure. Four of the vessels collapsed through plastic non-symmetric bifurcation buckling and one vessel collapsed through plastic axisymmetric buckling. A sixth vessel failed in a mixed mode of plastic non-symmetric bifurcation buckling, combined with plastic axisymmetric buckling. The theoretical and experimental investigations appeared to indicate that there was a link between plastic non-symmetric bifurcation buckling and plastic axisymmetric buckling. The theoretical investigations were via the finite element method and were used to provide a design chart for these vessels.",
    "keywords": [
      "conical",
      "shells",
      "buckling",
      "lobar",
      "axisymmetric",
      "finite element",
      "plastic",
      "non-linear"
    ]
  },
  {
    "id": "660",
    "title": "meta-heuristics for reconstructing cross cut shredded text documents",
    "abstract": "In this work, we present two new approaches based on variable neighborhood search (VNS) and ant colony optimization (ACO) for the reconstruction of cross cut shredded text documents. For quickly obtaining initial solutions, we consider four different construction heuristics. While one of them is based on the well known algorithm of Prim, another one tries to match shreds according to the similarity of their borders. Two further construction heuristics rely on the fact that in most cases the left and right edges of paper documents are blank, i.e. no text is written on them. Randomized variants of these construction heuristics are applied within the ACO. Experimental tests reveal that regarding the solution quality the proposed ACO variants perform better than the VNS approaches in most cases, while the running times needed are shorter for VNS. The high potential of these approaches for reconstructing cross cut shredded text documents is underlined by the obtained results.",
    "keywords": [
      "ant colony optimization",
      "document reconstruction",
      "variable neighborhood search",
      "integer linear programming"
    ]
  },
  {
    "id": "661",
    "title": "The cybernetics of architecture: a tribute to the contribution of Gordon Pask",
    "abstract": "Discusses the relation between cybernetics and architecture and pays tribute to Gordon Pask's role and influence. Indicates Pask's contribution to an increasingly environmentally responsive architectural theory that may lead to a more humane and ecologically conscious environment.",
    "keywords": [
      "architecture",
      "cybernetics"
    ]
  },
  {
    "id": "662",
    "title": "Forecasting volatility based on wavelet support vector machine",
    "abstract": "One of the challenging problems in forecasting the conditional volatility of stock market returns is that general kernel functions in support vector machine (SVM) cannot capture the cluster feature of volatility accurately. While wavelet function yields features that describe of the volatility time series both at various locations and at varying time granularities, so this paper construct a multidimensional wavelet kernel function and prove it meeting the mercer condition to address this problem. The applicability and validity of wavelet support vector machine (WSVM) for volatility forecasting are confirmed through computer simulations and experiments on real-world stock data.",
    "keywords": [
      "volatility forecasting",
      "wavelet support vector machine ",
      "mercer condition"
    ]
  },
  {
    "id": "663",
    "title": "THE ROLE OF USER CAPABILITY AND INCENTIVES IN GROUP AND INDIVIDUAL DECISION-SUPPORT SYSTEMS - AN ECONOMICS PERSPECTIVE",
    "abstract": "We model the decision making processes in decision support systems and programs as sequential information acquisition processes and compare their usefulness. A Bayesian decision maker is shown to be indifferent between the two approaches. In contrast, a decision maker with bounded rationality prefers the decision support systems approach. The model is extended to group decision support systems where the interaction between the decision support systems approach. The model is extended to group decision support systems where the interaction between the decision makers and the group facilitator is modelled as a non-cooperative economics game. We show that in some instances the group facilitator would prefer precommitment to an interaction plan rather than allow evolutionary planning of the interaction. This planning is similar to that in a program and may take the form of an organization chart.",
    "keywords": [
      "decision support systems",
      "programs",
      "group dss",
      "bayesian",
      "bounded rationality",
      "economics model",
      "incentive conflict",
      "noncooperative game",
      "organization chart"
    ]
  },
  {
    "id": "664",
    "title": "using chi-scores to reward honest feedback from repeated interactions",
    "abstract": "Online communities increasingly rely on reputation information to foster cooperation and deter cheating. As rational agents can often benefit from misreporting their observations, explicit incentives must be created to reward honest feedback. Reputation side-payments (e.g., agents get paid for submitting feedback) can be designed to make truth-telling optimal. In this paper, we present a new side-payment scheme adapted for settings where agents repeatedly submit feedback. We rate the feedback set of an agent, rather than individual reports. The CHI-Score of the feedback set is computed based on a Chi-square independence test that assesses the correlation between the agent's feedback and the feedback submitted by the rest of the community. The mechanism has intuitive appeal and generates significantly lower costs than existing incentive-compatible reporting mechanisms.",
    "keywords": [
      "reputation mechanisms",
      "honest reporting"
    ]
  },
  {
    "id": "665",
    "title": "Embedding {0,1/2}-cuts in a branch-and-cut framework: A computational study",
    "abstract": "Embedding cuts into a branch-and-cut framework is a delicate task, especially when a large set of cuts is available. In this paper we describe a separation heuristic for {10, 1/2}-cuts, a special case of Chvatal-Gomory cuts, that tends to produce many violated inequalities within relatively short time. We report computational results on a large testbed of integer linear programming (ILP) instances of combinatorial problems including satisfiability, max-satisfiability, and linear ordering problems, showing that a careful cut-selection strategy produces a considerable speedup with respect to the cases in which either the separation heuristic is not used at all, or all of the cuts it produces are added to the LP relaxation.",
    "keywords": [
      "programming, integer, algorithms, cutting plane",
      "programming, integer, applications"
    ]
  },
  {
    "id": "666",
    "title": "2-ANTBAL: An ant colony optimisation algorithm for balancing two-sided assembly lines",
    "abstract": "Two-sided assembly lines are a special type of assembly lines in which workers perform assembly tasks in both sides of the line. This type of lines is of crucial importance, especially in the assembly of large-sized products, like automobiles, buses or trucks, in which some tasks must be performed at it specific side of the product. This paper presents ail approach to address the two-sided mixed-model assembly line balancing problem. First, a mathematical programming model is presented to formally describe the problem. Then, an ant colony optimisation algorithm is proposed to solve the problem. In the proposed procedure two ants 'work' simultaneously, one at each side of the line, to build a balancing solution which verifies the precedence, zoning, capacity, side and synchronism constraints of the assembly process. The main goal is to minimise the number of workstations of the line, but additional goals are also envisaged. The proposed procedure is illustrated with a numerical example and results of a computational experience that exhibit its superior performance are presented.  ",
    "keywords": [
      "assembly line balancing",
      "ant colony optimisation",
      "two-sided assembly lines",
      "mixed-model assembly lines"
    ]
  },
  {
    "id": "667",
    "title": "Evaluation of ICL670, a Once-Daily Oral Iron Chelator in a Phase III Clinical Trial of ?-Thalassemia Patients with Transfusional Iron Overload",
    "abstract": "Abstract: Osteoporosis and osteopenia are frequent complications of thalassemia major (TM) and intermedia (TI). Osteoporosis was found in 23/25 patients with TI and in 115/239 patients with TM. In TM, no association was found with specific polymorphisms in candidate genes (vitamin D receptor, estrogen receptor, calcitonin receptor, and collagen type 1 alpha 1). Osteoporosis in TM female was strongly associated with primary amenorrhea (P < .0001), while in male patients with TM hypogonadism was not significantly related to BMD (P= .0001). Low BMD was also associated with cardiomiopathy (P= .01), diabetes mellitus (P= .0001), chronic hepatitis (P= .0029), and increased ALT (P= .01).",
    "keywords": [
      "deferasirox ",
      "?-thalassemia",
      "iron chelator",
      "pediatrics",
      "iron overload"
    ]
  },
  {
    "id": "668",
    "title": "Maximum local energy: An effective approach for multisensor image fusion in beyond wavelet transform domain",
    "abstract": "The benefits of multisensor fusion have motivated research in this area in recent years. Redundant fusion methods are used to enhance fusion system capability and reliability. The benefits of beyond wavelets have also prompted scholars to conduct research in this field. In this paper, we propose the maximum local energy method to calculate the low-frequency coefficients of images and compare the results with those of different beyond wavelets. An image fusion step was performed as follows: first, we obtained the coefficients of two different types of images through beyond wavelet transform. Second, we selected the low-frequency coefficients by maximum local energy and obtaining the high-frequency coefficients using the sum modified Laplacian method. Finally, the fused image was obtained by performing an inverse beyond wavelet transform. In addition to human vision analysis, the images were also compared through quantitative analysis. Three types of images (multifocus, multimodal medical, and remote sensing images) were used in the experiments to compare the results among the beyond wavelets. The numerical experiments reveal that maximum local energy is a new strategy for attaining image fusion with satisfactory performance.",
    "keywords": [
      "image fusion",
      "beyond wavelet transform",
      "maximum local energy ",
      "sum modified laplacian "
    ]
  },
  {
    "id": "669",
    "title": "hla federate migration",
    "abstract": "The High Level Architecture (HLA) is a standardized framework for distributed simulation that promotes reuse and interoperability of simulation components (federates). Federates are processes which communicate with each other in the simulation via the Run Time Infrastructure (RTI). When running a large scale simulation over many nodes/workstations, some may get more workload than others. To run the simulation as efficiently as possible, the workload should be uniformly distributed over the nodes. Current RTI implementations are very static, and do not allow any load balancing. Load balancing of a HLA federation can be achieved by scheduling new federates on the node with least load and migrating executing federates from a highly loaded node to a lightly loaded node. Process migration has been a topic of research for many years, but not within the context of HLA. This paper focuses on process migration within the HLA framework.",
    "keywords": [
      "high level architecture",
      "load balancing",
      "federate migration",
      "distributed simulation"
    ]
  },
  {
    "id": "670",
    "title": "Online activity, motivation, and reasoning among adult learners",
    "abstract": "College students motivational beliefs influence their online behavior and ability to think critically. In the present study, doctoral health science students reports of motivation, as measured by the California Measure of Mental Motivation, reasoning skill, as measured by the Health Science Reasoning Test, and Web-CT records of online activity during a Web-CT-based statistics course were explored. Critical thinking skill and disposition each contributed unique variance to student grades, with age, organization disposition, and analysis skill as the strongest predictors. The youngest students, those so-called millennial age, and born after 1982, were those with the lowest critical thinking skill and dispositions, and the lowest grades in the class. Future research must take into consideration discrepancies between skill and disposition and interactions with age or cohort. At present, and contrary to popular wisdom, older students may make better online learners than younger.",
    "keywords": [
      "critical thinking dispositions",
      "critical thinking skills",
      "health science students",
      "online communication"
    ]
  },
  {
    "id": "671",
    "title": "Automatic simulation and verification of pipelined microcontrollers",
    "abstract": "This paper presents a methodology for automatic simulation and verification of pipelined microcontrollers. Using this methodology, we can generate the simulation for the instruction set architecture (ISA), abstract finite slate machine (FSM) and pipelined register transfer level design and compare the simulation results across different levels quickly. We have implemented our method in the simulation and verification of a synthesized microcontroller HT_4 using our behavioral synthesis tool.",
    "keywords": [
      "functional verification",
      "simulation",
      "high level synthesis",
      "microcontrollers",
      "microprocessors"
    ]
  },
  {
    "id": "672",
    "title": "Constraint satisfaction for planning and scheduling problems",
    "abstract": "The areas of planning and scheduling (from the Artificial Intelligence point of view) have seen important advances thanks to application of constraint satisfaction techniques. Currently, many important real-world problems require efficient constraint handling for planning, scheduling and resource allocation to competing goal activities over time in the presence of complex state-dependent constraints. Solutions to these problems require integration of resource allocation and plan synthesis capabilities. Hence to manage such complex problems planning, scheduling and constraint satisfaction must be interrelated. This special issue on Constraint Satisfaction for Planning and Scheduling Problems compiles a selection of papers dealing with various aspects of applying constraint satisfaction techniques in planning and scheduling. The core of submitted papers was formed by the extended versions of papers presented at COPLAS'2009: ICAPS 2009 Workshop on Constraint Satisfaction Techniques for Planning and Scheduling Problems. This issue presents novel advances on planning, scheduling, constraint programming/constraint satisfaction problems (CSPs) and many other common areas that exist among them. On the whole, this issue mainly focus on managing complex problems where planning, scheduling, constraint satisfaction and search must be combined and/or interrelated, which entails an enormous potential for practical applications and future research.",
    "keywords": [
      "planning",
      "scheduling",
      "constraint programming",
      "search"
    ]
  },
  {
    "id": "673",
    "title": "Induction of M-convex functions by linking systems",
    "abstract": "Induction (or transformation) by bipartite graphs is one of the most important operations on matroids, and it is well known that the induction of a matroid by a bipartite graph is again a matroid. As an abstract form of this fact, the induction of a matroid by a linking system is known to be a matroid. M-convex functions are quantitative extensions of matroidal structures, and they are known as discrete convex functions. As with matroids, it is known that the induction of an M-convex function by networks generates an M-convex function. As an abstract form of this fact, this paper shows that the induction of an M-convex function by linking systems generates an M-convex function. Furthermore, we show that this result also holds for M-convex functions on constant-parity jump systems. Previously known operations such as aggregation, splitting, and induction by networks can be understood as special cases of this construction.",
    "keywords": [
      "m-convex function",
      "jump system",
      "linking system"
    ]
  },
  {
    "id": "674",
    "title": "Some scheduling problems with sum-of-processing-times-based and job-position-based learning effects",
    "abstract": "In this paper we introduce a new scheduling model with learning effects in which the actual processing time of a job is a function of the total normal processing times of the jobs already processed and of the jobs scheduled position. We show that the single-machine problems to minimize makespan and total completion time are polynomially solvable. In addition, we show that the problems to minimize total weighted completion time and maximum lateness are polynomially solvable under certain agreeable conditions. Finally, we present polynomial-time optimal solutions for some special cases of the m-machine flowshop problems to minimize makespan and total completion time.",
    "keywords": [
      "scheduling",
      "learning effect",
      "single-machine",
      "flowshop"
    ]
  },
  {
    "id": "675",
    "title": "On the Expressiveness of Single-Pass Instruction Sequences",
    "abstract": "We perceive programs as single-pass instruction sequences. A single-pass instruction sequence under execution is considered to produce a behaviour to be controlled by some execution environment. Threads as considered in basic thread algebra model such behaviours. We show that all regular threads, i.e. threads that can only be in a finite number of states, can be produced by single-pass instruction sequences without jump instructions if use can be made of Boolean registers. We also show that, in the case where goto instructions are used instead of jump instructions, a bound to the number of labels restricts the expressiveness.",
    "keywords": [
      "single-pass instruction sequence",
      "regular thread",
      "expressiveness",
      "jump-free instruction sequence"
    ]
  },
  {
    "id": "676",
    "title": "Adaptive two-stage QoS provisioning schemes for CDMA networks",
    "abstract": "in this paper, adaptive QoS provisioning schemes are proposed for CDMA wireless networks. The proposed schemes consist of two stages. in the first stage, a call admission control (CAC) and a bandwidth allocation schemes are proposed to determine the bandwidth assignment for new connections according to available bandwidth and adaptable range of active connections. The proposed methods utilize the Markov chain model to estimate the required bandwidth for all connections. This Markov chain model considers the traffic characteristics of both the incoming call request and the existing connections, and estimates the resulting quality of service (QoS). In the second stage, Net-CBK and Net-Share schemes are presented to regulate the active connections according to call blocking rates and unused bandwidth. From the simulation results, the proposed schemes are able to carry more connections and improve system utilization.",
    "keywords": [
      "code division multiple access",
      "quality of service",
      "variable bit rate",
      "call admission control",
      "packet lost rate"
    ]
  },
  {
    "id": "677",
    "title": "A fine motor skill training system using multi-fingered haptic interface robot",
    "abstract": "We proposed optimal switching method for transferring multiple fingertip forces. Based on the proposed method, a haptic training system was developed. The proposed method enhanced the training effect compared with our earlier method.",
    "keywords": [
      "virtual reality",
      "humancomputer interaction",
      "haptic interface",
      "skill transfer",
      "fine motor skill in fingers"
    ]
  },
  {
    "id": "678",
    "title": "Visualization of Bubble-Fluid Interaction by a Moving Object Flow Image Analyzer System",
    "abstract": "This paper deals with interaction between a bubble and fluid around it, visualized by a moving object flow image analyzer (MOFIA) consisting of a three-dimensional (3D) moving object image analyzer (MOIA) and two-dimensional particle image velocimetry (PIV). The experiments were carried out for rising bubbles of various sizes and shapes in stagnant water in a vertical pipe. In the MOFIA employed, 3D-MOIA was used to measure bubble motion and PIV to measure fluid flow. The 3D position and shape of a bubble and the velocity field were measured simultaneously. The experimental results showed that the interaction was characterized by the shape, size and density of a bubble. Concretely, they showed the characteristics of bubble motion, wake shedding, and flow field.",
    "keywords": [
      "bubble motion",
      "trajectory",
      "flow field",
      "piv",
      "3d-moia",
      "mofia"
    ]
  },
  {
    "id": "679",
    "title": "Fibonacci (p p , r r )-cubes which are median graphs",
    "abstract": "The Fibonacci (p p , r r )-cube is an interconnection topology, which unifies a wide range of connection topologies, such as hypercube, Fibonacci cube, postal network, etc. It is known that the Fibonacci cubes are median graphs [S. Klavar, On median nature and enumerative properties of Fibonacci-like cubes, Discrete Math. 299 (2005) 145153]. The question for determining which Fibonacci (p p , r r )-cubes are median graphs is solved completely in this paper. We show that Fibonacci (p p , r r )-cubes are median graphs if and only if either r?p r ? p and r?2 r ? 2 , or p=1 p = 1 and r=n r = n .",
    "keywords": [
      "hypercube",
      "fibonacci -cube",
      "median graph"
    ]
  },
  {
    "id": "680",
    "title": "Fast isogeometric boundary element method based on independent field approximation",
    "abstract": "We present an alternative isogeometric BEM for elasticity on NURBS patches. Boundary data and geometry are approximated independently which avoids redundancies. Hierarchical matrices provide almost linear computational complexity. Comparison of different Ansatz functions on NURBS patches. The results show optimal convergence for all tested orders.",
    "keywords": [
      "subparametric formulation",
      "isogeometric analysis",
      "hierarchical matrices",
      "elasticity",
      "nurbs",
      "convergence"
    ]
  },
  {
    "id": "681",
    "title": "Expectations of Random Sets and Their Boundaries Using Oriented Distance Functions",
    "abstract": "Shape estimation and object reconstruction are common problems in image analysis. Mathematically, viewing objects in the image plane as random sets reduces the problem of shape estimation to inference about sets. Currently existing definitions of the expected set rely on different criteria to construct the expectation. This paper introduces new definitions of the expected set and the expected boundary, based on oriented distance functions. The proposed expectations have a number of attractive properties, including inclusion relations, convexity preservation and equivariance with respect to rigid motions. The paper introduces a special class of decomposable oriented distance functions for parametric sets and gives the definition and properties of decomposable random closed sets. Further, the definitions of the empirical mean set and the empirical mean boundary are proposed and empirical evidence of the consistency of the boundary estimator is presented. In addition, the paper discusses loss functions for set inference in frequentist framework and shows how some of the existing expectations arise naturally as optimal estimators. The proposed definitions are illustrated on theoretical examples and real data.",
    "keywords": [
      "random closed sets",
      "expected set",
      "oriented distance function",
      "set inference",
      "loss functions",
      "boundary estimator",
      "boundary reconstruction"
    ]
  },
  {
    "id": "682",
    "title": "A numerical comparison of several unconditional exact tests in problems of equivalence based on the difference of proportions",
    "abstract": "In order to compare two independent proportions (p(1) and p(2)) there are several useful tests for the parameter d = p(2) - p(1) : H(SG) : d   delta, H(SG2) : d = delta vs. K(SG2) : d not equal delta (where -1   Delta (where Delta >= 0) and H(PE) : vertical bar d vertical bar >= Delta vs. K(PE) : vertical bar d vertical bar   0). The exact unconditional test requires an ordering statistic, which is usually the z-pooled statistic, to be defined. The paper gives the definition of 10 new ordering statistics with a similar computational time, and compares the number of points which each introduces into the critical region obtained to error alpha = 5%. The article reaches the conclusion that the most generally powerful statistics are: the z-pooled one with a small continuity correction (c = 1/N if n(1) not equal n(2) or c = 2/N if n(1) = n(2), where N = {n(1)+1} {n(2) +1} and n(i) are the sample sizes) and those z-pooled with Yates' continuity correction (c = {n(1) +n(2)}/{2n(1)n(2)}). In this paper the author also showed that Barnard's two classic convexity conditions are redundant, because when one of them is verified the other is also verified. The programs for these tests may be obtained free of charge from the site http://www.ugr.es/local/bioest/software.htm.",
    "keywords": [
      "difference of proportions",
      "equivalence of proportions",
      "exact confidence intervals",
      "exact tests",
      "non-inferiority",
      "power",
      "unconditional tests",
      "2 x 2 tables"
    ]
  },
  {
    "id": "683",
    "title": "MINIMUM ENERGY CONTROL OF POSITIVE CONTINUOUS-TIME LINEAR SYSTEMS WITH BOUNDED INPUTS",
    "abstract": "The minimum energy control problem for positive continuous-time linear systems with bounded inputs is formulated and solved. Sufficient conditions for the existence of a solution to the problem are established. A procedure for solving the problem is proposed and illustrated with a numerical example.",
    "keywords": [
      "positive system",
      "continuous time",
      "minimum energy control",
      "bounded inputs"
    ]
  },
  {
    "id": "684",
    "title": "Relevance-Zone-Oriented Proof Search for Connect6",
    "abstract": "Wu and Huang (Advances in Computer Games, pp. 180-194, 2006) presented a new family of k-in-a-row games, among which Connect6 (a kind of six-in-a-row) attracted much attention. For Connect6 as well as the family of k-in-a-row games, this paper proposes a new threat-based proof search method, named relevance-zone-oriented proof (RZOP) search, developed from the lambda search proposed by Thomsen (Int. Comput. Games Assoc. J., vol. 23, no. 4, pp. 203-217, 2000). The proposed RZOP search is a novel, general, and elegant method of constructing and promoting relevance zones. Using this method together with a proof number search, this paper solved effectively and successfully many new Connect6 game positions, including several Connect6 openings, especially the Mickey Mouse opening, which used to be one of the popular openings before we solved it.",
    "keywords": [
      "board games",
      "connect6",
      "k-in-a-row games",
      "lambda search",
      "threat-based proof search",
      "threat-space search"
    ]
  },
  {
    "id": "685",
    "title": "Interaction of TCP flows as billiards",
    "abstract": "The aim of this paper is to analyze the performance of a large number of long-lived TCP controlled flows sharing many routers (or links), from the knowledge of the network parameters (capacity, buffer size, topology) and of the characteristics of each TCP flow (RTT, route etc.) when taking synchronization into account. It is shown that in the small buffer case, the dynamics of such a network can be described in terms of iterate of random piecewise affine maps, or geometrically as a billiards in the Euclidean space with as many dimensions as the number of flow classes and as many reflection facets as there are routers. This class of billiards exhibits both periodic and nonperiodic asymptotic oscillations, the characteristics of which are extremely sensitive to the parameters of the network. It is also shown that for large populations and in the presence of synchronization, aggregated throughputs exhibit fluctuations that are due to the network as a whole, that follow some complex fractal patterns, and that come on top of other and more classical flow or packet level fluctuations. The consequences on TCP's fairness are exemplified on a few typical cases of small dimension.",
    "keywords": [
      "bandwidth sharing",
      "dynamical system",
      "flow level modeling",
      "ip traffic",
      "product of random matrices",
      "tcp fairness"
    ]
  },
  {
    "id": "686",
    "title": "Feature selection, mutual information, and the classification of high-dimensional patterns",
    "abstract": "We propose a novel feature selection filter for supervised learning, which relies on the efficient estimation of the mutual information between a high-dimensional set of features and the classes. We bypass the estimation of the probability density function with the aid of the entropic-graphs approximation of Rnyi entropy, and the subsequent approximation of the Shannon entropy. Thus, the complexity does not depend on the number of dimensions but on the number of patterns/samples, and the curse of dimensionality is circumvented. We show that it is then possible to outperform algorithms which individually rank features, as well as a greedy algorithm based on the maximal relevance and minimal redundancy criterion. We successfully test our method both in the contexts of image classification and microarray data classification. For most of the tested data sets, we obtain better classification results than those reported in the literature.",
    "keywords": [
      "filter feature selection",
      "mutual information",
      "entropic spanning graphs",
      "microarray"
    ]
  },
  {
    "id": "687",
    "title": "Analyzing the techniques that improve fault tolerance of aggregation trees in sensor networks",
    "abstract": "Sensor networks are finding significant applications in large scale distributed systems. One of the basic operations in sensor networks is in-network aggregation. Among the various approaches to in-network aggregation, such as gossip and tree, including the hash-based techniques, the tree-based approaches have better performance and energy-saving characteristics. However, sensor networks are highly prone to failures. Numerous techniques suggested in the literature to counteract the effect Of failures have not been carefully analyzed. In this paper, we focus on the performance of these tree-based aggregation techniques in the presence of failures. First, we identify a fault model that Captures the important failure traits of the system. Then, we analyze the correctness of simple tree aggregation with Our fault model. We then use the same fault model to analyze the techniques that utilize redundant trees to improve the variance. The impact of techniques for maintaining the correctness under faults, Such as rebuilding or locally fixing the tree, is then studied under the same fault model. We also do the cost-benefit analysis Of using the hash-based schemes which are based on FM sketches, We conclude that these fault tolerance techniques for tree aggregation do not necessarily result in substantial improvement in fault tolerance. ",
    "keywords": [
      "fault tolerance",
      "reliability",
      "sensor network",
      "aggregation",
      "modeling faults"
    ]
  },
  {
    "id": "688",
    "title": "Lean government and platform-based governanceDoing more with less",
    "abstract": "Governments from all over the world are looking for ways to reduce costs while at the same time to stimulate innovation. While pursuing both objectives, governments face a major challengeto operate in a connected environment, engage stakeholders and solve societal problems by utilizing new methods, tools, practices and governance models. As result, fundamental changes are taking place on how government operates. Such changes are under the larger umbrella of lean government (l-Government). Lean government is a new wave which is appearing as a response to traditional approacheslike electronic government (e-Government) and transformational government (t-Government), and aims at reducing the complexity of the public sector by simplifying and streamlining organizational structures and processes, at the same time at stimulating innovation by mobilizing stakeholders. In l-Government, public organizations introduce platforms facilitating innovation and interactions with other public organizations, business and citizens, and focus on their orchestration role. Experimentation, assessment and gradual improvement based on user requirements are key factors for realizing l-Government.",
    "keywords": [
      "e-government",
      "open government",
      "public sector reform",
      "platform",
      "government as a platform",
      "infrastructure",
      "orchestration"
    ]
  },
  {
    "id": "689",
    "title": "Resolving IP Aliases in Building Traceroute-Based Internet Maps",
    "abstract": "Alias resolution, the task of identifying IP addresses belonging to the same router, is an important step in building traceroute-based Internet topology maps. Inaccuracies in alias resolution affect the representativeness of constructed topology maps. This in turn affects the conclusions derived from studies that use these maps. This paper presents two complementary studies on alias resolution. First, we present an experimental study to demonstrate the impact of alias resolution on topology measurement studies. Then, we introduce an alias resolution approach called analytic and probe-based alias resolver (APAR). APAR consists of an analytical component and a probe-based component. Given a set of path traces, the analytical component utilizes the common IP address assignment scheme to infer IP aliases. The probe-based component introduces a minimal probing overhead to improve the accuracy of APAR. Compared to the existing state-of-the-art tool ally, APAR uses an orthogonal approach to resolve a large number of IP aliases that ally fails to identify. Our extensive verification study on sample data sets shows that our approach is effective in resolving many aliases with good accuracy. Our evaluations also indicate that the two approaches (ally and APAR) should be used together to maximize the success of the alias resolution process.",
    "keywords": [
      "alias resolution",
      "internet topology",
      "network measurement"
    ]
  },
  {
    "id": "690",
    "title": "beta P: A novel approach to filter out malicious rating profiles from recommender systems",
    "abstract": "Recommender systems are widely deployed to provide user purchasing suggestion on eCommerce websites. The technology that has been adopted by most recommender systems is collaborative filtering. However, with the open nature of collaborative filtering recommender systems, they suffer significant vulnerabilities from being attacked by malicious raters, who inject profiles consisting of biased ratings. In recent years, several attack detection algorithms have been proposed to handle the issue. Unfortunately, their applications are restricted by various constraints. PCA-based methods while having good performance on paper, still suffer from missing values that plague most user-item matrixes. Classification-based methods require balanced numbers of attacks and normal profiles to train the classifiers. The detector based on SPC (Statistical Process Control) assumes that the rating probability distribution for each item is known in advance. In this research, Beta-Protection ( beta P) is proposed to alleviate the problem without the abovementioned constraints. beta P grounds its theoretical foundation on Beta distribution for easy computation and has stable performance when experimenting with data derived from the public websites of MovieLens.  ",
    "keywords": [
      "shilling attacks detection",
      "collaborative filtering",
      "recommender systems"
    ]
  },
  {
    "id": "691",
    "title": "On the lifetime of node-to-node communication in wireless ad hoc networks",
    "abstract": "Lifetime of node-to-node communication in a wireless ad hoc network is defined as the duration that two nodes can communicate with each other. Failure of the two nodes or failure of the last available route between them ends their communication. In this paper, we analyze the maximum lifetime of node-to-node communication in static ad hoc networks when alternative routes that keep the two nodes connected to each other are node-disjoint. We target ad hoc networks with random topology modeled as a random geometric graph. The analysis is provided for (1) networks that support automatic repeat request (ARQ) at the medium access control level and (2) networks that do not support ARQ. On the basis of this analysis, we propose numerical algorithms to predict at each moment of network operation, the maximum duration that two nodes can still communicate with each other. Then, we derive a closed-form expression for the expected value of maximum node-to-node communication lifetime in the network. As a byproduct of our analysis, we also derive upper and lower bounds on the lifetime of node-disjoint routes in static ad hoc networks. We verify the accuracy of our analysis using extensive simulation studies.  ",
    "keywords": [
      "node-to-node communication lifetime",
      "node-disjoint routes",
      "route lifetime",
      "network connectivity",
      "ad hoc networks"
    ]
  },
  {
    "id": "692",
    "title": "Human factors in the adoption and performance of advanced manufacturing technology in unionized firms",
    "abstract": "Some researchers have found that unionized firms are less likely to pursue automation because high wage demands deprive them of the necessary capital required to invest in advanced manufacturing technology (AMT). it has also been suggested that stringent work rules and technology agreements can make the substitution of new technology for union labor too expensive. Others have found, however, that the pursuit of high wage policies and the resultant requirement for improved worker and machine productivity can create a positive environment for technological change. This exploratory study examines the relationships between firm-level union status and the adoption and performance of AMT in the discrete parts durable-goads manufacturing industry. Analyses of our sample, which included Chi-square tests, t-tests, correlation analyses and multiple linear regression analyses. revealed a union effect an the adoption of just-in-time technology and a moderately positive union effect on performance. Results of analyses of the impact of union status, firm sire and several human factor variables on firm performance are also presented and discussed.",
    "keywords": [
      "industrial relations",
      "trade unions",
      "advanced manufacturing technologies",
      "implementation"
    ]
  },
  {
    "id": "693",
    "title": "Dividing protein interaction networks for modular network comparative analysis",
    "abstract": "The increasing growth of data on protein-protein interaction (PPI) networks has boosted research on their comparative analysis. In particular, recent studies proposed models and algorithms for performing network alignment, that is, the comparison of networks across species for discovering conserved functional complexes. In this paper, we present an algorithm for dividing PPI networks, prior to their alignment, into small sub-graphs that are likely to cover conserved complexes. This allows one to perform network alignment in a modular fashion, by acting on pairs of resulting small sub-graphs from different species. The proposed dividing algorithm combines a graph-theoretical property (articulation) with a biological one (orthology). Extensive experiments on various PPI networks are conducted in order to assess how well the sub-graphs generated by this dividing algorithm cover protein functional complexes and whether the proposed pre-processing step can be used for enhancing the performance of network alignment algorithms. Source code of the dividing algorithm is available upon request for academic use.  ",
    "keywords": [
      "protein interaction network division",
      "modular network alignment",
      "conserved protein complexes"
    ]
  },
  {
    "id": "694",
    "title": "Clustering with the multivariate normal inverse Gaussian distribution",
    "abstract": "Many model-based clustering methods are based on a finite Gaussian mixture model. The Gaussian mixture model implies that the data scatter within each group is elliptically shaped. Hence non-elliptical groups are often modeled by more than one component, resulting in model over-fitting. An alternative is to use a meanvariance mixture of multivariate normal distributions with an inverse Gaussian mixing distribution (MNIG) in place of the Gaussian distribution, to yield a more flexible family of distributions. Under this model the component distributions may be skewed and have fatter tails than the Gaussian distribution. The MNIG based approach is extended to include a broad range of eigendecomposed covariance structures. Furthermore, MNIG models where the other distributional parameters are constrained is considered. The Bayesian Information Criterion is used to identify the optimal model and number of mixture components. The method is demonstrated on three sample data sets and a novel variation on the univariate KolmogorovSmirnov test is used to assess goodness of fit.",
    "keywords": [
      "model-based clustering",
      "multivariate normal inverse gaussian distribution",
      "mclust",
      "information metrics",
      "kolmogorovsmirnov goodness of fit"
    ]
  },
  {
    "id": "695",
    "title": "Linear multiuser receivers: Effective interference, effective bandwidth and user capacity",
    "abstract": "Multiuser receivers improve the performance of spread-spectrum and antenna-array systems by exploiting the structure of the multiaccess interference when demodulating the signal of a user. Much of the previous work on the performance analysis of multiuser receivers has focused on their ability to reject worst case interference. Their performance in a power-controlled network and the resulting user capacity are less well-understood. In this paper, me show that in a large system with each user using random spreading sequences, the limiting interference effects under several linear multiuser receivers can be decoupled, such that each interferer can be ascribed a level of effective interference that it provides to the user to be demodulated, Applying these results to the uplink of a single power-controlled cell, we derive an effective bandwidth characterization of the user capacity: the signal-to-interference requirements of all the users can be met if and only if the sum of the effective bandwidths of the users is less than the total number of degrees of freedom in the system. The effective bandwidth of a user depends only on its own SIR requirement, and simple expressions are derived for three linear receivers: the conventional matched filter, the decorrelator, and the MMSE receiver. The effective bandwidths under the three receivers serve as a basis for performance comparison.",
    "keywords": [
      "decorrelator",
      "effective bandwidth",
      "effective interference",
      "mmse receiver",
      "multiuser detection",
      "power control",
      "user capacity",
      "random spreading sequences"
    ]
  },
  {
    "id": "696",
    "title": "The multiple originator broadcasting problem in graphs",
    "abstract": "Given a graph G and a vertex subset S of V(G) V ( G ) , the broadcasting time with respect to S , denoted by b(G,S) b ( G , S ) , is the minimum broadcasting time when using S as the broadcasting set. And the k-broadcasting number , denoted by bk(G) b k ( G ) , is defined by bk(G)=min{b(G,S)|S?V(G),|S|=k} b k ( G ) = min { b ( G , S ) | S ? V ( G ) , | S | = k } . Given a graph G and two vertex subsets S , S? S ? of V(G) V ( G ) , define d ( v , S ) = min u ? S d ( v , u ) , d(S,S?)=min{d(u,v)|u?S d ( S , S ? ) = min { d ( u , v ) | u ? S , v?S?} v ? S ? } , and d ( G , S ) = max v ? V ( G ) d ( v , S ) for all v?V(G) v ? V ( G ) . For all k , 1?k?|V(G)| 1 ? k ? | V ( G ) | , the k-radius  of G , denoted by rk(G) r k ( G ) , is defined as rk(G)=min{d(G,S)|S?V(G) r k ( G ) = min { d ( G , S ) | S ? V ( G ) , |S|=k} | S | = k } . In this paper, we study the relation between the k-radius and the k-broadcasting numbers of graphs. We also give the 2 -radius and the 2 -broadcasting numbers of the grid graphs, and the k-broadcasting numbers of the complete n-partite graphs and the hypercubes.",
    "keywords": [
      "k k-radius",
      "k k-broadcasting number",
      "cartesian product",
      "path",
      "hypercube"
    ]
  },
  {
    "id": "697",
    "title": "A concurrent rule scheduling algorithm for active rules",
    "abstract": "The use of rules in a distributed environment creates new challenges for the development of active rule execution models. In particular, since a single event can trigger multiple rules that execute over distributed sources of data, it is important to make use of concurrent rule execution whenever possible. This paper presents the details of the integration rule scheduling (IRS) algorithm. Integration rules are active database rules that are used for component integration in a distributed environment. The IRS algorithm identifies rule conflicts for multiple rules triggered by the same event through static, compile-time analysis of the read and write sets of each rule. A unique aspect of the algorithm is that the conflict analysis includes the effects of nested rule execution that occurs as a result of using an execution model with an immediate coupling mode. The algorithm therefore identifies conflicts that may occur as a result of the concurrent execution of different rule triggering sequences. The rules are then formed into a priority graph before execution, defining the order in which rules triggered by the same event should be processed. Rules with the same priority can be executed concurrently. The IRS algorithm guarantees confluence in the final state of the rule execution. The IRS algorithm is applicable for rule scheduling in both distributed and centralized rule execution environments.",
    "keywords": [
      "active rules",
      "concurrent rule execution",
      "rule scheduling algorithm",
      "confluence analysis"
    ]
  },
  {
    "id": "698",
    "title": "Identification of human implicit visual search intention based on eye movement and pupillary analysis",
    "abstract": "We propose a novel approach for the identification of human implicit visual search intention based on eye movement patterns and pupillary analysis, in general, as well as pupil size, gradient of pupil size variation, fixation length and fixation count corresponding to areas of interest, and fixation count corresponding to non-areas of interest, in particular. The proposed model identifies human implicit visual search intention as task-free visual browsing or task-oriented visual search. Task-oriented visual search is further identified as task-oriented visual search intent generation, task-oriented visual search intent maintenance, or task-oriented visual search intent disappearance. During a visual search, measurement of the pupillary response is greatly influenced by external factors such the intensity and size of the visual stimulus. To alleviate the effects of external factors, we propose a robust baseline model that can accurately measure the pupillary response. Graphical representation of the measured parameter values shows significant differences among the different intent conditions, which can then be used as features for identification. By using the eye movement patterns and pupillary analysis, we can detect the transitions between different implicit intentionstask-free visual browsing intent to task-oriented visual search intent and task-oriented visual search intent maintenance to task-oriented visual search intent disappearanceusing a hierarchical support vector machine. In the proposed model, the hierarchical support vector machine is able to identify the transitions between different intent conditions with greater than 90% accuracy.",
    "keywords": [
      "implicit intention detection",
      "task-free visual browsing intent",
      "task-oriented visual search intent",
      "intention recognition",
      "human computer interface & interaction",
      "pupillary analysis",
      "eye tracking",
      "pupil dilation"
    ]
  },
  {
    "id": "699",
    "title": "A methodology and tools for applying context-specific usability guidelines to interface design",
    "abstract": "This paper presents a methodology and an associated technology to create context-specific usability guidelines. The objective is to transform usability guidelines into a proactive resource that software developers can employ early and often in the development process. The methodology ensures conformance with established guidelines, but has the flexibility to use design experiences to adapt the guidelines to meet the emergent and diverse requirements of modern user interface design. Case-based and organizational learning technology is used to support the methodology and provide valuable resources for software developers.  ",
    "keywords": [
      "guidelines",
      "human-computer interaction",
      "software development methodologies",
      "software engineering",
      "standards",
      "tools for working with guidelines",
      "usability"
    ]
  },
  {
    "id": "700",
    "title": "towards an engineering tool for implementing reusable distributed control systems",
    "abstract": "The IEC model for distributed control systems (DCSs) was adopted for the implementation of a new generation engineering tool. However, it was found that this approach does not exploit all the benefits of the object and component technologies. In this paper, we present the enhanced 4-layer architecture that proved to be very helpful in the identification of the key abstractions required for the design of the new generation of function block based engineering tools. Despite being IEC-compliant, the proposed approach introduces a number of extensions and modifications to the IEC-model to improve the development process. The Unified Modelling Language is exploited during the requirements phase of DCSs, but the use of the FB construct is confected during the design phase.",
    "keywords": [
      "corfu ess",
      "corfu fbdk",
      "iec61499",
      "case tool",
      "distributed control systems",
      "engineering tool",
      "function block"
    ]
  },
  {
    "id": "701",
    "title": "Power Allocation in Cooperative Communication System Based on Stackelberg Game",
    "abstract": "Cooperative communication has great potential to improve the wireless channel capacity by exploiting the antennas on wireless devices for spatial diversity. The performance in cooperative communication depends on careful resource allocation such as relay selection and power control. In this paper, the network is expanded and more than one source is used. What is proposed is a distributed buyer/seller game theoretic framework over multiuser cooperative communication networks in order to stimulate cooperation and improve the system performance. A two-level Stackelberg game is employed to jointly consider the benefits of the source node and the relay nodes in which the source node is modeled as a buyer and the relay nodes are modeled as sellers, respectively. In this work we proposed coded method in which relays amplify and code Source data and send it to destination at the same time and then signal detection occur in destination, but in the codeless network relays send source data separately to destination. So, here coded and codeless networks are compared and contrasted. The stimulation results revealed that the proposed coded method performed better than the codeless ones; furthermore, the research shows that relays near the sources can play a significant role in increasing source nodes utility, so every source would like to buy more power from these preferred relays. Also, the proposed algorithm enforces truthful power demands.",
    "keywords": [
      "cooperative communication",
      "power control",
      "stackelberg game",
      "game theory"
    ]
  },
  {
    "id": "702",
    "title": "Inhibitory conductance dynamics in cortical neurons during activated states",
    "abstract": "During activated states in vivo, neocortical neurons are subject to intense synaptic activity and high-amplitude membrane potential (Vm) ( V m ) fluctuations. These high-conductance states may strongly affect the integrative properties of cortical neurons. We investigated the responsiveness of cortical neurons during different states using a combination of computational models and in vitro experiments (dynamic-clamp) in the visual cortex of adult guinea pigs. Spike responses were monitored following stochastic conductance injection in both experiments and models. We found that cortical neurons can operate in a continuum between two different modes: during states with equal excitatory and inhibitory conductances, the firing is mostly correlated with an increase in excitatory conductance, which is a rather classic scenario. In contrast, during states dominated by inhibition, the firing is mostly related to a decrease in inhibitory conductances (dis-inhibition). This model prediction was tested experimentally using dynamic-clamp, and the same modes of firing were identified. We also found that the signature of spikes evoked by dis-inhibition is a transient drop of the total membrane conductance prior to the spike, which is typical of states with dominant inhibitory conductances. Such a drop should be identifiable from intracellular recordings in vivo, which would provide an important test for the presence of inhibition-dominated states. In conclusion, we show that in artificial activated states, not only inhibition can determine the conductance state of the membrane, but inhibitory inputs may also have a determinant influence on spiking. Future analyses and models should focus on verifying if such a determinant influence of inhibitory conductance dynamics is also present in vivo.",
    "keywords": [
      "spike-triggered average",
      "conductance dynamics",
      "dynamic-clamp"
    ]
  },
  {
    "id": "703",
    "title": "Centralized fleet management system for cybernetic transportation",
    "abstract": "In this article, we present a centralized fleet management system (CFMS) for cybernetic vehicles called cybercars. Cybercars are automatically guided vehicles for passenger transport on dedicated networks like amusement parks, shopping centres etc. The users make reservations for the vehicles through phone, internet, kiosk etc and the CFMS schedules the cybercars to pick the users at their respective stations at desired time intervals. The CFMS has centralized control of the routing network and performs pooling of customer requests, scheduling and routing of cybercars to customers, empty cybercars to new services or parking stations and those running below their threshold battery levels to recharging stations. The challenges before CFMS are to assure conflict-free routing, accommodate immediate requests from customers, dynamic updation of vehicle paths and minimize congestion on the whole network. We present the approaches used by CFMS to ensure these functionalities and demonstrate a numerical illustration on a test network.",
    "keywords": [
      "fleet management",
      "intelligent vehicle",
      "decision support"
    ]
  },
  {
    "id": "704",
    "title": "Signal processing interpolation educational workbench",
    "abstract": "This article presents an educational tool to be used in signal processing interpolation-related subjects. The aim is to contribute to the better consolidation of acquired theoretical knowledge, allowing students to test signal reconstruction algorithms and visualize the results obtained by the usage of such algorithms, and how several parameters affect their convergence and performance.  ",
    "keywords": [
      "signal",
      "reconstruction",
      "educational",
      "interpolation",
      "over-sampling"
    ]
  },
  {
    "id": "705",
    "title": "Unsupervised view and rate invariant clustering of video sequences",
    "abstract": "Videos play an ever increasing role in our everyday lives with applications ranging from news, entertainment, scientific research, security and surveillance. Coupled with the fact that cameras and storage media are becoming less expensive, it has resulted in people producing more video content than ever before. This necessitates the development of efficient indexing and retrieval algorithms for video data. Most state-of-the-art techniques index videos according to the global content in the scene such as color, texture, brightness, etc. In this paper, we discuss the problem of activity-based indexing of videos. To address the problem, first we describe activities as a cascade of dynamical systems which significantly enhances the expressive power of the model while retaining many of the computational advantages of using dynamical models. Second, we also derive methods to incorporate view and rate-invariance into these models so that similar actions are clustered together irrespective of the viewpoint or the rate of execution of the activity. We also derive algorithms to learn the model parameters from a video stream and demonstrate how a single video sequence may be clustered into different clusters where each cluster represents an activity. Experimental results for five different databases show that the clusters found by the algorithm correspond to semantically meaningful activities.",
    "keywords": [
      "video clustering",
      "summarization",
      "surveillance",
      "cascade of linear dynamical systems",
      "view invariance",
      "affine invariance",
      "rate invariance"
    ]
  },
  {
    "id": "706",
    "title": "Closed inter-sequence pattern mining",
    "abstract": "Inter-sequence pattern mining can find associations across several sequences in a sequence database, which can discover both a sequential pattern within a transaction and sequential patterns across several different transactions. However, inter-sequence pattern mining algorithms usually generate a large number of recurrent frequent patterns. We have observed mining closed inter-sequence patterns instead of frequent ones can lead to a more compact yet complete result set. Therefore, in this paper, we propose a model of closed inter-sequence pattern mining and an efficient algorithm called CISP-Miner for mining such patterns, which enumerates closed inter-sequence patterns recursively along a search tree in a depth-first search manner. In addition, several effective pruning strategies and closure checking schemes are designed to reduce the search space and thus accelerate the algorithm. Our experiment results demonstrate that the proposed CISP-Miner algorithm is very efficient and outperforms a compared EISP-Miner algorithm in most cases.",
    "keywords": [
      "closed patterns",
      "data mining",
      "inter-sequence pattern"
    ]
  },
  {
    "id": "707",
    "title": "Symmetry detection by generalized complex (GC) moments: A close-form solution",
    "abstract": "This paper presents a unified method for detecting both reflection-symmetry and rotation-symmetry of 2D images based on an identical set of features, i.e., the first three nonzero generalized complex (GC) moments. This method is theoretically guaranteed to detect all the axes of symmetries of every 2D image, if more nonzero GC moments are used in the feature set. Furthermore, we establish the relationship between reflectional symmetry and rotational symmetry in an image, which can be used to check the correctness of symmetry detection. This method has been demonstrated experimentally using more than 200 images.",
    "keywords": [
      "symmetry detection",
      "reflectional and rotational symmetry",
      "symmetric axis",
      "generalized complex  moments",
      "fold number",
      "fold axes",
      "rotationally symmetric image",
      "reflection-symmetric image"
    ]
  },
  {
    "id": "708",
    "title": "On \"drunken sinusoids\" and their Fourier series",
    "abstract": "The Fourier series of trigonometric-exponential functions f(alpha)(x) = sin(x + alpha) exp(cos(x)) are studied. Dual recursive formulae for the corresponding Fourier coefficients are derived. The above coefficients are transcendental.  ",
    "keywords": [
      "fourier series and coefficients",
      "wave function",
      "transcendental number"
    ]
  },
  {
    "id": "709",
    "title": "Automated Abstractions for Contract Validation",
    "abstract": "Pre/postcondition-based specifications are commonplace in a variety of software engineering activities that range from requirements through to design and implementation. The fragmented nature of these specifications can hinder validation as it is difficult to understand if the specifications for the various operations fit together well. In this paper, we propose a novel technique for automatically constructing abstractions in the form of behavior models from pre/postcondition-based specifications. Abstraction techniques have been used successfully for addressing the complexity of formal artifacts in software engineering; however, the focus has been, up to now, on abstractions for verification. Our aim is abstraction for validation and hence, different and novel trade-offs between precision and tractability are required. More specifically, in this paper, we define and study enabledness-preserving abstractions, that is, models in which concrete states are grouped according to the set of operations that they enable. The abstraction results in a finite model that is intuitive to validate and which facilitates tracing back to the specification for debugging. The paper also reports on the application of the approach to two industrial strength protocol specifications in which concerns were identified.",
    "keywords": [
      "requirements/specifications",
      "validation",
      "automated abstraction"
    ]
  },
  {
    "id": "710",
    "title": "Communication-Delay-Distribution-Dependent Networked Control for a Class of T-S Fuzzy Systems",
    "abstract": "This paper investigates a robust networked control for a class of Takagi-Sugeno (T-S) fuzzy systems. The controller design specifically takes probabilistic interval distribution of the communication delay into account. A general framework of networked control is first proposed. The two main features are 1) the zero-order hold can choose the latest control input signal when the packets received are out-of-order, and 2) as the result of 1), the models of the all kinds of uncertainties in networked signal transfer-including network-induced delay and data packet dropout-are under a unified framework. Next, if the probability distribution of communication delay is known or specified in a design process, sufficient stability conditions for networked T-S fuzzy systems are derived, which are based on the Lyapunov theory. Following this, a stabilizing controller design method is developed, which shows that the solvability of the design depends not only on the upper and lower bounds of the delay but on its probability distribution as well. Finally, a numerical example is used to show the application of the theoretical results obtained in this paper.",
    "keywords": [
      "linear matrix inequalities ",
      "networked control systems ",
      "probability distribution",
      "takagi-sugeno  fuzzy systems"
    ]
  },
  {
    "id": "711",
    "title": "Branched Polymers and Hyperplane Arrangements",
    "abstract": "We generalize the construction of connected branched polymers and the notion of the volume of the space of connected branched polymers studied by Brydges and Imbrie (Ann Math, 158:1019-1039, 2003), and Kenyon and Winkler (Am Math Mon, 116(7):612-628, 2009) to any central hyperplane arrangement . The volume of the resulting configuration space of connected branched polymers associated to the hyperplane arrangement is expressed through the value of the characteristic polynomial of at 0. We give a more general definition of the space of branched polymers, where we do not require connectivity, and introduce the notion of q-volume for it, which is expressed through the value of the characteristic polynomial of at . Finally, we relate the volume of the space of branched polymers to broken circuits and show that the cohomology ring of the space of branched polymers is isomorphic to the Orlik-Solomon algebra.",
    "keywords": [
      "polymers",
      "braid arrangement",
      "hyperplane arrangement",
      "characteristic polynomial",
      "broken circuit",
      "orlik-solomon algebra"
    ]
  },
  {
    "id": "712",
    "title": "mobile dynamic content distribution networks",
    "abstract": "Mobile networks are becoming increasingly popular as means for distributing inform tion to large number of users. In comparison to wired networks, mobile networks distinguished by potentially much higher variability in demand due to user mobility. Most previous content techniques ssume static client demand distribution and, thus, may not perform well in mobile networks.This paper proposes and analyzes Mobile Dynamic Content Distribution Network model, which takes demand variations into account to decide whether to replicate content and whether to remove previously created replicas in order to minimize total network traffic. We develop two solutions to our model: an offline optimal, which provides an ideal lower-bound on the total traffic, and practical heuristic online algorithm, which uses demand forecasting to make replication decisions. We provide thorough evaluation of our solutions, comparing them against ACDN, the only previous dynamic content placement algorithm targeting bandwidth minimization that we are aware of. Our results show that our online algorithm significantly outperforms the ACDN one, reducing total network traffic by up to 85% in a number of experiments covering large system design space.",
    "keywords": [
      "cdn",
      "simulation",
      "demand forecasting",
      "mobile network",
      "dynamic content placement",
      "online algorithm"
    ]
  },
  {
    "id": "713",
    "title": "An empirical investigation of the effects of data warehousing on decision performance",
    "abstract": "Organizations implement data warehouses to overcome the limitations of DSS by adding this database component and thereby improve decision performance. However, no empirical evidence is available to show the effects of a data warehouse (DW) on decision quality and performance. To examine this, a laboratory experiment was conducted. The data warehouse variables considered were the time horizon of the data and its level of aggregation. It was found that using a full data warehouse resulted in significantly better performance and that using it resulted in better performance than using a partial data warehouse (long-time history with no aggregated data). However, using a partial data warehouse was not significantly better than not using a data warehouse at all.",
    "keywords": [
      "data warehouse",
      "decision support systems",
      "is success",
      "database management",
      "decision performance"
    ]
  },
  {
    "id": "714",
    "title": "Measuring and modelling the performance of a parallel ODMG compliant object database server",
    "abstract": "Object database management systems (ODBMSs) are now established as the database management technology of choice for a range of challenging data intensive applications. Furthermore, the applications associated with object databases typically have stringent performance requirements, and some are associated with very large data sets. An important feature for the performance of object databases is the speed at which relationships can be explored. In queries, this depends on the effectiveness of different join algorithms into which queries that follow relationships can be compiled. This paper presents a performance evaluation of the Polar parallel object database system, focusing in particular on the performance of parallel join algorithms. Polar is a parallel, shared-nothing implementation of the Object Database Management Group (ODMG) standard for object databases. The paper presents an empirical evaluation of queries expressed in the ODMG Query Language (OQL), as well as a cost model for the parallel algebra that is used to evaluate OQL queries. The cost model is validated against the empirical results for a collection of queries using four different join algorithms, one that is value based and three that are pointer based. ",
    "keywords": [
      "object database",
      "parallel databases",
      "odmg",
      "oql",
      "benchmark",
      "cost model"
    ]
  },
  {
    "id": "715",
    "title": "Performance comparison of machine learning methods for prognosis of hormone receptor status in breast cancer tissue samples",
    "abstract": "We examined the classification and prognostic scoring performances of several computer methods on different feature sets to obtain objective and reproducible analysis of estrogen receptor status in breast cancer tissue samples. Radial basis function network, k-nearest neighborhood search, support vector machines, naive bayes, functional trees, and k-means clustering algorithm were applied to the test datasets. Several features were employed and the classification accuracies of each method for these features were examined. The assessment results of the methods on test images were also experimentally compared with those of two experts. According to the results of our experimental work, a combination of functional trees and the naive bayes classifier gave the best prognostic scores indicating very good kappa agreement values (?=0.899 and ?=0.949, p<0.001) with the experts. This combination also gave the best dichotomization rate (96.3%) for assessment of estrogen receptor status. Wavelet color features provided better classification accuracy than Laws texture energy and co-occurrence matrix features.",
    "keywords": [
      "image processing",
      "classification",
      "nucleus segmentation",
      "estrogen receptor  status evaluation",
      "breast cancer prognosis"
    ]
  },
  {
    "id": "716",
    "title": "Corpus-based HIT-MW database for offline recognition of general-purpose Chinese handwritten text",
    "abstract": "A Chinese handwriting database named HIT-MW is presented to facilitate the offline Chinese handwritten text recognition. Both the writers and the texts for handcopying are carefully sampled with a systematic scheme. To collect naturally written handwriting, forms are distributed by postal mail or middleman instead of face to face. The current version of HIT-MW includes 853 forms and 186,444 characters that are produced under an unconstrained condition without preprinted character boxes. The statistics show that the database has an excellent representation of the real handwriting. Many new applications concerning real handwriting recognition can be supported by the database.",
    "keywords": [
      "standardization",
      "data acquisition",
      "optical character recognition",
      "handwritten chinese text"
    ]
  },
  {
    "id": "717",
    "title": "Fault diagnosis based on Walsh transform and rough sets",
    "abstract": "An accurate and fast method for fault diagnosis is an important issue most techniques have sought to. For this reason, a fast fault diagnosis method based on Walsh transform and rough sets is proposed in this paper. Firstly, fault signals are fast transformed by Walsh matrix, and the Walsh spectrums are obtained, whose statistical characteristics constitute feature vectors. Secondly, the feature vectors are discretized and reduced by the rough sets theory, as a result, key features are retained and diagnosis rules are provided. Finally, utilized these diagnosis rules, fault diagnosis is carried out experimentally in the spectrum domain and its performance is compared with that of other methods, the higher accuracy is achieved and much time is saved, which fully validates the effectiveness of our approach.",
    "keywords": [
      "fault diagnosis",
      "walsh transform",
      "rough sets",
      "discernable matrix",
      "attribution reduction"
    ]
  },
  {
    "id": "718",
    "title": "Weighted fuzzy interpolative reasoning for sparse fuzzy rule-based systems",
    "abstract": "In this paper, we present a weighted fuzzy interpolative reasoning method for sparse fuzzy rule-based systems, where the antecedent variables appearing in the fuzzy rules have different weights. We also present a weights-learning algorithm to automatically learn the optimal weights of the antecedent variables of the fuzzy rules for the proposed weighted fuzzy interpolative reasoning method. We also apply the proposed weighted fuzzy interpolative reasoning method and the proposed weights-learning algorithm to handle the truck backer-upper control problem. The experimental results show that the proposed fuzzy interpolative reasoning method using the optimally learned weights by the proposed weights-learning algorithm gets better truck backer-upper control results than the ones by the traditional fuzzy inference system and the existing fuzzy interpolative reasoning methods. The proposed method provides us with a useful way for fuzzy rules interpolation in sparse fuzzy rule-based systems.",
    "keywords": [
      "fuzzy interpolative reasoning",
      "sparse fuzzy rule-based systems",
      "weighted antecedent variables",
      "weights-learning algorithm"
    ]
  },
  {
    "id": "719",
    "title": "A Plug-in Approach to Neyman-Pearson Classification",
    "abstract": "The Neyman-Pearson (NP) paradigm in binary classification treats type I and type II errors with different priorities. It seeks classifiers that minimize type II error, subject to a type I error constraint under a user specified level a. In this paper, plug-in classifiers are developed under the NP paradigm. Based on the fundamental Neyman-Pearson Lemma, we propose two related plug-in classifiers which amount to thresholding respectively the class conditional density ratio and the regression function. These two classifiers handle different sampling schemes. This work focuses on theoretical properties of the proposed classifiers; in particular, we derive oracle inequalities that can be viewed as finite sample versions of risk bounds. NP classification can be used to address anomaly detection problems, where asymmetry in errors is an intrinsic property. As opposed to a common practice in anomaly detection that consists of thresholding normal class density, our approach does not assume a specific form for anomaly distributions. Such consideration is particularly necessary when the anomaly class density is far from uniformly distributed.",
    "keywords": [
      "plug-in approach",
      "neyman-pearson paradigm",
      "nonparametric statistics",
      "oracle inequality",
      "anomaly detection"
    ]
  },
  {
    "id": "720",
    "title": "Analytical transformation of the volume integral in the boundary integral equation for 3D anisotropic elastostatics involving body force",
    "abstract": "In the boundary element method (BEM), it is well known that the presence of body force shall give rise to an additional volume integral that conventionally requires domain discretization for numerical computations. To restore the BEMs distinctive notion of boundary discretization, the present work analytically transforms the volume integral to surface ones for the body-force effect in the 3D anisotropic elasticity. On applying Greens Theorem, new fundamental solutions with explicit forms of Fourier series are introduced to facilitate the volume-to-surface transformation. The coefficients of the Fourier-series representations are determined by solving a banded matrix formulated from integrations of the constrained equation. Of no doubt, such an approach has fully restored the boundary element method as a truly boundary solution technique for analyzing 3D anisotropic elasticity involving body force. At the end, numerical verifications of the volume-to-surface integral transformation are presented. Also, such an approach has been implemented in an existing BEM code. For demonstrating the implementation, numerical examples are presented with comparisons with ANSYS analysis. To the authors knowledge, this is the first work in the open literature that reports the successful transformation for 3D anisotropic elasticity.",
    "keywords": [
      "3d anisotropic elasticity",
      "body force",
      "volume-to-surface integral transformation",
      "boundary element method"
    ]
  },
  {
    "id": "721",
    "title": "An integrated framework for adapting WS-BPEL scenario execution using QoS and collaborative filtering techniques",
    "abstract": "We present an algorithm combining QoS and collaborative filtering for BPEL adaptation. The combination introduces collaborating filtering functionality maintaining high QoS. We exploit the sparsity of the rating matrix to tackle known issues of CF. We evaluate the approach both in terms of performance and adaptation QoS.",
    "keywords": [
      "ws-bpel",
      "adaptation",
      "quality of service",
      "collaborative filtering",
      "metasearch algorithms"
    ]
  },
  {
    "id": "722",
    "title": "Basic skeletons in 11c",
    "abstract": "11c is a high-level parallel language that provides support for some of the most widely used algorithmic skeletons. The language has a syntax based on OpenMP-like directives and the compiler uses direct translation to MPI to produce parallel code. To evaluate the performance of our prototype compiler we present computational results for some of the skeletons available in the language on different architectures. Particularly in the presence of coarse-grain parallelism, the results reflect similar performances for the 11c compiled version and ad hoc MPI or OpenMP implementations. In all cases, the performance loss with respect to a direct MPI implementation is clearly compensated by a significantly smaller development effort.  ",
    "keywords": [
      "parallel programming",
      "high-level language",
      "algorithmic skeletons",
      "compilers",
      "openmp",
      "mpi"
    ]
  },
  {
    "id": "723",
    "title": "A multisolution method for cell formation - Exploring practical alternatives in group technology manufacturing",
    "abstract": "An approach based on the decision maker's judgment is proposed by furnishing multiple solutions of part-family and machine-cell formation of a cellular manufacturing system. The reason for relying on the judgment of the decision-maker is due to the complexity and the many constraints encountered in practice. Some examples of these practical constraints are workload balancing, ill-defined systems, the existence of exceptional elements, and the presence of the various uncertain factors in the system. The basic approach is based on the concept of nearest-neighborhood between machines and parts. The procedure, which can be used to identify multiple grouping pat terns of machines and parts, consists of two algorithms: grouping and branching, association, and combining. Numerical examples are provided, especially for ill-structured problems, to illustrate the approach.  ",
    "keywords": [
      "group technology",
      "cellular manufacturing",
      "multiple grouping patterns",
      "interactive decision making"
    ]
  },
  {
    "id": "724",
    "title": "A complex communication network for distribution automation using a fiber optic network and WLANs",
    "abstract": "In order to provide electricity economically and safely to users, a Distribution Automation System (DAS) monitors and operates the components of distribution systems remotely through communication networks. Fiber optic communication networks have primarily been used for DASs in Korea because of their huge bandwidth and dielectric noise immunity. However, the fiber optic communication network has some shortcomings, particularly that its installation cost and communication fees are expensive. This paper proposes a complex communication network, where WLANs are linked into a fiber optic network to expand DASs in distribution lines inexpensively. A DAS wireless bridge (DWB) is designed for the proposed communication network using IEEE 802.11a WLAN technology. Feasibility of the proposed network is checked experimentally.",
    "keywords": [
      "distribution automation",
      "ieee 802.11a wlan",
      "fiber optic cable",
      "wireless bridge",
      "complex communication network"
    ]
  },
  {
    "id": "725",
    "title": "Multiscale morphological segmentation of gray-scale images",
    "abstract": "In this paper, the authors have proposed a method of segmenting gray level images using multiscale morphology. The approach resembles watershed algorithm in the sense that the dark (respectively bright) features which are basically canyons (respectively mountains) on the surface topography of the gray level image are gradually filled (respectively clipped) using multiscale morphological closing (respectively opening) by reconstruction with isotropic structuring element. The algorithm detects valid segments at each scale using three criteria namely growing, merging and saturation. Segments extracted at various scales are integrated in the final result. The algorithm is composed of two passes preceded by a preprocessing step for simplifying small scale details of the image that might cause over-segmentation. In the first pass feature images at various scales are extracted and kept in respective level of morphological towers. In the second pass, potential features contributing to the formation of segments at various scales are detected. Finally the algorithm traces the contours of all such contributing features at various scales. The scheme after its implementation is executed on a set of test images (synthetic as well as real) and the results are compared with those of few other standard methods. A quantitative measure of performance is also formulated for comparing the methods.",
    "keywords": [
      "closing by reconstruction",
      "gray-level image segmentation",
      "morphological towers",
      "multiscale morphology",
      "opening by reconstruction",
      "performance analysis"
    ]
  },
  {
    "id": "726",
    "title": "Data mining in Web services discovery and monitoring",
    "abstract": "The business needs, the availability of huge volumes of data and the continuous evolution in Web services functions derive the need of application of data mining in the Web service domain. This article recommends several data mining applications that can leverage problems concerned with the discovery and monitoring of Web services. This article then presents a case study on applying the clustering data mining technique to the Web service usage data to improve the Web service discovery process. This article also discusses the challenges that arise when applying data mining to Web services usage data and abstract information.",
    "keywords": [
      "data mining",
      "knowledge discovery",
      "service discovery",
      "web services"
    ]
  },
  {
    "id": "727",
    "title": "Reasoning theories",
    "abstract": "Our long-term goal is the development of a general framework for specifying, structuring, and interoperating provers. Our main focus is on the formalization of the architectural and implementational choices that underlie the construction of such systems. This paper has two main goals. The first is to introduce the main intuitions underlying the proposed framework. We concentrate on its use in the integration of provers. The second is the development of the notion of reasoning theory, meant as the formalization of the notion of \"implementation of the logic\" of a prover. As an example we sketch an analysis, at the reasoning theory level, of the integration of linear arithmetic into the NQTHM simplification process.",
    "keywords": [
      "open mechanized reasoning system ",
      "reasoning theory",
      "nqthm",
      "linear arithmetic"
    ]
  },
  {
    "id": "728",
    "title": "Automatic generation of human animation based on motion programming",
    "abstract": "In motion simulations, video games and animation films, lots of interactions between characters and virtual environments are needed. Even though realistic motion data can be derived from MoCap system, motion editing and synthesis, animators must adapt these motion data to specific virtual environment manually, which is a boring and time-consuming job. Here we propose a framework to program the movements of characters and generate navigation animations in virtual environment. Given a virtual environment, a visual user interface is provided for animators to interactively generate motion scripts, describing the characters' movements in this scene and finally used to retrieve motion clips from MoCap database and generate navigation animations automatically. This framework also provides flexible mechanism for animators to get varied resulting animations by configurable table of motion bias coefficients and interactive visual user interface. ",
    "keywords": [
      "human animation",
      "motion programming",
      "motion scripts"
    ]
  },
  {
    "id": "729",
    "title": "Bipartite graph-based mismatch removal for wide-baseline image matching",
    "abstract": "The conventional wide-baseline image matching aims to establish point-to-point correspondence pairs across the two images under matching. This is normally accomplished by identifying those feature points with most similar local features represented by feature descriptors and measuring the feature-vector distance based on the nearest neighbor matching criterion. However, a large number of mismatches would be incurred especially when the two images under comparison have a large viewpoint variation with respect to each other or involve very different backgrounds. In this paper, a new mismatch removal method is proposed by utilizing the bipartite graph to first establish one-to-one coherent region pairs (CRPs), which are then used to verify whether each point-to-point matching pair is a correct match or not. The generation of CRPs is achieved by applying the Hungarian method to the bipartite graph, together with the use of the proposed region-to-region similarity measurement metric. Extensive experimental results have demonstrated that our proposed mismatch removal method is able to effectively remove a significant number of mismatched point-to-point correspondences.",
    "keywords": [
      "wide-baseline image matching",
      "region correspondence",
      "point correspondence",
      "coherent region pair",
      "sift feature descriptor",
      "bipartite graph matching",
      "region similarity measurement metric",
      "hungarian method"
    ]
  },
  {
    "id": "730",
    "title": "performance evaluation of evolutionary algorithms for road detection",
    "abstract": "In this paper we present the first comparative study of evolutionary classifiers for the problem of road detection. We use seven evolutionary algorithms ( GAssist-ADI, XCS, UCS, cAnt, EvRBF,Fuzzy-AB and FuzzySLAVE ) for this purpose and to develop better understanding we also compare their performance with two well-known non-evolutionary classifiers ( kNN, C4.5 ). Further we identify vision based features that enable a single classifier to learn to successfully classify a variety of regions in various roads as opposed to training a new classifier for each type of road. For this we collect a real-world dataset of road images of various roads taken at different times of the day. Then, using Information Gain (I.G) and CfsSubsetMerit values we evaluate the efficacy of our features in facilitating the detection. Our results indicate that intelligent features coupled with right evolutionary technique provides a promising solution for the domain of road detection.",
    "keywords": [
      "road detection"
    ]
  },
  {
    "id": "731",
    "title": "Modeling dynamic scenarios for local sensor-based motion planning",
    "abstract": "This paper addresses the modeling of the static and dynamic parts of the scenario and how to use this information with a sensor-based motion planning system. The contribution in the modeling aspect is a formulation of the detection and tracking of mobile objects and the mapping of the static structure in such a way that the nature (static/dynamic) of the observations is included in the estimation process. The algorithm provides a set of filters tracking the moving objects and a local map of the static structure constructed on line. In addition, this paper discusses how this modeling module is integrated in a real sensor-based motion planning system taking advantage selectively of the dynamic and static information. The experimental results confirm that the complete navigation system is able to move a vehicle in unknown and dynamic scenarios. Furthermore, the system overcomes many of the limitations of previous systems associated to the ability to distinguish the nature of the parts of the scenario.",
    "keywords": [
      "mobile robots",
      "mapping dynamic environments",
      "sensor-based motion planning"
    ]
  },
  {
    "id": "732",
    "title": "Large-margin feature selection for monotonic classification",
    "abstract": "Monotonic classification plays an important role in the field of decision analysis, where decision values are ordered and the samples with better feature values should not be classified into a worse class. The monotonic classification tasks seem conceptually simple, but difficult to utilize and explain the order structure in practice. In this work, we discuss the issue of feature selection under the monotonicity constraint based on the principle of large margin. By introducing the monotonicity constraint into existing margin based feature selection algorithms, we design two new evaluation algorithms for monotonic classification. The proposed algorithms are tested with some artificial and real data sets, and the experimental results show its effectiveness.",
    "keywords": [
      "monotonic classification",
      "ordinal classification",
      "monotonicity constraint",
      "feature selection",
      "classification margin"
    ]
  },
  {
    "id": "733",
    "title": "Using formal metamodels to check consistency of functional views in information systems specification",
    "abstract": "UML notations require adaptation for applications such as Information Systems (IS). Thus we have defined IS-UML. The purpose of this article is twofold. First, we propose an extension to this language to deal with functional aspects of IS. We use two views to specify IS transactions: the first one is defined as a combination of behavioural UML diagrams (collaboration and state diagrams), and the second one is based on the definition of specific classes of an extended class diagram. The final objective of the article is to consider consistency issues between the various diagrams of an IS-UML specification. In common with other UML languages, we use a metamodel to define IS-UML. We use class diagrams to summarize the metamodel structure and a formal language, B, for the full metamodel. This allows us to formally express consistency checks and mapping rules between specific metamodel concepts.",
    "keywords": [
      "information system design",
      "unified modelling language notation",
      "metamodel",
      "formal notation"
    ]
  },
  {
    "id": "734",
    "title": "Denoising and enhancing digital mammographic images for visual screening",
    "abstract": "Dense regions in digital mammographic images are usually noisy and have low contrast, and their visual screening is difficult. This paper describes a new method for mammographic image noise suppression and enhancement, which can be effective particularly for screening image dense regions. Initially, the image is preprocessed to improve its local contrast and the discrimination of subtle details. Next, image noise suppression and edge enhancement are performed based on the wavelet transform. At each resolution, coefficients associated with noise are modelled by Gaussian random variables; coefficients associated with edges are modelled by Generalized Laplacian random variables, and a shrinkage function is assembled based on posterior probabilities. The shrinkage functions at consecutive scales are combined, and then applied to the wavelets coefficients. Given a resolution of analysis, the image denoising process is adaptive (i.e. does not require further parameter adjustments), and the selection of a gain factor provides the desired detail enhancement. The enhancement function was designed to avoid introducing artifacts in the enhancement process, which is essential in mammographic image analysis. Our preliminary results indicate that our method allows to enhance local contrast, and detect microcalcifications and other suspicious structures in situations where their detection would be difficult otherwise. Compared to other approaches, our method requires less parameter adjustments by the user.",
    "keywords": [
      "mammography",
      "contrast equalization",
      "image denoising",
      "image enhancement",
      "multiresolution analysis",
      "wavelets"
    ]
  },
  {
    "id": "735",
    "title": "Principles for modeling language design",
    "abstract": "Modeling languages, like programming languages, need to be designed if they are to be practical, usable, accepted, and of lasting value. We present principles for the design of modeling languages. To arrive at these principles, we consider the intended use of modeling languages. We conject that the principles are applicable to the development of new modeling languages, and for improving the design of existing modeling languages that have evolved, perhaps through a process of unification. The principles are illustrated and explained by several examples, drawing on object-oriented and mathematical modeling languages.",
    "keywords": [
      "modeling languages",
      "design principles",
      "uml",
      "unification"
    ]
  },
  {
    "id": "736",
    "title": "Systems, systems of systems, and the education of engineers",
    "abstract": "The thesis presented here is that the result of engineering is the design, construction, or operation of systems or their subsystems and components and that the teaching of systems must be central to engineering education. It is maintained that current undergraduate engineering curricula do not give the student adequate appreciation of this major intellectual element of their profession. Five proposals for approaches to correct this deficiency are offered: opportunities for clinical practice throughout all the undergraduate years; the use of distributed interactive simulation technology in semester-long projects; courses or course material on the phenomenology and behavior of systems; use of project management tools in engineering clinics; and encouraging engineering faculty to spend some part of their sabbaticals engaged in system design or operation. Issues of implementation are addressed, including the scaling of these ideas to universities that must meet the needs of large numbers of students.",
    "keywords": [
      "system",
      "systems of systems",
      "engineering education",
      "clinical practice",
      "engineering practice",
      "simulation"
    ]
  },
  {
    "id": "737",
    "title": "Optimal erasure protection for scalably compressed video streams with limited retransmission on channels with IID and bursty loss characteristics",
    "abstract": "In this paper we combine priority encoding transmission (PET) with a limited retransmission (LR) capacity. We propose the resulting LR-PET scheme as a framework for efficient RD optimized delivery of streaming media. Previous work on scalable media protection with PET has largely ignored the possibility of retransmission. In the proposed LR-PET framework, an optimization algorithm determines the level of protection for each element in each transmission slot, subject to transmission bandwidth constraints. To balance the protection assigned to elements being transmitted for the first time with those being retransmitted, the proposed algorithm formulates a collection of hypotheses concerning its own behavior in future transmission slots. We show that this formulation of hypotheses is central to the success of the proposed LR-PET algorithm. Indeed, without this element, a greedy version of LR-PET performs only slightly better than PET without retransmission. Experimental results are reported using both IID and GE channel models, with a Motion JPEG2000 video source, demonstrating substantial performance benefits from the proposed framework.",
    "keywords": [
      "channel coding",
      "forward error correction",
      "retransmission",
      "scalable video",
      "erasure channels",
      "priority encoding transmission",
      "bursty loss"
    ]
  },
  {
    "id": "738",
    "title": "Verification of various pipeline models",
    "abstract": "The paper deals with the verification of three pipeline models: the non-linear distributed parameters model, the linear distributed parameter model and the linear lumped parameters model. All the models were comparatively verified on the basis of the measurements on a real pipeline.  ",
    "keywords": [
      "pipeline models",
      "pade approximation",
      "pipesim"
    ]
  },
  {
    "id": "739",
    "title": "On some test statistics for testing homogeneity of variances: a comparative study",
    "abstract": "In this paper, we have reviewed 25 test procedures that are widely reported in the literature for testing the hypothesis of homogeneity of variances under various experimental conditions. Since a theoretical comparison was not possible, a simulation study has been conducted to compare the performance of the test statistics in terms of robustness and empirical power. Monte Carlo simulation was performed for various symmetric and skewed distributions, number of groups, sample size per group, degree of group size inequalities, and degree of variance heterogeneity. Using simulation results and based on the robustness and power of the tests, some promising test statistics are recommended for practitioners.",
    "keywords": [
      "anova",
      "homogeneity of variances",
      "levene's test",
      "monte carlo simulation",
      "power of test",
      "robustness",
      "type i error rate"
    ]
  },
  {
    "id": "740",
    "title": "Synchronization control of stochastic memristor-based neural networks with mixed delays",
    "abstract": "In this paper, the synchronization control of stochastic memristor-based neural networks with mixed delays is studied. Based on the drive-response concept, the stochastic differential inclusions theory and Lyapunov functional method some new criteria are established to guarantee the exponential synchronization in the pth moment of stochastic memristor-based neural networks. The obtained sufficient conditions can be checked easily and improve the results in earlier publications. Finally, a numerical example is given to illustrate the effectiveness of the new scheme.",
    "keywords": [
      "memristor",
      "stochastic neural networks",
      "mixed delays",
      "synchronization control"
    ]
  },
  {
    "id": "741",
    "title": "Methods for parallel computation of complex flow problems",
    "abstract": "This paper is an overview of some of the methods developed by the Team for Advanced Flow Simulation and Modeling (T*AFSM) [http://www, mems, rice. edu/TAFSM/] to support flow simulation and modeling in a number of \"Targeted Challenges\". The \"Targeted Challenges\" include unsteady flows with interfaces, fluid-object and fluid-structure interactions, airdrop systems, and air circulation and contaminant dispersion. The methods developed include special numerical stabilization methods for compressible and incompressible flows, methods for moving boundaries and interfaces, advanced mesh management methods, and multi-domain computational methods. We include in this paper a number of numerical examples from the simulation of complex flow problems.  ",
    "keywords": [
      "computational fluid dynamics",
      "flow simulation",
      "stabilization methods",
      "compressible flow",
      "incompressible flow",
      "multidomain computational methods"
    ]
  },
  {
    "id": "742",
    "title": "Environmental sensor bridge system for communication robots",
    "abstract": "This paper proposes an environmental sensor bridge system named COSPI for autonomous communication robots. In the near future, various sensors will get installed in day-to-day environments and robots will use information from these sensors to recognize their surroundings. To this end, different sensors can be installed in different environments. COSPI enables the robots to work with different sensor configuration environments without requiring any robot software reconfiguration. The basic idea of COSPI is the abstraction of recognition types. We investigate 278 recognition routines in behavior modules of a communication robot that works in practical situations and classify them based on communication cues of the communication robot. COSPI facilitates robot behavior development as robot behavior and sensor processing procedures can be independently developed. Furthermore, COSPI encourages the reuse of environmental sensor modules for other robot tasks or applications within the same environment. We have conducted an experiment to confirm that a robot can work in a visual sensor environment, in an infrared ray and pyroelectric sensor environment, and in an optical motion capture system environment. The results showed that robots can work using sensor information from environmental sensors in all these environments.",
    "keywords": [
      "human-robot interaction",
      "communication robot",
      "sensor network",
      "robot behavior development"
    ]
  },
  {
    "id": "743",
    "title": "A note on machine scheduling with sum-of-logarithm-processing-time-based and position-based learning effects",
    "abstract": "Recently, Biskup [2] classifies the learning effect models in scheduling environments into two types: position-based and sum-of-processing-time-based. In this paper, we study scheduling problem with sum-of-logarithm-processing-time-based and position-based learning effects. We show that the single machine scheduling problems to minimize the makespan and the total completion time can both be solved by the smallest (normal) processing time first (SPT) rule. We also show that the problems to minimize the maximum lateness, the total weighted completion times and the total tardiness have polynomial-time solutions under agreeable WSPT rule and agreeable EDD rule. In addition, we show that m-machine permutation flowshop problems are still polynomially solvable under the proposed learning model.",
    "keywords": [
      "scheduling",
      "single-machine",
      "flowshop",
      "learning effect"
    ]
  },
  {
    "id": "744",
    "title": "Towards a global control strategy for induction motor: Speed regulation, flux optimization and power factor correction",
    "abstract": "A great deal of interest has been paid to induction machine control over the last years. However, most previous works have focused on the speed/flux/torque regulation supposing the machine magnetic circuit to be linear and ignoring the machine power conversion equipments. The point is that speed regulation cannot be ensured in optimal efficiency conditions, for a wide range of speed-set-point and load torque, unless the magnetic circuit nonlinearity is explicitly accounted for in the motor model. On the other hand, the negligence of the power conversion equipments makes it impossible to deal properly with the harmonic pollution issue due to motor  power supply grid interaction. This paper presents a theoretical framework for a global control strategy of the induction machine and related power equipments. The proposed strategy involves a multi-loop nonlinear adaptive controller designed to meet the three main control objectives, i.e. tight speed regulation for a wide range speed-reference variation, flux optimization for energy consumption and power factor correction (PFC). Tools from the averaging theory are resorted to formally describe the control performances.",
    "keywords": [
      "induction machine",
      "magnetic circuit nonlinearity",
      "ac/dc/ac converters",
      "speed regulation",
      "power factor correction",
      "backstepping technique"
    ]
  },
  {
    "id": "745",
    "title": "Solving fuzzy Fredholm linear integral equations using Sinc method and double exponential transformation",
    "abstract": "In this paper, numerical solution of fuzzy Fredholm linear integral equations is considered by applying Sinc method based on double exponential transformation with dual fuzzy linear systems. For this purpose, we convert the given fuzzy integral equation to a fuzzy linear system of equation. In this case, the Sinc collocation method with double exponential transformation is used. Numerical examples are provided to verify the validity of the proposed algorithm.",
    "keywords": [
      "fuzzy number",
      "fuzzy fredholm integral equations",
      "sinc method",
      "dual systems",
      "double exponential  transformation"
    ]
  },
  {
    "id": "746",
    "title": "Fuzzy-Approximation-Based Adaptive Control of Strict-Feedback Nonlinear Systems With Time Delays",
    "abstract": "This paper focuses on the problem of adaptive control for a class of nonlinear time-delay systems with unknown nonlinearities and strict-feedback structure. Based on the Lyapunov-Krasovskii functional approach, a state-feedback adaptive controller is constructed by backstepping. The proposed adaptive controller guarantees that the system output converges into a small neighborhood of the reference signal, and all the signals of the closed-loop system remain bounded. Compared with the results that exist, the main advantage of the proposed method is that the controller design is independent of the choice of the fuzzy-membership functions; therefore, a priori knowledge of fuzzy approximators is not necessary for control design, and the proposed approach requires only one adaptive law for an nth-order system. Two numerical examples are used to illustrate the effectiveness of the proposed approach.",
    "keywords": [
      "adaptive control",
      "backstepping",
      "fuzzy-logic systems",
      "nonlinear systems",
      "time delays"
    ]
  },
  {
    "id": "747",
    "title": "Some Results on Greedy Embeddings in Metric Spaces",
    "abstract": "Geographic Routing is a family of routing algorithms that uses geographic point locations as addresses for the purposes of routing. Such routing algorithms have proven to be both simple to implement and heuristically effective when applied to wireless sensor networks. Greedy Routing is a natural abstraction of this model in which nodes are assigned virtual coordinates in a metric space, and these coordinates are used to perform point-to-point routing. Here we resolve a conjecture of Papadimitriou and Ratajczak that every 3-connected planar graph admits a greedy embedding into the Euclidean plane. This immediately implies that all 3-connected graphs that exclude K (3,3) as a minor admit a greedy embedding into the Euclidean plane. We also prove a combinatorial condition that guarantees nonembeddability. We use this result to construct graphs that can be greedily embedded into the Euclidean plane, but for which no spanning tree admits such an embedding.",
    "keywords": [
      "greedy embedding",
      "papadimitriou-ratajczak conjecture",
      "christmas cactus graph",
      "excluded minor"
    ]
  },
  {
    "id": "748",
    "title": "Investigating the Effects of Multiple Factors Towards More Accurate 3-D Object Retrieval",
    "abstract": "This paper proposes a novel framework for 3-D object retrieval, taking into account most of the factors that may affect the retrieval performance. Initially, a novel 3-D model alignment method is introduced, which achieves accurate rotation estimation through the combination of two intuitive criteria, plane reflection symmetry and rectilinearity. After the pose normalization stage, a low-level descriptor extraction procedure follows, using three different types of descriptors, which have been proven to be effective. Then, a novel combination procedure of the above descriptors takes place, which achieves higher retrieval performance than each descriptor does separately. The paper provides also an in-depth study of the factors that can further improve the 3-D object retrieval accuracy. These include selection of the appropriate dissimilarity metric, feature selection/dimensionality reduction on the initial low-level descriptors, as well as manifold learning for re-ranking of the search results. Experiments performed on two 3-D model benchmark datasets confirm our assumption that future research in 3-D object retrieval should focus more on the efficient combination of low-level descriptors as well as on the selection of the best features and matching metrics, than on the investigation of the optimal 3-D object descriptor.",
    "keywords": [
      "3-d object retrieval",
      "descriptor extraction",
      "feature selection",
      "manifold learning",
      "rotation estimation"
    ]
  },
  {
    "id": "749",
    "title": "Nonlinear hydrodynamic models of traffic flow modelling and mathematical problems",
    "abstract": "This paper deals with nonlinear hydrodynamic modelling of traffic flow on roads and with the solution of related nonlinear initial and boundary value problems. The paper is in two parts. The first one provides the general framework of hydrodynamic modelling of traffic flow. Some new models are proposed and related to the ones which are known in the literature. The second one is on mathematical methods related to the solution of initial-boundary value problems. A critical analysis and an overview on research perspectives conclude the paper.  ",
    "keywords": [
      "nonlinear hydrodynamics",
      "traffic models",
      "nonlinear sciences",
      "evolution equations"
    ]
  },
  {
    "id": "750",
    "title": "Skeleton-enhanced line drawings for 3D models",
    "abstract": "We present a novel line drawing approach for 3D models by introducing their skeleton information into the rendering process. Based on the silhouettes of the input 3D models, we first extract feature lines in geometric regions by utilizing their curvature, torsion and view-dependent information. Then, the skeletons of the models are extracted by our newly developed skeleton extraction algorithm. After that, we draw the skeleton-guided lines from non-geometric regions through the skeleton information. These lines are combined with the feature lines to render the final line drawing result using the line optimization. Experimental results show that our algorithm can render line drawings more effectively with enhanced skeletons. The resulting artistic effects can capture the local geometries as well as the global skeletons of the input 3D models.",
    "keywords": [
      "non-photorealistic rendering",
      "line drawing",
      "skeleton",
      "geometric feature"
    ]
  },
  {
    "id": "751",
    "title": "Low-Complexity Video Coding Based on Two-Dimensional Singular Value Decomposition",
    "abstract": "In this paper, we propose a low-complexity video coding scheme based upon 2-D singular value decomposition (2-D SVD), which exploits basic temporal correlation in visual signals without resorting to motion estimation (ME). By exploring the energy compaction property of 2-D SVD coefficient matrices, high coding efficiency is achieved. The proposed scheme is for the better compromise of computational complexity and temporal redundancy reduction, i.e., compared with the existing video coding methods. In addition, the problems caused by frame decoding dependence in hybrid video coding, such as unavailability of random access, are avoided. The comparison of the proposed 2-D SVD coding scheme with the existing relevant non-ME-based low-complexity codecs shows its advantages and potential in applications.",
    "keywords": [
      "computational complexity",
      "energy compaction",
      "frame independence",
      "simultaneous low-rank approximation of matrices",
      "video decomposition"
    ]
  },
  {
    "id": "752",
    "title": "Use of OSWALD for analyzing longitudinal data with informative dropout",
    "abstract": "OSWALD (Object-oriented Software for the Analysis of Longitudinal Data) is flexible and powerful software written for S-PLUS for the analysis of longitudinal data with dropout for which there is little other software available in the public domain. The implementation of OSWALD is described through analysis of a psychiatric clinical trial that compares antidepressant effects in an elderly depressed sample and a simulation study. In the simulation study, three different dropout mechanisms: completely random dropout (CRD), random dropout (RD) and informative dropout (ID), are considered and the results from using OSWALD are compared across mechanisms. The parameter estimates for ID-simulated data show less bias with OSWALD under the ID missing data assumption than under the CRD or RD assumptions. Under an ID mechanism, OSWALD does not provide standard error estimates. We supplement OSWALD with a bootstrap procedure to derive the standard errors. This report illustrates the usage of OSWALD for analyzing longitudinal data with dropouts and how to draw appropriate conclusions based on the analytic results under different assumptions regarding the dropout mechanism.  ",
    "keywords": [
      "oswald",
      "non-ignorable missing",
      "longitudinal data analyses",
      "simulation"
    ]
  },
  {
    "id": "753",
    "title": "A Survey of NAT Behavior Discovery in VoIP Applications",
    "abstract": "Because of the foreseeing depletion of Internet Protocol (IP) addresses, Network Address Translation (NAT) is ubiquitously deployed to allow hosts to connect to the Internet through a single shared public IP address, which is a popular approach in deploying wireless local area network (WLAN). Although NAT proves to work well with traditional client/server applications, its existence and non-standard behaviors are the major problem which cripples voice over IP (VoIP) applications. In addition to some efforts which attempt to devise complicated protocols to tackle all NAT varieties, there are also efforts in Internet communities trying to standardize the behaviors of NAT. Therefore, it becomes crucial for a network device to discover the existence of NAT in its subnet and to determine the NAT behaviors, so that it can choose the optimal NAT traversal mechanisms to apply. In this paper, we surveyed the divergent NAT behaviors and then proposed a simplified NAT behavior discovery approach which is more suitable for VoIP applications. The proposed approach can reduce the call establishment time of VoIP applications, which is useful in scenarios where VoIP devices are administrated within a specific domain, e.g., 3G cellular networks.",
    "keywords": [
      "nat",
      "stun",
      "nat behavior discovery"
    ]
  },
  {
    "id": "754",
    "title": "An automorphic approach to verification pattern generation for SoC design verification using port-order fault model",
    "abstract": "Embedded cores are being increasingly used in the design of large system-on-a-chip (SoC). Because of the high complexity of SoC, the design verification is a challenge for system integrators. To reduce the verification complexity, the port-order fault (POF) model was proposed. It has been used for verifying core-based designs and the corresponding verification pattern generation has been developed. Here, the authors present an automorphic technique to improve the efficiency of the automatic verification pattern generation (AVPG) for SoC design verification based on the POF model. On average, the size of pattern sets obtained on the ISCAS-85 and MCNC benchmarks are 45 % smaller and the run time decreases 16 % as compared with the previous results of AVPG.",
    "keywords": [
      "automatic verification pattern generation ",
      "automorphism",
      "characteristic vector ",
      "port-order fault ",
      "soc",
      "superset of all automorphism ",
      "verification"
    ]
  },
  {
    "id": "755",
    "title": "Data page layouts for relational databases on deep memory hierarchies",
    "abstract": "Relational database systems have traditionally optimized for I/O performance and organized records sequentially on disk pages using the N-ary Storage Model (NSM) (a.k.a., slotted pages). Recent research, however, indicates that cache utilization and performance is becoming increasingly important on modern platforms. In this paper, we first demonstrate that in-page data placement is the key to high cache performance and that NSM exhibits low cache utilization on modern platforms. Next, we propose a new data organization model called PAX (Partition Attributes Across), that significantly improves cache performance by grouping together all values of each attribute within each page. Because PAX only affects layout inside the pages, it incurs no storage penalty and does not affect I/O behavior. According to our experimental results (which were obtained without using any indices on the participating relations), when compared to NSM: (a) PAX exhibits superior cache and memory bandwidth utilization, saving at least 75% of NSM's stall time due to data cache accesses; (b) range selection queries and updates on memory-resident relations execute 17-25% faster; and (c) TPC-H queries involving I/O execute 11-48% faster. Finally, we show that PAX performs well across different memory system designs.",
    "keywords": [
      "relational data placement",
      "disk page layout",
      "cache-conscious database systems"
    ]
  },
  {
    "id": "756",
    "title": "A soft computing approach to projecting locational marginal price",
    "abstract": "The increased deregulation of electricity markets in most nations of the world in recent years has made it imperative that electricity utilities design accurate and efficient mechanisms for determining locational marginal price (LMP) in power systems. This paper presents a comparison of two soft computing-based schemes: Artificial neural networks and support vector machines for the projection of LMP. Our system has useful power system parameters as inputs and the LMP as output. Experimental results obtained suggest that although both methods give highly accurate results, support vector machines slightly outperform artificial neural networks and do so with manageable computational time costs.",
    "keywords": [
      "locational marginal price",
      "artificial neural networks",
      "support vector machines",
      "back propagation learning algorithm",
      "radial basis function"
    ]
  },
  {
    "id": "757",
    "title": "Composite Right/Left Handed Artificial Transmission Line Structures in CMOS for Controlled Insertion Phase at 30 GHz",
    "abstract": "Two CMOS integrated circuits are presented that utilize metamaterial composite right/left handed (CRLH) transmission lines (TLs) for zero insertion phase at 30 GHz. Specifically, 2 and 3 unit cell structures are presented with controlled insertion phase that is achieved by cascading lumped element capacitors and spiral inductors in an LC network configuration defining the TL unit cells. Furthermore, the fixed TL structures suggest the possibility of zero, advanced or delayed insertion phases by element variation, or by the use of simple active components. Simulation and measured results are in good agreement with CRLH TL theory. and display a linear insertion phase and flat group delay values that are dependent on the number of unit cells with an insertion loss of similar to 0.8 dB per cell. These findings suggest that such high speed CRLH TLs structures can he implemented for linear array feeding networks and compact antenna designs in CMOS at millimeter wave frequencies. .",
    "keywords": [
      "composite right and left handed ",
      "transmission lines ",
      "right-handed ",
      "left-handed ",
      "group dealy ",
      "metal-insulator-metal ",
      "complementary metal oxide semiconductor "
    ]
  },
  {
    "id": "758",
    "title": "On the hardness of offline multi-objective optimization",
    "abstract": "It has been empirically established that multiobjective evolutionary algorithms do not scale well with the number of conflicting objectives. This paper shows that the convergence rate of all comparison-based multi-objective algorithms, for the Hausdorff distance, is not much better than the convergence rate of the random search under certain conditions. The number of objectives must be very moderate and the framework should hold the following assumptions: the objectives are conflicting and the computational cost is lower bounded by the number of comparisons is a good model. Our conclusions are: (i) the number of conflicting objectives is relevant (ii) the criteria based on comparisons with random-search for multi-objective optimization is also relevant (iii) having more than 3-objectives optimization is very hard. Furthermore, we provide some insight into cross-over operators.",
    "keywords": [
      "theory",
      "randomized search heuristics",
      "multi-objective optimization"
    ]
  },
  {
    "id": "759",
    "title": "The spatial market of business advice and consultancy to SMEs",
    "abstract": "This paper demonstrates the existence of spatial markets for business advice services. A large sample of 3245 clientadvisor links is investigated using GIS software. Seventy per cent of links are less than 25 km in extent, 93% are to the nearest local business centre, and only a few are with hinterlands or areas peripheral to main centres. The maximum reach of market areas varies by advisor type, averaging only 2540 km for chambers of commerce and public sector advice services such as Business Link. The maximum reach is 48 km for accountants and banks, and increases to 64 km for customers and suppliers and 74 km for consultants. A threshold for regional level services in major centres can be identified, which ranges from 12,000 to 24,000 businesses in size, depending on advisor type. Service sector firms are generally more localised than manufacturing, and local sourcing of advisors generally declines with firm size and size of business centre. Regional differences are relatively small, but Scotland, Yorkshire and Humberside are the most self-contained for advice, whilst London and the South-East are the least self-contained. This is a contrast to earlier findings by OFarrell and others. The paper demonstrates a hierarchical and spatial market structure for business advice services that is similar to that in retailing, with firm size and advisor type being the primary influence on differences in demand, and with regional centres most distinct from local centres of supply. Intense localised sourcing of advice from customers and suppliers does not appear to be frequent.",
    "keywords": [
      "consultancy",
      "business services",
      "local networks",
      "agglomeration",
      "gis",
      "business link",
      "chambers of commerce"
    ]
  },
  {
    "id": "760",
    "title": "A calculus for stochastic QoS analysis",
    "abstract": "The issue of Quality of Service (QoS) performance analysis in packet-switched networks has drawn a lot of attention in the networking community. There is a lot of work including an elegant theory under the name of network calculus, which focuses on analysis of deterministic worst case QoS performance bounds. In the meantime, researchers have studied stochastic QoS performance for specific schedulers. However, most previous works on deterministic QoS analysis or stochastic QoS analysis have only considered a server that provides deterministic service, i.e.deterministically bounded rate service. Few have considered the behavior of a stochastic server that provides input flows with variable rate service, for example wireless links. In this paper, we propose a stochastic network calculus to analyze the end-to-end stochastic QoS performance of a system with stochastically bounded input traffic over a series of deterministic and stochastic servers. We also prove that a server serving an aggregate of flows can be regarded as a stochastic server for individual flows within the aggregate. Based on this, the proposed framework is further applied to analyze per-flow stochastic QoS performance under aggregate scheduling.",
    "keywords": [
      "network calculus",
      "quality of service",
      "generalized stochastically bounded burstiness ",
      "stochastic service curve"
    ]
  },
  {
    "id": "761",
    "title": "Dynamic consistency for stochastic optimal control problems",
    "abstract": "For a sequence of dynamic optimization problems, we aim at discussing a notion of consistency over time. This notion can be informally introduced as follows. At the very first time stept 0, the decision maker formulates an optimization problem that yields optimal decision rules for all the forthcoming time stepst 0,t 1,,T; at the next time stept 1, he is able to formulate a new optimization problem starting at timet 1 that yields a new sequence of optimal decision rules. This process can be continued until the final timeT is reached. Afamily of optimization problems formulated in this way is said to be dynamically consistent if the optimal strategies obtained when solving the original problem remain optimal for all subsequent problems. The notion of dynamic consistency, well-known in the field of economics, has been recently introduced in the context of risk measures, notably by Artzner et al. (Ann. Oper. Res. 152(1):522, 2007) and studied in the stochastic programming framework by Shapiro (Oper. Res. Lett. 37(3):143147, 2009) and for Markov Decision Processes (MDP) by Ruszczynski (Math. Program. 125(2):235261, 2010). We here link this notion with the concept of state variable in MDP, and show that a significant class of dynamic optimization problems are dynamically consistent, provided that an adequate state variable is chosen.",
    "keywords": [
      "stochastic optimal control",
      "dynamic consistency",
      "dynamic programming",
      "risk measures"
    ]
  },
  {
    "id": "762",
    "title": "Multiobjective optimization of expensive-to-evaluate deterministic computer simulator models",
    "abstract": "Many engineering design optimization problems contain multiple objective functions all of which are desired to be minimized, say. This paper proposes a method for identifying the Pareto Front and the Pareto Set of the objective functions when these functions are evaluated by expensive-to-evaluate deterministic computer simulators. The method replaces the expensive function evaluations by a rapidly computable approximator based on a Gaussian process (GP) interpolator. It sequentially selects new input sites guided by values of an improvement function given the current data. The method introduced in this paper provides two advances in the interpolator/improvement framework. First, it proposes an improvement function based on the modified maximin fitness function which is known to identify well-spaced non-dominated outputs when used in multiobjective evolutionary algorithms. Second, it uses a family of GP models that allows for dependence among output function values but which permits zero covariance should the data be consistent with this model. A closed-form expression is derived for the improvement function when there are two objective functions; simulation is used to evaluate it when there are three or more objectives. Examples from the multiobjective optimization literature are presented to show that the proposed procedure can improve substantially previously proposed statistical improvement criteria for the computationally intensive multiobjective optimization setting.",
    "keywords": [
      "computer experiment",
      "gaussian process",
      "kriging",
      "pareto optimization",
      "nonseparable gp model",
      "computer simulator model"
    ]
  },
  {
    "id": "763",
    "title": "Early Activation and Induction of Apoptosis in T Cells Is Independent of c-Fos",
    "abstract": "We used c-Fos-deficient activated T cells from the spleen and c-Fos-deficient thymocytes to address the capacity of these cells to undergo apoptosis in response to various stimuli. To determine the role of c-Fos in apoptosis regulation in thymocytes, we challenged thymocytes from wild-type and c-Fos-deficient mice with either TPA or the glucocorticoid dexamethasone. After various time points cells were stained according to the Nicoletti method and analyzed by FACS. Thymocytes from both genotypes exhibited similar efficiency of apoptosis in response to treatment with TPA or dexamethasone. Our data provide clear evidence that c-Fos is not required for apoptosis regulation in activated T cells as well as in thymocytes.",
    "keywords": [
      "c-fos",
      "apoptosis",
      "early activation",
      "induction",
      "t cells"
    ]
  },
  {
    "id": "764",
    "title": "A shear-deformable beam element for the analysis of laminated composites",
    "abstract": "A 21 degree-of-freedom element, based on the FSDT, is derived to study the response of unsymmetrically laminated composite structures subject to both static and dynamic problems. In the FSDT model used here we have employed an accurate model to obtain the transverse shear correction factor. The dynamic version of the principle of virtual work for laminated composites is expressed in its nondimensional form and the element tangent stiffness and mass matrices are obtained using analytical integration. The element consists of four equally spaced nodes and a node at the middle. The results for the one-dimensional case are within 5% when compared to equivalent one and two-dimensional problems of static loading, free vibrations and buckling loads.",
    "keywords": [
      "nonlinear finite element",
      "condensation",
      "shear deformation",
      "laminated composites"
    ]
  },
  {
    "id": "765",
    "title": "Identifying potential adverse effects using the web: A new approach to medical hypothesis generation",
    "abstract": "Medical message boards are online resources where users with a particular condition exchange information, some of which they might not otherwise share with medical providers. Many of these boards contain a large number of posts and contain patient opinions and experiences that would be potentially useful to clinicians and researchers. We present an approach that is able to collect a corpus of medical message board posts, de-identify the corpus, and extract information on potential adverse drug effects discussed by users. Using a corpus of posts to breast cancer message boards, we identified drug event pairs using co-occurrence statistics. We then compared the identified drug event pairs with adverse effects listed on the package labels of tamoxifen, anastrozole, exemestane, and letrozole. Of the pairs identified by our system, 7580% were documented on the drug labels. Some of the undocumented pairs may represent previously unidentified adverse drug effects.",
    "keywords": [
      "data mining",
      "information extraction",
      "medical message board",
      "drug adverse effect"
    ]
  },
  {
    "id": "766",
    "title": "Analytic hierarchy based policy design method (AHPo) for solving societal problems that require a multifaceted approach",
    "abstract": "This paper proposes an AHP based statistical method for the design of a comprehensive policy alternative, AHPo, for solving societal problems that require a multifaceted approach. In the proposed method, criteria relevant to the goal or focus are structured in the same way as in the conventional AHP. However, these two methods are quite different in regard to the method of quantification. The new method predicts or analyses the impact of the policy alternatives on the overall goal. In other words, it predicts or rationalizes the way people appreciate the situation in which an alternative is adopted and implemented. It will serve as a tool for supporting (especially political) decision making.",
    "keywords": [
      "analytic hierarchy process ",
      "analytic network process ",
      "multicriteria decision",
      "policy design",
      "household adoption of seismic hazard adjustments"
    ]
  },
  {
    "id": "767",
    "title": "growing a language environment with editor libraries",
    "abstract": "Large software projects consist of code written in a multitude of different (possibly domain-specific) languages, which are often deeply interspersed even in single files. While many proposals exist on how to integrate languages semantically and syntactically, the question of how to support this scenario in integrated development environments (IDEs) remains open: How can standard IDE services, such as syntax highlighting, outlining, or reference resolving, be provided in an extensible and compositional way, such that an open mix of languages is supported in a single file? Based on our library-based syntactic extension language for Java, SugarJ, we propose to make IDEs extensible by organizing editor services in editor libraries . Editor libraries are libraries written in the object language, SugarJ, and hence activated and composed through regular import statements on a file-by-file basis. We have implemented an IDE for editor libraries on top of SugarJ and the Eclipse-based Spoofax language workbench. We have validated editor libraries by evolving this IDE into a fully-fledged and schema-aware XML editor as well as an extensible Latex editor, which we used for writing this paper.",
    "keywords": [
      "library",
      "language workbench",
      "dsl embedding",
      "language extensibility"
    ]
  },
  {
    "id": "768",
    "title": "Assessment of the classification capability of prediction and approximation methods for HRV analysis",
    "abstract": "The goal of this paper is to examine the classification capabilities of various prediction and approximation methods and suggest which are most likely to be suitable for the clinical setting. Various prediction and approximation methods are applied in order to detect and extract those which provide the better differentiation between control and patient data, as well as members of different age groups. The prediction methods are local linear prediction, local exponential prediction, the delay times method, autoregressive prediction and neural networks. Approximation is computed with local linear approximation, least squares approximation, neural networks and the wavelet transform. These methods are chosen since each has a different physical basis and thus extracts and uses time series information in a different way.",
    "keywords": [
      "heart rate variability",
      "prediction",
      "approximation",
      "mean error",
      "cardiogram classification",
      "ecg"
    ]
  },
  {
    "id": "769",
    "title": "hyperspeech",
    "abstract": "Hyperspeech is a speech-only hypermedia application that explores issues of speech user interfaces, navigation, and system architecture in a purely audio environment without a visual display. The system uses speech recognition input and synthetic speech feedback to aid in navigating through a database of digitally recorded speech segments.",
    "keywords": [
      "conversational interfaces",
      "speech applications",
      "speech synthesis",
      "speech recognition",
      "speech user interfaces",
      "speech as data",
      "hypermedia"
    ]
  },
  {
    "id": "770",
    "title": "Outline of a centralised multihop ad hoc wireless network",
    "abstract": "A concept of a multihop ad hoc network and associated algorithms for adaptive clustering in wireless ad hoc networks are presented in this paper. The algorithms take into account the connectivity of the stations as well as the quality of service requirements. The concept of a centralised ad hoc network is adopted, in which a cluster is defined by a Central Controller granting access to the radio interface to all terminals in its cluster. By these means the CC contributes to provide quality of service guarantees to the users. This concept is also used in the HiperLAN/2 (HL/2) Home Environment Extension (HEE), an ad hoc wireless LAN standardised by the European Telecommunications Standardisation Institute (ETSI). The HEE is restricted to one single cluster. It is shown in this article how the network can be extended over several clusters by the introduction of so-called forwarding stations. These forwarders interconnect the clusters and enable multihop connections of users roaming in different clusters. A solution is presented to ensure, as far as possible, an interconnection of clusters by means of the clustering algorithm.",
    "keywords": [
      "ad hoc networks",
      "clustering",
      "forwarding",
      "routing",
      "hiperlan/2",
      "mobility management",
      "handover"
    ]
  },
  {
    "id": "771",
    "title": "An architecture for security-oriented perfective maintenance of legacy software",
    "abstract": "This work presents an implementation strategy which exploits the separation of concerns and reuse in a multi-tier architecture to improve the security (availability, integrity, and confidentiality) level of an existing application. Functional properties are guaranteed via wrapping of the existing software modules. Security mechanisms are handled by the business logic of the middle-tier: availability and integrity are achieved via replication of the functional modules and the confidentiality is obtained via cryptography. The technique is presented with regard to a case study application. We believe that our experience can be used as a guideline for software practitioners to solve similar problems. We thus describe the conceptual model behind the architecture, discuss implementation issues, and present technical solutions.",
    "keywords": [
      "security",
      "perfective maintenance",
      "legacy software",
      "corba",
      "replication"
    ]
  },
  {
    "id": "772",
    "title": "Current and future models for nursing e-journals: making the most of the webs potential",
    "abstract": "We are presently witnessing an increasing number of nursing, medical and health-related electronic journals (e-journals) being made available on the World Wide Web, a minority of which are specifically devoted to informatics. We would expect, given the potential of interacting multimedia and computer-mediated communications (i.e. telematics), to also see an increasing diversity of models, but this is not currently the case. Following a brief discussion of some of the issues relevant to electronic publications, the authors present a taxonomy of current nursing e-journal models, including discussion of some examples from around the world that fall into categories within this taxonomy. We describe the model and levels of usage of one particular e-journal, Nursing Standard Online. Some of the issues presented may account for the current relative paucity of high quality content and innovative models in the development of Web-based e-journals for nurses and other health professionals. We believe it likely that nursing e-journals using current models will need to be specialist rather than generalist if they are to attract a larger audience. In concluding our paper, we advocate the development of innovative and increasingly interactive nursing e-journals as the way forward, discussing one particular model which holds promise.",
    "keywords": [
      "nursing",
      "publishingtrends ",
      "medical informaticseducation ",
      "online systems",
      "computer communication networks",
      "peer review"
    ]
  },
  {
    "id": "773",
    "title": "Trust management in vehicular ad hoc network: a systematic review",
    "abstract": "The basis of vehicular ad hoc networks (VANETs) is the exchange of data between entities, and making a decision on received data/event is usually based on information provided by other entities. Many researchers utilize the concept of trust to assess the trustworthiness of the received data. Nevertheless, the lack of a review to sum up the best available research on specific questions on trust management in vehicular ad hoc networks is sensible. This paper presents a systematic literature review to provide comprehensive and unbiased information about various current trust conceptions, proposals, problems, and solutions in VANETs to increase quality of data in transportation. For the purpose of the writing of this paper, a total of 111 articles related to the trust model in VANETs published between 2005 and 2014 were extracted from the most relevant scientific sources (IEEE Computer Society, ACM Digital Library, Springer Link, Science Direct, and Wiley Online Library). Finally, ten articles were eventually analyzed due to several reasons such as relevancy and comprehensiveness of discussion presented in the articles. Using the systematic method of review, this paper succeeds to reveal the main challenges and requirements for trust in VANETs and future research within this scope.",
    "keywords": [
      "systematic literature review",
      "trust management",
      "vanet",
      "trust metric"
    ]
  },
  {
    "id": "774",
    "title": "QSPR study of the Henry's Law constant for hydrocarbons",
    "abstract": "We establish a QSPR inodel between the Henry's Law constant in the air - water system and the molecular structure of 150 aliphatic hydrocarbons. The simultaneous linear regression analyzes on 1086 numerical descriptors reflecting topological, geometrical, and electronic aspects lead to a seven parameter equation that, when compared to previously reported models, exhibits good calibration and cross-validated parameters: R=0.996, R(1 - 10% - o)=0.997. As a realistic application, we employ this relationship to estimate the partition coefficient for 39 non-yet measured chemicals.  ",
    "keywords": [
      "qspr theory",
      "molecular descriptors",
      "multivariable regression analysis",
      "henry's law constant",
      "replacement method"
    ]
  },
  {
    "id": "775",
    "title": "sleep transistor distribution in row-based mtcmos designs",
    "abstract": "The Multi-Threshold CMOS (MTCMOS) technology has become a popular technique for standby power reduction. This technology utilizes high-Vth sleep transistors to reduce sub threshold leakage currents during the standby mode of CMOS VLSI Circuits. The performance of MTCMOS circuits strongly depends on the size of the sleep transistors and the parasitics on the virtual ground network. Given a placed net list of a row-based MTCMOS design and the number of sleep transistor cells on each standard cell row, this paper introduces an optimal algorithm for linearly placing the allocated sleep transistors on each standard cell row so as to minimize the performance degradation of the MTCMOS circuit, which is in part due to unwanted voltage drops on its virtual ground network. Experimental results show that, compared to existing methods of placing the sleep transistors on cell rows, the proposed technique results in up to 11% reduction in the critical path delay of the circuit.",
    "keywords": [
      "mtcmos",
      "leakage minimization",
      "placement"
    ]
  },
  {
    "id": "776",
    "title": "Learning the mean: A neural network approach",
    "abstract": "One of the key problems in machine learning theory and practice is setting the correct value of the regularization parameter; this is particularly crucial in Kernel Machines such as Support Vector Machines, Regularized Least Square or Neural Networks with Weight Decay terms. Well known methods such as Leave-One-Out (or GCV) and Evidence Maximization offer a way of predicting the regularization parameter. This work points out the failure of these methods for predicting the regularization parameter when coping with the, apparently trivial and here introduced, regularized mean problem; this is the simplest form of Tikhonov regularization, that, in turn, is the primal form of the learning algorithm Regularized Least Squares. This controlled environment gives the possibility to define oracular notions of regularization and to experiment new methodologies for predicting the regularization parameter that can be extended to the more general regression case. The analysis stems from JamesStein theory, shows the equivalence of shrinking and regularization and is carried using multiple kernels learning for regression and SVD analysis; a mean value estimator is built, first via a rational function and secondly via a balanced neural network architecture suitable for estimating statistical quantities and gaining symmetric expectations. The obtained results show that a non-linear analysis of the sample and a non-linear estimation of the mean obtained by neural networks can be profitably used to improve the accuracy of mean value estimations, especially when a small number of realizations is provided.",
    "keywords": [
      "model selection",
      "regularization",
      "mean problem",
      "back-propagation",
      "multiple kernel learning",
      "jamesstein theory",
      "svd",
      "shrinkage"
    ]
  },
  {
    "id": "777",
    "title": "clustering by pattern similarity in large data sets",
    "abstract": "Clustering is the process of grouping a set of objects into classes of similar objects. Although definitions of similarity vary from one clustering model to another, in most of these models the concept of similarity is based on distances, e.g., Euclidean distance or cosine distance. In other words, similar objects are required to have close values on at least a set of dimensions. In this paper, we explore a more general type of similarity. Under the pCluster model we proposed, two objects are similar if they exhibit a coherent pattern on a subset of dimensions. For instance, in DNA microarray analysis, the expression levels of two genes may rise and fall synchronously in response to a set of environmental stimuli. Although the magnitude of their expression levels may not be close, the patterns they exhibit can be very much alike. Discovery of such clusters of genes is essential in revealing significant connections in gene regulatory networks. E-commerce applications, such as collaborative filtering, can also benefit from the new model, which captures not only the closeness of values of certain leading indicators but also the closeness of (purchasing, browsing, etc.) patterns exhibited by the customers. Our paper introduces an effective algorithm to detect such clusters, and we perform tests on several real and synthetic data sets to show its effectiveness.",
    "keywords": [
      "collaborative-filtering",
      "applications",
      "analysis",
      "definition",
      "express",
      "concept",
      "object",
      "browse",
      "general",
      "response",
      "group",
      "model",
      "paper",
      "gene regulatory network",
      "exploration",
      "coherence",
      "pattern",
      "distance",
      "values",
      "custom",
      "process",
      "data",
      "discoveries",
      "similarity",
      "algorithm",
      "connection",
      "effect",
      "microarray",
      "class",
      "cluster"
    ]
  },
  {
    "id": "778",
    "title": "Measuring the performance of underplatform dampers for turbine blades by rotating laser Doppler Vibrometer",
    "abstract": "Underplatform friction dampers are commonly used to control the vibration level of turbine blades in order to prevent high-cycle fatigue failures. Experimental validation of highly non-linear response predictions obtained from FEM bladed disk models incorporating underplatform dampers models has proved to be very difficult so as the assessment of the performance of a chosen design. In this paper, the effect of wedge-shaped underplatform dampers on the dynamics of a simple bladed disk under rotating conditions is measured and the effect of the excitation level on the UPDs performances is investigated at different number of the engine order excitation nearby resonance frequencies of the 1st blade bending modes of the system. The measurements are performed with an improved configuration of a rotating test rig, designed with a non-contact magnetic excitation and a non-contact rotating SLDV measurement system.",
    "keywords": [
      "friction damping",
      "laser vibrometry",
      "bladed disks",
      "experimental mechanics",
      "non-linear dynamics",
      "underplatform dampers"
    ]
  },
  {
    "id": "779",
    "title": "Failure analysis of R/C columns using a triaxial concrete model",
    "abstract": "Inelastic failure analysis of concrete structures has been one of the central issues in concrete mechanics. Especially, the effect of confinement has been of great importance to capture the transition from brittle to ductile fracture of concrete under triaxial loading scenarios. Moreover, it has been a challenge to implement numerically material descriptions, which are susceptible to loss of stability and localization. In this article, a novel triaxial concrete model is presented, which captures the full spectrum of triaxial stress and strain histories in reinforced concrete structures. Thereby, inelastic dilatation is controlled by a non-associated flow rule to attain realistic predictions of inelastic volume change at various confinement levels. Different features of distributed and localized failure of the concrete model are examined under confined compression, uniaxial tension, pure shear, and simple shear. The performance at the structural level is illustrated with the example of a reinforced concrete column subjected to combined axial and transverse loading.",
    "keywords": [
      "triaxial concrete model",
      "elasto-plastic hardening/softening",
      "localization properties in tension",
      "compression and shear",
      "r/c column subject to axial loading and shearing"
    ]
  },
  {
    "id": "780",
    "title": "toward sophisticated detection with distributed triggers",
    "abstract": "Recent research has proposed efficient protocols for distributed triggers, which can be used in monitoring infrastructures to maintain system-wide invariants and detect abnormal events with minimal communication overhead. To date, however, this work has been limited to simple thresholds on distributed aggregate functions like sums and counts. In this paper, we present our initial results that show how to use these simple threshold triggers to enable sophisticated anomaly detection in near-real time, with modest communication overheads. We design a distributed protocol to detect \"unusual traffic patterns\" buried in an Origin-Destination network flow matrix that: a) uses a Principal Components Analysis decomposition technique to detect anomalies via a threshold function on residual signals [10]; and b) efficiently tracks this threshold function in near-real time using a simple distributed protocol. In addition, we speculate that such simple thresholding can be a powerful tool for a variety of monitoring tasks beyond the one presented here, and we propose an agenda to explore additional sophisticated applications.",
    "keywords": [
      "distributed triggers",
      "anomaly detection",
      "pca"
    ]
  },
  {
    "id": "781",
    "title": "Fully abstract trace semantics for protected module architectures",
    "abstract": "Formalises A+I A + I : an assembly language extended with protected module architectures  an isolation mechanism found in emerging processors. Presents two trace semantics for A+I A + I programs and proves that both are fully abstract w.r.t. the operational semantics. Details which problems arise when considering readout and writeout labels in the trace semantics of A+I A + I programs.",
    "keywords": [
      "fully abstract semantics",
      "trace semantics",
      "untyped assembly language",
      "protected modules architectures",
      "formal languages"
    ]
  },
  {
    "id": "782",
    "title": "A new inversion method for (T2, D) 2D NMR logging and fluid typing",
    "abstract": "One-dimensional nuclear magnetic resonance (1D NMR) logging technology has some significant limitations in fluid typing. However, not only can two-dimensional nuclear magnetic resonance (2D NMR) provide some accurate porosity parameters, but it can also identify fluids more accurately than 1D NMR. In this paper, based on the relaxation mechanism of (T2, D) 2D NMR in a gradient magnetic field, a hybrid inversion method that combines least-squares-based QR decomposition (LSQR) and truncated singular value decomposition (TSVD) is examined in the 2D NMR inversion of various fluid models. The forward modeling and inversion tests are performed in detail with different acquisition parameters, such as magnetic field gradients (G) and echo spacing (TE) groups. The simulated results are discussed and described in detail, the influence of the above-mentioned observation parameters on the inversion accuracy is investigated and analyzed, and the observation parameters in multi-TE activation are optimized. Furthermore, the hybrid inversion can be applied to quantitatively determine the fluid saturation. To study the effects of noise level on the hybrid method and inversion results, the numerical simulation experiments are performed using different signal-to-noise-ratios (SNRs), and the effect of different SNRs on fluid typing using three fluid models are discussed and analyzed in detail.",
    "keywords": [
      "two-dimensional nmr  logging",
      "transverse relaxation time ",
      "diffusion coefficient ",
      "fluid typing"
    ]
  },
  {
    "id": "783",
    "title": "TESTING COLLABORATIVE STRATEGIES BY COMPUTATIONAL SIMULATION - COGNITIVE AND TASK EFFECTS",
    "abstract": "A theory of communication between autonomous agents should make testable predictions about which communicative behaviors are collaborative, and provide a framework for determining the features of a communicative situation that affect whether a behavior is collaborative. The results presented here are derived from a two-phase empirical method. First, we analyze a corpus of naturally occurring problem-solving dialogues in order to identify potentially collaborative communicative strategies. Second, we experimentally test hypotheses that arise from the corpus analysis in Design-World, an experimental environment for simulating dialogues. The results indicate that collaborative behaviors must be defined relative to the cognitive limitations of the agents and the cognitive demands of the task. The method of computational simulation provides an additional empirical basis for theories of human-computer collaboration.",
    "keywords": [
      "collaboration",
      "communication",
      "simulation"
    ]
  },
  {
    "id": "784",
    "title": "Pedestrian navigation aids: information requirements and design implications",
    "abstract": "Recent years have seen an increased interest in navigational services for pedestrians. To ensure that these services are successful, it is necessary to understand the information requirements of pedestrians when navigating, and in particular, what information they need and how it is used. A requirements study was undertaken to identify these information requirements within an urban navigation context. Results show that landmarks were by far the most predominant navigation cue, that distance information and street names were infrequently used, and that information is used to enable navigation decisions, but also to enhance the pedestrians confidence and trust. The implications for the design of pedestrian navigation aids are highlighted.",
    "keywords": [
      "design",
      "navigation",
      "pedestrian",
      "requirements",
      "wayfinding"
    ]
  },
  {
    "id": "785",
    "title": "Cognitive Representation of a Complex Motor Action Executed by Different Motor Systems",
    "abstract": "The present study evaluates the cognitive representation of a kicking movement performed by a human and a humanoid robot, and how they are represented in experts and novices of soccer and robotics, respectively. To learn about the expertise-dependent development of memory structures, we compared the representation structures of soccer experts and robot experts concerning a human and humanoid robot kicking movement. We found different cognitive representation structures for both expertise groups under two different motor performance conditions (human vs. humanoid robot). In general, the expertise relies on the perceptual-motor knowledge of the human motor system. Thus, the soccer experts cognitive representation of the humanoid robot movement is dominated by their representation of the corresponding human movement. Additionally, our results suggest that robot experts, in contrast to soccer experts, access functional features of the technical system of the humanoid robot in addition to their perceptual-motor knowledge about the human motor system. Thus, their perceptual-motor and neuro-functional machine representation are integrated into a cognitive representation of the humanoid robot movement.",
    "keywords": [
      "neuro-functional machine representation",
      "perceptual-motor representation",
      "expertise",
      "motor system",
      "humanoid robot",
      "human movement"
    ]
  },
  {
    "id": "786",
    "title": "Postmarketing surveillance based on electronic patient records: The IPCI project",
    "abstract": "Researchers claim that data in electronic patient records can be used for a variety of purposes including individual patient care, management, and resource planning for scientific research. Our objective in the project Integrated Primary Care Information (IPCI) was to assess whether the electronic patient records of Dutch general practitioners contain sufficient data to perform studies in the area of postmarketing surveillance studies, We determined the data requirements for postmarketing surveillance studies, implemented additional software in the electronic patient records of the general practitioner, developed an organization to monitor the use of data, and performed validation studies to test the quality of the data. Analysis of the data requirements showed that additional software had to be installed to collect data that is not recorded in routine practice. To avoid having to obtain informed consent from each enrolled patient, we developed IPCI as a semianonymous system: both patients and participating general practitioners are anonymous for the researchers. Under specific circumstances, the researcher can contact indirectly (through a trusted third party) the physician that made the data available. Only the treating general practitioner is able to decode the identity of his patients. A Board of Supervisors predominantly consisting of participating general practitioners monitors the use of data. Validation studies show the data can be used for postmarketing surveillance. With additional software to collect data not normally recorded in routine practice, data from electronic patient record of general practitioners can be used for postmarketing surveillance.",
    "keywords": [
      "postmarketing surveillance",
      "electronic patient record",
      "general practitioner",
      "validation"
    ]
  },
  {
    "id": "787",
    "title": "Efficient algorithms for multichromosomal genome rearrangements",
    "abstract": "Hannenhalli and Pevzner (36th Annual Symposium on Foundations of Computer Science, Milwaukee, WI, IEEE Computer Soc. Press, Los Alamitos, CA, 1995, p. 581) gave a polynomial time algorithm for computing the minimum number of reversals, translocations, fissions, and fusions, that would transform one multichromosomal genome to another when both have the same set of genes without repeats. We fixed some problems with the construction: (1) They claim it can exhibit such a sequence of steps, but there was a gap in the construction. (2) Their construction had an asymmetry in the number of chromosomes in the two genomes, whereby forward scenarios could have fissions but not fusions. We also improved the speed by combining the algorithm with the algorithm of Bader et al. (J. Comput. Biol. 8 (5) (2001) 483) that computes reversal distances for permutations in linear time.",
    "keywords": [
      "genome rearrangements",
      "fusion",
      "fission",
      "translocation",
      "reversal",
      "inversion",
      "breakpoint graph",
      "genes",
      "chromosomes",
      "homology"
    ]
  },
  {
    "id": "788",
    "title": "eye-gaze interfaces using electro-oculography (eog)",
    "abstract": "Using electro-oculography (EOG), two types of eye-gaze interfaces have been developed; \"EOG Pointer\" and \"EOG Switch\". The former enables a user to move a computer cursor or to control a machine using only eye-gaze, regardless of drifting signal and blinking artifacts. In contrast, the latter output an ON/OFF signal only. Although it has the least simple function, it enables every user easily to turn ON/OFF a nurse-call device or to send one bit signal to a personal computer with high stability and reliability. Since the EOG Switch was commercialized in 2003, it has been widely used among amyotrophic lateral sclerosis (ALS) patients in Japan.",
    "keywords": [
      "electro-oculography ",
      "dc amplifier",
      "ac amplifier",
      "eye-gaze interface"
    ]
  },
  {
    "id": "789",
    "title": "Creational Object-oriented Design Pattern Applied to the Development of Software Tools for Electric Power Systems Dynamic Simulations",
    "abstract": "The development of software for dynamic simulation of electrical power systems requires a comprehensive range of complex studies, which encompasses many areas of electrical engineering as well as software engineering. This study aims at to develop an efficient strategy applied to the development of software tools for dynamic power systems simulations studies. The proposed strategy is based on the object-oriented creational pattern. This approach has the advantage of makes easy the application development process, by performing a mapping between block diagram model representation and the corresponding specialized classes. Firstly, a conceptual mapping between block diagram and the object-oriented paradigm, based on the Factory Method, is carried out. After that, some flexible strategies are presented in order to obtain an improved efficiency for the numerical routines, based on the Builder standard. This allows for the parameterization of the selected numerical integration techniques. The proposed strategy was evaluated by using a 4-generator multi-machine power system. The simulation results shown that the proposed strategy was able to provides a good power system dynamic performance.",
    "keywords": [
      "design patterns",
      "factory method",
      "mapping",
      "builder",
      "object-oriented",
      "dynamic simulation",
      "numerical routines",
      "power systems"
    ]
  },
  {
    "id": "790",
    "title": "Algorithm for faster computation of non-zero graph based invariants",
    "abstract": "This paper presents a detailed study of the graph based algorithm used to generate geometric moment invariant functions. The graph based algorithm has been found to suffer from high computational complexity. One major cause of this problem is that the algorithm generates too many graphs that produce zero moment invariant functions. Hence, we propose an algorithm to determine and eliminate the zero moment invariant generating graphs and thereby generate non-zero moment invariant functions with reduced computational complexity. The correctness of the algorithm has been verified and discussed with suitable induction proofs and sample graphs. Asymptotic analysis has been presented to clearly illustrate the reduction in computational complexity achieved by the proposed algorithm. It has been found and illustrated with examples that the computational time for identifying non-zero invariants could be largely reduced with the help of our proposed algorithm.",
    "keywords": [
      "computational complexity",
      "geometric moments",
      "image transforms",
      "orthogonal moments",
      "moment invariants"
    ]
  },
  {
    "id": "791",
    "title": "Removing photography artifacts using gradient projection and flash-exposure sampling",
    "abstract": "Flash images are known to suffer from several problems: saturation of nearby objects, poor illumination of distant objects, reflections of objects strongly lit by the flash and strong highlights due to the reflection of flash itself by glossy surfaces. We propose to use a flash and no-flash (ambient) image pair to produce better flash images. We present a novel gradient projection scheme based on a gradient coherence model that allows removal of reflections and highlights from flash images. We also present a brightness-ratio based algorithm that allows us to compensate for the falloff in the flash image brightness due to depth. In several practical scenarios, the quality of flash/no-flash images may be limited in terms of dynamic range. In such cases, we advocate using several images taken under different flash intensities and exposures. We analyze the flash intensity-exposure space and propose a method for adaptively sampling this space so as to minimize the number of captured images for any given scene. We present several experimental results that demonstrate the ability of our algorithms to produce improved flash images.",
    "keywords": [
      "flash",
      "reflection removal",
      "gradient projection",
      "flash-exposure sampling",
      "high dynamic range  imaging"
    ]
  },
  {
    "id": "792",
    "title": "A stable 3D energetic Galerkin BEM approach for wave propagation interior problems",
    "abstract": "We consider 3D interior wave propagation problems with vanishing initial and mixed boundary conditions, reformulated as a system of two boundary integral equations with retarded potentials. These latter are then set in a weak form, based on a natural energy identity satisfied by the solution of the differential problem, and discretized by the energetic Galerkin boundary element method. Numerical results are presented and discussed in order to show the stability and accuracy of the proposed technique.",
    "keywords": [
      "wave propagation",
      "boundary integral equation",
      "energetic galerkin boundary element method"
    ]
  },
  {
    "id": "793",
    "title": "a differential notion of place for local search",
    "abstract": "For extracting the characteristics a specific geographic entity, and notably a place, we propose to use dynamic Extreme Tagging Systems in combination with the classic approach of static KR models like ontologies, thesauri and gazetteers. Indeed, we argue that in local search , the what that is queried is implicitly about places. However existing knowledge representation (KR) models, such as ontologies based on logical theories, conceptual spaces, affordance or other, cannot capture in isolation all aspects of the meaning of a place. Therefore we propose to use a combination of them based on the underlying notion of differences , linked elements of meaning without commitment to any KR model. Mapping to elements of different KR models can be made later to follow the requirements of a given task, supported by a KR representation of the elements that support this task. We show the usefulness of the approach for local search by applying it to the notion of place defined as a location that supports a homogeneous affordance field , i.e. the spatial area which allows me the do a particular thing, while allowing the homogeneity of movement , meaning that the previous field is not interrupted by any boundaries.",
    "keywords": [
      "local search",
      "image schemata",
      "ai",
      "multi-representation",
      "similarity",
      "affordances",
      "knowledge representation",
      "wordnet",
      "conceptual spaces",
      "differences",
      "extreme tagging"
    ]
  },
  {
    "id": "794",
    "title": "Innovation outsourcing: Risks and quality issues",
    "abstract": "Innovation is the creation of new idea, practice, object, or even product by an individual or company. A competitive organization needs to continuously offer new line of products and services to the market for their customers. In order to cut down their R&D costs, companies seek external or even global vendors to pursue their research and development (R&D) tasks. This paper discusses the issues related to innovation outsourcing, including uncertainty, risks, productivity and quality issues.",
    "keywords": [
      "innovation",
      "outsourcing",
      "quality",
      "productivity",
      "risks"
    ]
  },
  {
    "id": "795",
    "title": "Firm orientation, community of practice, and Internet-enabled interfirm communication: Evidence from Chinese firms",
    "abstract": "What motivates firms to develop Internet-enabled interfirm communication? We draw upon the work of Alavi et al. (2005-2006) and propose that the use of the Internet in interfirm communication is influenced by a firm's firm orientation and its internal communities of practice. Based on data collected from 307 international trade firms in the Beijing area, we find that Internet-enabled interfirm communication is directly driven by internal community of practices and customer orientation, and indirectly by competitor orientation and learning orientation. The internal community of practice is affected by learning orientation and competitor orientation, but not by customer orientation. The present study contributes to the literature by providing empirical investigation on firm's strategic communications from the perspective of firm orientations, delineating how different firm orientations vary in impacting firm's strategic communications, and exploring the bridging effect of communities of practices on the influences of firm orientations on knowledge management initiatives. ",
    "keywords": [
      "firm orientation",
      "learning orientation",
      "customer orientation",
      "competitor orientation",
      "community of practice",
      "internet-enabled interfirm communication"
    ]
  },
  {
    "id": "796",
    "title": "Index of a point of 3-D digital binary image and algorithm for computing its Euler characteristic",
    "abstract": "In this work a concept of index of a point of a 3-D (26, 6) digital image is defined. Basing on this concept a new characterization of the so-called simple points [1] as well as an algorithm for computing Euler characteristics of 3-D (26, 6) digital pictures is proposed.  ",
    "keywords": [
      "digital picture",
      "index of a point",
      "euler number",
      "invariant transformation"
    ]
  },
  {
    "id": "797",
    "title": "Fluxing botnet command and control channels with URL shortening services",
    "abstract": "URL shortening services (USSes), which provide short aliases to registered long URLs, have become popular owing to Twitter. Despite their popularity, researchers do not carefully consider their security problems. in this paper, we explore botnet models based on USSes to prepare for new security threats before they evolve. Specifically, we consider using USSes for alias flux to hide botnet command and control (C&C) channels. In alias flux, a botmaster obfuscates the IP addresses of his C&C servers, encodes them as URLs, and then registers them to USSes with custom aliases generated by an alias generation algorithm. Later, each bot obtains the encoded IP addresses by contacting USSes using the same algorithm. For USSes that do not support custom aliases, the botmaster can use shared alias lists instead of the shared algorithm. DNS-based botnet detection schemes cannot detect an alias flux botnet, and network-level detection and blacklisting of the fluxed aliases are difficult. We also discuss possible countermeasures to cope with these new threats and investigate operating USSes.  ",
    "keywords": [
      "botnet",
      "dns",
      "domain flux",
      "url shortening service"
    ]
  },
  {
    "id": "798",
    "title": "a generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval",
    "abstract": "Opinion retrieval is a task of growing interest in social life and academic research, which is to find relevant and opinionate documents according to a user's query. One of the key issues is how to combine a document's opinionate score (the ranking score of to what extent it is subjective or objective) and topic relevance score. Current solutions to document ranking in opinion retrieval are generally ad-hoc linear combination, which is short of theoretical foundation and careful analysis. In this paper, we focus on lexicon-based opinion retrieval. A novel generation model that unifies topic-relevance and opinion generation by a quadratic combination is proposed in this paper. With this model, the relevance-based ranking serves as the weighting factor of the lexicon-based sentiment ranking function, which is essentially different from the popular heuristic linear combination approaches. The effect of different sentiment dictionaries is also discussed. Experimental results on TREC blog datasets show the significant effectiveness of the proposed unified model. Improvements of 28.1% and 40.3% have been obtained in terms of MAP and p@10 respectively. The conclusion is not limited to blog environment. Besides the unified generation model, another contribution is that our work demonstrates that in the opinion retrieval task, a Bayesian approach to combining multiple ranking functions is superior to using a linear combination. It is also applicable to other result re-ranking applications in similar scenario.",
    "keywords": [
      "generation model",
      "opinion generation model",
      "opinion retrieval",
      "topic relevance",
      "sentiment analysis"
    ]
  },
  {
    "id": "799",
    "title": "Anomalous behavior of the pulse transfer characteristic of a selectively doped AlxGa1?xAs/GaAs heterostructure containing deep traps",
    "abstract": "The pulse transfer characteristic of a normal selectively doped AlxGa1?xAs/GaAs heterostructure containing deep traps in the AlxGa1?xAs layer is considered. It is shown that these deep traps are responsible for an undershoot in the drain-source current at the end of a positive voltage pulse applied to the gate (the pulse voltage is measured from the initial gate bias) and the trap depth can be determined from this undershoot.",
    "keywords": [
      "selectively doped heterostucture",
      "high electron mobility transistor",
      "transfer characteristic",
      "deep trap"
    ]
  },
  {
    "id": "800",
    "title": "Wireless image sensor networks: event acquisition in attack-prone and uncertain environments",
    "abstract": "Wireless Image Sensor Networks (WISNs) consisting of untethered camera nodes and sensors may be deployed in a variety of unattended and possibly hostile environments to obtain surveillance data. In such settings, the WISN nodes must perform reliable event acquisition to limit the energy, computation and delay drains associated with forwarding large volumes of image data wirelessly to a sink node. In this work we investigate the event acquisition properties of WISNs that employ various techniques at the camera nodes to distinguish between event and non-event frames in uncertain environments that may include attacks. These techniques include lightweight image processing, decisions from n sensors with/without cluster head fault and attack detection, and a combination approach relying on both lightweight image processing and sensor decisions. We analyze the relative merits and limitations of each approach in terms of the resulting probability of event detection and false alarm in the face of occasional errors, attacks and stealthy attacks.",
    "keywords": [
      "image sensor networks",
      "lightweight event acquisition",
      "sensor network security"
    ]
  },
  {
    "id": "801",
    "title": "Constructing Comprehensive Summaries of Large Event Sequences",
    "abstract": "Event sequences capture system and user activity over time. Prior research on sequence mining has mostly focused on discovering local patterns appearing in a sequence. While interesting, these patterns do not give a comprehensive summary of the entire event sequence. Moreover, the number of patterns discovered can be large. In this article, we take an alternative approach and build short summaries that describe an entire sequence, and discover local dependencies between event types. We formally define the summarization problem as an optimization problem that balances shortness of the summary with accuracy of the data description. We show that this problem can be solved optimally in polynomial time by using a combination of two dynamic-programming algorithms. We also explore more efficient greedy alternatives and demonstrate that they work well on large datasets. Experiments on both synthetic and real datasets illustrate that our algorithms are efficient and produce high-quality results, and reveal interesting local structures in the data.",
    "keywords": [
      "algorithms",
      "experimentation",
      "theory",
      "event sequences",
      "summarization",
      "log mining"
    ]
  },
  {
    "id": "802",
    "title": "Combining data remapping and voltage/frequency scaling of second level memory for energy reduction in embedded systems",
    "abstract": "In this paper we show that the energy reductions obtained from using two techniques, data remapping (DR) and voltage/frequency scaling of off-chip bus and memory, combine to provide interesting trade offs between energy, execution time and power. Both methods aim to reduce the energy consumed by the memory subsystem. DR is a fully automatic compile time technique applicable to pointer-intensive dynamic applications. Voltage/frequency scaling of off-chip memory is a technique applied at the hardware level. When combined together, energy reductions can be as high as 49.45%. The improvements are verified in the context of three OLDEN pointer-centric benchmarks, namely Perimeter, Health and TSP.",
    "keywords": [
      "low power",
      "embedded systems",
      "energy model",
      "voltage/frequency scaling",
      "compiler optimizations"
    ]
  },
  {
    "id": "803",
    "title": "Benefits and Barriers of User Evaluation in Software Engineering Research",
    "abstract": "In this paper, we identify trends about, benefits from, and barriers to performing user evaluations in software engineering research. From a corpus of over 3,000 papers spanning ten years, we report on various subtypes of user evaluations (e.g., coding tasks vs. questionnaires) and relate user evaluations to paper topics (e.g., debugging vs. technology transfer). We identify the external measures of impact, such as best paper awards and citation counts, that are correlated with the presence of user evaluations. We complement this with a survey of over 100 researchers from over 40 different universities and labs in which we identify a set of perceived barriers to performing user evaluations.",
    "keywords": [
      "experimentation",
      "human factors",
      "human study",
      "user evaluation"
    ]
  },
  {
    "id": "804",
    "title": "Quasi-one-dimensional approximation in the HMO model of polymethine dyes",
    "abstract": "In the framework of the Hueckel molecular orbital (HMO) model, an analytical method has been elaborated which enables calculation of energy levels and wave functions for polymethine dye molecules with arbitrary end groups characterized by two effective additive parameters. The method represents a generalization of the known long-chain approximation (LCA) manipulating only frontier pi-MOs and yields analytical relations for molecular characteristics based on all occupied dye pi-MOs.",
    "keywords": [
      "polymethine compounds",
      "long-chain approximation",
      "quasi-one-dimensional approximation",
      "green's functions",
      "atomic charges",
      "bond orders"
    ]
  },
  {
    "id": "805",
    "title": "Ontology Combined Structural and Operational Semantics for Resource-Oriented Service Composition",
    "abstract": "Resource-oriented Services recently become an enabling technology to integrate and configure information from different heterogeneous systems so as to meet ever-changing environment which not only need the concepts for entities but also require the semantics for operations. By the aim of combining structural and operational semantics agilely, a Semantic Resource Service Model (SRSM) is proposed. Firstly, SRSM describes Entity-Oriented and Transition-Oriented Resource by semantic meta-model which contains data structures and operation semantics. Secondly, by describing structural semantics for Entity-Oriented Resource, heterogonous inputs/outputs of a service can be automatically matched. Thirdly, by describing operational semantics for Transition-Oriented Resource, the service composition sequence can be inferred after ontology reasoning. Then, both Entity-Oriented and Transition-Oriented Resources are encapsulated into composited RESTful service. At last, a case study and several comparisons are applied in a prototype system. The result shows that the proposed approach provides a flexible way for resource-oriented service composition.",
    "keywords": [
      "structural semantic",
      "operational semantic",
      "ontology",
      "restful service",
      "resource-oriented architecture",
      "entity-oriented resource",
      "transition-oriented resource"
    ]
  },
  {
    "id": "806",
    "title": "Analysis of sidewall quality in through-wafer deep reactive-ion etching",
    "abstract": "The quality of channel sidewalls resulting from through-wafer deep reactive-ion etching is analysed using scanning electron microscopy, atomic-force microscopy and interferometry. Sidewall quality and profile are highly dependent on the width of the etched channel. Channels narrower than 100 ?m show generally good sidewall smoothness, though with a bowed profile. This profile leads to ion-induced damage towards the bottom of the channel sidewall. Wider channels, in contrast, exhibit overpassivation of the sidewalls with a region of thick polymer build-up followed by vertical striations and a very rough surface, but with an overall vertical profile. Redeposition of the passivation from the trench bottom to the sidewalls as suggested by other researchers is supported by our observations.",
    "keywords": [
      "deep reactive-ion etching",
      "mems",
      "fluorocarbon redeposition",
      "sidewall morphology"
    ]
  },
  {
    "id": "807",
    "title": "Design, modelling and analysis of a six component force balance for hypervelocity wind tunnel testing",
    "abstract": "A combination of modelling and analysis techniques was used to design a six component force balance. The balance was designed specifically for the measurement of impulsive aerodynamic forces and moments characteristic of hypervelocity shock tunnel testing using the stress wave force measurement technique. Aerodynamic modelling was used to estimate the magnitude and distribution of forces and finite element modelling to determine the mechanical response of proposed balance designs. Simulation of balance performance was based on aerodynamic loads and mechanical responses using convolution techniques. Deconvolution was then used to assess balance performance and to guide further design modifications leading to the final balance design.",
    "keywords": [
      "force balance design",
      "force measurement",
      "finite element modelling",
      "deconvolution",
      "shock tunnel",
      "hypersonic"
    ]
  },
  {
    "id": "808",
    "title": "Towards a pivotal-based approach for business process alignment",
    "abstract": "This article focuses on business process engineering, especially on alignment between business analysis and implementation. Through a business process management approach, different transformations interfere with process models in order to make them executable. To keep the consistency of process model from business model to IT model, we propose a pivotal metamodel-centric methodology. It aims at keeping or giving all requisite structural and semantic data needed to perform such transformations without loss of information. Through this we can ensure the alignment between business and IT. This article describes the concept of pivotal metamodel and proposes a methodology using such an approach. In addition, we present an example and the resulting benefits.",
    "keywords": [
      "business process engineering",
      "metamodelling",
      "transformation",
      "alignment"
    ]
  },
  {
    "id": "809",
    "title": "Ensembles of relational classifiers",
    "abstract": "Relational classification aims at including relations among entities into the classification process, for example taking relations among documents such as common authors or citations into account. However, considering more than one relation can further improve classification accuracy. Here we introduce a new approach to make use of several relations as well as both, relations and local attributes for classification using ensemble methods. To accomplish this, we present a generic relational ensemble model that can use different relational and local classifiers as components. Furthermore, we discuss solutions for several problems concerning relational data such as heterogeneity, sparsity, and multiple relations. Especially the sparsity problem will be discussed in more detail. We introduce a new method called PRNMultiHop that tries to handle this problem. Furthermore we categorize relational methods in a systematic way. Finally, we provide empirical evidence, that our relational ensemble methods outperform existing relational classification methods, even rather complex models such as relational probability trees (RPTs), relational dependency networks (RDNs) and relational Bayesian classifiers (RBCs).",
    "keywords": [
      "relational data mining",
      "ensemble classification",
      "sparse graphs",
      "relational autocorrelation"
    ]
  },
  {
    "id": "810",
    "title": "3D feature surface properties and their application in geovisualization",
    "abstract": "New acquisition methods have increased the availability of surface property data that capture location-dependent data on feature surfaces. However, these data are not supported as fully in the geovisualization of the Digital City as established data categories such as feature attributes, 2D rasters, or geometry. Consequently, 3D surface properties are largely excluded from the information extraction and knowledge creation process of geovisualization despite their potential for being an effective tool in many such tasks. To overcome this situation, this paper examines the benefits of a better integration into geovisualization systems in terms of two examples and discusses technological foundations for surface property support. The main contribution is the identification of computer graphics techniques as a suitable basis for such support. This way, the processing of surface property data fits well into existing visualization systems. This finding is demonstrated through an interactive prototypic visualization system that extends an existing system with surface property support. While this prototype concentrates on technology and neglects user-related and task-related aspects, the paper includes a discussion on challenges for making surface properties accessible to a wider audience.",
    "keywords": [
      "geovisualization",
      "exploratory data analysis",
      "3d surface properties",
      "textures",
      "computer graphics",
      "gpu"
    ]
  },
  {
    "id": "811",
    "title": "An optimization of the icosahedral grid modified by spring dynamics",
    "abstract": "We have investigated an optimum form of the modified icosahedral grid that is generated by applying the spring dynamics to the standard icosahedral grid System. The spring dynamics can generate a more homogeneous grid system than the standard icosahedral grid system by tuning the natural spring lenght: as the natural spring length becomes longer, the ratio of maximum grid interval to minimum one becomes closer to unit. When the natural spring length is larger than a critical value, however, the spring dynamic system does not have a stable equilibrium. By setting the natural spring length to be the marginally critical value, we can obtain the most homogeneous grid system, which is most efficient in terms of the CFL condition. We have analyzed eigenmodes involved in the initial error of the geostrophic balance problem [test case 2 of D. L. Williamson et al. (1992, J. Comput. Phys. 102, 211)]. Since the balance state in the discrete system differs slightly from the exact solution of the analytic system, the initial error field includes both the gravity wave mode and the Rossby wave mode. As the results of the analysis are based on Hough harmonics decompositions, we detected Rossby and gravity wave modes with zonal wavenumber 5, which are asymmetric against the equator. These errors are associated with icosahedral grid structure. The symmetric gravity wave mode with zonal wavenumber 0 also appears in the error field. To clarify the evolution of Rossby waves, we introduce divergence damping to reduce the gravity wave mode. From the simulated results of the geostrophic problem with various grid systems, we found that the spuriously generated Rossby wave mode is eliminated most effectively when the most homogeneously distributed grid system is used. It is therefore, concluded that the most homogeneous grid system is the best choice from the viewpoint of numerical accuracy as well as computational efficiency. ",
    "keywords": [
      "shallow water model",
      "icosahedral grid",
      "spring dynamics",
      "climate model"
    ]
  },
  {
    "id": "812",
    "title": "design and implementation of the glue-nail database system",
    "abstract": "We describe the design and implementation of the Glue-Nail database system. The Nail language is a purely declarative query language; Glue is a procedural language used for non-query activities. The two languages combined are sufficient to write a complete application. Nail and Glue code both compile into the target language IGlue. The Nail compiler uses variants of the magic sets algorithm, and supports well-founded models. Static optimization is performed by the Glue compiler using techniques that include peephole methods and data flow analysis. The IGlue code is executed by the IGlue interpreter, which features a run-time adaptive optimizer. The three optimizers each deal with separate optimization domains, and experiments indicate that an effective synergism is achieved. The Glue-Nail system is largely complete and has been tested using a suite of representative applications.",
    "keywords": [
      "activation",
      "applications",
      "design",
      "domain",
      "experience",
      "timing",
      "model",
      "writing",
      "data flow analysis",
      "method",
      "adapt",
      "systems",
      "optimality",
      "procedure",
      "interpretation",
      "code",
      "language",
      "implementation",
      "compilation",
      "algorithm",
      "feature",
      "effect",
      "database",
      "completeness",
      "query"
    ]
  },
  {
    "id": "813",
    "title": "Analog integrated circuit for detection of an approaching object with simple-shape recognition based on lower animal vision",
    "abstract": "A network for the detection of an approaching object with simple-shape recognition is proposed based on lower animal vision. The locust can detect an approaching object through a simple process in the descending contralateral movement detector (DCMD) in the locust brain, by which the approach velocity and direction of the object is determined. The frog can recognize simple shapes through a simple process in the tectum and thalamus in the frog brain. The proposed network is constructed of simple analog complementary metal oxide semiconductor (CMOS) circuits. The integrated circuit of the proposed network is fabricated with the 1.2 mu m CMOS process. Measured results for the proposed circuit indicate that the approach velocity and direction of an object can be detected by the output current of the analog circuit based on the DCMD response. The shape of moving objects having simple shapes, such as circles, squares, triangles and rectangles, was recognized using the proposed frog-visual-system-based circuit.",
    "keywords": [
      "analog integrated circuit",
      "edge detection",
      "motion sensor",
      "shape recognition",
      "vision chip"
    ]
  },
  {
    "id": "814",
    "title": "The safety of electronic prescribing: manifestations, mechanisms, and rates of system-related errors associated with two commercial systems in hospitals",
    "abstract": "Objectives To compare the manifestations, mechanisms, and rates of system-related errors associated with two electronic prescribing systems (e-PS). To determine if the rate of system-related prescribing errors is greater than the rate of errors prevented. Methods Audit of 629 inpatient admissions at two hospitals in Sydney, Australia using the CSC MedChart and Cerner Millennium e-PS. System related errors were classified by manifestation (eg, wrong dose), mechanism, and severity. A mechanism typology comprised errors made: selecting items from drop-down menus; constructing orders; editing orders; or failing to complete new e-PS tasks. Proportions and rates of errors by manifestation, mechanism, and e-PS were calculated. Results 42.4% (n=493) of 1164 prescribing errors were system-related (78/100 admissions). This result did not differ by e-PS (MedChart 42.6% (95% CI 39.1 to 46.1); Cerner 41.9% (37.1 to 46.8)). For 13.4% (n=66) of system-related errors there was evidence that the error was detected prior to study audit. 27.4% (n=135) of system-related errors manifested as timing errors and 22.5% (n=111) wrong drug strength errors. Selection errors accounted for 43.4% (34.2/100 admissions), editing errors 21.1% (16.5/100 admissions), and failure to complete new e-PS tasks 32.0% (32.0/100 admissions). MedChart generated more selection errors (OR=4.17; p=0.00002) but fewer new task failures (OR=0.37; p=0.003) relative to the Cerner e-PS. The two systems prevented significantly more errors than they generated (220/100 admissions (95% CI 180 to 261) vs 78 (95% CI 66 to 91)). Conclusions System-related errors are frequent, yet few are detected. e-PS require new tasks of prescribers, creating additional cognitive load and error opportunities. Dual classification, by manifestation and mechanism, allowed identification of design features which increase risk and potential solutions. e-PS designs with fewer drop-down menu selections may reduce error risk.",
    "keywords": [
      "cpoe",
      "prescribing errors",
      "unintended consequences",
      "information technology",
      "clinical information systems"
    ]
  },
  {
    "id": "815",
    "title": "Sliding Windows and Persistence: An Application of Topological Methods to Signal Analysis",
    "abstract": "We develop in this paper a theoretical framework for the topological study of time series data. Broadly speaking, we describe geometrical and topological properties of sliding window embeddings, as seen through the lens of persistent homology. In particular, we show that maximum persistence at the point-cloud level can be used to quantify periodicity at the signal level, prove structural and convergence theorems for the resulting persistence diagrams, and derive estimates for their dependency on window size and embedding dimension. We apply this methodology to quantifying periodicity in synthetic data sets and compare the results with those obtained using state-of-the-art methods in gene expression analysis. We call this new method SW1PerS, which stands for Sliding Windows and 1-Dimensional Persistence Scoring.",
    "keywords": [
      "persistent homology",
      "time-delay embeddings",
      "periodicity",
      "primary secondary "
    ]
  },
  {
    "id": "816",
    "title": "Stochastic learning solution for distributed discrete power control game in wireless data networks",
    "abstract": "Distributed power control is an important issue in wireless networks. Recently, noncooperative game theory has been applied to investigate interesting solutions to this problem. The majority of these studies assumes that the transmitter power level can take values in a continuous domain. However, recent trends such as the GSM standard and Qualcomm's proposal to the IS-95 standard use a finite number of discretized power levels. This motivates the need to investigate solutions for distributed discrete power control which is the primary objective of this paper. We first note that, by simply discretizing, the previously proposed continuous power adaptation techniques will not suffice. This is because a simple discretization does not guarantee convergence and uniqueness. We propose two probabilistic power adaptation algorithms and analyze their theoretical properties along with the numerical behavior. The distributed discrete power control problem is formulated as an N-person, nonzero sum game. In this game, each user evaluates a power strategy by computing a utility value. This evaluation is performed using a stochastic iterative procedures. We approximate the discrete power control iterations by an equivalent ordinary differential equation to prove that the proposed stochastic learning power control algorithm converges to a stable Nash equilibrium. Conditions when more than one stable Nash equilibrium or even only mixed equilibrium may exist are also studied. Experimental results are presented for several cases and compared with the continuous power level adaptation solutions.",
    "keywords": [
      "game theory",
      "power control",
      "stochastic learning",
      "wireless networking"
    ]
  },
  {
    "id": "817",
    "title": "A Novel Faster-Than-at-Speed Transition-Delay Test Method Considering IR-Drop Effects",
    "abstract": "Interconnect defects such as weak resistive opens, shorts, and bridges increase the path delay affected by a pattern during manufacturing test but are not significant enough to cause a failure at functional frequency. In this paper, a new faster-than-at-speed method is presented for delay test pattern application to screen small delay defects. Given a test pattern set, the technique groups the patterns into multiple subsets with close path delay distribution and determines an optimal test frequency considering both positive slack and performance degradation due to IR-drop effects. Since, the technique does not increase the test frequency to an extent that any paths exercised at the rated functional frequency may fail, it avoids any scan flip-flop masking. As most semiconductor companies currently deploy compression technologies to reduce test costs, scan-cell masking is highly undesirable for pattern modification as it would imply pattern count increase and might result in pattern regeneration. Therefore, our solution is more practical as the test engineer can run the same pattern set without any changes to the test flow other than the at-speed test frequency.",
    "keywords": [
      "delay test",
      "supply noise",
      "test generation"
    ]
  },
  {
    "id": "818",
    "title": "A fuzzy MCDM approach for evaluating banking performance based on Balanced Scorecard",
    "abstract": "The paper proposed a Fuzzy Multiple Criteria Decision Making (FMCDM) approach for banking performance evaluation. Drawing on the four perspectives of a Balanced Scorecard (BSC), this research first summarized the evaluation indexes synthesized from the literature relating to banking performance. Then, for screening these indexes, 23 indexes fit for banking performance evaluation were selected through expert questionnaires. Furthermore, the relative weights of the chosen evaluation indexes were calculated by Fuzzy Analytic Hierarchy Process (FAHP). And the three MCDM analytical tools of SAW, TOPSIS, and VIKOR were respectively adopted to rank the banking performance and improve the gaps with three banks as an empirical example. The analysis results highlight the critical aspects of evaluation criteria as well as the gaps to improve banking performance for achieving aspired/desired level. It shows that the proposed FMCDM evaluation model of banking performance using the BSC framework can be a useful and effective assessment tool.",
    "keywords": [
      "fmcdm",
      "balance scorecard ",
      "fuzzy analytic hierarchy process ",
      "topsis",
      "vikor"
    ]
  },
  {
    "id": "819",
    "title": "Geographical information systems and location science",
    "abstract": "Since the 1970s the field of Geographical Information Systems (GIS) has evolved into a mature research and application area involving a number of academic fields including Geography, Civil Engineering, Computer Science, Land Use Planning, and Environmental Science. GIS can support a wide range of spatial queries that can be used to support location studies. GIS will play a significant role in future location model development and application. We review existing work that forms the interface between GIS and Location Science and discuss some of the potential research areas involving both GIS and Location Science. During the past 30 years there have been many developments in spatial data analysis, spatial data storage and retrieval, and mapping. Many of these developments have occurred in the field of Geographical Information Science. Geographical Information Systems software now supports many elementary and advanced spatial analytic approaches including the production of high quality maps. GIS will have a major impact on the field of Location Science in terms of model application and model development. The purpose of this paper is to explore the interface between the field of Location Science and GIS.",
    "keywords": [
      "geographical information systems ",
      "geographical information science",
      "facility location",
      "site selection"
    ]
  },
  {
    "id": "820",
    "title": "Introduction of r(m(rank))(2) metric incorporating rank-order predictions as an additional tool for validation of QSAR/QSPR models",
    "abstract": "In silica techniques involving the development of quantitative regression models have been extensively used for prediction of activity, property and toxicity of new chemicals. The acceptability and subsequent applicability of the models for predictions is determined based on several internal and external validation statistics. Among different validation metrics, Q(2) and R-pred(2) represent the classical metrics for internal validation and external validation respectively. Additionally, the r(m)(2) metrics introduced by Roy and coworkers have been widely used by several groups of authors to ensure the close agreement of the predicted response data with the observed ones. However, none of the currently available and commonly used validation metrics provides any information regarding the rank-order predictions for the test set. Thus, to incorporate the concept of ranking order predictions while calculating the common validation metrics originally using the Pearson's correlation coefficient-based algorithm, the new r(m(rank))(2) metric has been introduced in this work as a new variant of the r(m)(2) series of metrics. The ability of this new metric to perform the rank-order prediction is determined based on its application in judging the quality of predictions of regression - based quantitative structure-activity/property relationship (QSAR/QSPR) models for four different data sets. The different validation metrics calculated in each case were compared for their ability to reflect the rank-order predictions based on their correlation with the conventional Spearman's rank correlation coefficient. Based on the results of the sum of ranking differences analysis performed using the Spearman's rank correlation coefficient as the reference, it was observed that the (2)(m(rank)) metric exhibited the least difference in ranking from that of the reference metric. Thus, the close correlation of the (2)(m(rank)) metric with the Spearman's rank correlation coefficient inferred that the new metric could aptly perform the rank-order prediction for the test data set and can be utilized as an additional validation tool, besides the conventional metrics, for assessing the acceptability and predictive ability of a QSAR/QSPR model.  ",
    "keywords": [
      "qsar",
      "qspr",
      "qstr",
      "validation",
      "pearson's correlation coefficient",
      "spearman's rank correlation coefficient"
    ]
  },
  {
    "id": "821",
    "title": "Designing ubiquitous information systems for a community of homeless young people: precaution and a way forward",
    "abstract": "Drawing upon and distinguishing themselves from domestic, public, work, and natural settings, homeless communities offer new cultural frontiers into which ubiquitous computing could diffuse. We report on one such frontier, a community of homeless young people, located in Seattle, WA, seeking both to foresee the consequences of pervasive access to digital media and communications and to prepare for its seemingly inevitable uptake. The community consists of hundreds of young people living without stable housing, often in the public, and an alliance of nine service agencies that seek to stabilize youth and equip them to escape homelessness. We examine the opportunities for ubiquitous computing in this community by, in part, developing a precautionary stance on intervention. This stance is then used to critically examine a scenario in which information about the service agencies is made public. From this scenario, and a description of the social and material constraints of this community, we argue that precaution offers productive counsel on decisions on whether and how to intervene with ubiquitous computing. A precautionary point of view is especially important as ubiquitous computing diffuses into communities that, by their social and material conditions, are vulnerable. In such communities, the active avoidance of harms and plans for their mitigation is particularly important.",
    "keywords": [
      "homelessness",
      "poverty",
      "youth",
      "community informatics",
      "non-profit service agencies",
      "precautionary principle",
      "designer value",
      "value sensitive design",
      "value scenario",
      "envisioning"
    ]
  },
  {
    "id": "822",
    "title": "On the equivalence of two optimization methods for fuzzy linear programming problems",
    "abstract": "The paper analyses the linear programming problem with fuzzy coefficients in the objective function. The set of nondominated (ND) solutions with respect to an assumed fuzzy preference relation, according to Orlovsky's concept, is supposed to be the solution of the problem. Special attention is paid to unfuzzy nondominated (UND) solutions (the solutions which are nondominated to the degree one). The main results of the paper are sufficient conditions on a fuzzy preference relation allowing to reduce the problem of determining UND solutions to that of determining the optimal solutions of a classical linear programming problem. These solutions can thus be determined by means of classical linear programming methods.",
    "keywords": [
      "fuzzy programming",
      "linear programming",
      "fuzzy relation",
      "nondominated solution"
    ]
  },
  {
    "id": "823",
    "title": "Identifying failure causes in Java programs: An application of change impact analysis",
    "abstract": "During program maintenance, a programmer may make changes that enhance program functionality or fix bugs in code. Then, the programmer usually will run unit/regression tests to prevent invalidation of previously tested functionality. If a test fails unexpectedly, the programmer needs to explore the edit to find the failure-inducing changes for that test. Crisp uses results from Chianti, a tool that performs semantic change impact analysis [ 1], to allow the programmer to examine those parts of the edit that affect the failing test. Crisp then builds a compilable intermediate version of the program by adding a programmer-selected partial edit to the original code, augmenting the selection as necessary to ensure compilation. The programmer can reexecute the test on the intermediate version in order to locate the exact reasons for the failure by concentrating on the specific changes that were applied. In nine initial case studies on pairs of versions from two real Java programs, Daikon [ 2] and Eclipse jdt compiler [ 3], we were able to use Crisp to identify the failure-inducing changes for all but 1 of 68 failing tests. On average, 33 changes were found to affect each failing test ( of the 67), but only 1-4 of these changes were found to be actually failure-inducing.",
    "keywords": [
      "fault localization",
      "semantic change impact analysis",
      "edit change dependence",
      "regression testing",
      "intermediate versions of programs"
    ]
  },
  {
    "id": "824",
    "title": "Dexterous workspace optimization of an asymmetric six-degree of freedom Stewart-Gough platform type manipulator",
    "abstract": "In this paper, an asymmetric Generalized Stewart-Gough Platform (GSP) type parallel manipulator is designed by considering the type synthesis approach. The asymmetric six-Degree Of Freedom (DOE) manipulator optimized in this paper is selected among the GSPs classified under the name of 6D. The dexterous workspace optimization of Asymmetric parallel Manipulator with tEn Different Linear Actuator Lengths (AMEDIAL) subject to kinematics and geometric constraints is performed by using the Particle Swarm Optimization (PSO). The condition number and Minimum Singular Value (MSV) of homogenized Jacobian matrix are employed to obtain the dexterous workspace of AMEDLAL. Finally, the six-DOF AMEDLAL is also compared with the optimized Traditional Stewart-Gough Platform Manipulator (TSPM) considering the volume of the dexterous workspace in order to demonstrate its kinematic performance. Comparisons show that the manipulator proposed in this study illustrates better kinematic performance than TSPM.  ",
    "keywords": [
      "type synthesis",
      "singular values",
      "dexterous workspace",
      "pso and gsp"
    ]
  },
  {
    "id": "825",
    "title": "SOLVING SYSTEMS OF TWO-SIDED (MAX, MIN)-LINEAR EQUATIONS",
    "abstract": "A finite iteration method for solving systems of (max, min)-linear equations is presented. The systems have variables on both sides of the equations. The algorithm has polynomial complexity and may be extended to wider classes of equations with a similar structure.",
    "keywords": [
      "-linear equations",
      "two-sided system"
    ]
  },
  {
    "id": "826",
    "title": "Fuzzy association degree with delayed time in temporal data model",
    "abstract": "This paper presents an expression of the semantic proximity. Based on the temporal data model, a method of the temporal approximation is given. Using these concepts, this paper provides an evaluated method of fuzzy and dynamic association degree with delayed time and a superposition method of association degrees. Particularly, by means of the fuzzy and dynamic association degree, the connection between the weather data of two regions can be discovered.",
    "keywords": [
      "temporal data model",
      "fuzzy association degree",
      "delayed time",
      "weather forecast"
    ]
  },
  {
    "id": "827",
    "title": "On-line prediction of micro-turning multi-response variables by machine vision system using adaptive neuro-fuzzy inference system (ANFIS)",
    "abstract": "In this paper, a new attempt has been made in the area of tool-based micromachining for automated, non-contact, and flexible prediction of quality responses such as average surface roughness (R (a)), tool wear ratio (TWR) and metal removal rate (MRR) of micro-turned miniaturized parts through a machine vision system (MVS) which is integrated with an adaptive neuro-fuzzy inference system (ANFIS). The images of machined surface grabbed by the MVS could be extracted using the algorithm developed in this work, to get the features of image texture [average gray level (G (a))]. This work presents an area-based surface characterization technique which applies the basic light scattering principles used in other optimal optical measurement systems. These principles are applied in a novel fashion which is especially suitable for in-process prediction and control. The main objective of this study is to design an ANFIS for estimation of R (a), TWR, and MRR in micro-turning process. Cutting speed (S), feed rate (F), depth of cut (D), G (a) were taken as input parameters and R (a), TWR, MRR as the output parameters. The results obtained from the ANFIS model were compared with experimental values. It is found that the predicted values of the responses are in good agreement with the experimental values.",
    "keywords": [
      "micro-turning",
      "machine vision",
      "anfis",
      "surface roughness",
      "twr",
      "mrr"
    ]
  },
  {
    "id": "828",
    "title": "Gallium arsenide passivation method for the employment of High Electron Mobility Transistors in liquid environment",
    "abstract": "We report on effective prevention of GaAs corrosion in a cell culture liquid environment by means of polymerized (3-mercaptopropyl)-trimethoxysilane thin film coatings. Aging in physiological solution kept at 37C revealed no significant oxidation after 2weeks, which is the typical period of incubation of a neuron cells culture. The method was also applied to High Electron Mobility Transistors (HEMT) arrays with unmetallized gate regions, in view of their application as neural signal transducers. Significant reduction of the degradation of the HEMT behavior was obtained, as compared to uncoated HEMTs, with good channel modulation efficiency still after 30days aging.",
    "keywords": [
      "gaas",
      "surface passivation",
      "hemt",
      "biosensor"
    ]
  },
  {
    "id": "829",
    "title": "using shape distributions to compare solid models",
    "abstract": "Our recent work has described how to use feature and topology in-formation to compare 3-D solid models. In this work we describe a new method to compare solid models based on shape distributions. Shape distribution functions are common in the computer graphics and computer vision communities. The typical use of shape dis-tributions is to compare 2-D objects, such as those obtained from imaging devices (cameras and other computer vision equipment). Recent work has applied shape distribution metrics for compari-son of approximate models found in the graphics community, such as polygonal meshes, faceted representation, and Virtual Reality Modeling Language (VRML) models. This paper examines how to adapt these techniques to comparison of 3-D solid models, such as those produced by commercial CAD systems. We provide a brief review of shape matching with distribution functions and present an approach to matching solid models. First, we show how to ex-tend basic distribution-based techniques to handle CAD data that has been exported to VRML format. These extensions address specific geometries that occur in mechanical CAD data. Second, we describe how to use shape distributions to directly interrogate solid models. Lastly, we show how these techniques can be put together to provide a \"query by example\" interface to a large, het-erogeneous, CAD database: The National Design Repository. One significant contribution of our work is the systematic technique for performing consistent, engineering content-based comparisons of CAD models produced by different CAD systems.",
    "keywords": [
      "mesh",
      "use",
      "solid model databases",
      "3d search",
      "communities",
      "engine",
      "modelling language",
      "approximation",
      "design",
      "metrication",
      "addressing",
      "object",
      "topologies",
      "camera",
      "interfaces",
      "extensibility",
      "imaging",
      "graphics",
      " virtual reality ",
      "paper",
      "shape",
      "model",
      "representation",
      "shape matching",
      "computer vision",
      "comparisons",
      "computer graphics",
      "review",
      "shape recognition",
      "contention",
      "query-by-example",
      "method",
      "systems",
      "solid modeling",
      "device",
      "matching",
      "data",
      "consistency",
      "distributed",
      "repositories",
      "feature",
      "database"
    ]
  },
  {
    "id": "830",
    "title": "Navigation-based self-optimization handover mechanism for mobile relay stations in WiMAX networks",
    "abstract": "Organic Computing has similar characteristics of organism which can be self-adjustment for a variety of conditions. Moreover, during the wireless communication technological evolution progress, WiMAX (Worldwide Interoperability for Microwave Access) offers ability of high capacity and far distance transmission. WiMAX provides high-speed access and a coverage range across several kilometers, but the actual coverage range was merely a few kilometers due to the shelter of buildings or terrain. IEEE 802.16 working group designed 802.16j-based RS (Relay Station) to overcome above problem. In this paper, we present a mechanism called Self-Optimization Handover Mechanism. This mechanism is using GPS (Global Positioning System) navigation system to gather the related information for the position and combine the mobility characteristics of Mobile Relay Station. Especially, the concept of Self-Optimization of Organic Computing has been integrated into this mechanism. There are some advantages for this new mechanism, including: (1)The base station can provide advance plan and select the path. (2)The mechanism can reduce the number of possible handover and hop. (3)The mechanism can reduce the time of channel scan.",
    "keywords": [
      "802.16j",
      "handover",
      "mobile relay station",
      "gps navigation",
      "organic computing"
    ]
  },
  {
    "id": "831",
    "title": "Evolutionary computation approaches for real offshore wind farm layout: A case study in northern Europe",
    "abstract": "This paper presents new evolutionary computation algorithms for a problem of wind farm design. The algorithms tackle two different problems of offshore wind farm layout. Experiments in a real offshore wind farm layout case are shown and discussed.",
    "keywords": [
      "offshore wind farm design",
      "optimal layouts",
      "evolutionary computation",
      "real case study"
    ]
  },
  {
    "id": "832",
    "title": "Scalability comparison of Peer-to-Peer similarity search structures",
    "abstract": "Due to the increasing complexity of current digital data, similarity search has become a fundamental computational task in many applications. Unfortunately, its costs are still high and grow linearly on single server structures, which prevents them from efficient application on large data volumes. In this paper, we shortly describe four recent scalable distributed techniques for similarity search and study their performance in executing queries on three different datasets. Though all the methods employ parallelism to speed up query execution, different advantages for different objectives have been identified by experiments. The reported results would be helpful for choosing the best implementations for specific applications. They can also be used for designing new and better indexing structures in the future.",
    "keywords": [
      "similarity search",
      "scalability",
      "metric space",
      "distributed index structures",
      "peer-to-peer networks"
    ]
  },
  {
    "id": "833",
    "title": "Estimation of power quality indices in distributed generation systems during power islanding conditions",
    "abstract": "This paper presents a new, fast Modified Recursive GaussNewton (MRGN) method for the estimation of power quality indices in distributed generating systems during both islanding and non-islanding conditions. A forgetting factor weighted error cost function is minimized by the well known GaussNewton algorithm and the resulting Hessian matrix is approximated by ignoring the off-diagonal terms. This simplification produces a decoupled algorithm, for the fundamental and harmonic components and results in a large reduction of computational effort, when the power signal contains a large number of harmonics. Numerical experiments have shown that the proposed approach results in higher speed of convergence, accurate tracking of power signal parameters in the presence noise, waveform distortion, etc., which are suitable for the estimation of power quality indices. In the case of a distribution network, power islands occur when power supply from the main utility is interrupted due to faults or otherwise and the distributed generation system (DG) keeps supplying power into the network. Further, due to unbalanced load conditions the DG is subject to unbalanced voltages at its terminals and suffers from increased total harmonic distortion (THD). Thus, the power quality indices estimation, along with the power system frequency estimation will play a vital role in detecting power islands in distributed generating systems. Extensive studies, both on simulated and real, benchmark hybrid distribution networks, involving distributed generation systems reveal the effectiveness of the proposed approach to calculate the power quality indices accurately.",
    "keywords": [
      "power quality indices",
      "total harmonic distortion",
      "sequence voltages and currents",
      "power measurements",
      "distributed generation",
      "islanding condition"
    ]
  },
  {
    "id": "834",
    "title": "The Bud Scar-Based Screening System for Hunting Human Genes Extending Life Span",
    "abstract": "We developed a high-throughput screening system that allows identification of genes prolonging life span in the budding yeast Saccharomyces cerevisiae. The method is based on isolating yeast mother cells with an extended number of cell divisions as indicated by the increased number of bud scars on their surface. Fluorescently labeled wheat germ agglutinin (WGA) was used for specific staining of bud scars. Screening of a human HepG2 cDNA expression library in yeast resulted in the isolation of several yeast transformants with a potentially prolonged life span. The budding yeast S. cerevisiae, one of the favorite models used to study aging, has been studied extensively for the better understanding of the mechanisms of human aging. Because human disease genes often have yeast counterparts, they can be studied efficiently in this organism. One interesting example is the WRN gene, the human DNA helicase, which participates in the DNA repair pathway. The mutation of the WRN gene causes Werner syndrome showing premature-aging phenotype. Budding yeast contains WRN homologue, SGS1, and its mutation results in shortening yeast life span. The knowledge gained from the studies of budding yeast will benefit studies in humans for better understanding of aging and aging-related disease.",
    "keywords": [
      "aging",
      "budding yeast",
      "wga",
      "bud scar",
      "life span"
    ]
  },
  {
    "id": "835",
    "title": "Matching office firms types and location characteristics: An exploratory analysis using Bayesian classifier networks",
    "abstract": "While most models of location decisions of firms are based on the principle of utility maximizing behavior, the present study assumes that location decisions are just part of business cycle models, in which location is considered along other business decisions. The business model results in a series of location requirements and these are matched against location characteristics. Given this theoretical perspective, the modeling challenge then becomes how to find the match between firm types and the set of location characteristics using observations of the spatial distribution of firms. In this paper, several Bayesian classifier networks are compared in terms of their performance, using a large data set collected for the Netherlands. Results demonstrate that by taking relationships between predictor variables into account the Bayesian classifiers can improve prediction accuracy compared to commonly used decision tree. From a substantive point of view, our results indicate that different sets of urban characteristics and accessibility requirements are relevant to different office types as reflected in the spatial distribution of these office firms.",
    "keywords": [
      "office location",
      "bayesian classifier networks",
      "decision trees",
      "luti  models"
    ]
  },
  {
    "id": "836",
    "title": "mental models of recursive computations vs. recursive analysis in the problem domain",
    "abstract": "The work outlined here was inspired by [1, 3], where the authors analyze the mental models of recursion by looking at how students trace simple recursive computations. Besides trying to understand if their results generalize to a different context, I was interested to see the correlations between the mental models of the computation process and the ability to establish recursive relationships in the problem domain. My investigation essentially lends further support to the findings of [3]. However, a consistent mental model of recursive computations, although implied by the ability to use recursion in problem-solving, does not seem to be sufficient for the achievement of this higher-level skill.",
    "keywords": [
      "mental models",
      "programming learning",
      "recursion"
    ]
  },
  {
    "id": "837",
    "title": "Scaling up multi-touch selection and querying: Interfaces and applications for combining mobile multi-touch input with large-scale visualization displays",
    "abstract": "We present a mobile multi-touch interface for selecting, querying, and visually exploring data visualized on large, high-resolution displays. Although emerging large (e.g., ?10m wide), high-resolution displays provide great potential for visualizing dense, complex datasets, their utility is often limited by a fundamental interaction problem  the need to interact with data from multiple positions around a large room. Our solution is a selection and querying interface that combines a hand-held multi-touch device with 6 degree-of-freedom tracking in the physical space that surrounds the large display. The interface leverages context from both the user's physical position in the room and the current data being visualized in order to interpret multi-touch gestures. It also utilizes progressive refinement, favoring several quick approximate gestures as opposed to a single complex input in order to most effectively map the small mobile multi-touch input space to the large display wall. The approach is evaluated through two interdisciplinary visualization applications: a multi-variate data visualization for social scientists, and a visual database querying tool for biochemistry. The interface was effective in both scenarios, leading to new domain-specific insights and suggesting valuable guidance for future developers.",
    "keywords": [
      "multi-touch",
      "progressive refinement",
      "3d user interface",
      "mobile device",
      "3d tracking",
      "ray casting",
      "selection"
    ]
  },
  {
    "id": "838",
    "title": "Knowledge-based linguistic equations for defect detection through functional testing of printed circuit boards",
    "abstract": "Increasing globalization of the economy is imposing tough challenges to manufacturing companies. The ability to produce highly customized products, in order to satisfy market niches, requires the introduction of new features in automation systems. Flexible manufacturing processes must be able to handle unforeseen events, but their complexity makes the supervision and maintenance task difficult to perform by human operators. This paper describes how linguistic equations (LE), an intelligent method derived from Fuzzy Algorithms, has been used in a decision-helping tool for electronic manufacturing. In our case the company involved in the project is mainly producing control cards for the automotive industry. In their business, nearly 70% of the cost of a product is material cost. Detecting defects and repairing the printed circuit boards is therefore a necessity. With an ever increasing complexity of the products, defects are very likely to occur, no matter how much attention is put into their prevention. Therefore, the system described in this paper comes into use only during the final testing of the product and is purely oriented towards the detection and localization of defects. Final control is based on functional testing. Using linguistic equations and expert knowledge, the system is able to analyze that data and successfully detect and trace a defect in a small area of the printed circuit board. If sufficient amount of data is provided, self-tuning and self-learning methods can be used. Diagnosis effectiveness can therefore be improved from detection of a functional area towards component level analysis.",
    "keywords": [
      "linguistic equations",
      "defect detection",
      "diagnosis",
      "knowledge",
      "fuzzy logic"
    ]
  },
  {
    "id": "839",
    "title": "Dynamic neural network approach for tool cutting force modelling of end milling operations",
    "abstract": "This paper uses the artificial neural networks (ANNs) approach to evolve an efficient model for estimation of cutting forces, based on a set of input cutting conditions. Neural network (NN) algorithms are developed for use as a direct modelling method, to predict forces for ball-end milling operations. Prediction of cutting forces in ball-end milling is often needed in order to establish automation or optimization of the machining processes. Supervised NNs are used to successfully estimate the cutting forces developed during end milling processes. The training of the networks is preformed with experimental machining data. The predictive capability of using analytical and NN approaches is compared. NN predictions for three cutting force components were predicted with 4% error by comparing with the experimental measurements. Exhaustive experimentation is conduced to develop the model and to validate it. By means of the developed method, it is possible to forecast the development of events that will take place during the milling process without executing the tests. The force model can be used for simulation purposes and for defining threshold values in cutting tool condition monitoring system. It can be used also in the combination for monitoring and optimizing of the machining process-cutting parameters.",
    "keywords": [
      "machining",
      "cutting forces",
      "modeling",
      "neural network",
      "experimental measurements",
      "milling"
    ]
  },
  {
    "id": "840",
    "title": "Contexts built and found: a pilot study on the process of archival meaning-making",
    "abstract": "Over the last 20years, humanities and archival scholars have theorized the ways in which archives imbue records with meaning. However, archival scholars have not sufficiently examined how users understand the meaning of the records they find. Building on the premise that how users come to make meaning from records is greatly in need of examination, this paper reports on a pilot study of four book history students and their processes of archival meaning-making. We focus in particular on behaviors of an interpretive rather than forensic nature. This article includes a discussion of the theoretical concepts and scholarly literature that shaped our goals for this paper. It then discusses the methodology and our interpretations of the research findings, before turning to a discussion of the findings implications and directions for future work.",
    "keywords": [
      "meaning-making",
      "information use",
      "book history"
    ]
  },
  {
    "id": "841",
    "title": "Localizing web videos using social images",
    "abstract": "While inferring the geo-locations of web images has been widely studied, there is limited work engaging in geo-location inference of web videos due to inadequate labeled samples available for training. However, such a geographical localization functionality is of great importance to help existing video sharing websites provide location-aware services, such as location-based video browsing, video geo-tag recommendation, and location sensitive video search on mobile devices. In this paper, we address the problem of localizing web videos through transferring large-scale web images with geographic tags to web videos, where near-duplicate detection between images and video frames is conducted to link the visually relevant web images and videos. To perform our approach, we choose the trustworthy web images by evaluating the consistency between the visual features and associated metadata of the collected images, therefore eliminating the noisy images. In doing so, a novel transfer learning algorithm is proposed to align the landmark prototypes across both domains of images and video frames, leading to a reliable prediction of the geo-locations of web videos. A group of experiments are carried out on two datasets which collect Flickr images and YouTube videos crawled from the Web. The experimental results demonstrate the effectiveness of our video geo-location inference approach which outperforms several competing approaches using the traditional frame-level video geo-location inference.",
    "keywords": [
      "web video analysis",
      "cross-domain",
      "social media",
      "landmark recognition",
      "classification"
    ]
  },
  {
    "id": "842",
    "title": "biXid: A bidirectional transformation language for XML",
    "abstract": "Often, independent organizations de. ne and advocate different XML formats for a similar purpose and, as a result, application programs need to mutually convert between such formats. Existing XML transformation languages, such as XSLT and XDuce, are unsatisfactory for this purpose since we would have to write, e. g., two programs for the forward and the backward transformations in case of two formats, incur high developing and maintenance costs. This paper proposes the bidirectional XML transformation language biXid, allowing us to write only one program for both directions of conversion. Our language adopts a common paradigm programming-by-relation, where a program defines a relation over documents and transforms a document to another in a way satisfying this relation. Our contributions here are specific language features for facilitating realistic conversions whose target formats are loosely in parallel but have many discrepancies in details. Concretely, we ( 1) adopt XDuce-style regular expression patterns for describing and analyzing XML structures, ( 2) fully permit ambiguity for treating formats that do not have equivalent expressivenesses, and ( 3) allow non-linear pattern variables for expressing non-trivial transformations that cannot be written only with linear patterns, such as conversion between unordered and ordered data. We further develop an efficient evaluation algorithm for biXid, consisting of the \"parsing\" phase that transforms the input document to an intermediate \"parse tree\" structure and the \"unparsing\" phase that transforms it to an output document. Both phases use a variant of finite tree automata for performing a one-pass scan on the input or the parse tree by using a standard technique that \"maintains the set of all transitable states.\" However, the construction of the \"unparsing\" phase is challenging since ambiguity causes different ways of consuming the parse tree and thus results in multiple possible outputs that may have different structures. We have implemented a prototype system of biXid and confirmed that it has enough expressiveness and a linear-time performance from experiments with several realistic bidirectional transformations including one between vCard-XML and ContactXML.",
    "keywords": [
      "xml",
      "tree automata"
    ]
  },
  {
    "id": "843",
    "title": "real time self-maintenable data warehouse",
    "abstract": "Data warehousing is an approach to data integration wherein integrated information is stored in a data warehouse for direct querying and analysis. To provide fast access, a data warehouse stores materialized views of the sources of its data. As a result, a data warehouse needs to be maintained to keep its contents consistent with the contents of its data sources. Incremental maintenance is generally regarded as a more efficient way to maintain materialized views in a data warehouse. In this paper a strategy for the maintenance of data warehouse is presented. It has the following characteristics: it is self-maintainable (weak), incremental, non-blocking (the analysts transactions and the maintenance transaction are executed concurrently) and is performed in real time. The proposed algorithm is implemented for view definition SPJ (Select Project Join) queries and it calculates the aggregate functions: sum, avg, count, min and max. Aggregate functions are calculated like algebraic functions (the new result of the function can be computed using some small, constant size storage that accompanies the existing value of the aggregate). We have named this improved algorithm ?VNLTR (unlimited ?V (versions), NL (non-blocking), TR (in real time)).",
    "keywords": [
      "self-maintenable",
      "data warehouse"
    ]
  },
  {
    "id": "844",
    "title": "content-based tag generation to enable a tag-based collaborative tv-recommendation system.",
    "abstract": "With the application of the Web 2.0 philosophy to more and more online services and platforms, tagging has become a well established collaboration method. It is often used to simplify organization, navigation and discovery of information and resources in huge archives. In parallel, due to recent developments in digital television, audiences are confronted with a rising amount of available content and demand for better ways to discover programs of interest. In this paper, we propose a tagging-based solution to this problem. Using a content-based filtering approach, we present an individualized and flexible tag generation process. User specific as well as collaborative tag generation is enabled. Based on generated and user added tags, program recommendations are derived in a collaborative filtering step.",
    "keywords": [
      "metadata-based filtering",
      "tag generation",
      "folksonomy",
      "bayesian classifier",
      "tv recommender",
      "collaborative filtering",
      "recommendation system",
      "tags",
      "epg"
    ]
  },
  {
    "id": "845",
    "title": "The M/M/1 queue with inventory, lost sale, and general lead times",
    "abstract": "We consider an M/M/1 queueing system with inventory under the ((r,Q)) policy and with lost sales, in which demands occur according to a Poisson process and service times are exponentially distributed. All arriving customers during stockout are lost. We derive the stationary distributions of the joint queue length (number of customers in the system) and on-hand inventory when lead times are random variables and can take various distributions. The derived stationary distributions are used to formulate long-run average performance measures and cost functions in some numerical examples.",
    "keywords": [
      "queueing",
      "inventory",
      "stationary distribution",
      "lost sale",
      "regenerative process",
      ""
    ]
  },
  {
    "id": "846",
    "title": "Dynamic data rectification using particle filters",
    "abstract": "The basis of dynamic data rectification is a dynamic process model. The successful application of the model requires the fulfilling of a number of objectives that are as wide-ranging as the estimation of the process states, process signal denoising and outlier detection and removal. Current approaches to dynamic data rectification include the conjunction of the Extended Kalman Filter (EKF) and the expectation-maximization algorithm. However, this approach is limited due to the EKF being less applicable where the state and measurement functions are highly non-linear or where the posterior distribution of the states is non-Gaussian. This paper proposes an alternative approach whereby particle filters, based on the sequential Monte Carlo method, are utilized for dynamic data rectification. By formulating the rectification problem within a probabilistic framework, the particle filters generate Monte Carlo samples from the posterior distribution of the system states, and thus provide the basis for rectifying the process measurements. Furthermore, the proposed technique is capable of detecting changes in process operation and thus complements the task of process fault diagnosis. The appropriateness of particle filters for dynamic data rectification is demonstrated through their application to an illustrative non-linear dynamic system, and a benchmark pH neutralization process.  ",
    "keywords": [
      "dynamic data rectification",
      "filtering",
      "particle filters",
      "sequential monte carlo",
      "state estimation"
    ]
  },
  {
    "id": "847",
    "title": "Neighbor sensor networks: Increasing lifetime and eliminating partitioning through cooperation",
    "abstract": "In this paper we consider neighbor sensor networks which are defined as multiple wireless sensor networks under the administration of different authorities but located physically on the same area or close to each other. We construct a Linear Programming framework to characterize the cooperation of neighbor sensor networks in comparison to non-cooperating networks. We show that if neighbor sensor networks cooperate with each other for relaying data packets then this cooperation brings two advantages as compared to no cooperation case. First, lifetime of both networks is prolonged  the results of our analysis show that cooperation between neighbor sensor networks can significantly extend the overall network lifetime. Second, cooperation reduces the probability of disjoint partitions arising due to the limited transmission ranges of sensor nodes. When neighbor sensor networks cooperate, eliminating disjoint partitions is possible with sensors having shorter transmission ranges as demonstrated and quantified by our analysis.",
    "keywords": [
      "wireless sensor networks",
      "linear programming",
      "network lifetime",
      "disjoint partition",
      "cooperation"
    ]
  },
  {
    "id": "848",
    "title": "IMAGE ERROR CONCEALMENT BASED ON QIM DATA HIDING IN DUAL-TREE COMPLEX WAVELETS",
    "abstract": "Transmission of block-coded images through error-prone radio mobile channel often results in lost blocks. Error concealment (EC) techniques exploit inherent redundancy and reduce visual artifacts through post-processing at the decoder side. In this paper, we propose an efficient quantization index modulation (QIM)-based data hiding scheme using dual-tree complex wavelet transform (DTCWT) for the application of image error concealment. The goal is achieved by embedding important information (image digest) as watermark signal that is extracted from the original image itself and is used to introduce sufficient redundancy in the transmitted image. At the decoder side, the extracted image digest is used to correct the damaged regions. DTCWT offers three-fold advantages viz. (1) high embedding capacity due to inherent redundancy that leads to the better reconstruction of high volume missing data, (2) better imperceptibly after data embedding since it most closely captures human visual system (HVS) characteristics than conventional DWT and (3) better watermark decoding reliability. Simulation results duly support the claims and relative performance improvement with respect to the existing results.",
    "keywords": [
      "error concealment",
      "data hiding",
      "dtcwt",
      "halftoning",
      "qim",
      "image digest"
    ]
  },
  {
    "id": "849",
    "title": "Visualizing \"typical\" and \"exotic\" Internet traffic data",
    "abstract": "The threat of cyber attacks motivates the need to monitor Internet traffic data for potentially abnormal behavior. Due to the enormous volumes of such data, statistical process monitoring tools, such as those traditionally used on data in the product manufacturing arena, are inadequate. \"Exotic\" data may indicate a potential attack; detecting such data requires a characterization of \"typical\" data. We devise some new graphical displays, including a \"skyline plot,\" that permit ready visual identification of unusual Internet traffic patterns in \"streaming\" data, and use appropriate statistical measures to help identify potential cyberattacks. These methods are illustrated on a moderate-sized data set (135,605 records) collected at George Mason University.  ",
    "keywords": [
      "logarithmic transformation",
      "computational methods",
      "recursive computation",
      "graphical displays",
      "exploratory data analysis"
    ]
  },
  {
    "id": "850",
    "title": "scheduling and student performance",
    "abstract": "We present data showing strong correlation between students' time management and a successful outcome on programming assignments. Students who spread their work over more time will produce a better result without additional expenditure of total effort. We examined performance of students who sometimes did well and sometimes did poorly, and found that their good performance occurred on the projects where they displayed better time management. While these results will not surprise most instructors, hard data is more compelling than intuition when trying to train students to use good time management.",
    "keywords": [
      "time management",
      "student performance",
      "scheduling"
    ]
  },
  {
    "id": "851",
    "title": "Access control and key management for mobile agents",
    "abstract": "Security is a fundamental precondition for the acceptance of mobile agent systems. In this paper we present a mobile agent structure which supports authentication, security management and access control for mobile agents.",
    "keywords": [
      "mobile agent security",
      "agent authentication",
      "key management",
      "access control",
      "access groups agent confidentiality"
    ]
  },
  {
    "id": "852",
    "title": "Empirical distribution of k-word matches in biological sequences",
    "abstract": "This study focuses on an alignment-free sequence comparison method: the number of words of length k  shared between two sequences, also known as the D2 D 2 statistic. The advantages of the use of this statistic over alignment-based methods are firstly that it does not assume that homologous segments are contiguous, and secondly that the algorithm is computationally extremely fast, the runtime being proportional to the size of the sequence under scrutiny. Existing applications of the D2 D 2 statistic include the clustering of related sequences in large EST databases such as the STACK database. Such applications have typically relied on heuristics without any statistical basis. Rigorous statistical characterisations of the distribution of D2 D 2 have subsequently been undertaken, but have focussed on the distribution's asymptotic behaviour, leaving the distribution of D2 D 2 uncharacterised for most practical cases. The work presented here bridges these two worlds to give usable approximations of the distribution of D2 D 2 for ranges of parameters most frequently encountered in the study of biological sequences.",
    "keywords": [
      "alignment-free sequence comparison",
      "biological sequences",
      "genomic data"
    ]
  },
  {
    "id": "853",
    "title": "replications types in experimental disciplines",
    "abstract": "Experiment replication is a key component of the scientific paradigm. The purpose of replication is to verify previously observed findings. Although some Software Engineering (SE) experiments have been replicated, yet, there is still disagreement about how replications should be run in our field. With the aim of gaining a better understanding of how replications are carried out, this paper examines different replication types in other scientific disciplines. We believe that by analysing the replication types proposed in other disciplines it is possible to clarify some of the question marks still hanging over experimental SE replication.",
    "keywords": [
      "software engineering",
      "experimental paradigm",
      "classifications of replications",
      "types of replications",
      "experimental replication"
    ]
  },
  {
    "id": "854",
    "title": "Optimization models to characterize the broadcast capacity of vehicular ad hoc networks",
    "abstract": "Broadcast capacity of the entire network is one of the fundamental properties of vehicular ad hoc networks (VANETs). It measures how efficiently the information can be transmitted in the network and usually it is limited by the interference between the concurrent transmissions in the physical layer of the network. This study defines the broadcast capacity of vehicular ad hoc network as the maximum successful concurrent transmissions. In other words, we measure the maximum number of packets which can be transmitted in a VANET simultaneously, which characterizes how fast a new message such as a traffic incident can be transmitted in a VANET. Integer programming (IP) models are first developed to explore the maximum number of successful receiving nodes as well as the maximum number of transmitting nodes in a VANET. The models embed an traffic flow model in the optimization problem. Since IP model cannot be efficiently solved as the network size increases, this study develops a statistical model to predict the network capacity based on the significant parameters in the transportation and communication networks. MITSIMLab is used to generate the necessary traffic flow data. Response surface method and linear regression technologies are applied to build the statistical models. Thus, this paper brings together an array of tools to solve the broadcast capacity problem in VANETs. The proposed methodology provides an efficient approach to estimate the performance of a VANET in real-time, which will impact the efficacy of travel decision making.",
    "keywords": [
      "vehicular ad hoc networks",
      "atis",
      "broadcast capacity",
      "information flow",
      "optimization",
      "integer program"
    ]
  },
  {
    "id": "855",
    "title": "Improving hierarchical cluster analysis: A new method with outlier detection and automatic clustering",
    "abstract": "Techniques based on agglomerative hierarchical clustering constitute one of the most frequent approaches in unsupervised clustering. Some are based on the single linkage methodology, which has been shown to produce good results with sets of clusters of various sizes and shapes. However, the application of this type of algorithms in a wide variety of fields has posed a number of problems, such as the sensitivity to outliers and fluctuations in the density of data points. Additionally, these algorithms do not usually allow for automatic clustering. In this work we propose a method to improve single linkage hierarchical cluster analysis (HCA), so as to circumvent most of these problems and attain the performance of most sophisticated new approaches. This completely automated method is based on a self-consistent outlier reduction approach, followed by the building-up of a descriptive function. This, in turn, allows to define natural clusters. Finally, the discarded objects may be optionally assigned to these clusters. The validation of the method is carried out by employing widely used data sets available from literature and others for specific purposes created by the authors. Our method is shown to be very efficient in a large variety of situations.  ",
    "keywords": [
      "clustering",
      "unsupervised pattern recognition",
      "hierarchical cluster analysis",
      "single linkage",
      "outlier removal"
    ]
  },
  {
    "id": "856",
    "title": "The sensitivity of the inventory model with partial backorders",
    "abstract": "This paper uses the rigorous methods of mathematics to explore the analysis of the sensitivity of Park [Int. J. Syst. Sci. 13 (1982) 1313]. However, Park discusses the analysis of the sensitivity by numerical examples. The results obtained by this paper show that the sensitivity of Park is not always true sometimes. Therefore, the researchers may be very careful to use the conclusions of the analysis of the sensitivity made by numerical examples in general.",
    "keywords": [
      "inventory model",
      "partial backorders",
      "sensitivity"
    ]
  },
  {
    "id": "857",
    "title": "Performance Evaluation of an Advanced DWDM RoFSO System for Transmitting Multiple RF Signals",
    "abstract": "With the increase of communication demand and the emergence of new services. various innovative wireless technologies have been deployed recently. Free Space Optics (FSO) links combined with Radio over Fiber (RoF) technology can realize a cost-effective heterogeneous wireless access system for both urban and rural areas. In this paper, we introduce a newly developed advanced DWDM Radio-on-FSO (RoFSO) system capable of simultaneously transmitting multiple Radio Frequency (RF) signals carrying various wireless services including W-CDMA, WLAN IEEE802.1 lg and ISDB-T signals over FSO link. We present an experimental performance evaluation of transmitting RF signals using the RoFSO system over a I kin link under different deployment environment conditions, This, work represents a pioneering attempt, based on a realistic operational scenario, aiming at demonstrating the RoFSO system can be conveniently used as a reliable alternative broadband wireless technology for complementing optical fiber networks in areas where the deployment of optical fiber is not feasible.",
    "keywords": [
      "radio on free space optics ",
      "radio over fiber ",
      "w-cdma",
      "isdb-t",
      "wlan"
    ]
  },
  {
    "id": "858",
    "title": "Sonar based mobile robot localization by using fuzzy triangulation",
    "abstract": "The objective of this paper is to identify the robot's location in a global map from solely sonar based information. This is achieved by using fuzzy sets to model sonar data and by using fuzzy triangulation to identify robot's position and orientation. As a result we obtain a fuzzy position region where each point in the region has a degree of certainty of being the actual position of the robot.  ",
    "keywords": [
      "fuzzy sets",
      "uncertainty",
      "sonar",
      "mobile robots",
      "localization"
    ]
  },
  {
    "id": "859",
    "title": "fuzzy representation for classification of basic bruise colours",
    "abstract": "An application of the Fuzzy Inference System (FIS) for bruise colour recognition is suggested in the paper. Input information to the system will be taken from the images, which includes a bruise and surrounding healthy skin. There are formulated six basic colour groups for the bruise images - red, blue, yellow, brown, green and purple. The input variables of the FIS are connected with the information from the pixels of the images in some colour models (RGB, HSV or Lab). The output variables are the classes - the basic colour groups. Matlab environment was used for representation of the membership functions.",
    "keywords": [
      "bruise age determination",
      "bruise age",
      "colours and image analysis",
      "bruises"
    ]
  },
  {
    "id": "860",
    "title": "Silicon template fabrication for imprint process with good demolding characteristics",
    "abstract": "Demolding force for thermal imprint process to polymethylmethacrylate (PMMA) film is examined by use of Si templates with various side wall profiles. Patterns with tapered side wall profile can be fabricated by control of etching conditions. Side wall profile can be smoothened by anisotropic etching by use of mixed solution of potassium hydroxide (KOH) solution and isopropyl-alcohol. It is confirmed that demolding force can be reduced when mold with tapered side wall pattern is used. Demolding force can be greatly reduced by KOH treatment. Especially, when the template with taper and smooth side wall patterns is used, demolding force is below our measurement system limit of 0.1kgf. It is confirmed that the KOH treatment is very effective in order to reduce demolding force.",
    "keywords": [
      "imprint template",
      "silicon deep etching",
      "bosch process",
      "scalloping",
      "anisotropic etching",
      "potassium hydroxide"
    ]
  },
  {
    "id": "861",
    "title": "By all these lovely tokens... Merging conflicting tokenizations",
    "abstract": "Given the contemporary trend to modular NLP architectures and multiple annotation frameworks, the existence of concurrent tokenizations of the same text represents a pervasive problem in everydays NLP practice and poses a non-trivial theoretical problem to the integration of linguistic annotations and their interpretability in general. This paper describes a solution for integrating different tokenizations using a standoff XML format, and discusses the consequences from a corpus-linguistic perspective.",
    "keywords": [
      "linguistic annotation",
      "multi-layer annotation",
      "conflicting tokenizations",
      "tokenization alignment",
      "corpus linguistics"
    ]
  },
  {
    "id": "862",
    "title": "An application of quantum finite automata to interactive proof systems",
    "abstract": "Quantum finite automata have been studied intensively since their introduction in late 1990s as a natural model of a quantum computer working with finite-dimensional quantum memory space. This paper seeks their direct application to interactive proof systems in which a mighty quantum prover communicates with a quantum-automaton verifier through a common communication cell. Our quantum interactive proof systems are juxtaposed to Dwork-Stockmeyer's classical interactive proof systems whose verifiers are two-way probabilistic finite automata. We demonstrate strengths and weaknesses of our systems by studying how various restrictions on the behaviors of quantum-automaton verifiers affect the power of quantum interactive proof systems. ",
    "keywords": [
      "quantum finite automaton",
      "quantum interactive proof system",
      "quantum measurement",
      "quantum circuit"
    ]
  },
  {
    "id": "863",
    "title": "An accurate radio channel model for wireless sensor networks simulation",
    "abstract": "Simulations are currently an essential tool to develop and test wireless sensor networks (WSNs) protocols and to analyze future WSNs applications performance. Researchers often simulate their proposals rather than deploying high-cost test-beds or develop complex mathematical analysis. However, simulation results rely on physical layer assumptions, which are not usually accurate enough to capture the real behavior of a WSN. Such an issue can lead to mistaken or questionable results. Besides, most of the envisioned applications for WSNs consider the nodes to be at the ground level. However, there is a lack of radio propagation characterization and validation by measurements with nodes at ground level for actual sensor hardware. In this paper, we propose to use a low-computational cost, two slope, log-normal path-loss near ground outdoor channel model at 868 MHz in WSN simulations. The model is validated by extensive real hardware measurements obtained in different scenarios. In addition, accurate model parameters are provided. This model is compared with the well-known one slope path-loss model. We demonstrate that the two slope log-normal model provides more accurate WSN simulations at almost the same computational cost as the single slope one. It is also shown that the radio propagation characterization heavily depends on the adjusted model parameters for a target deployment scenario: The model parameters have a considerable impact on the average number of neighbors and on the network connectivity.",
    "keywords": [
      "channel modeling",
      "near ground propagation",
      "simulation"
    ]
  },
  {
    "id": "864",
    "title": "Personalized text snippet extraction using statistical language models",
    "abstract": "In knowledge discovery in a text database, extracting and returning a subset of information highly relevant to a user's query is a critical task. In a broader sense, this is essentially identification of certain personalized patterns that drives such applications as Web search engine construction, customized text summarization and automated question answering. A related problem of text snippet extraction has been previously studied in information retrieval. In these studies, common strategies for extracting and presenting text snippets to meet user needs either process document fragments that have been delimitated a priori or use a sliding window of a fixed size to highlight the results. In this work, we argue that text snippet extraction can be generalized if the user's intention is better utilized. It overcomes the rigidness of existing approaches by dynamically returning more flexible startend positions of text snippets, which are also semantically more coherent. This is achieved by constructing and using statistical language models which effectively capture the commonalities between a document and the user intention. Experiments indicate that our proposed solutions provide effective personalized information extraction services.",
    "keywords": [
      "text snippet extraction",
      "personalization",
      "language model",
      "information retrieval",
      "natural language processing",
      "pattern discovery",
      "hidden markov model"
    ]
  },
  {
    "id": "865",
    "title": "Upper and lower bounds on the makespan of schedules for tree dags on linear arrays",
    "abstract": "We find, in polynomial time, a schedule for a complete binary tree directed acyclic graph (dag) with n unit execution time tasks on a linear array whose makespan is optimal within a factor of 1 + o(1). Further, given a binary tree dag T with n tasks and height h, we find, in polynomial time, a schedule for T on a linear array whose makespan is optimal within a factor of 5 + o(1). On the other hand, we prove that explicit lower and upper bounds on the makespan of optimal schedules of binary tree dags on linear arrays differ at least by a factor of 1 + root 2/2. We also find, in polynomial time, schedules for bounded tree dags with n unit execution time tasks, degree d, and height h is an element of o(n(1/2)) boolean OR omega(n(1/2)) on a linear array which are optimal within a factor of 1 + o(1), this time under the assumption of links with unlimited bandwidth. Finally, we compute an improved upper bound on the makespan of an optimal schedule for a tree dag on the architecture independent model of Papadimitriou and Yannakakis [14], provided that its height is not too large.",
    "keywords": [
      "multiprocessing",
      "parallel computation",
      "parallel architectures",
      "communication delay",
      "scheduling",
      "tree dags",
      "linear array",
      "mesh array",
      "tree decomposition"
    ]
  },
  {
    "id": "866",
    "title": "Domain Adaptation for Face Recognition: Targetize Source Domain Bridged by Common Subspace",
    "abstract": "In many applications, a face recognition model learned on a source domain but applied to a novel target domain degenerates even significantly due to the mismatch between the two domains. Aiming at learning a better face recognition model for the target domain, this paper proposes a simple but effective domain adaptation approach that transfers the supervision knowledge from a labeled source domain to the unlabeled target domain. Our basic idea is to convert the source domain images to target domain (termed as targetize the source domain hereinafter), and at the same time keep its supervision information. For this purpose, each source domain image is simply represented as a linear combination of sparse target domain neighbors in the image space, with the combination coefficients however learnt in a common subspace. The principle behind this strategy is that, the common knowledge is only favorable for accurate cross-domain reconstruction, but for the classification in the target domain, the specific knowledge of the target domain is also essential and thus should be mostly preserved (through targetization in the image space in this work). To discover the common knowledge, specifically, a common subspace is learnt, in which the structures of both domains are preserved and meanwhile the disparity of source and target domains is reduced. The proposed method is extensively evaluated under three face recognition scenarios, i.e., domain adaptation across view angle, domain adaptation across ethnicity and domain adaptation across imaging condition. The experimental results illustrate the superiority of our method over those competitive ones.",
    "keywords": [
      "face recognition",
      "domain adaptation",
      "common subspace learning",
      "targetize the sourece domain"
    ]
  },
  {
    "id": "867",
    "title": "threat analysis of online health information system",
    "abstract": "Electronic health records are increasingly used to enhance availability, recovery, and transfer of health records. Newly developed online health systems such as Google-Health create new security and privacy risks. In this paper, we elucidate a clear threat model for online health information systems. We distinguish between privacy and security threats. In response to these risks, we propose a traitor-tracing solution, which embeds proof to trace an attacker who leaks data from a repository. We argue that the application of traitor-tracing techniques to online health systems can align incentives and decrease risks.",
    "keywords": [
      "traitor-tracing schemes",
      "information health systems",
      "privacy",
      "legal aspects"
    ]
  },
  {
    "id": "868",
    "title": "Models and Mechanisms for Artificial Morphogenesis",
    "abstract": "Embryological development provides an inspiring example of the creation of complex hierarchical structures by self-organization. Likewise, biological metamorphosis shows how these complex systems can radically restructure themselves. Our research investigates these principles and their application to artificial systems in order to create intricately structured systems that are ordered from the nanoscale up to the macroscale. However these processes depend on mutually interdependent unfoldings of an information process and of the \"body\" in which it is occurring. Such embodied computation provides challenges as well as opportunities, and in order to fulfill its promise, we need both formal and informal models for conceptualizing, designing, and reasoning about embodied computation. This paper presents a preliminary design for one such model especially oriented toward artificial morphogenesis.",
    "keywords": [
      "algorithmic assembly",
      "embodied computation",
      "embodiment",
      "embryological development",
      "metamorphosis",
      "morphogenesis",
      "nanotechnology",
      "post-moore's law computing",
      "reconfigurable systems",
      "self-assembly",
      "self-organization"
    ]
  },
  {
    "id": "869",
    "title": "ICP registration using invariant features",
    "abstract": "This paper investigates the use of Euclidean invariant features in a generalization of iterative closest point registration of range images. Pointwise correspondences are chosen as the closest point with respect to a weighted linear combination of positional and feature distances. It is shown that under ideal noise-free conditions, correspondences formed using this distance function are correct more often than correspondences formed using the positional distance alone. In addition, monotonic convergence to at least a local minimum is shown to hold for this method. When noise is present, a method that automatically sets the optimal relative contribution of features and positions is described. This method trades off error in feature values due to noise against error in positions due to misalignment, Experimental results suggest that using invariant features decreases the probability of being trapped in a local minimum and may be an effective solution for difficult range image registration problems where the scene is very small compared to the model.",
    "keywords": [
      "registration",
      "range images",
      "feature detection",
      "invariance"
    ]
  },
  {
    "id": "870",
    "title": "Where were you? Development of a time-geographic approach for activity destination re-construction",
    "abstract": "With the use of individual-level travel survey datasets describing the detailed activities of households, it is possible to analyze human movements with a high degree of precision. However, travel survey data are not without quality issues. Potential exists for origins and destinations of reported trips not to be geo-referenced, perhaps due to misreported information or inconsistencies in spatial address databases, which can limit the usefulness of the survey data. From an analytical standpoint, this is a serious problem because a single unreferenced stop in a trip record in effect renders that individuals data useless, especially in cases where analyzing chains of activity locations is of interest. This paper presents a framework and basic computational approach for exploring unlocatable activity locations inherent to travel surveys. Derived from recent work in developing a network-based, probabilistic time geography, the proposed methods are able to estimate the likely locations of missing trip origins and destinations. The methods generate probabilistic potential path trees which are used to visualize and quantify potential locations for the unreferenced destinations. The methods are demonstrated with simulated survey data from a smaller metropolitan area.",
    "keywords": [
      "time geography",
      "travel surveys",
      "spatial behavior",
      "geocoding",
      "visualization",
      "probability",
      "error",
      "networks"
    ]
  },
  {
    "id": "871",
    "title": "New primal-dual algorithms for Steiner tree problems",
    "abstract": "We present new primal-dual algorithms for several network design problems. The problems considered are the generalized Steiner tree problem (GST), the directed Steiner tree problem (DST), and the set cover problem (SC) which is a subcase of DST. All our problems are NP-hard; so we are interested in their approximation algorithms. First, we give an algorithm for DST which is based on the traditional approach of designing primal-dual approximation algorithms. We show that the approximation factor of the algorithm is k, where k is the number of terminals. in the case when the problem is restricted to quasi-bipartite graphs. We also give pathologically bad examples for the algorithm performance. To overcome the problems exposed by the bad examples, we design a new framework for primal-dual algorithms which can be applied to all of our problems. The main feature of the new approach is that, unlike the traditional primal-dual algorithms, it keeps the dual solution in the interior of the dual feasible region. The new approach allows us to avoid including too many arcs in the solution, and thus achieves a smaller-cost solution. Our computational results show that the interior-point version of the primal-dual most of the time performs better than the original primal-dual method. ",
    "keywords": [
      "steiner tree",
      "integer programming",
      "approximation algorithm",
      "primal-dual algorithm"
    ]
  },
  {
    "id": "872",
    "title": "An expert trajectory design for control of nuclear research reactors",
    "abstract": "In this study, an expert trajectory was proposed for control of nuclear research reactors. The trajectory being followed by the reactor power is composed of three parts. In order to calculate periods at the midpoint of each part of the trajectory, a period generator was designed based on artificial neural networks. The contribution of the expert trajectory to the reactor control system was investigated. Furthermore, the behavior of the controller with the expert trajectory was tested for various initial and desired power levels, as well as under disturbance. It was seen that the controller could control the system successfully under all conditions within the acceptable error tolerance.",
    "keywords": [
      "nuclear reactor control",
      "neural networks",
      "trajectory planning"
    ]
  },
  {
    "id": "873",
    "title": "Facebook or Renren? A comparative study of social networking site use and social capital among Chinese international students in the United States",
    "abstract": "Facebook and Renren use are positively associated with bridging social capital. Facebooks relationship with bridging social capital is stronger than Renren. Renren use is positively associated with maintaining home country social capital.",
    "keywords": [
      "facebook",
      "renren",
      "social networking sites",
      "social capital",
      "chinese international students"
    ]
  },
  {
    "id": "874",
    "title": "Detection of phishing attacks in Iranian e-banking using a fuzzyrough hybrid system",
    "abstract": "Identifying outstanding phishing features that best fit the Iranian bank websites. Extracting a reduct of influential indicators in phishing detection for Iranian e-banking system using rough sets theory. Determining critical phishing detection rules and forming a flexible rule base for phishing detection. Building a fuzzyrough hybrid system as a core processing unit of phishing detection applications or web browser add-ons and extensions concentrated on Iranian e-banking. Applying the proposed system on Iranian phishing sites and achieving an efficiency of 88%.",
    "keywords": [
      "e-banking",
      "phishing",
      "fraud detection",
      "fuzzy expert system",
      "rough sets theory"
    ]
  },
  {
    "id": "875",
    "title": "Play-it-by-eye! Collect movies and improvise perspectives with tangible video objects",
    "abstract": "We present an alternative video-making framework for children with tools that integrate video capture with movie production. We propose different forms of interaction with physical artifacts to capture storytelling. Play interactions as input to video editing systems assuage the interface complexities of film construction in commercial software. We aim to motivate young users in telling their stories, extracting meaning from their experiences by capturing supporting video to accompany their stories, and driving reflection on the outcomes of their movies. We report on our design process over the course of four research projects that span from a graphical user interface to a physical instantiation of video. We interface the digital and physical realms using tangible metaphors for digital data, providing a spontaneous and collaborative approach to video composition. We evaluate our systems during observations with 4- to 14-year-old users and analyze their different approaches to capturing, collecting, editing, and performing visual and sound clips.",
    "keywords": [
      "children",
      "interaction design",
      "storytelling",
      "tangible user interfaces",
      "video"
    ]
  },
  {
    "id": "876",
    "title": "Exploring association between perceived importance of travel/traffic information and travel behaviour in natural disasters: A case study of the 2011 Brisbane floods",
    "abstract": "Using the 2011 Brisbane flood as a case study. Respondents perceptions of the importance of travel/traffic information were modelled. The hysteresis phenomenon in respondents perceived information importance. Socio-demographic features have a significant impact on such perceptions. No evidence of the influence of travel/traffic information on respondents travel mode.",
    "keywords": [
      "travel information",
      "traffic information",
      "travel behaviour",
      "adverse weather",
      "natural disaster",
      "random-effects ordered logit"
    ]
  },
  {
    "id": "877",
    "title": "A Shape Reconstructability Measure of Object Part Importance with Applications to Object Detection and Localization",
    "abstract": "We propose a computational model which computes the importance of 2-D object shape parts, and we apply it to detect and localize objects with and without occlusions. The importance of a shape part (a localized contour fragment) is considered from the perspective of its contribution to the perception and recognition of the global shape of the object. Accordingly, the part importance measure is defined based on the ability to estimate/recall the global shapes of objects from the local part, namely the parts shape reconstructability. More precisely, the shape reconstructability of a part is determined by two factorspart variation and part uniqueness. (i) Part variation measures the precision of the global shape reconstruction, i.e. the consistency of the reconstructed global shape with the true object shape; and (ii) part uniqueness quantifies the ambiguity of matching the part to the object, i.e. taking into account that the part could be matched to the object at several different locations. Taking both these factors into consideration, an information theoretic formulation is proposed to measure part importance by the conditional entropy of the reconstruction of the object shape from the part. Experimental results demonstrate the benefit with the proposed part importance in object detection, including the improvement of detection rate, localization accuracy, and detection efficiency. By comparing with other state-of-the-art object detectors in a challenging but common scenario, object detection with occlusions, we show a considerable improvement using the proposed importance measure, with the detection rate increased over (10~%). On a subset of the challenging PASCAL dataset, the Interpolated Average Precision (as used in the PASCAL VOC challenge) is improved by 48%. Moreover, we perform a psychological experiment which provides evidence suggesting that humans use a similar measure for part importance when perceiving and recognizing shapes.",
    "keywords": [
      "shape part",
      "part importance",
      "shape reconstruction",
      "object recognition and detection"
    ]
  },
  {
    "id": "878",
    "title": "In situ fabrication of a poly-acrylamide membrane in a microfluidic channel",
    "abstract": "We present a novel technological approach for the in situ realization of a micron-thin poly-acrylamide membrane in the center of a microfluidic channel. The membrane is formed by interfacial polymerization of an inner stream of monomer solution in between two streams of initiator/catalyst solution in a hydrodynamic focusing chip. 20?m thick SU-8 structures are used to replicate the chip in poly dimethylsiloxane (PDMS). The chip is fitted within an adaptor allowing easy fluidic connections, temperature control and optical monitoring under a microscope. With this system, we can easily tune the internal stream width from 5 to 100?m, by varying the internal/external flow ratio.",
    "keywords": [
      "microfluidics",
      "hydrodynamic focusing",
      "poly-acrylamide membrane",
      "interfacial polymerization"
    ]
  },
  {
    "id": "879",
    "title": "Enhanced colliding bodies optimization for design problems with continuous and discrete variables",
    "abstract": "A new multi-agent algorithm inspired by a collision between two objects in one-dimension is presented. An enhanced colliding bodies optimization which uses memory to save some best solutions is developed. A mechanism is utilized to escape from local optima. Performance of the proposed algorithm is compared to those of standard CBO and some optimization techniques.",
    "keywords": [
      "colliding bodies optimization ",
      "coefficient of restitution ",
      "enhanced colliding bodies optimization ",
      "colliding memory ",
      "discrete and continuous optimization",
      "optimum design of truss structures"
    ]
  },
  {
    "id": "880",
    "title": "detecting online commercial intention (oci)",
    "abstract": "Understanding goals and preferences behind a user's online activities can greatly help information providers, such as search engine and E-Commerce web sites, to personalize contents and thus improve user satisfaction. Understanding a user's intention could also provide other business advantages to information providers. For example, information providers can decide whether to display commercial content based on user's intent to purchase. Previous work on Web search defines three major types of user search goals for search queries: navigational, informational and transactional or resource [1][7]. In this paper, we focus our attention on capturing commercial intention from search queries and Web pages, i.e., when a user submits the query or browse a Web page, whether he/she is about to commit or in the middle of a commercial activity, such as purchase, auction, selling, paid service, etc. We call the commercial intentions behind a user's online activities as OCI (Online Commercial Intention). We also propose the notion of \"Commercial Activity Phase\" (CAP), which identifies in which phase a user is in his/her commercial activities: Research or Commit. We present the framework of building machine learning models to learn OCI based on any Web page content. Based on that framework, we build models to detect OCI from search queries and Web pages. We train machine learning models from two types of data sources for a given search query: content of algorithmic search result page(s) and contents of top sites returned by a search engine. Our experiments show that the model based on the first data source achieved better performance. We also discover that frequent queries are more likely to have commercial intention. Finally we propose our future work in learning richer commercial intention behind users' online activities.",
    "keywords": [
      "help",
      " framework ",
      "understandability",
      "activation",
      "search intention",
      "paging",
      "online commercial intention",
      "oci",
      "user satisfaction",
      "examples",
      "goals",
      "personality",
      "performance",
      "experience",
      "browse",
      "model",
      "paper",
      "informal",
      "business",
      "user",
      "online",
      "search",
      "preference",
      "training",
      "attention",
      "svm",
      "web search",
      "contention",
      "research",
      "users",
      "search engine",
      "web page",
      "types",
      "auction",
      "intention",
      "data",
      "web site",
      "queries",
      "machine learning",
      "commit",
      "learning",
      "future",
      "transaction",
      "resource",
      "query"
    ]
  },
  {
    "id": "881",
    "title": "Easy doesnt do it: skill and expression in tangible aesthetics",
    "abstract": "In this paper, we articulate the role of movement within a perceptual-motor view of tangible interaction. We argue that the history of humanproduct interaction design has exhibited an increasing neglect of the intrinsic importance of movement. On one hand, humanproduct interaction design has shown little appreciation in practice of the centrality of our bodily engagement in the world. This has resulted in technologies that continue to place demands on our cognitive abilities, and deny us the opportunity of building bodily skill. On the other hand, the potential for movement in products to be a meaningful component of our interaction with them has also been ignored. Both of these directions (design for bodily engagement and the expressiveness of product movements) are sketched out, paying particular respect for their potential to impact both interaction aesthetics and usability. We illustrate a number of these ideas with examples.",
    "keywords": [
      "movement",
      "tangible interaction",
      "aesthetics",
      "motor skill",
      "expression",
      "robotics"
    ]
  },
  {
    "id": "882",
    "title": "RLC coupling-aware simulation and on-chip bus encoding for delay reduction",
    "abstract": "This paper shows. that the worst case switching pattern that incurs the longest bus delay While considering the RLC effect is quite different from that while considering the RC effect alone. It implies that the existing encoding schemes based on the RC model may not improve or possibly worsen the delay when the inductance effects become dominant. A bus-invert method is also proposed to reduce the on-chip bus delay based on the RLC model. Simulation results show that the proposed encoding scheme significantly reduces the worst case coupling delay of the inductance-dominated buses.",
    "keywords": [
      "bus-invert method",
      "coupling",
      "inductance",
      "interconnect delay",
      "worst case switching pattern"
    ]
  },
  {
    "id": "883",
    "title": "Studying knowledge management in information systems research: Discourses and theoretical assumptions",
    "abstract": "In information systems, most research on knowledge management assumes that knowledge has positive implications for organizations. However, knowledge is a double-edged sword: while too little might result in expensive mistakes, too much might result in unwanted accountability. The purpose of this paper is to highlight the lack of attention paid to the unintended consequences of managing organizational knowledge and thereby to broaden the scope of IS-based knowledge management research. To this end, this paper analyzes the IS literature on knowledge management. Using a framework developed by Deetz (1996), research articles published between 1990 and 2000 in six IS journals are classified into one of four scientific discourses. These discourses are the normative, the interpretive, the critical, and the dialogic. For each of these discourses, we identify the research focus, the metaphors of knowledge, the theoretical foundations, and the implications apparent in the articles representing it. The metaphors of knowledge that emerge from this analysis are knowledge as object, asset, mind, commodity, and discipline. Furthermore, we present a paper that is exemplary of each discourse. Our objective with this analysis is to raise IS researchers' awareness of the potential and the implications of the different discourses in the study of knowledge and knowledge management.",
    "keywords": [
      "epistemology",
      "knowledge",
      "knowledge management"
    ]
  },
  {
    "id": "884",
    "title": "Partial Information Network Queries",
    "abstract": "Introducing the Partial Information Network Query (PINQ) problem. Developing a parameterized algorithm for PINQ. For topology-free network queries: improving upon previous running times. For two types of alignment network queries: improving upon previous running times.",
    "keywords": [
      "partial information network query ",
      "alignment network query ",
      "topology-free network query "
    ]
  },
  {
    "id": "885",
    "title": "Rule-based and case-based reasoning approach for internal audit of bank",
    "abstract": "Banks currently have a great interest in internal audits to reduce risks, to prevent themselves from insolvency, and to take quick action for financial incidents. This study presents an integrated audit approach of rule-based and case-based reasoning, which includes two stages of reasoning, i.e., screening stage based on rule-based reasoning and auditing stage based on case-based reasoning. Rule-based reasoning uses induction rules to determine whether a new problem should be inspected further or not. Case-based reasoning performs similarity-based matching to find the most similar case in case base to the new problem. The method presented is applied to data of internal audits of a bank.",
    "keywords": [
      "bank audit",
      "rule base",
      "case base",
      "reasoning",
      "similarity"
    ]
  },
  {
    "id": "886",
    "title": "Under-approximating loops in C programs for fast counterexample detection",
    "abstract": "Many software model checkers only detect counterexamples with deep loops after exploring numerous spurious and increasingly longer counterexamples. We propose a technique that aims at eliminating this weakness by constructing auxiliary paths that represent the effect of a range of loop iterations. Unlike acceleration, which captures the exact effect of arbitrarily many loop iterations, these auxiliary paths may under-approximate the behaviour of the loops. In return, the approximation is sound with respect to the bit-vector semantics of programs. Our approach supports arbitrary conditions and assignments to arrays in the loop body, but may as a result introduce quantified conditionals. To reduce the resulting performance penalty, we present two quantifier elimination techniques specially geared towards our application. Loop under-approximation can be combined with a broad range of verification techniques. We paired our techniques with lazy abstraction and bounded model checking, and evaluated the resulting tool on a number of buffer overflow benchmarks, demonstrating its ability to efficiently detect deep counterexamples in C programs that manipulate arrays.",
    "keywords": [
      "model checking",
      "loop acceleration",
      "underapproximation",
      "counterexamples"
    ]
  },
  {
    "id": "887",
    "title": "Robust MIMO radar target localization via nonconvex optimization",
    "abstract": "This communication addresses the problem of robust target localization in distributed multiple-input multiple-output (MIMO) radar using possibly outlier range measurements. To achieve robustness against outliers, we construct an objective function for MIMO target localization via the maximum correntropy criterion. To deal with such a nonconvex and nonlinear function, we apply a half-quadratic optimization technique to determine the target position and auxiliary variables alternately. Especially, we derive a semidefinite relaxation formulation for the aforementioned position determination step. The robust performance of the developed approach is demonstrated by comparing with several conventional localization methods via computer simulation.",
    "keywords": [
      "target localization",
      "multiple-input multiple-output  radar",
      "nonlinear optimization",
      "nonconvex optimization",
      "semidefinite relaxation",
      "maximum correntropy criterion",
      "non-line-of-sight "
    ]
  },
  {
    "id": "888",
    "title": "Plant invasions across the Northern Hemisphere: a deep-time perspective",
    "abstract": "Few invasion biologists consider the long-term evolutionary context of an invading organism and its invaded ecosystem. Here, I consider patterns of plant invasions across Eastern North America, Europe, and East/Far East Asia, and explore whether biases in exchanges of plants from each region reflect major selection pressures present within each region since the late Miocene, during which temperate Northern Hemisphere floras diverged taxonomically and ecologically. Although there are many exceptions, the European flora appears enriched in species well adapted to frequent, intense disturbances such as cultivation and grazing; the North American composite (Asteraceae) flora appears particularly well adapted to nutrient-rich meadows and forest openings; and the East Asian flora is enriched in shade-tolerant trees, shrubs, and vines of high forest-invasive potential. I argue that such directionality in invasions across different habitat types supports the notion that some species are preadapted to become invasive as a result of differences in historical selection pressures between regions.",
    "keywords": [
      "preadaptation",
      "eastern north america",
      "naturalized plants",
      "invasion biology"
    ]
  },
  {
    "id": "889",
    "title": "On classification with bags, groups and sets",
    "abstract": "Many classification tasks are not entirely suitable for supervised learning. Instead of individual feature vectors, bags of feature vectors can be considered. Many learning scenarios with bags in training and/or test phase have been proposed. We provide an overview and taxonomy of these learning scenarios.",
    "keywords": [
      "multiple instance learning",
      "set classification",
      "group-based classification",
      "label dependencies",
      "weakly labeled data"
    ]
  },
  {
    "id": "890",
    "title": "Technological support for the enactment of collaborative scripted learning activities across multiple spatial locations",
    "abstract": "Support for Computer Supported Collaborative Blended Learning scripts is proposed. Requirements are: replicability, adaptability, flexibility, scalability. The system integrates several technologies and derives a more general architecture. An experiment, based on a previous script, was conducted to validate the proposal. The findings show that the script reduces the management workload.",
    "keywords": [
      "cscl scripts",
      "orchestration",
      "ims learning design",
      "service integration",
      "case study"
    ]
  },
  {
    "id": "891",
    "title": "unboundedly parallel simulations via recurrence relations",
    "abstract": "New methods are presented for parallel simulation of discrete event systems that, when applicable, can usefully employ a number of processors much larger than the number of objects in the system being simulated. Abandoning the distributed event list approach, the simulation problem is posed using recurrence relations. We bring three algorithmic ideas to bear on parallel simulation: parallel prefix computation, parallel merging , and iterative folding . Efficient parallel simulations are given for (in turn) the G/G/1 queue, a variety of queueing networks having a global first come first served structure (e.g., a series of queues with finite buffers), acyclic networks of queues, and networks of queues with feedbacks and cycles. In particular, the problem of simulating the arrival and departure times for the first N jobs to a single G/G/1 queue is solved in time proportional to N/P + log P using P processors.",
    "keywords": [
      "network",
      "processor",
      "queueing networks",
      "structure",
      "method",
      "simulation",
      "systems",
      "efficiency",
      "parallel simulation",
      "log",
      "event",
      "parallel",
      "computation",
      "object",
      "timing",
      "feedback",
      "queue",
      "distributed",
      "buffers",
      "global"
    ]
  },
  {
    "id": "892",
    "title": "Assembly and disassembly: An overview and framework for cooperation requirement planning with conflict resolution",
    "abstract": "Assembly, one of the oldest forms of industrial production, and its twin area, disassembly, have enjoyed tremendous modernization in the era of the information revolution. New enabling technologies, including prominent examples such as virtual CAD, Design for Assembly and Disassembly ( DFAD), robotic and intelligent assembly, and Flexible Assembly ( FA), are now becoming commonplace. This article reviews some of the newer solutions, and an extended framework for Cooperation Requirement Planning (ECRP) in robotic assembly/ disassembly is developed. Recent research under the PRISM program at Purdue University to enable ECRP and other relevant projects is presented. The challenges to researchers in this field, in adapting these solutions to the emerging environment of global and local supply networks are also discussed.",
    "keywords": [
      "error recovery",
      "conflict resolution",
      "design for assembly",
      "artificial intelligence",
      "design for disassembly"
    ]
  },
  {
    "id": "893",
    "title": "Robust optimisation for self-scheduling and bidding strategies of hybrid CSPfossil power plants",
    "abstract": "A robust MILP approach is proposed for self-scheduling of hybrid CSPfossil fuel plant. Uncertainty is introduced in the model by asymmetric prediction intervals. The robustness cost is controlled by the budget of uncertainty. Plant self-scheduling and bidding strategies in a day-ahead market are simultaneously considered.",
    "keywords": [
      "asymmetric uncertainty",
      "backup system",
      "bidding strategies",
      "hybrid csp plant",
      "robust optimisation",
      "self-scheduling"
    ]
  },
  {
    "id": "894",
    "title": "self-managing, disconnected processes and mechanisms for mobile e-business",
    "abstract": "With the tremendous advances in hand-held computing and communication capabilities, rapid proliferation of mobile devices, and decreasing device costs, we are seeing a growth in mobile e-business in various consumer and business markets. In this paper, we present a novel architecture and framework for end-to-end mobile e-business applications such as purchasing, retail point of sales, and order management. The design takes into consideration disconnection, application context and failure modes to provide mobile users with seamless and transparent access to commerce and content activities. In our architecture, we consider a novel business process design based on state-machines and event management to handle disconnection and resource limitations. We designed, implemented and deployed a system for mobile e-business on clients integrated with private exchanges and sell-side servers. The e-business framework on mobile clients is implemented based on J2ME and open XML standards. A performance study of simple e-business transactions was done on the client using the above mechanisms and programming environment. We show that the performance of a purchasing process using the framework does reasonably well.",
    "keywords": [
      "self-managing systems",
      "context-driven computing",
      "mobile computing",
      "workflow",
      "disconnected computing",
      "mobile e-business"
    ]
  },
  {
    "id": "895",
    "title": "PARAFAC - PARALLEL FACTOR-ANALYSIS",
    "abstract": "We review the method of Parallel Factor Analysis, which simultaneously fits multiple two-way arrays or 'slices' of a three-way array in terms of a common set of factors with differing relative weights in each 'slice'. Mathematically, it is a straightforward generalization of the bilinear model of factor (or component) analysis (x(ij) = SIGMA(r=1)R a(ir)b(jr)) to a trilinear model (x(ijk) = SIGMA(r=1)R a(ir)b(jr)c(kr)). Despite this simplicity, it has an important property not possessed by the two-way model: if the latent factors show adequately distinct patterns of three-way variation, the model is fully identified; the orientation of factors is uniquely determined by minimizing residual error, eliminating the need for a separate 'rotation' phase of analysis. The model can be used several ways. It can be directly fit to a three-way array of observations with (possibly incomplete) factorial structure, or it can be indirectly fit to the original observations by fitting a set of covariance matrices computed from the observations, with each matrix corresponding to a two-way subset of the data. Even more generally, one can simultaneously analyze covariance matrices computed from different samples, perhaps corresponding to different treatment groups, different kinds of cases, data from different studies, etc. To demonstrate the method we analyze data from an experiment on right vs. left cerebral hemispheric control of the hands during various tasks. The factors found appear to correspond to the causal influences manipulated in the experiment, revealing their patterns of influence in all three ways of the data. Several generalizations of the parallel factor analysis model are currently under development, including ones that combine parallel factors with Tucker-like factor 'interactions'. Of key importance is the need to increase the method's robustness against nonstationary factor structures and qualitative (nonproportional) factor change.",
    "keywords": [
      "3-way exploratory factor analysis",
      "unique axes",
      "parallel proportional profiles",
      "factor rotation problem",
      "3-way data preprocessing",
      "3 mode principal components",
      "trilinear decomposition",
      "trilinear model",
      "multidimensional scaling",
      "longitudinal factor analysis",
      "factor analysis of spectra",
      "interpretation of factors",
      "real or causal or explanatory factors",
      "tucker,l.r.",
      "cattel,r.b."
    ]
  },
  {
    "id": "896",
    "title": "Helper threads via virtual multithreading on an experimental ltanium (R) 2 processor-based platform",
    "abstract": "Helper threading is a technology to accelerate a program by exploiting a processor's multithreading capability to run \"assist\" threads. Previous experiments on hyper-threaded processors have demonstrated significant speedups by using helper threads to prefetch hard-to-predict delinquent data accesses. In order to apply this technique to processors that do not have built-in hardware support for multithreading, we introduce virtual multithreading (VMT), a novel form of switch-on-event user-level multithreading, capable of fly-weight multiplexing of event-driven thread executions on a single processor without additional operating system support. The compiler plays a key role in minimizing synchronization cost by judiciously partitioning register usage among the user-level threads. The VMT approach makes it possible to launch dynamic helper thread instances in response to long-latency cache miss events, and to run helper threads in the shadow of cache misses when the main thread would be otherwise stalled. The concept of VMT is prototyped on an Itanium (R) 2 processor using features provided by the Processor Abstraction Layer (PAL) firmware mechanism already present in currently shipping processors. On a 4-way MP physical system equipped with VMT-enabled Itanium 2 processors, helper threading via the VMT mechanism can achieve significant performance gains for a diverse set of real-world workloads, ranging from single-threaded workstation benchmarks to heavily multithreaded large scale decision support systems (DSS) using the IBM DB2 Universal Database. We measure a wall-clock speedup of 5.8% to 38.5% for the workstation benchmarks, and 5.0% to 12.7% on various queries in the DSS workload.",
    "keywords": [
      "helper thread",
      "cache miss prefetching",
      "multithreading",
      "switch-on-event",
      "itanium processor",
      "pal",
      "db2 database"
    ]
  },
  {
    "id": "897",
    "title": "A Systematic Review of Simulation Studies Investigating Emergency Department Overcrowding",
    "abstract": "The problem of emergency department (ED) overcrowding has reached crisis proportions in the last decade. In 2005, the National Academy of Engineering and the Institute of Medicine reported on the important role of simulation as a systems analysis tool that can have an impact on care processes at the care-team, organizational, and environmental levels. Simulation has been widely used to understand causes of ED overcrowding and to test interventions to alleviate its effects. In this paper, we present a systematic review of ED simulation literature from 1970 to 2006 from healthcare, systems engineering, operations research and computer science publication venues. The goals of this review are to highlight the contributions of these simulation studies to our understanding of ED overcrowding and to discuss how simulation can be better used as a tool to address this problem. We found that simulation studies provide important insights into ED overcrowding but they also had major limitations that must be addressed.",
    "keywords": [
      "emergency department simulations",
      "literature review",
      "emergency department",
      "overcrowding",
      "simulation"
    ]
  },
  {
    "id": "898",
    "title": "An effective lower bound on L-max in a worker-constrained job shop",
    "abstract": "A common industrial operation is a dual resource constrained job shop where: (a) the objective is to minimize L-max, the maximum job lateness; (b) machines are organized into groups; and  ",
    "keywords": [
      "job shop scheduling",
      "dual resource constrained systems",
      "maximum lateness",
      "worker allocation"
    ]
  },
  {
    "id": "899",
    "title": "A local tree alignment approach to relation extraction of multiple arguments",
    "abstract": "In this paper, we address the problem of relation extraction of multiple arguments where the relation of entities is framed by multiple attributes. Such complex relations are successfully extracted using a syntactic tree-based pattern matching method. While induced subtree patterns are typically used to model the relations of multiple entities, we argue that hard pattern matching between a pattern database and instance trees cannot allow us to examine similar tree structures. Thus, we explore a tree alignment-based soft pattern matching approach to improve the coverage of induced patterns. Our pattern learning algorithm iteratively searches the most influential dependency tree patterns as well as a control parameter for each pattern. The resulting method outperforms two baselines, a pairwise approach with the tree-kernel support vector machine and a hard pattern matching method, on two standard datasets for a complex relation extraction task.",
    "keywords": [
      "relation extraction",
      "multiple arguments",
      "pattern induction",
      "local tree alignment",
      "soft pattern matching"
    ]
  },
  {
    "id": "900",
    "title": "EDM in the Danish public sector: the FESD project",
    "abstract": "Purpose - To confirm that the purpose of the FESD project has been to provide a framework Accepted 24 February 2005 contract for the whole public sector covering the purchase of an EDM system, technical and organisational consulting for implementation and organisational change. Design/methodology/approach - The project took the approach of working closely together with 11 partnering organisations on developing the functional requirements for the system and participating in the tender negotiations with the bidding consortia. This has proved valuable, since the project has gained a profound legitimacy for its demands and a strong basis for the roll-out in the rest of the public sector. Findings - The results of the project are manifold: for the first time in the Danish public sector a mutual framework contract has made it possible to put the same requirements forward to the bidding vendors. It has made it possible to develop mutual technical standards and to develop standardised work processes supported by the systems. Furthermore, a number of long-term findings will become evident over the next two years when the implementation projects begin to show results. Practical implications - Originally it was one of the major tasks of the FESD project to show efficiency gains and return of investment within the project's life span. This has not been possible due to the fact that the implementation projects in the partnering organisations are far from finished. Also, efficiency gains are not always part of the success criteria and it may turn out that efficiency gains weigh more in the minds of planners than in the real implementation projects. Originality/value - The article is a report from a country highly esteemed for its efforts in pushing public digital administration in order to create better service and higher efficiency.",
    "keywords": [
      "document management",
      "electronic document delivery",
      "public sector organizations",
      "organizational change"
    ]
  },
  {
    "id": "901",
    "title": "Locally conservative discontinuous PetrovGalerkin finite elements for fluid problems",
    "abstract": "We develop a locally conservative formulation of the discontinuous PetrovGalerkin finite element method (DPG) for convectiondiffusion type problems using Lagrange multipliers to exactly enforce conservation over each element. We provide a proof of convergence as well as extensive numerical experiments showing that the method is indeed locally conservative. We also show that standard DPG, while not guaranteed to be conservative, is nearly conservative for many of the benchmarks considered. The new method preserves many of the attractive features of DPG, but turns the normally symmetric positive-definite DPG system into a saddle point problem.",
    "keywords": [
      "discontinuous petrovgalerkin",
      "local conservation",
      "convection-diffusion",
      "stokes flow",
      "least squares",
      "minimum residual",
      "higher order",
      "adaptive mesh refinement"
    ]
  },
  {
    "id": "902",
    "title": "Reassortment Networks for Investigating the Evolution of Segmented Viruses",
    "abstract": "Many viruses of interest, such as influenza A, have distinct segments in their genome. The evolution of these viruses involves mutation and reassortment, where segments are interchanged between viruses that coinfect a host. Phylogenetic trees can be constructed to investigate the mutation-driven evolution of individual viral segments. However, reassortment events among viral genomes are not well depicted in such bifurcating trees. We propose the concept of reassortment networks to analyze the evolution of segmented viruses. These are layered graphs in which the layers represent evolutionary stages such as a temporal series of seasons in which influenza viruses are isolated. Nodes represent viral isolates and reassortment events between pairs of isolates. Edges represent evolutionary steps, while weights on edges represent edit costs of reassortment and mutation events. Paths represent possible transformation series among viruses. The length of each path is the sum edit cost of the events required to transform one virus into another. In order to analyze tau stages of evolution of n viruses with segments of maximum length m, we first compute the pairwise distances between all corresponding segments of all viruses in O(m(2)n(2)) time using dynamic programming. The reassortment network, with O(tau n(2)) nodes, is then constructed using these distances. The ancestors and descendents of a specific virus can be traced via shortest paths in this network, which can be found in O(tau n(3)) time.",
    "keywords": [
      "influenza a",
      "dynamic programming",
      "reassortment",
      "segmented virus",
      "shortest paths"
    ]
  },
  {
    "id": "903",
    "title": "Scaffolding problem solving in technology-enhanced learning environments (TELEs): Bridging research and theory with practice",
    "abstract": "With the expanding availability and capability of varied technologies, classroom-based problem solving has become an increasingly attainable, yet still elusive, goal. Evidence of technology-enhanced problem-solving teaching and learning in schools has been scarce, understanding how to support students' problem solving in classroom-based, technology-enhanced learning environments has been limited, and coherent frameworks to guide implementation have been slow to emerge. Whereas researchers have examined the use and impact of scaffolds in mathematics, science, and reading, comparatively little research has focused on scaffolding learning in real-world, everyday classroom settings. Web-based systems have been developed to support problem solving, but implementations suggest variable enactment and inconsistent impact. The purpose of this article is to identify critical issues in scaffolding students' technology-enhanced problem solving in everyday classrooms. First, we examine two key constructs (problem solving and scaffolding) and propose a framework that includes essential dimensions to be considered when teachers scaffold student problem solving in technology-rich classes. We then investigate issues related to peer-, teacher-, and technology-enhanced scaffolds, and conclude by examining implications for research.  ",
    "keywords": [
      "scaffolding",
      "scaffolds",
      "technology-enhanced classrooms",
      "problem solving",
      "scientific inquiry",
      "technology-enhanced learning environments ",
      "technology integration"
    ]
  },
  {
    "id": "904",
    "title": "A NEW APPROACH TO OBJECTIVE QUALITY-MEASURES BASED ON ATTRIBUTE-MATCHING",
    "abstract": "In this paper the results of a study of objective quality measures for a broad range of coding systems are presented. These objective measures take the linear and the nonlinear distortions of the coder into account. A correlation analysis was performed, in order to find out those measures which are most effective in predicting perceivable parametric attributes of speech quality. The results of this experiment, the so-called attribute-matching, yield a good composite measure for predicting the total quality for a wide range of coding systems and can be computed in pseudo-realtime. Furthermore, we describe the test signal we have used in our study, which was not natural speech but a speech-model process.",
    "keywords": [
      "speech quality",
      "objective quality-measures",
      "attribute-matching",
      "speech-model process"
    ]
  },
  {
    "id": "905",
    "title": "Building additive utilities for Multi-Group Hierarchical Discrimination: The MHDIS method",
    "abstract": "The discrimination problem is of major interest in fields such as environmental management, human resources management, production management, finance, marketing, medicine, etc. For decades this problem has been studied from a multivariate statistical point of view. Recently the possibilities of new approaches have been explored, based mainly on mathematical programming. This paper follows the methodological framework of multicriteria decision aid (MCDA), to propose a new method for multigroup discrimination based on a hierarchical procedure (Multi-Group Hierarchical Discrimination-M.H.DIS). The performance of the M.H.DIS method is evaluated along with eight real world case studies from the fields of finance and marketing. A comparison is also performed with other MCDA methods.",
    "keywords": [
      "discrimination",
      "multicriteria decision aid",
      "preference disaggregation",
      "goal programming"
    ]
  },
  {
    "id": "906",
    "title": "The effects of static versus dynamic 3D representations on 10th grade students atomic orbital mental model construction: Evidence from eye movement behaviors",
    "abstract": "Dynamic 3D representations enhanced students performance. Dynamic 3D representations fostered students to allocate greater attention. Eye movements could predict students 3D mental models of an atomic orbital. Low-spatial-ability students with dynamic 3D representations spent more attention.",
    "keywords": [
      "atomic orbital concept",
      "mental model",
      "spatial ability",
      "eye-tracking"
    ]
  },
  {
    "id": "907",
    "title": "A novel switching local evolutionary PSO for quantitative analysis of lateral flow immunoassay",
    "abstract": "This paper presents a novel particle swarm optimization (PSO) based on a non-homogeneous Markov chain and differential evolution (DE) for quantification analysis of the lateral flow immunoassay (LFIA), which represents the first attempt to estimate the concentration of target analyte based on the well-established state-space model. A new switching local evolutionary PSO (SLEPSO) is developed and analyzed. The velocity updating equation jumps from one mode to another based on the non-homogeneous Markov chain, where the probability transition matrix is updated by calculating the diversity and current optimal solution. Furthermore, DE mutation and crossover operations are implemented to improve local best particles searching in PSO. Compared with some well-known PSO algorithms, the experiments results show the superiority of proposed SLEPSO. Finally, the new SLEPSO is successfully exploited to quantification analysis of the LFIA system, which is essentially nonlinear and dynamic. Therefore, this can provide a new method for the area of quantitative interpretation of LFIA system.  ",
    "keywords": [
      "lateral flow immunoassay",
      "particle swarm optimization",
      "differential evolution",
      "non-homogeneous markov chain",
      "immunochromatographic strip"
    ]
  },
  {
    "id": "908",
    "title": "The Frobenius anatomy of word meanings I: subject and object relative pronouns",
    "abstract": "This article develops a compositional vector-based semantics of subject and object relative pronouns within a categorical framework. Frobenius algebras are used to formalize the operations required to model the semantics of relative pronouns, including passing information between the relative clause and the modified noun phrase, as well as copying, combining, and discarding parts of the relative clause. We develop two instantiations of the abstract semantics, one based on a truth-theoretic approach and one based on corpus statistics.",
    "keywords": [
      "computational linguistics",
      "type logical grammars",
      "distributional vector space semantics",
      "compact closed categories",
      "string diagrams pregroups"
    ]
  },
  {
    "id": "909",
    "title": "Blow-up of Compressible NavierStokesKorteweg Equations",
    "abstract": "Based on the results of Xin (Commun. Pure Appl. Math. 51(3):229240, 1998), Zhang and Tan (Acta Math. Sin. Engl. Ser. 28(3):645652, 2012), we show the blow-up phenomena of smooth solutions to the non-isothermal compressible NavierStokesKorteweg equations in arbitrary dimensions, under the assumption that the initial density has compact support. Here the coefficients are generalized to a more general case which depends on density and temperature. Our work extends the previous corresponding results.",
    "keywords": [
      "blow-up",
      "compressible navierstokeskorteweg equations",
      ""
    ]
  },
  {
    "id": "910",
    "title": "A coordinated planning model for the design of a distributed database system",
    "abstract": "Motivated by a problem faced by a multimedia entertainment retailer, we explore the problem of planning the design of a distributed database system. The problem consists of planning the design/expansion of the distributed database system by introducing new database servers and retiring possibly some existing ones in order to reduce telecommunication costs for processing user queries and server acquisition, operations and maintenance cost in a multiperiod environment where user processing demand varies over time. We develop a mathematical programming model and an effective solution approach to determine the best decisions regarding acquisition and retirement of database servers and assignment of user processing demand to the servers over time. Through a computational study, we investigate the impact of important parameters such as length of the planning horizon and demand growth on the solution quality and utilization of server capacity and examine the effectiveness of the solution approach in comparison with the commercial package LINDO. We also discuss some extensions to the problem as directions for future research.",
    "keywords": [
      "planning",
      "distributed database system",
      "database servers",
      "mathematical programming",
      "heuristic"
    ]
  },
  {
    "id": "911",
    "title": "Computationally feasible estimation of the covariance structure in generalized linear mixed models",
    "abstract": "In this paper, we discuss how a regression model, with a non-continuous response variable, which allows for dependency between observations, should be estimated when observations are clustered and measurements on the subjects are repeated. The cluster sizes are assumed to be large. We find that the conventional estimation technique suggested by the literature on generalized linear mixed models (GLMM) is slow and sometimes fails due to non-convergence and lack of memory on standard PCs. We suggest to estimate the random effects as fixed effects by generalized linear model and to derive the covariance matrix from these estimates. A simulation study shows that our proposal is feasible in terms of mean-square error and computation time. We recommend that our proposal be implemented in the software of GLMM techniques so that the estimation procedure can switch between the conventional technique and our proposal, depending on the size of the clusters.",
    "keywords": [
      "monte carlo simulations",
      "large sample",
      "interdependence",
      "cluster errors"
    ]
  },
  {
    "id": "912",
    "title": "Are there time and cost savings by using telemanagement for patients on intensified insulin therapy? A randomised, controlled trial",
    "abstract": "Background: Patients with insulin dependent diabetes require frequent advice if their metabolic control is not optimal. This study focuses on the fiscal and administrative aspects of telemanagement, which was used to establish a supervised autonomy of patients on intensified insulin therapy. Methods: A prospective, randomised trial with 43 patients on intensified insulin therapy was conducted. Travelling distance to the diabetes centre was 50 min one way; all patients had undergone a diabetes education course with lessons in dose adaptation. Patients were randomly assigned to telecare (n = 27) or conventional care (n = 16). They used BG-meters with a storage capacity of 120 values (Precision QID(TM) Abbott/Medisense) and transmitted their data over a combined modem/interface via telephone line to the diabetes centre. Data were displayed and stored by a customised software (Precision Link Plus(TM), Abbott/Medisense). Advice for proper dose adjustment was given by telephone. Results: Average time needed for instruction in the telemedical system was 15 min. Data were transmitted every 1-3 weeks and a teleconsultation was performed by phone every 2-4 weeks, depending on the extent of specific problems. On average, personal visits in the control group were performed once a month. Physician's time expenditure for telemanagement, compared to conventional advice was moderately higher (50 vs. 42 min per month). A substantial amount of time on the patients side could be saved through replacing personal communications by telephone contacts and data transmission reduction (96 vs. 163 min/month including data transmission time). Setting up an optimal telemanagement scenario, a cost analysis was carried out yielding savings of 650 EURO per year per patient. HbA(1c) dropped significantly from 8.2 to 7.0% after 8 months of observation, but there was no significant difference between the intervention and control groups. Major technical problems with the telematic system did not occur during the study. Conclusions: Telemanagement of insulin-requiring diabetic patients is a cost and time saving procedure for the patients and results in metabolic control comparable to conventional outpatient management.  ",
    "keywords": [
      "telemedicine",
      "diabetes",
      "telecare",
      "insulin therapy",
      "glucose monitoring"
    ]
  },
  {
    "id": "913",
    "title": "Clinical and Personality Correlates of MMO Gaming: Anxiety and Absorption in Problematic Internet Use",
    "abstract": "Massively-multiplayer online games (MMOs) are increasingly popular worldwide. MMO gaming can result in problematic Internet use (PIU; or Internet addiction), which is characterized by dysfunction in areas such as work or relationships. Because PIU in online gaming is increasingly seen in clinical populations, we explored PIU in the context of MMO gaming. Using a cross-sectional design, we sought to identify clinical and personality factors, as well as motivations for gaming, that differentiated between people who scored high or low on a measure of problematic Internet use. Subjects completed all study procedures via an online survey. Participants were 163 MMO users recruited from the community, from gaming websites, and from online forums. Subjects completed a series of demographic, mood, anxiety, and personality questionnaires. The study found that individuals in the high PIU group (n = 79) were more likely to have higher levels of social phobia (p = .000), state (p = .000) and trait (p = .000) anxiety, introversion (p = .000), neuroticism (p = .000) and absorption (p = .019) than individuals in the low-PIU group (n = 84). Different reasons for gaming also characterized the group with more problematic Internet use. Our findings provide support for the idea that high anxiety and absorption may be risk factors for problematic Internet use within the MMO gaming environment and suggest that gamers who endorse problematic Internet use identify different motivations for online gaming than gamers who do not.",
    "keywords": [
      "internet addiction",
      "anxiety",
      "personality",
      "online gaming"
    ]
  },
  {
    "id": "914",
    "title": "Convergence of LiuStorey conjugate gradient method",
    "abstract": "The conjugate gradient method is a useful and powerful approach for solving large-scale minimization problems. Liu and Storey developed a conjugate gradient method, which has good numerical performance but no global convergence result under traditional line searches such as Armijo, Wolfe and Goldstein line searches. In this paper a convergent version of LiuStorey conjugate gradient method (LS in short) is proposed for minimizing functions that have Lipschitz continuous partial derivatives. By estimating the Lipschitz constant of the derivative of objective functions, we can find an adequate step size at each iteration so as to guarantee the global convergence and improve the efficiency of LS method in practical computation.",
    "keywords": [
      "unconstrained optimization",
      "liustorey conjugate gradient method",
      "global convergence"
    ]
  },
  {
    "id": "915",
    "title": "A space- and power-efficient multi-match packet classification technique combining TCAMs and SRAMs",
    "abstract": "Packet classification is implemented in modern network routers for providing differentiated services based on packet header information. Traditional packet classification only reports a single matched rule with the highest priority for an incoming packet and takes an action accordingly. With the emergence of new Internet applications such as network intrusion detection system, all matched rules need to be reported. This multi-match problem is more challenging and is attracting attentions in recent years. Because of the stringent time budget on classification, architectural solutions using ternary content addressable memory (TCAM) are the preferred choice for backbone network routers. However, despite its advantage on search speed, TCAM is much more expensive than SRAM, and is notorious for its extraordinarily high power consumption. These problems limit the application and scalability of TCAM-based solutions. This paper presents a tree-based multi-match packet classification technique combining the benefits of both TCAMs and SRAMs. The experiments show that the proposed solution achieves significantly more savings on both memory space and power consumption on packet matching compared to existing solutions.",
    "keywords": [
      "network router",
      "packet classification",
      "multi-match",
      "ternary content addressable memory ",
      "network intrusion detection system "
    ]
  },
  {
    "id": "916",
    "title": "Oscillation and comparison theorems for half-linear second-order difference equations",
    "abstract": "The authors consider second-order difference equations of the type Delta((Deltay(n))(alpha)) + q(n)y(sigma (n))(alpha) =0, (E) where alpha > 0 is the ratio of odd positive integers, {q(n)} is a positive sequence, and {sigma (n)} is a positive increasing sequence of integers with sigma (n) --> infinity as n --> infinity. They give some oscillation and comparison results for equation (E).  ",
    "keywords": [
      "comparison theorems",
      "difference equations",
      "half-linear equations",
      "second-order",
      "oscillation"
    ]
  },
  {
    "id": "917",
    "title": "Automated 3D surface scanning based on CAD model",
    "abstract": "This paper presents a method to automate the process of surface scanning using optical range sensors and based on a priori known information from a CAD model. A volumetric model implemented through a 3D voxel map is generated from the object CAD model and used to define a sensing plan composed of a set of viewpoints and the respective scanning trajectories. Surface coverage with high data quality and scanning costs are the main aspects in sensing plan definition. A surface following scheme is used to define collision free and efficient scanning path trajectories. Results of experimental tests performed on a typical industrial scanning system with 5 dof are shown.  ",
    "keywords": [
      "automatic surface scanning",
      "viewpoint set computation",
      "optical range sensors",
      "cad model",
      "next best viewpoint",
      "range data"
    ]
  },
  {
    "id": "918",
    "title": "A NEW MECHANISM FOR TRACKING A MOBILE TARGET USING GRID SENSOR NETWORKS",
    "abstract": "Tracking moving targets is one of the important problems of wireless sensor networks. We have considered a sensor network where numerous sensor nodes are spread in a grid like manner. These sensor nodes are capable of storing data and thus act as a separate datasets. The entire network of these sensors act as a set of distributed datasets. Each of these datasets has its local temporal dataset along with Spatial data. and the geographical coordinates of a, given object or target. In this paper an algorithm is introduced that mines global temporal patterns from these datasets and results in the discovery of linear or nonlinear trajectories of moving objects tinder supervision. The main objective here is to perforin in-network aggregation between the data contained in the various datasets to discover global spatio-temporal patterns; the main constraint is that there should be minimal communication among the participating nodes. We present the algorithm and analyze it in terms of the communication costs.",
    "keywords": [
      "target tracking",
      "sensor networks",
      "in-network aggregation",
      "spatio-temporal mining"
    ]
  },
  {
    "id": "919",
    "title": "Supply chain simulator: A scenario-based educational tool to enhance student learning",
    "abstract": "Simulation-based educational products are excellent set of illustrative tools that proffer features like visualization of the dynamic behavior of a real system, etc. Such products have great efficacy in education and are known to be one of the first-rate student centered learning methodologies. These products allow students to practice skills such as critical thinking and decision-making. In this paper, a case is presented where a scenario-based e-learning product namely 'supply chain simulator' is developed at KFUPM for an introductory technology course. The product simulates a supply chain - a network of facilities and distribution systems that carries out the task of procurement and transformation of materials from manufacturer to customer. The product was put to test during four semesters and results of the survey conducted by the instructors and the students are presented. The results clearly suggest the benefits of using such a tool in enhancing student learning.  ",
    "keywords": [
      "scenario-based e-learning",
      "teaching/learning strategies",
      "interactive learning environments",
      "active learning",
      "supply chain"
    ]
  },
  {
    "id": "920",
    "title": "Obtaining traceability codes from Chinese Reminder Theorem codes",
    "abstract": "Traceability codes are used in schemes that prevent illegal redistribution of digital content. In this Letter, we use Chinese Reminder Theorem codes to construct traceability codes. Both the code parameters and the traitor identification process take into account the non-uniformity of the alphabet of Chinese Reminder Theorem codes. Moreover it is shown that the identification process can be done in polynomial time using list decoding techniques.",
    "keywords": [
      "fingerprinting",
      "traitor tracing",
      "chinese reminder theorem"
    ]
  },
  {
    "id": "921",
    "title": "Learning and Control Model of the Arm for Loading",
    "abstract": "We propose a learning and control model of the arm for a loading task in which an object is loaded onto one hand with the other hand, in the sagittal plane. Postural control during object interactions provides important points to motor control theories in terms of how humans handle dynamics changes and use the information of prediction and sensory feedback. For the learning and control model, we coupled a feedback-error-learning scheme with an Actor-Critic method used as a feedback controller. To overcome sensory delays, a feedforward dynamics model (FDM) was used in the sensory feedback path. We tested the proposed model in simulation using a two-joint arm with six muscles, each with time delays in muscle force generation. By applying the proposed model to the loading task, we showed that motor commands started increasing, before an object was loaded on, to stabilize arm posture. We also found that the FDM contributes to the stabilization by predicting how the hand changes based on contexts of the object and efferent signals. For comparison with other computational models, we present the simulation results of a minimum-variance model.",
    "keywords": [
      "motor control",
      "fdm",
      "loading",
      "actor-critic",
      "feedback-error-learning"
    ]
  },
  {
    "id": "922",
    "title": "A finite-capacity queue with exhaustive vacation/close-down/setup times and Markovian arrival processes",
    "abstract": "We consider a finite-capacity single-server vacation model with close-down/setup times and Markovian arrival processes (MAP). The queueing model has potential applications in classical IP over ATM or IP switching systems, where the close-down time corresponds to an inactive timer and the setup time to the time delay to set up a switched virtual connection (SVC) by the signaling protocol. The vacation time may be considered as the time period required to release an SVC or as the time during which the server goes to set up other SVCs. By using the supplementary variable technique, we obtain the queue length distribution at an arbitrary instant, the loss probability, the setup rate, as well as the Laplace-Stieltjes transforms of both the virtual and actual waiting time distributions.",
    "keywords": [
      "markovian arrival process ",
      "finite capacity queue",
      "vacation",
      "setup time",
      "close-down time",
      "supplementary variable method"
    ]
  },
  {
    "id": "923",
    "title": "Adaptive sequential Monte Carlo by means of mixture of experts",
    "abstract": "Appropriately designing the proposal kernel of particle filters is an issue of significant importance, since a bad choice may lead to deterioration of the particle sample and, consequently, waste of computational power. In this paper we introduce a novel algorithm adaptively approximating the so-called optimal proposal kernel by a mixture of integrated curved exponential distributions with logistic weights. This family of distributions, referred to as mixtures of experts, is broad enough to be used in the presence of multi-modality or strongly skewed distributions. The mixtures are fitted, via online-EM methods, to the optimal kernel through minimisation of the Kullback-Leibler divergence between the auxiliary target and instrumental distributions of the particle filter. At each iteration of the particle filter, the algorithm is required to solve only a single optimisation problem for the whole particle sample, yielding an algorithm with only linear complexity. In addition, we illustrate in a simulation study how the method can be successfully applied to optimal filtering in nonlinear state-space models.",
    "keywords": [
      "optimal proposal kernel",
      "adaptive algorithms",
      "kullback-leibler divergence",
      "coefficient of variation",
      "expectation-maximisation",
      "particle filter",
      "sequential monte carlo",
      "shannon entropy"
    ]
  },
  {
    "id": "924",
    "title": "Personalized Recommendation over a Customer Network for Ubiquitous Shopping",
    "abstract": "Personalization services in a ubiquitous computing environment-ubiquitous personalization services computing-are expected to emerge in diverse environments. Ubiquitous personalization must address limited computational power of personal devices and potential privacy issues. Such characteristics require managing and maintaining a client-side recommendation model for ubiquitous personalization. To implement the client-side recommendation model, this paper proposes Buying-net, a customer network in ubiquitous shopping spaces. Buying-net is operated in a community, called the Buying-net space, of devices, customers, and services that cooperate together to achieve common goals. The customers connect to the Buying-net space using their own devices that contain software performing tasks of learning the customers' preferences, searching for similar customers for network formation, and generating recommendation lists of items. Buying-net attempts to improve recommendation accuracy with less computational time by focusing on local relationship of customers and newly obtained information. We experimented with such customer networks in the area of multimedia content recommendation and validated that Buying-net outperformed a typical collaborative-filtering-based recommender system on accuracy as well as computational time. This shows that Buying-net has good potential to be a system for ubiquitous shopping.",
    "keywords": [
      "mobile commerce",
      "recommender systems",
      "ubiquitous computing",
      "ubiquitous personalization services"
    ]
  },
  {
    "id": "925",
    "title": "Modelling and analysis of a competitive model with stage structure",
    "abstract": "A two-species Lotka-Volterra type competition model with stage structures for both species is proposed and investigated. In our model, the individuals of each species are classified as belonging either the immature or the mature. First, we consider the stage-structured model with constant coefficients. By constructing suitable Lyapunov functions, sufficient conditions are derived for the global stability of nonnegative equilibria of the proposed model. It is shown that three typical dynamical behaviors (coexistence, bistability, dominance) are possible in stage-structured competition model. Next, we consider the stage-structured competitive model in which the coefficients are assumed to be positively continuous periodic functions. By using Gaines and Mawhin's continuation theorem of coincidence degree theory, a set of easily verifiable sufficient conditions are obtained for the existence of positive periodic solutions to the model. Numerical simulations are also presented to illustrate the feasibility of our main results.  ",
    "keywords": [
      "stage structure",
      "competition",
      "global stability",
      "periodic solution"
    ]
  },
  {
    "id": "926",
    "title": "Continuous K-Nearest Neighbor Query for Moving Objects with Uncertain Velocity",
    "abstract": "One of the most important queries in spatio-temporal databases that aim at managing moving objects efficiently is the continuous K-nearest neighbor (CKNN) query. A CKNN query is to retrieve the K-nearest neighbors (KNNs) of a moving user at each time instant within a user-given time interval [t s , t e ]. In this paper, we investigate how to process a CKNN query efficiently. Different from the previous related works, our work relieves the past assumption, that an object moves with a fixed velocity, by allowing that the velocity of the object can vary within a known range. Due to the introduction of this uncertainty on the velocity of each object, processing a CKNN query becomes much more complicated. We will discuss the complications incurred by this uncertainty and propose a cost-effective P2 KNN algorithm to find the objects that could be the KNNs at each time instant within the given query time interval. Besides, a probability-based model is designed to quantify the possibility of each object being one of the KNNs. Comprehensive experiments demonstrate the efficiency and the effectiveness of the proposed approach.",
    "keywords": [
      "continuous k-nearest neighbor query",
      "k-nearest neighbors",
      "moving objects",
      "moving query object",
      "spatio-temporal databases"
    ]
  },
  {
    "id": "927",
    "title": "detecting topical events in digital video",
    "abstract": "The detection of events is essential to high-level semantic querying of video databases. It is also a very challenging problem requiring the detection and integration of evidence for an event available in multiple information modalities, such as audio, video and language. This paper focuses on the detection of specific types of events, namely, topic of discussion events that occur in classroom/lecture environments. Specifically, we present a query-driven approach to the detection of topic of discussion events with foils used in a lecture as a way to convey a topic. In particular, we use the image content of foils to detect visual events in which the foil is displayed and captured in the video stream. The recognition of a foil in video frames exploits the color and spatial layout of regions on foils using a technique called region hashing. Next, we use the textual phrases listed on a foil as an indication of a topic, and detect topical audio events as places in the audio track where the best evidence for the topical phrases was heard. Finally, we use a probabilistic model of event likelihood to combine the results of visual and audio avent detection that exploits their time cooccurrence. The resulting identification of topical events is evaluated in the domain of classroom lectures and talks.",
    "keywords": [
      "color",
      "video",
      "hashing",
      "query-driven topic detection",
      "use",
      "digital video",
      "topic of discussion events",
      "slide detection",
      "event",
      "topical audio events",
      "layout",
      "timing",
      "evidence",
      "paper",
      "informal",
      "audio",
      "place",
      "spatial",
      "video stream",
      "contention",
      "visualization",
      "detection",
      "environments",
      "semantic",
      "modal",
      "region",
      "exploit",
      "recognition",
      "multi-modal fusion",
      "language",
      "identification",
      "types",
      "queries",
      "image",
      "probabilistic models",
      "tracking",
      "database",
      "integrability",
      "query"
    ]
  },
  {
    "id": "928",
    "title": "On the guessing number of shift graphs",
    "abstract": "In this paper we investigate guessing number, a relatively new concept linked to network coding and certain long standing open questions in circuit complexity. Here we study the bounds and a variety of properties concerning this parameter. As an application, we obtain the lower and upper bounds for shift graphs, a subclass of directed circulant graphs.",
    "keywords": [
      "guessing number",
      "shift graph",
      "network coding"
    ]
  },
  {
    "id": "929",
    "title": "intelligent understanding of handwritten geometry theorem proving",
    "abstract": "Computer-based geometry systems have been widely used for teaching and learning, but largely based on mouse-and-keyboard interaction, these systems usually require users to draw figures by following strict task structures defined by menus, buttons, and mouse and keyboard actions. Pen-based designs offer a more natural way to develop geometry theorem proofs with hand-drawn figures and scripts. This paper describes a pen-based geometry theorem proving system that can effectively recognize hand-drawn figures and hand-written proof scripts, and accurately establish the correspondence between geometric components and proof steps. Our system provides dynamic and intelligent visual assistance to help users understand the process of proving and allows users to manipulate geometric components and proof scripts based on structures rather than strokes. The results from evaluation study show that our system is well perceived and users have high satisfaction with the accuracy of sketch recognition, the effectiveness of visual hints, and the efficiency of structure-based manipulation.",
    "keywords": [
      "geometry theorem proving",
      "hand-drawn figures",
      "hand-written proof scripts",
      "structure based manipulation",
      "recognition"
    ]
  },
  {
    "id": "930",
    "title": "A Semi-Infinite Programming Approach to Preoperative Planning of Robotic Cardiac Surgery Under Geometric Uncertainty",
    "abstract": "In this paper, a computational framework for patient-specific preoperative planning of robotics-assisted minimally invasive cardiac surgery (RAMICS) is presented. It is expected that the preoperative planning of RAMICS will improve the success rate by considering robot kinematics, patient-specific thoracic anatomy, and procedure-specific intraoperative conditions. Given the significant anatomical features localized in the preoperative computed tomography images of a patient's thorax, port locations, and robot orientations (with respect to the patient's body coordinate frame) are determined to optimize qualities such as dexterity, reachability, tool approach angles, and maneuverability. To address intraoperative geometric uncertainty, the problem is formulated as a generalized semi-infinite program (GSIP) with a convex lower-level problem to seek a plan that is less sensitive to geometric uncertainty in the neighborhood of surgical targets. It is demonstrated that with a proper formulation of the problem, the GSIP can be replaced by a tractable constrained nonlinear program that uses a multicriteria objective function to balance between the nominal task performance and robustness to collisions and joint limit violations. Finally, performance of the proposed formulation is demonstrated by a comparison between the plans generated by the algorithm and those recommended by an experienced surgeon for several case studies.",
    "keywords": [
      "medical robotics",
      "planning under uncertainty",
      "port placement",
      "preoperative planning"
    ]
  },
  {
    "id": "931",
    "title": "An accelerated IEEE 802.11 handoff process based on the dynamic cluster chain method",
    "abstract": "The latency of the IEEE 802.11 handoff process in wireless local area network (WLAN) is much higher than 50 ms. Since the bearable maximum delay is 50 ms. Since the bearable maximum delay 50 ms in multimedia applications, e.g., voice over IP (VOIP), such large handoff gap may bring up excessive jitter. Therefore, many researches make much effort about how to fast handoff. In this paper, we propose an accelerated handoff mechanism in which three methods are involved: (1) dynamic cluster chain, (2) PMSKA caching, and (3) fast reassociation with the pairwise transient key security association (PTKSA) establishment. Access points (APs) are arranged as a cluster for each client station (STA). APs that are cluster members can cache PMKSA of STA in advance to reduce the extensible authentication protocol-transport Laver Security (EAP-TLS) authentication delay. the dynamic cluster chain which is arranged by a dynamic cluster selection and transition method, is proposed to assure that STA stays within a cluster. Furthermore, the fast reassociation with the PTSA establishment process incorporates four-way handshake into the IEEE 802.11 reassociation process to further accelerate handoff process.  ",
    "keywords": [
      "authentication",
      "bss transition",
      "cluster",
      "cluster roaming key",
      "handoff",
      "wlan"
    ]
  },
  {
    "id": "932",
    "title": "Algorithmic aspects of acyclic edge colorings",
    "abstract": "A proper coloring of the edges of a graph G is called acyclic if there is no two-colored cycle in G. The acyclic edge chromatic number of G, denoted by a'(G), is the least number of colors in an acyclic edge coloring of G. For certain graphs G, a'(G) greater than or equal to Delta(G) + 2 where Delta(G) is the maximum degree in G. It is known that a'(G) less than or equal to Delta + 2 for almost all Delta-regular graphs, including all Delta-regular graphs whose girth is at least cDelta log Delta. We prove that determining the acyclic edge chromatic number of an arbitrary graph is an NP-complete problem, ror graphs G with sufficiently large girth in terms of Delta(G), we present deterministic polynomial-time algorithms that color the edges of G acyclically using at most Delta(G) + 2 colors.",
    "keywords": [
      "acyclic edge coloring",
      "girth"
    ]
  },
  {
    "id": "933",
    "title": "An improved meshless method with almost interpolation property for isotropic heat conduction problems",
    "abstract": "In the paper an improved element free Galerkin method is presented for heat conduction problems with heat generation and spatially varying conductivity. In order to improve computational efficiency of meshless method based on Galerkin weak form, the nodal influence domain of meshless method is extended to have arbitrary polygon shape. When the dimensionless size of the nodal influence domain approaches 1, the Gauss quadrature point only contributes to those nodes in whose background cell the Gauss quadrature point is located. Thus, the bandwidth of global stiff matrix decreases obviously and the node search procedure is also avoided. Moreover, the shape functions almost possess the Kronecker delta function property, and essential boundary conditions can be implemented without any difficulties. Numerical results show that arbitrary polygon shape nodal influence domain not only has high computational accuracy, but also enhances computational efficiency of meshless method greatly.",
    "keywords": [
      "meshless method",
      "heat conduction",
      "spatial varying conductivity",
      "computational efficiency",
      "interpolation property"
    ]
  },
  {
    "id": "934",
    "title": "Function-defined shape metamorphoses in visual cyberworlds",
    "abstract": "Animated shape transformations should be an intrinsic part of visual cyberworlds. However, quite often only limited animation of the polygon-based shapes can be found there, specifically when using the virtual reality modeling language (VRML) and its successor extensible 3D (X3D). This greatly limits the expressive power of visual cyberworlds and has motivated our research in this direction. In this paper, we present function-based extensions of VRML and X3D, which allow for time-dependent shape modeling on the web. Our shape modeling approach is based on the concurrent use of implicit, explicit and parametric functions defining geometry, appearance and their transformations through time. The functions are typed straight in VRML/X3D code as individual formulas and as function scripts. We have also developed a web enabled interactive software tool for modeling function-based VRML/X3D objects.",
    "keywords": [
      "function-based shape modeling",
      "computer animation",
      "3d web visualization",
      "vrml",
      "x3d"
    ]
  },
  {
    "id": "935",
    "title": "Crosstalk analysis for a CMOS-gate-driven coupled interconnects",
    "abstract": "This paper deals in crosstalk analysis of a CMOS-gate-driven capacitively and inductively coupled interconnect. Alpha power-law model of a MOS transistor is used to represent a CMOS driver. This is combined with a transmission-line-based coupled-interconnect model to develop a composite driver-interconnect-load model for analytical purposes. On this basis, a transient analysis of crosstalk noise is carried out. Comparison of the analytical results with SPICE extracted results shows that the average error involved in estimating noise peak and their time of occurrence is less than 7%.",
    "keywords": [
      "coupling",
      "crosstalk noise",
      "inductance",
      "integrated-circuit interconnect",
      "signal integrity",
      "transmission lines"
    ]
  },
  {
    "id": "936",
    "title": "Blocking reduction strategies in hierarchical text classification",
    "abstract": "One common approach in hierarchical text classification involves associating classifiers with nodes in the category tree and classifying text documents in a top-down manner. Classification methods using this top-down approach can scale well and cope with changes to the category trees. However, all these methods suffer from blocking which refers to documents wrongly rejected by the classifiers at higher-levels and cannot be passed to the classifiers at lower-levels. In this paper, we propose a classifier-centric performance measure known as blocking factor to determine the extent of the blocking. Three methods are proposed to address the blocking problem, namely, Threshold Reduction, Restricted Voting, and Extended Multiplicative. Our experiments using Support Vector Machine (SVM) classifiers on the Reuters collection have shown that they all could reduce blocking and improve the classification accuracy. Our experiments have also shown that the Restricted Voting method delivered the best performance.",
    "keywords": [
      "data mining",
      "text mining",
      "classification"
    ]
  },
  {
    "id": "937",
    "title": "Kernel ellipsoidal trimming",
    "abstract": "Ellipsoid estimation is important in many practical areas such as control, system identification, visual/audio tracking, experimental design, data mining, robust statistics and statistical outlier or novelty detection. A new method, called kernel minimum volume covering ellipsoid (KMVCE) estimation, that finds an ellipsoid in a kernel-defined feature space is presented. Although the method is very general and can be applied to many of the aforementioned problems, the main focus is on the problem of statistical novelty/outlier detection. A simple iterative algorithm based on Mahalanobis-type distances in the kernel-defined feature space is proposed for practical implementation. The probability that a non-outlier is misidentified by our algorithms is analyzed using bounds based on Rademacher complexity. The KMVCE method performs very well on a set of real-life and simulated datasets, when compared with standard kernel-based novelty detection methods.",
    "keywords": [
      "minimum volume covering ellipsoid",
      "rademacher complexity",
      "kernel methods",
      "outlier detection",
      "novelty detection"
    ]
  },
  {
    "id": "938",
    "title": "Supervised fuzzy logic modeling for building earthquake hazard assessment",
    "abstract": "Building hazard assessment prior to earthquake occurrence exposes interesting problems especially in earthquake prone areas. Such an assessment provides an early warning system for building owners as well as the local and central administrators about the possible hazards that may occur in the next scenario earthquake event, and hence pre- and post-earthquake preparedness can be arranged according to a systematic program. For such an achievement, it is necessary to have efficient models for the prediction of hazard scale of each building within the study area. Although there are subjective intensity index methods for such evaluations, the objective of this paper is to propose a useful tool through fuzzy logic (FL) to classify the buildings that would be vulnerable to earthquake hazard. The FL is a soft computing intelligent reasoning methodology, which is rapid, simple and easily applicable with logical and rational association between the building-hazard categories and the most effective factors. In this paper, among the most important factors are the story number (building height), story height ratio, cantilever extension ratio, moment of inertia (stiffness), number of frames, column and shear wall area percentages. Their relationships with the five hazard categories are presented through a supervised hazard center classification method. These five categories are none, slight, moderate, extensive, and complete hazard classes. A new supervised FL classification methodology is proposed similar to the classical fuzzy c-means procedure for the allocation of hazard categories to individual buildings. The application of the methodology is presented for Zeytinburnu quarter of Istanbul City, Turkey. It is observed that out of 747 inventoried buildings 7.6%, 50.0%, 14.6%, 20.1%, and 7.7% are subject to expected earthquake with none, slight, moderate, extensive, and complete hazard classes, respectively.",
    "keywords": [
      "earthquake",
      "hazard",
      "categories",
      "moment of inertia",
      "stiffness",
      "supervised fuzzy model"
    ]
  },
  {
    "id": "939",
    "title": "Color texture segmentation based on image pixel classification",
    "abstract": "Image segmentation partitions an image into nonoverlapping regions, which ideally should be meaningful for a certain purpose. Thus, image segmentation plays an important role in many multimedia applications. In recent years, many image segmentation algorithms have been developed, but they are often very complex and some undesired results occur frequently. By combination of Fuzzy Support Vector Machine (FSVM) and Fuzzy C-Means (FCM), a color texture segmentation based on image pixel classification is proposed in this paper. Specifically, we first extract the pixel-level color feature and texture feature of the image via the local spatial similarity measure model and localized Fourier transform, which is used as input of FSVM model (classifier). We then train the FSVM model (classifier) by using FCM with the extracted pixel-level features. Color image segmentation can be then performed through the trained FSVM model (classifier). Compared with three other segmentation algorithms, the results show that the proposed algorithm is more effective in color image segmentation.",
    "keywords": [
      "image segmentation",
      "fuzzy support vector machine",
      "fuzzy c-means",
      "local spatial similarity measure model",
      "localized angular phase"
    ]
  },
  {
    "id": "940",
    "title": "MuSeQoR: Multi-path failure-tolerant security-aware QoS routing in ad hoc wireless networks",
    "abstract": "In this paper, we present MuSeQoR: a new multi-path routing, protocol that tackles the twin issues of reliability (protection against failures Of Multiple paths) and security, while ensuring minimum data redundancy. Unlike ill all the previous studies. reliability is addressed in the context of both erasure and corruption channels. We also quantify the security of the protocol in terms of the number of eavesdropping nodes. The reliability and security requirements of a session are specified by a User and are related to the parameters of the protocol adaptively. This relationship is of central importance and shows how the protocol attempts to simultaneously achieve reliability and security. In addition. by using optimal coding schemes and by dispersing the original data. we minimize the redundancy. Finally, extensive simulations were performed to assess the performance of the protocol Under varying network conditions. The simulation Studies clearly indicate the gains in using Such a protocol and also highlight the enormous flexibility of the protocol.  ",
    "keywords": [
      "multi-path routing",
      "qos",
      "security",
      "dispersity routing",
      "diversity coding",
      "erasure channel",
      "corruption channel",
      "ad hoc wireless networks"
    ]
  },
  {
    "id": "941",
    "title": "Robust location estimation under dependence",
    "abstract": "We discuss the approximation of the mean of autocorrelated data under contaminations of different types. Many robust location estimators have been investigated carefully for independent data, but their properties have not been studied in detail under dependencies. We pay attention to estimators based on subranges like minimum volume ellipsoid and minimum covariance determinant estimators, mid-ranges and trimmed means, of which the sample mean and median are special cases, and also include the Hodges - Lehmann and the Bickel - Hodges estimators. Our interest is in small to moderate sample sizes.",
    "keywords": [
      "time series",
      "robustness",
      "additive outliers",
      "innovative outliers",
      "sensitivity function",
      "bias curve"
    ]
  },
  {
    "id": "942",
    "title": "Determination of the rank of an integration lattice",
    "abstract": "The continuing and widespread use of lattice rules for high-dimensional numerical quadrature is driving the development of a rich and detailed theory. Part of this theory is devoted to computer searches for rules, appropriate to particular situations. In some applications, one is interested in obtaining the (lattice) rank of a lattice rule Q(Lambda) directly from the elements of a generator matrix B (possibly in upper triangular lattice form) of the corresponding dual lattice Lambda(perpendicular to). We treat this problem in detail, demonstrating the connections between this (lattice) rank and the conventional matrix rank deficiency of modulo p versions of B.",
    "keywords": [
      "lattice rules",
      "rank",
      "integration lattice"
    ]
  },
  {
    "id": "943",
    "title": "Using cell phone data to measure quality of service and passenger flows of Paris transit system",
    "abstract": "Cell-phone data are used to measure passenger flows in the underground part of Paris transit system. Travel times, trains level of occupancy and origindestination flows are measured. The measures are consistent with field observations, and with estimates from automated fare collection data. Having an independent real-time measure of train occupancy can be beneficial to the quality of the system.",
    "keywords": [
      "quality of service",
      "transit network",
      "cellular phone data"
    ]
  },
  {
    "id": "944",
    "title": "A genetic algorithm based heuristic to the multi-period fixed charge distribution problem",
    "abstract": "This paper proposes a genetic algorithm (GA) based heuristic to the multi-period fixed charge distribution problem associated with backorder and inventories. The objective is to determine the size of the shipments, backorder and inventories at each period, so that, the total cost incurred during the entire period towards transportation, backorder and inventories is minimum. The model is formulated as pure integer nonlinear programming and 01 mixed integer linear programming problems, and proposes a GA based heuristic to provide solution to the above problem. The proposed GA based heuristic is evaluated by comparing their solutions with lower bound, LINGO solver and approximate solutions. The comparisons reveal that the GA generates better solutions than the approximate solutions, and is capable of providing solutions equal to LINGO solutions and closer to the lower bound value of the problems.",
    "keywords": [
      "genetic algorithm",
      "multi-period distribution problem",
      "fixed charge"
    ]
  },
  {
    "id": "945",
    "title": "an explorational exhibit of a pig's heart",
    "abstract": "Coronary heart diseases (CHD) are one of the main causes of deaths in the United States. Although it is well known that CHD mainly occurs due to blocked arteries, many of the specifics of this disease are still subject to current research. It is commonly accepted that certain factors, such as a cholesterol high diet, increase the risk of coronary heart disease. As a consequence, people should be educated to adhere a diet low in low-density lipoprotein (LDL or bad cholesterol). In order for children to become familiar with these facts, educational, explorative computer systems can be employed to raise some awareness. This poster describes an educational computer system for children that serves this purpose. While practicing their navigation skills, the children can learn about the various types of blood cells and particles within the blood stream. A geometric model of the arterial vascular system of the heart has been developed, which considers vessels of different sizes. An interactive fly-through using a standard game controller facilitates the exploration of the interior structure of the vasculature. A blood flow simulation including several different particles within the blood stream allows the young explorer to understand their functionality. This system has been deployed as an interactive museum exhibit for children. The primary age group addressed by the science museum where it is currently being displayed is 4-9 years. With proper guidance by the museum personnel and the instructional material provided at the exhibit the game is also suitable for slightly younger and much older children.The implemented system simulates a submarine-style navigation through the blood stream inside an arterial vascular tree of a heart. The vasculature is based on a computed tomography (CT) scan of a pig's heart. The user has full control over the navigation by using a Logitech WingMan Cordless Rumblepad as input device. This controller provides two analog joysticks that can be used to achieve six-degrees-of-freedom input. In this application, the user controls forward and backward movement (acceleration and deceleration) with the left joystick while changing the orientation (left, right, up and down) by using the right joystick. Collision detection with the vessel walls ensures that the vasculature cannot be left. On collision with the vessel wall as well as with any of the particles within the blood stream force feedback is provided by using the rumble feature of the input device. In addition, audio feedback with different types of sounds allows the player to distinguish between the different types of collisions. Consequently, the user has complete manual control over the navigation while visual, audio, and force feedback provided by the system results in an easy to understand assessment of what is happening. This is especially important since the targeted audience are children of a relatively young age.The software is scalable in terms of the physical size of the blood vessel systems and the amount of geometry data that is used to represent it. It can be ported to various virtual environments (VEs). At this point, it has been tested on a regular desktop computer and a large projection screen at the museum site. Especially the projection screen, which was used for the interactive exhibit, allows a user to fully immerse herself/himself into the scene. Overall, this computer system gives hands-on experience of the functions of the circulatory system of the heart and exposes the user to the various particles present in the blood stream. As a museum exhibit, it was very well received by the targeted audience, i.e., by children between the age of four and nine, and beyond. The learning experience in the virtual environment was validated in a conversation during a complementing stage performance, which included a scientist dissecting a real pig's heart, where the children were asked to identify anatomical parts and discuss the importance of the circulatory system.",
    "keywords": [
      "fly-through",
      "educational computer game",
      "cardiovascular",
      "biomedical visualization",
      "navigation"
    ]
  },
  {
    "id": "946",
    "title": "Education reform and its needs for technical standards",
    "abstract": "This paper discusses some of the leading concepts in education reform and their need for technical standards. It also discusses the efforts by several organizations to develop such standards and specifications, and how new stakeholders can get involved or monitor this work. There are a variety of reform efforts being advocated and pursued by researchers, educators, learning institutions, corporate trainers, and government leaders. Concepts such as student-centered learning, computer-based training, on-line learning, distance learning, just-in-time learning, and self-learning are widely accepted as having the potential to substantially improve the efficiency and effectiveness of learning. All of these concepts have the need for one or more underlying technical standards. This paper describes a number of these standards and the work that is being done to develop them. It is important to note that these technical standards are independent from the content material (known as content standards) that students would be required to learn, as well as from the amount of that content (known as performance standards) a student would be expected to master.",
    "keywords": [
      "education",
      "computer based learning",
      "technical standards"
    ]
  },
  {
    "id": "947",
    "title": "Rationing mechanisms and inventory control-policy parameters for a divergent supply chain operating with lost sales and costs of review",
    "abstract": "We consider a static divergent two-stage supply chain with one distributor and many retailers. The unsatisfied demands at the retailers end are treated as lost sales, whereas the unsatisfied demand is assumed to be backlogged at the distributor. The distributor uses an inventory rationing mechanism to distribute the available on-hand inventory among the retailers, when the sum of demands from the retailers is greater than the on-hand inventory at the distributor. The present study aims at determining the best installation inventory control-policy or order-policy parameters such as the base-stock levels and review periods, and inventory rationing quantities, with the objective of minimizing the total supply chain costs (TSCC) consisting of holding costs, shortage costs and review costs in the supply chain over a finite planning horizon. An exact solution procedure involving a mathematical programming model is developed to determine the optimum TSCC, base-stock levels, review periods and inventory rationing quantities (in the class of periodic review, order-up-to S policy) for the supply chain model under study. On account of the computational complexity involved in optimally solving problems over a large finite time horizon, a genetic algorithm (GA) based heuristic methodology is presented.",
    "keywords": [
      "divergent supply chain",
      "lost sales",
      "inventory rationing",
      "base-stock levels",
      "periodic review periods",
      "allocation rules",
      "mathematical programming model",
      "genetic algorithm"
    ]
  },
  {
    "id": "948",
    "title": "A globally conforming method for solving flow in discrete fracture networks using the Virtual Element Method",
    "abstract": "The Virtual Element method allows for meshes made up by arbitrary polygonal elements. Guaranteed local and global conformity with no alteration of the geometry of the DFN. Unconstrained fracture-independent meshing. Application of domain decomposition preconditioners.",
    "keywords": [
      "vem",
      "fracture flows",
      "darcy flows",
      "discrete fracture networks"
    ]
  },
  {
    "id": "949",
    "title": "Multi-objective optimization of facility planning for energy intensive companies",
    "abstract": "Because of the energy shortage and energy price rise, energy efficiency becomes a worldwide hot spot problem. It is not only a problem about cost reduction, but also a great contribute to the environmental protection. However, the energy efficiency was always ignored in the past decades. In order to gain more benefit and become more competitive in the market, energy efficiency should be considered as an essential factor in early planning phase. To overcome these problems, a new approach, which introduces energy efficiency as a key criterion into the planning process, is presented in this article. An energy recovery network is built according to the analysis of process and product demands. Afterwards the energy loss of the whole system, transport performance and space demand are simultaneously taken into account with the purpose of finding good facility planning from both energy and economic aspects. Finally, a practical expanding case is used to validate the correctness and effectiveness of the proposed approach.",
    "keywords": [
      "energy efficiency",
      "facility planning",
      "multi objective optimization",
      "local search"
    ]
  },
  {
    "id": "950",
    "title": "Sparse margin-based discriminant analysis for feature extraction",
    "abstract": "The existing margin-based discriminant analysis methods such as nonparametric discriminant analysis use K-nearest neighbor (K-NN) technique to characterize the margin. The manifold learning-based methods use K-NN technique to characterize the local structure. These methods encounter a common problem, that is, the nearest neighbor parameter K should be chosen in advance. How to choose an optimal K is a theoretically difficult problem. In this paper, we present a new margin characterization method named sparse margin-based discriminant analysis (SMDA) using the sparse representation. SMDA can successfully avoid the difficulty of parameter selection. Sparse representation can be considered as a generalization of K-NN technique. For a test sample, it can adaptively select the training samples that give the most compact representation. We characterize the margin by sparse representation. The proposed method is evaluated by using AR, Extended Yale B database, and the CENPARMI handwritten numeral database. Experimental results show the effectiveness of the proposed method; its performance is better than some other state-of-the-art feature extraction methods.",
    "keywords": [
      "sparse margin",
      "dimensional reduction",
      "feature extraction"
    ]
  },
  {
    "id": "951",
    "title": "Market application of the percolation model: Relative price distribution",
    "abstract": "We study a variant of the Cont-Bouchaud model, which utilizes the percolation approach of multi-agent simulations of the stock market fluctuations. Here, instead of considering the relative price change as the difference of the total demand and total supply, we consider the relative price change to be proportional to the \"relative\" difference of demand and supply (the ratio of the difference in total demand and total supply to the sum of the total demand and total supply). We then study the probability distribution of the price changes.",
    "keywords": [
      "econophysics",
      "monte carlo",
      "simulation",
      "cont-bouchaud model"
    ]
  },
  {
    "id": "952",
    "title": "Feedback control strategies for spatial navigation revealed by dynamic modelling of learning in the Morris water maze",
    "abstract": "The Morris water maze is an experimental procedure in which animals learn to escape swimming in a pool using environmental cues. Despite its success in neuroscience and psychology for studying spatial learning and memory, the exact mnemonic and navigational demands of the task are not well understood. Here, we provide a mathematical model of rat swimming dynamics on a behavioural level. The model consists of a random walk, a heading change and a feedback control component in which learning is reflected in parameter changes of the feedback mechanism. The simplicity of the model renders it accessible and useful for analysis of experiments in which swimming paths are recorded. Here, we used the model to analyse an experiment in which rats were trained to find the platform with either three or one extramaze cue. Results indicate that the 3-cues group employs stronger feedback relying only on the actual visual input, whereas the 1-cue group employs weaker feedback relying to some extent on memory. Because the model parameters are linked to neurological processes, identifying different parameter values suggests the activation of different neuronal pathways.",
    "keywords": [
      "autoregression",
      "dynamic modelling",
      "learning and memory",
      "random walk",
      "navigation",
      "spatial memory",
      "water maze",
      "autocorrelation",
      "autoregressive model"
    ]
  },
  {
    "id": "953",
    "title": "A new regime for highly robust gamma oscillation with co-exist of accurate and weak synchronization in excitatoryinhibitory networks",
    "abstract": "A great number of biological experiments show that gamma oscillation occurs in many brain areas after the presentation of stimulus. The neural systems in these brain areas are highly heterogeneous. Specifically, the neurons and synapses in these neural systems are diversified; the external inputs and parameters of these neurons and synapses are heterogeneous. How the gamma oscillation generated in such highly heterogeneous networks remains a challenging problem. Aiming at this problem, a highly heterogeneous complex network model that takes account of many aspects of real neural circuits was constructed. The network model consists of excitatory neurons and fast spiking interneurons, has three types of synapses (GABAA, AMPA, and NMDA), and has highly heterogeneous external drive currents. We found a new regime for robust gamma oscillation, i.e. the oscillation in inhibitory neurons is rather accurate but the oscillation in excitatory neurons is weak, in such highly heterogeneous neural networks. We also found that the mechanism of the oscillation is a mixture of interneuron gamma (ING) and pyramidal-interneuron gamma (PING). We explained the mixture ING and PING mechanism in a consistent-way by a compound post-synaptic current, which has a slowly rising-excitatory stage and a sharp decreasing-inhibitory stage.",
    "keywords": [
      "gamma oscillation",
      "heterogeneity",
      "synapse",
      "balanced networks"
    ]
  },
  {
    "id": "954",
    "title": "Oxygen incorporation into CdS/CdTe thin film solar cells",
    "abstract": "CdS/CdTe thin films with 2.1(upmu hbox {m}) thickness were grown using R.F. magnetron sputtering in two different mixtures of Ar and (hbox {O}_{2}). The substrate was a commercially available Pilkington glass with TCO deposited. The concentration of (hbox {O}_{2}) was selected to be 0, 1 and 5%. The crystallographic, morphological, optical and electrical properties of the as-deposited samples were compared with the ones treated with (hbox {CdCl}_{2}) and subsequently annealed at high temperature. The films morphology and crystallinity were studied by X-ray diffraction and scanning electron microscopy. X-ray diffraction shows a transition of zinc blend cubic phase to hexagonal as the oxygen content increases from 0 to 5%. The measurements show the larger band gap and grain sizes for the films with higher oxygen content. The band gap and transmission rate of the O(_2)-free and oxygenated devices is different and the grains size is greatly affected by the oxygen content.",
    "keywords": [
      "cdte thin film",
      "oxygen incorporation",
      "sem",
      "x-ray diffraction"
    ]
  },
  {
    "id": "955",
    "title": "Abstracting Runtime Heaps for Program Understanding",
    "abstract": "Modern programming environments provide extensive support for inspecting, analyzing, and testing programs based on the algorithmic structure of a program. Unfortunately, support for inspecting and understanding runtime data structures during execution is typically much more limited. This paper provides a general purpose technique for abstracting and summarizing entire runtime heaps. We describe the abstract heap model and the associated algorithms for transforming a concrete heap dump into the corresponding abstract model as well as algorithms for merging, comparing, and computing changes between abstract models. The abstract model is designed to emphasize high-level concepts about heap-based data structures, such as shape and size, as well as relationships between heap structures, such as sharing and connectivity. We demonstrate the utility and computational tractability of the abstract heap model by building a memory profiler. We use this tool to identify, pinpoint, and correct sources of memory bloat for programs from DaCapo.",
    "keywords": [
      "heap structure",
      "runtime analysis",
      "memory profiling",
      "program understanding"
    ]
  },
  {
    "id": "956",
    "title": "On total domination vertex critical graphs of high connectivity",
    "abstract": "A graph is called-critical if the removal of any vertex from the graph decreases the domination number, while a graph with no isolated vertex is ?t ? t-critical if the removal of any vertex that is not adjacent to a vertex of degree 1 decreases the total domination number. A ?t ? t-critical graph that has total domination numberk k , is called k -?t ? t-critical. In this paper, we introduce a class of k -?t ? t-critical graphs of high connectivity for each integer k?3 k ? 3 . In particular, we provide a partial answer to the question Which graphs are-critical and ?t ? t-critical or one but not the other? posed in a recent work [W. Goddard, T.W. Haynes, M.A. Henning, L.C. van der Merwe, The diameter of total domination vertex critical graphs, Discrete Math. 286 (2004) 255261].",
    "keywords": [
      "total domination",
      "vertex critical",
      "connectivity",
      "diameter"
    ]
  },
  {
    "id": "957",
    "title": "IDD-based model validation of biochemical networks",
    "abstract": "This paper presents efficient techniques for the qualitative and quantitative analysis of biochemical networks, which are modeled by means of qualitative and stochastic Petri nets, respectively. The analysis includes standard Petri net properties as well as model checking of the Computation Tree Logic and the Continuous Stochastic Logic. Efficiency is achieved by using Interval decision diagrams to alleviate the well-known problem of state space explosion, and by applying operations exploiting the Petri structure and the principle of locality. All presented techniques are implemented in our tool IDD-MC which is available on our website.  ",
    "keywords": [
      "biochemical networks",
      "petri nets",
      "interval decision diagrams",
      "ct",
      "csl",
      "model checking"
    ]
  },
  {
    "id": "958",
    "title": "The Form of The Solution and Dynamics of a Rational Recursive Sequence",
    "abstract": "We discuss in this paper the form of the solutions of the following recursive sequences x(n+1) = x(n-3)x(n-4)/x(n)(+/-1 +/- x(n-3)x(n-4)), n = 0, 1, ..., where the initial conditions are arbitrary real numbers. Moreover, we study the dynamics and behavior of the solutions.",
    "keywords": [
      "difference equations",
      "recursive sequences",
      "stability",
      "periodic solution"
    ]
  },
  {
    "id": "959",
    "title": "real time self-maintenable data warehouse",
    "abstract": "Data warehousing is an approach to data integration wherein integrated information is stored in a data warehouse for direct querying and analysis. To provide fast access, a data warehouse stores materialized views of the sources of its data. As a result, a data warehouse needs to be maintained to keep its contents consistent with the contents of its data sources. Incremental maintenance is generally regarded as a more efficient way to maintain materialized views in a data warehouse. In this paper a strategy for the maintenance of data warehouse is presented. It has the following characteristics: it is self-maintainable (weak), incremental, non-blocking (the analysts transactions and the maintenance transaction are executed concurrently) and is performed in real time. The proposed algorithm is implemented for view definition SPJ (Select Project Join) queries and it calculates the aggregate functions: sum, avg, count, min and max. Aggregate functions are calculated like algebraic functions (the new result of the function can be computed using some small, constant size storage that accompanies the existing value of the aggregate). We have named this improved algorithm ?VNLTR (unlimited ?V (versions), NL (non-blocking), TR (in real time)).",
    "keywords": [
      "self-maintenable",
      "data warehouse"
    ]
  },
  {
    "id": "960",
    "title": "Constructing error-correcting pooling designs with symplectic space",
    "abstract": "We construct a family of error-correcting pooling designs with the incidence matrix of two types of subspaces of symplectic spaces over finite fields. We show that the new construction gives better ratio of efficiency compared with previously known three constructions associated with subsets of a set, its analogue over a vector space, and the dual spaces of a symplectic space.",
    "keywords": [
      "pooling designs",
      "d-disjunct matrix",
      "symplectic space",
      "totally isotropic subspaces",
      "non-isotropic subspaces"
    ]
  },
  {
    "id": "961",
    "title": "improving the memory behavior of vertical filtering in the discrete wavelet transform",
    "abstract": "The discrete wavelet transform (DWT) is used in several image and video compression standards, in particular JPEG2000. A 2D DWT consists of horizontal filtering along the rows followed by vertical filtering along the columns. It is well-known that a straightforward implementation of vertical filtering (assuming a row-major layout) induces many cache misses, due to lack of spatial locality. This can be avoided by interchanging the loops. This paper shows, however, that the resulting implementation suffers significantly from 64K aliasing, which occurs in the Pentium 4 when two data blocks are accessed that are a multiple of 64K apart, and we propose two techniques to avoid it. In addition, if the filter length is longer than four, the number of ways of the L1 data cache of the Pentium 4 is insufficient to avoid cache conflict misses. Consequently, we propose two methods for reducing conflict misses. Although experimental results have been collected on the Pentium 4, the techniques are general and can be applied to other processors with different cache organizations as well. The proposed techniques improve the performance of vertical filtering compared to already optimized baseline implementations by a factor of 3.11 for the (5,3) lifting scheme, 3.11 for Daubechies' transform of four coefficients, and by a factor of 1.99 for the Cohen, Daubechies, and Feauveau 9/7 transform.",
    "keywords": [
      "cache",
      "discrete wavelet transform",
      "memory hierarchy",
      "performance"
    ]
  },
  {
    "id": "962",
    "title": "ANALYSIS OF MULTIBACKGROUND MEMORY TESTING TECHNIQUES",
    "abstract": "March tests are widely used in the process of RAM testing. This family of tests is very efficient in the case of simple faults such as stuck-at or transition faults. In the case of a complex fault model-such as pattern sensitive faults-their efficiency is not sufficient. Therefore we have to use other techniques to increase fault coverage for complex faults. Multibackground memory testing is one of such techniques. In this case a selected March test is run many times. Each time it is run with new initial conditions. One of the conditions which we can change is the initial memory background. In this paper we compare the efficiency of multibackground tests based on four different algorithms of background generation.",
    "keywords": [
      "ram testing",
      "pattern sensitive faults",
      "march tests",
      "multibackground testing"
    ]
  },
  {
    "id": "963",
    "title": "detecting image spam using visual features and near duplicate detection",
    "abstract": "Email spam is a much studied topic, but even though current email spam detecting software has been gaining a competitive edge against text based email spam, new advances in spam generation have posed a new challenge: image-based spam. Image based spam is email which includes embedded images containing the spam messages, but in binary format. In this paper, we study the characteristics of image spam to propose two solutions for detecting image-based spam, while drawing a comparison with the existing techniques. The first solution, which uses the visual features for classification, offers an accuracy of about 98%, i.e. an improvement of at least 6% compared to existing solutions. SVMs (Support Vector Machines) are used to train classifiers using judiciously decided color, texture and shape features. The second solution offers a novel approach for near duplication detection in images. It involves clustering of image GMMs (Gaussian Mixture Models) based on the Agglomerative Information Bottleneck (AIB) principle, using Jensen-Shannon divergence (JS) as the distance measure.",
    "keywords": [
      "machine learning",
      "email spam",
      "image analysis"
    ]
  },
  {
    "id": "964",
    "title": "Exploiting local intensity information in ChanVese model for noisy image segmentation",
    "abstract": "This manuscript presents an improved region-based active contour model for noisy image segmentation. We define a local energy according to intensity information within the neighborhood of each point in image domain. By introducing a kernel function, our method employs intensity information in local region to guide the motion of active contour. Experiments on synthetic and real world images show that our model is robust to image noise while preserving the segmentation efficacy.",
    "keywords": [
      "noisy image segmentation",
      "robust chanvese model",
      "level set method",
      "variational method"
    ]
  },
  {
    "id": "965",
    "title": "A probabilistic spectral framework for grouping and segmentation",
    "abstract": "This paper presents an iterative spectral framework for pairwise clustering and perceptual grouping. Our model is expressed in terms of two sets of parameters. Firstly, there are cluster memberships which represent the affinity of objects to clusters. Secondly, there is a matrix of link weights for pairs of tokens. We adopt a model in which these two sets of variables are governed by a Bernoulli model. We show how the likelihood function resulting from this model may be maximised with respect to both the elements of link-weight matrix and the cluster membership variables. We establish the link between the maximisation of the log-likelihood function and the eigenvectors of the link-weight matrix. This leads us to an algorithm in which we iteratively update the link-weight matrix by repeatedly refining its modal structure. Each iteration of the algorithm is a three-step process. First, we compute a link-weight matrix for each cluster by taking the outer-product of the vectors of current cluster-membership indicators for that cluster. Second, we extract the leading eigenvector from each modal link-weight matrix. Third, we compute a revised link weight matrix by taking the sum of the outer products of the leading eigenvectors of the modal link-weight matrices.",
    "keywords": [
      "graph-spectral methods",
      "maximum likelihood",
      "perceptual grouping",
      "motion segmentation"
    ]
  },
  {
    "id": "966",
    "title": "composition of least privilege analysis results in software architectures (position paper)",
    "abstract": "Security principles are often neglected by software architects, due to the lack of precise definitions. This results in potentially high-risk threats to systems. Our own previous work tackled this by introducing formal foundations for the least privilege (LP) principle in software architectures and providing a technique to identify violations to this principle. This work shows that this technique can scale by composing the results obtained from the analysis of the sub-parts of a larger system. The technique decomposes the system into independently described subsystems and a description listing the interactions between these subsystems. These descriptions are thence analyzed to obtain LP violations and subsequently composed to obtain the violations of the overall system.",
    "keywords": [
      "least privilege",
      "software architecture",
      "security analysis"
    ]
  },
  {
    "id": "967",
    "title": "a software-based eye tracking system for the study of air-traffic displays",
    "abstract": "This paper describes a software-based system for offline tracking of eye and head movements using stored video images, designed for use in the study of air-traffic displays. These displays are typically dense with information; to address the research questions, we wish to be able to localize gaze within a single word within a line of text (a few minutes of arc), while at the same time allowing some freedom of movement to the subject. Accurate gaze tracking in the presence of head movements requires high precision head tracking, and this was accomplished by registration of images from a forward-looking scene camera with a narrow field of view.",
    "keywords": [
      "head and eye tracking",
      "scan-path analysis",
      "air traffic displays",
      "image registration"
    ]
  },
  {
    "id": "968",
    "title": "A new Lagrangian Relaxation Algorithm for scheduling dissimilar parallel machines with release dates",
    "abstract": "In this article we investigate the parallel machine scheduling problem with job release dates, focusing on the case that machines are dissimilar with each other. The goal of scheduling is to find an assignment and sequence for a set of jobs so that the total weighted completion time is minimised. This type of production environment is frequently encountered in process industry, such as chemical and steel industries, where the scheduling of jobs with different purposes is an important goal. This article formulates the problem as an integer linear programming model. Because of the dissimilarity of machines, the ordinary job-based decomposition method is no longer applicable, a novel machine-based Lagrangian relaxation algorithm is therefore proposed. Penalty terms associated with violations of coupling constraints are introduced to the objective function by Lagrangian multipliers, which are updated using subgradient optimisation method. For each machine-level subproblem after decomposition, a forward dynamic programming algorithm is designed together with the weighted shortest processing time rule to provide an optimal solution. A heuristics is developed to obtain a feasible schedule from the solution of subproblems to provide an upper bound. Numerical results show that the new approach is computationally effective to handle the addressed problem and provide high quality schedules.",
    "keywords": [
      "lagrangian relaxation",
      "dissimilar parallel machine",
      "release dates",
      "dynamic programming",
      "machine-based decomposition",
      "heuristics"
    ]
  },
  {
    "id": "969",
    "title": "Fast chromatography of complex biocide mixtures using diode array detection and multivariate curve resolution",
    "abstract": "Use of Multivariate Curve Resolution Alternating Least Squares (MCR-ALS) is evaluated in the analysis of complex biocide environmental sample mixtures by liquid chromatography with diode array detection (LC-DAD). Chromatographic coelution problems caused either because of the presence of unknown matrix interferences or because of using short chromatographic columns to reduce analysis times are investigated. Under such circumstances, lack of chromatographic resolution and lack of spectral selectivity of UV-VIS diode array detection is compensated by chemometric resolution using Multivariate Curve Resolution. Resolution of complex environmental mixtures and quantitative calibration curves for two types of chromatographic columns (25 and 7.5 cm) with different resolution and analysis times are shown. The limits of the proposed approach are investigated in the analysis of complex environmental samples with short LC columns and UV-VIS diode array detection.  ",
    "keywords": [
      "multivariate curve resolution",
      "mcr-als",
      "coelutions",
      "short columns",
      "lc-dad",
      "biocides"
    ]
  },
  {
    "id": "970",
    "title": "16-Channel micro magnetic flux sensor array for IGBT current distribution measurement",
    "abstract": "Current crowding of IGBT and power diode in a chip or among chips is a barrier to the realization of highly-reliable power module. The author developed and demonstrated 16-channel flat sensitivity sensor array for IGBT current distribution measurement. The sensor array consists of tiny-scale film sensors with analog amps and shield case against noise.",
    "keywords": [
      "igbt",
      "current distribution",
      "current crowding",
      "film sensor",
      "reliability analysis",
      "magnetic flux",
      "digital calibration",
      "flat sensitivity"
    ]
  },
  {
    "id": "971",
    "title": "State space reduction in modeling checking parameterized cache coherence protocol by two-dimensional abstraction",
    "abstract": "Scalability of cache coherence protocol is a key component in future shared-memory multi-core or multi-processor systems. The state space explosion is the first hurdle while applying model-checking to scalable protocols. In order to validate parameterized cache coherence protocols effectively, we present a new method of reducing the state space of parameterized systems, two-dimensional abstraction (TDA). Drawing inspiration from the design principle of parameterized systems, an abstract model of an unbounded system is constructed out of finite states. The mathematical principles underlying TDA is presented. Theoretical reasoning demonstrates that TDA is correct and sound. An example of parameterized cache coherence protocol based on MESI illustrates how to produce a much smaller abstract model by TDA. We also demonstrate the power of our method by applying it to various well-known classes of protocols. During the development of TH-1A supercomputer system, TDA was used to verify the coherence protocol in FT-1000 CPU and showed the potential advantages in reducing the verification complexity.",
    "keywords": [
      "parameterized cache coherence protocol",
      "true concurrency",
      "model checking",
      "two-dimensional abstraction"
    ]
  },
  {
    "id": "972",
    "title": "A new antialiased line drawing algorithm",
    "abstract": "Consider a line . Conventional line drawing algorithms sample (x,f(x)) on the line, where x must be an integer, and then map (x,f(x)) to the frame buffer according to the defined filter and f(x) . In this paper, we propose to simulate a sampled point (x,f(x)) by the four pixels around it where x and f(x) are not necessary to be integers. Based on the proposed low-pass filtering, we show that the effect of sampling at infinite number of points along a line segment can be achieved since the closed form of the intensities assigned to pixels exists. Furthermore, we show the coherence properties that can reduce the cost for computing these intensities.",
    "keywords": [
      "computer graphics",
      "line drawing algorithm",
      "antialiasing"
    ]
  },
  {
    "id": "973",
    "title": "High-performance short sequence alignment with GPU acceleration",
    "abstract": "Sequence alignment is a fundamental task for computational genomics research. We develop G-Aligner, which adopts the GPU as a hardware accelerator to speed up the sequence alignment process. A leading CPU-based alignment tool is based on the Bi-BWT index; however, a direct implementation of this algorithm on the GPU cannot fully utilize the hardware power due to its irregular algorithmic structure. To better utilize the GPU hardware resource, we propose a filtering-verification algorithm employing both the Bi-BWT search and direct matching. We further improve this algorithm on the GPU through various optimizations, e.g., the split of a large kernel, the warp based implementation to avoid user-level synchronization. As a result, G-Aligner outperforms another state-of-the-art GPU-accelerated alignment tools SOAP3 by 1.83.5 times for in-memory sequence alignment.",
    "keywords": [
      "sequence alignment",
      "gpgpu",
      "parallel systems"
    ]
  },
  {
    "id": "974",
    "title": "To feel or not to feel: The role of affect in human-computer interaction",
    "abstract": "The past decade has witnessed an unprecedented growth in user interface and human-computer interaction (HCI) technologies and methods. The synergy of technological and methodological progress on the one hand, and changing user expectations on the other, are contributing to a redefinition of the requirements for effective and desirable human-computer interaction. A key component of these emerging requirements, and of effective HCI in general, is the ability of these emerging systems to address user affect. The objective of this special issue is to provide an introduction to the emerging research area of affective HCI, some of the available methods and techniques, and representative systems and applications.  ",
    "keywords": [
      "affective hci",
      "affective computing",
      "affect recognition",
      "affect expression",
      "affective user modeling"
    ]
  },
  {
    "id": "975",
    "title": "Exponential stability of a class of generalized neural networks with time-varying delays",
    "abstract": "The dynamics of a class of generalized neural networks with time-varying delays are analyzed. Without constructing a Lyapunov function, general sufficient conditions for the existence, uniqueness and exponential stability of an equilibrium of the neural networks are obtained by the nonlinear Lipschitz measure approach. The new criteria are mild, independent of the delays and do not require the boundedness, differentiability or monotonicity assumption of the activation functions. Moreover, the proposed results extend and improve existing ones.",
    "keywords": [
      "neural networks",
      "time-varying delay",
      "exponential stability",
      "exponential decay",
      "nonlinear lipschitz measure"
    ]
  },
  {
    "id": "976",
    "title": "Sexism in online video games: The role of conformity to masculine norms and social dominance orientation",
    "abstract": "The Video Game Sexism Scale was created to assess attitudes toward female gamers. Conformity to some masculine norms predicted video game sexism. Social dominance orientation predicted video game sexism.",
    "keywords": [
      "video games",
      "sex role stereotypes",
      "gender roles",
      "masculinity",
      "sexual harassment",
      "social identity model of deindividuation effects"
    ]
  },
  {
    "id": "977",
    "title": "Pseudorandom generators for combinatorial checkerboards",
    "abstract": "We define a combinatorial checkerboard to be a function f : {1, . . . , m} (d) -> {1,-1} of the form for some functions f (i) : {1, . . . , m} -> {1,-1}. This is a variant of combinatorial rectangles, which can be defined in the same way but using {0, 1} instead of {1,-1}. We consider the problem of constructing explicit pseudorandom generators for combinatorial checkerboards. This is a generalization of small-bias generators, which correspond to the case m = 2. We construct a pseudorandom generator that -fools all combinatorial checkerboards with seed length . Previous work by Impagliazzo, Nisan, and Wigderson implies a pseudorandom generator with seed length . Our seed length is better except when 1/epsilon >= d(omega(log d)).",
    "keywords": [
      "pseudorandom generators",
      "combinatorial checkerboards",
      "explicit constructions",
      "derandomization"
    ]
  },
  {
    "id": "978",
    "title": "The Calendar is Crucial: Coordination and Awareness through the Family Calendar",
    "abstract": "Everyday family life involves a myriad of mundane activities that need to be planned and coordinated. We describe findings from studies of 44 different families' calendaring routines to understand how to best design technology to support them. We outline how a typology of calendars containing family activities is used by three different types of families-monocentric, pericentric, and polycentric-which vary in the level of family involvement in the calendaring process. We describe these family types, the content of family calendars, the ways in which they are extended through annotations and augmentations, and the implications from these findings for design.",
    "keywords": [
      "families",
      "coordination",
      "awareness",
      "calendars"
    ]
  },
  {
    "id": "979",
    "title": "The Myth and Reality of Reversal of Aging by Hormesis",
    "abstract": "Hormesis is an adaptive response to low doses of otherwise harmful agents by triggering a cascade of stress-specific resistance pathways. Evidence from protozoa, nematodes, flies, rodents, and primates indicate that stress-induced tolerance modulates survival and longevity. Realit is that hormesis can prolong the healthy life span. Genetic background provides the potential for longevity duration induced by stress. Senesence, or aging, is generally thought to be due to a different impact of selection for alleles positive for reproduction during early life but harmful in later life, a process called antagonistic pleiotropy (multiple phenotypic changes by a single gene). After reproduction, life span is invisible to selection. I propose the revision that mutations selected for survival until reproduction in early life may also extend later life (protagonistic pleiotropy). The protagonist candidate genes for extended life span are hormetic response genes, which activate the protective effect in both early and later life. My revision of the earlier evolutionary theory implies that natural selection of genes critical for early survival (life span until reproduction) can also be beneficial for extended longevity in old age, tipping the evolutionary balance in favor of a latent inducible life span extension unless excess stressor challenge exceeds the protection capacity. Mimetic triggers of the stress response promise the option of tricking the induction of metabolic pathways that confer resistance to environmental challenges, increased healthy life span, rejuvenation, and disease intervention without the danger of overwhelmiong damage by the stressor. Public policy should anticipate an increase in healthy life span.",
    "keywords": [
      "hormesis",
      "evolution",
      "longevity",
      "rejuvenation"
    ]
  },
  {
    "id": "980",
    "title": "bimodal hci-related affect recognition",
    "abstract": "Perhaps the most fundamental application of affective computing will be Human-Computer Interaction (HCI) in which the computer should have the ability to detect and track the user's affective states, and make corresponding feedback. The human multi-sensor affect system defines the expectation of multimodal affect analyzer. In this paper, we present our efforts toward audio-visual HCI-related affect recognition. With HCI applications in mind, we take into account some special affective states which indicate users' cognitive/motivational states. Facing the fact that a facial expression is influenced by both an affective state and speech content, we apply a smoothing method to extract the information of the affective state from facial features. In our fusion stage, a voting method is applied to combine audio and visual modalities so that the final affect recognition accuracy is greatly improved. We test our bimodal affect recognition approach on 38 subjects with 11 HCI-related affect states. The extensive experimental results show that the average person-dependent affect recognition accuracy is almost 90% for our bimodal fusion.",
    "keywords": [
      "emotion recognition",
      "affective computing",
      "multimodal human-computer interaction",
      "affect recognition"
    ]
  },
  {
    "id": "981",
    "title": "High order spacetime adaptive ADER-WENO finite volume schemes for non-conservative hyperbolic systems",
    "abstract": "Better than second order accurate spacetime adaptive mesh refinement (AMR). Time accurate local time stepping (LTS). High order ADER-WENO finite volume scheme for non-conservative hyperbolic systems. Applications to the BaerNunziato model of compressible multiphase flows in 2D and 3D. Very sharp resolution of material interfaces.",
    "keywords": [
      "adaptive mesh refinement ",
      "time accurate local time stepping",
      "high order ader approach",
      "path-conservative weno finite volume schemes",
      "compressible multi-phase flows",
      "baernunziato model"
    ]
  },
  {
    "id": "982",
    "title": "Evaluating covariance in prognostic and system health management applications",
    "abstract": "Simulated noisy data sets are used to compare the accuracy of four existing covariance estimation methodologies Among the discussed methodologies the NNVE algorithm provides the most accurate estimates of covariance. To further improve the accuracy of the covariance estimation, a new methodology based on a modification of the NNVE methodology is proposed. The proposed methodology is shown to exhibit improved performance in classification as well as anomaly detection applications.",
    "keywords": [
      "prognostics",
      "system health management",
      "covariance estimation"
    ]
  },
  {
    "id": "983",
    "title": "A Comprehensive Investigation of Wireless LAN for IEC 61850-Based Smart Distribution Substation Applications",
    "abstract": "Today's power grid is facing many challenges due to increasing load growth, aging of existing power infrastructures, high penetration of renewable, and lack of fast monitoring and control. Utilizing recent developments in Information and Communication Technologies (ICT) at the power-distribution level, various smart-grid applications can be realized to achieve reliable, efficient, and green power. Interoperable exchange of information is already standardized in the globally accepted smart-grid standard, IEC 61850, over the local area networks (LANs). Due to low installation cost, sufficient data rates, and ease of deployment, the industrial wireless LAN technologies are gaining interest among power utilities, especially for less critical smart distribution network applications. Extensive work is carried out to examine the wireless LAN (WLAN) technology within a power distribution substation. The first phase of the work is initiated with the radio noise interference measurements at 27.6- and 13.8-kV distribution substations, including circuit breaker switching operations. For a detailed investigation, the hardware prototypes of WLAN-enabled IEC 61850 devices are developed using industrial embedded systems, and the performance of smart distribution substation monitoring, control, and protection applications is analyzed for various scenarios using a round trip-time of IEC 61850 application messages. Finally, to examine the real-world field performance, the developed prototype devices are installed in the switchyard and control room of 27.6 power distribution substation, and testing results of various applications are discussed.",
    "keywords": [
      "distribution substation automation",
      "iec 61850",
      "ieee 802.11",
      "intelligent electronics devices ",
      "smart grid"
    ]
  },
  {
    "id": "984",
    "title": "An assessment of the effect of mass customization on suppliers inventory levels in a JIT supply chain",
    "abstract": "In some industries, mass customization requires a supplier to provide an Original Equipment Manufacturer (OEM) with a wide range of variants of a given part. We consider an OEM-parts suppliers system for an automotive supply chain where parts are delivered to the assembly line several times a day in a just-in-time environment. Simulating varying assembly schedule and parts delivery schemes, we assess the effect of mass customization on the level of inventory the supplier needs for each variant in order to prevent stockouts. We find, among other things, that as the level of mass customization increases, there tends to be an increase in the level of inventory the supplier needs to maintain for each part variant in order to prevent stockouts. Theoretical support is provided for the phenomenon. The presented framework is also useful for evaluating the levels of mass customization that will enable the manufacturer meet customers requirements in a cost effective manner. Furthermore, the study confirms the superiority, in terms of inventory levels, of the minmax over the minsum optimization framework.",
    "keywords": [
      "supply chain management",
      "manufacturing",
      "mass customization",
      "just-in-time assembly systems",
      "automotive"
    ]
  },
  {
    "id": "985",
    "title": "On the interaction between knowledge and social commitments in multi-agent systems",
    "abstract": "Both knowledge and social commitments have received considerable attention in Multi-Agent Systems (MASs), specially for multi-agent communication. Plenty of work has been carried out to define their semantics. However, the relationship between social commitments and knowledge has not been investigated yet. In this paper, we aim to explore such a relationship from the semantics and model checking perspectives with respect to CTLK logic (an extension of CTL logic with modality for reasoning about knowledge) and CTLC logic (an extension of CTL with modalities for reasoning about commitments and their fulfillments). To analyze this logical relationship, we simply combine the two logics in one new logic named CTLKC. The purpose of such a combination is not to advocate a new logic, but only to express and figure out some reasoning postulates merging both knowledge and commitments as they are currently defined in the literature. By so doing, we identify some paradoxes in the new logic showing that simply combining current versions of commitment and knowledge logics results in a logical language that violates some fundamental intuitions. Consequently, we propose CTLKC+, a new logic that fixes the identified paradoxes and allows us to reason about social commitments and knowledge simultaneously in a consistent manner. Furthermore, we address the problem of model checking CTLKC+ by reducing it to the problem of model checking GCTL?, a generalized version of CTL? with action formulae. By doing so, we directly benefit from CWB-NC, the model checker of GCTL?. Using this reduction, we also prove that the computational complexity of model checking CTLKC+ is still PSPACE-complete for concurrent programs as the complexity of model checking CTLK and CTLC separately.",
    "keywords": [
      "multi-agent systems",
      "social commitments",
      "agent communication",
      "knowledge"
    ]
  },
  {
    "id": "986",
    "title": "Geometric point interpolation method in R3 R 3 space with tangent directional constraint",
    "abstract": "This paper discusses a cubic B-spline interpolation problem with tangent directional constraint in R3 R 3 space. Given m points and their tangent directional vectors as well, the interpolation problem is to find a cubic B-spline curve which interpolates both the positions of the points and their tangent directional vectors. Given the knot vector of the resulting B-spline curve and parameter values to all of the data points, the corresponding control points can often be obtained by solving a system of linear equations. This paper presents a piecewise geometric interpolation method combining a unclamping technique with a knot extension technique, with which there is no need to solve a system of linear equations. It firstly uses geometric methods to construct a seed curve segment, which interpolates several data point pairs, i.e., positions and tangent directional vectors of the points. The seed segment is then extended to interpolate the remaining data point pairs one by one in a piecewise fashion. We show that a B-spline curve segment can always be extended to interpolate a new data point pair by adding two more control points. Methods for a curve segment extending to interpolate one more data point pair by adding one more control point are also provided, which are utilized to construct an interpolation B-spline curve with as small a number of control points as possible. Numerical examples show the effectiveness and the efficiency of the new method.",
    "keywords": [
      "geometric interpolation",
      "tangent directional constraint",
      "b b-spline curves",
      "knot extension",
      "unclamping"
    ]
  },
  {
    "id": "987",
    "title": "A software package for interactive motor unit potential classification using fuzzy k-NN classifier",
    "abstract": "We present an interactive software package for implementing the supervised classification task during electromyographic (EMG) signal decomposition process using a fuzzy k-NN classifier and utilizing the MATLAB high-level programming language and its interactive environment. The method employs an assertion-based classification that takes into account a combination of motor unit potential (MUP) shapes and two modes of use of motor unit firing pattern information: the passive and the active modes. The developed package consists of several graphical user interfaces used to detect individual MUP waveforms from a raw EMG signal, extract relevant features, and classify the MUPs into motor unit potential trains (MUPTs) using assertion-based classifiers.",
    "keywords": [
      "assertion-based classifiers",
      "computer interaction",
      "features extraction",
      "fuzzy k-nn",
      "motor unit potential classification",
      "user interfaces"
    ]
  },
  {
    "id": "988",
    "title": "Project selection for oil-fields development by using the AHP and fuzzy TOPSIS methods",
    "abstract": "The evaluation and selection of projects before investment decision is customarily done using, technical and information. In this paper, proposed a new methodology to provide a simple approach to assess alternative projects and help the decision-maker to select the best one for National Iranian Oil Company by using six criteria of comparing investment alternatives as criteria in an AHP and fuzzy TOPSIS techniques. The AHP is used to analyze the structure of the project selection problem and to determine weights of the criteria, and fuzzy TOPSIS method is used to obtain final ranking. This application is conducted to illustrate the utilization of the model for the project selection problems. Additionally, in the application, it is shown that calculation of the criteria weights is important in fuzzy TOPSIS method and they could change the ranking. The decision-maker can use these different weight combinations in the decision-making process according to priority.",
    "keywords": [
      "project selection",
      "ahp",
      "fuzzy topsis",
      "decision-maker",
      "criteria"
    ]
  },
  {
    "id": "989",
    "title": "On 2k-Variable Symmetric Boolean Functions With Maximum Algebraic Immunity k",
    "abstract": "Given a positive even integer n, it is found that the weight distribution of any n-variable symmetric Boolean function with maximum algebraic immunity ( AI) n/2 is determined by the binary expansion of n. Based on the foregoing, all n-variable symmetric Boolean functions with maximum AI are constructed. The amount is (2wt(n) + 1)2([log2n]).",
    "keywords": [
      "algebraic attack",
      "algebraic immunity ",
      "symmetric boolean function"
    ]
  },
  {
    "id": "990",
    "title": "A theoretical study of transition metal complexes of C60 and C70 and their ring-opened alternatives",
    "abstract": "Ring opened structures of C60 and C70 are shown to be stabilized by complexation with transition metal fragments of the form CnHnM, where n = 3 to 6 and M = Cr, Mn, Fe, Co, and Rh. The ring opening of C60 and C70 is compared with the reverse process of the well-known catalytic conversion of acetylene into benzene. Calculations at the semi-empirical PM3(tm) level show that the 6-membered ring in C60 and C70 can be opened up in different ways through complexation with transition metal fragment. The mode of ring opening depends on the number of external 5- and 6-membered rings around the 6-membered ring being cleaved. The structures and energetics of the various ring-opened structures are discussed.  2001 by Elsevier Science Inc.",
    "keywords": [
      "cage-opened fullerenes",
      "c60",
      "c70",
      "metal complexes",
      "pm3"
    ]
  },
  {
    "id": "991",
    "title": "An optimal Galerkin scheme to solve the kinematic dynamo eigenvalue problem in a full sphere",
    "abstract": "The kinematic dynamo approximation describes the generation of magnetic field in a prescribed flow of electrically-conducting liquid. One of its main uses is as a proof-of-concept tool to test hypotheses about self-exciting dynamo action. Indeed, it provided the very first quantitative evidence for the possibility of the geodynamo. Despite its utility, due to the requirement of resolving fine structures, historically, numerical work has proven difficult and reported solutions were often plagued by poor convergence. In this paper, we demonstrate the numerical superiority of a Galerkin scheme in solving the kinematic dynamo eigenvalue problem in a full sphere. After adopting a poloidaltoroidal decomposition and expanding in spherical harmonics, we express the radial dependence in terms of a basis of exponentially convergent orthogonal polynomials. Each basis function is constructed from a terse sum of one-sided Jacobi polynomials that not only satisfies the boundary conditions of matching to an electrically insulating exterior, but is everywhere infinitely differentiable, including at the origin. This Galerkin method exhibits more rapid convergence, for a given problem size, than any other scheme hitherto reported, as demonstrated by a benchmark of the magnetic diffusion problem and by comparison to numerous kinematic dynamos from the literature. In the axisymmetric flows we consider in this paper, at a magnetic Reynolds number of O(100), a convergence of 9 significant figures in the most unstable eigenvalue requires only 40 radial basis functions; alternatively, 4 significant figures requires 20 radial functions. The terse radial discretization becomes particularly advantageous when considering flows whose associated numerical solution requires a large number of coupled spherical harmonics. We exploit this new method to confirm the tentatively proposed positive growth rate of the planar flow of Bachtiar et al. [4], thereby verifying a counter-example to the Zeldovich anti-dynamo theorem in a spherical geometry.",
    "keywords": [
      "kinematic dynamo",
      "zeldovich theorem",
      "galerkin method",
      "jacobi polynomial",
      "basis function",
      "eigenvalue",
      "convergence"
    ]
  },
  {
    "id": "992",
    "title": "Logistic regression, AdaBoost and Bregman distances",
    "abstract": "We give a unified account of boosting and logistic regression in which each learning problem is cast in terms of optimization of Bregman distances. The striking similarity of the two problems in this framework allows us to design and analyze algorithms for both simultaneously, and to easily adapt algorithms designed for one problem to the other. For both problems, we give new algorithms and explain their potential advantages over existing methods. These algorithms are iterative and can be divided into two types based on whether the parameters are updated sequentially (one at a time) or in parallel (all at once). We also describe a parameterized family of algorithms that includes both a sequential- and a parallel-update algorithm as special cases, thus showing how the sequential and parallel approaches can themselves be unified. For all of the algorithms, we give convergence proofs using a general formalization of the auxiliary-function proof technique. As one of our sequential-update algorithms is equivalent to AdaBoost, this provides the first general proof of convergence for AdaBoost. We show that all of our algorithms generalize easily to the multiclass case, and we contrast the new algorithms with the iterative scaling algorithm. We conclude with a few experimental results with synthetic data that highlight the behavior of the old and newly proposed algorithms in different settings.",
    "keywords": [
      "logistic regression",
      "maximum-entropy methods",
      "boosting",
      "adaboost",
      "bregman distances",
      "convex optimization",
      "iterative scaling",
      "information geometry"
    ]
  },
  {
    "id": "993",
    "title": "Principles of scatter search",
    "abstract": "Scatter search is an evolutionary method that has been successfully applied to hard optimization problems. The fundamental concepts and principles of the method were first proposed in the 1970s, based on formulations dating back to the 1960s for combining decision rules and problem constraints. In contrast to other evolutionary methods like genetic algorithms, scatter search is founded on the premise that systematic designs and methods for creating new solutions afford significant benefits beyond those derived from recourse to randomization. It uses strategies for search diversification and intensification that have proved effective in a variety of optimization problems. This paper provides the main principles and ideas of scatter search and its generalized form path relinking. We first describe a basic design to give the reader the tools to create relatively simple implementations. More advanced designs derive from the fact that scatter search and path relinking are also intimately related to the tabu search (TS) metaheuristic, and gain additional advantage by making use of TS adaptive memory and associated memory-exploiting mechanisms capable of being tailored to particular contexts. These and other advanced processes described in the paper facilitate the creation of sophisticated implementations for hard problems that often arise in practical settings. Due to their flexibility and proven effectiveness, scatter search and path relinking can be successfully adapted to tackle optimization problems spanning a wide range of applications and a diverse collection of structures, as shown in the papers of this volume.",
    "keywords": [
      "metaheuristics",
      "evolutionary computations",
      "search theory",
      "path relinking"
    ]
  },
  {
    "id": "994",
    "title": "Weakly quasi-Hamiltonian-set-connected multipartite tournaments",
    "abstract": "A multipartite or c-partite tournament is an orientation of a complete c-partite graph. Lu and Guo (submitted for publication)[3] recently introduced strong quasi-Hamiltonian-connectivity of a multipartite tournament D as follows: For any two distinct vertices x and y of D , there is a path with at least one vertex from each partite set of D from x to y and from y to x . We obtain the definition for weak quasi-Hamiltonian-connectivity, where only one of those paths, and weak quasi-Hamiltonian-set-connectivity, where only one such path between every two distinct partite sets has to exist, in a natural way. In this paper, we characterize weakly quasi-Hamiltonian-set-connected multipartite tournaments which extends a result of Thomassen (1980)[6].",
    "keywords": [
      "multipartite tournament",
      "quasi-hamiltonian-connectivity",
      "weak quasi-hamiltonian-set-connectivity"
    ]
  },
  {
    "id": "995",
    "title": "Analysing knowledge requirements: a case study",
    "abstract": "This paper presents the findings of a knowledge audit conducted to determine the knowledge requirements of a large service-based enterprise in South Africa. The objective of the knowledge audit was to identify and describe the current and future knowledge requirements of the enterprise. The results indicated that employees have some basic knowledge and information needs that must be satisfied before any further investigations take place. Once the fundamental building blocks of knowledge content are established, it is recommended that more sophisticated solutions can be developed. Broad recommendations for establishing a knowledge management strategy that will be a source of sustainable competitive advantage are proposed.",
    "keywords": [
      "knowledge management",
      "south africa",
      "service industries"
    ]
  },
  {
    "id": "996",
    "title": "NEGATIVE RESISTANCE ACTIVE RESISTOR WITH IMPROVED LINEARITY AND FREQUENCY RESPONSE",
    "abstract": "An original active resistor circuit will be presented. The main advantages of the new proposed implementations are the improved linearity, small area consumption and improved frequency response. An original technique for linearizing the I( V) characteristic of the active resistor will be proposed, based on the utilization of a new linear differential amplifier, and on a current-pass circuit. The linearization of the original differential structure is achieved by compensating the quadratic characteristic of the MOS transistor operating in the saturation region by an original square-root circuit. The errors introduced by the second-order effects will be strongly reduced, while the circuit frequency response of the circuit is very good as a result of operating all MOS transistors in the saturation region. In order to design a circuit having a negative equivalent resistance, an original method specific to the proposed implementation of the active resistor circuit will be presented. The circuit is implemented in 0.35 mu m CMOS technology, the SPICE simulation con. firming the theoretical estimated results and showing a linearity error under a percent for an extended input range (+/- 500mV) and a small value of the supply voltage (+/- 3V).",
    "keywords": [
      "active resistor",
      "linearity",
      "negative equivalent resistance"
    ]
  },
  {
    "id": "997",
    "title": "Quantum computation, quantum theory and AI",
    "abstract": "The main purpose of this paper is to examine some (potential) applications of quantum computation in AI and to review the interplay between quantum theory and AI. For the readers who are not familiar with quantum computation, a brief introduction to it is provided, and a famous but simple quantum algorithm is introduced so that they can appreciate the power of quantum computation. Also, a (quite personal) survey of quantum computation is presented in order to give the readers a (unbalanced) panorama of the field. The author hopes that this paper will be a useful map for AI researchers who are going to explore further and deeper connections between AI and quantum computation as well as quantum theory although some parts of the map are very rough and other parts are empty, and waiting for the readers to fill in.  ",
    "keywords": [
      "quantum computation",
      "quantum theory",
      "search",
      "learning",
      "discrimination and recognition",
      "bayesian network",
      "semantic analysis",
      "communication"
    ]
  },
  {
    "id": "998",
    "title": "automatic recognition of lower facial action units",
    "abstract": "The face is an important source of information in multimodal communication. Facial expressions are generated by contractions of facial muscles, which lead to subtle changes in the area of the eyelids, eye brows, nose, lips and skin texture, often revealed by wrinkles and bulges. To measure these subtle changes, Ekman et al.[5] developed the Facial Action Coding System (FACS). FACS is a human-observer-based system designed to detect subtle changes in facial features, and describes facial expressions by action units (AUs). We present a technique to automatically recognize lower facial Action Units, independently from one another. Even though we do not explicitly take into account AU combinations, thereby making the classification process harder, an average F 1 score of 94.83% is achieved.",
    "keywords": [
      "facial action units",
      "svm",
      "ovl",
      "adaboost"
    ]
  },
  {
    "id": "999",
    "title": "Development, implementation, and analysis of direct integration offline programming method",
    "abstract": "Wire bond programming (WBP) consists of information required to drive a wire bond machines movement during the wire bonding process. Wire bond programs consist of three key components: material handling, bonding parameter, and bonding path instructions. Of the three components, the bonding path component requires effort and time the most to prepare, as the preparation of bond path is currently being carried out manually. The manual process is tedious and error prone. In comparison to a manual process, offline programming (OLP) of bonding path creation provides a much more reliable and a less tedious method as it is error proof. OLP can be categorized into two versions, mainly vendor specific OLP and direct integration offline programming (Di-OLP), which is presented in this paper. Vendor specific OLP utilizes bonding diagrams created by a computer-aided design program to generate wire bonding paths. Di-OLP on the other hand utilizes the numeric coordinate data extracted from the bonding diagram creation software to generate the bonding path component of the wire bond program. Di-OLP is a more flexible method as it has the potential to be adapted to different machine platforms. This paper explains the challenges in the implementation of Di-OLP. The effectiveness and efficiency of the program created by Di-OLP are evaluated as compared to a manual programming method. Final results indicate that the offline programming is more efficient as it greatly reduces the time required to create the bonding paths for wire bond programs as compared to the manual methodology.",
    "keywords": [
      "wire bonding",
      "offline programming",
      "computer-aided design",
      "direct integration offline programming",
      "bondlist"
    ]
  },
  {
    "id": "1000",
    "title": "Transport protocols for wireless sensor networks: State-of-the-art and future directions",
    "abstract": "Characteristics of wireless sensor networks, specifically dense deployment, limited processing power, and limited power supply ,provide unique design challenges at the transport layer. Message transmission between sensor nodes over a wireless medium is especially expensive. Care must be taken to design an efficient transport layer protocol that combines reliable message delivery and congestion control with minimal overhead and retransmission. Sensor networks are created using low cost, low power nodes. Wireless sensors are assumed to have a finite lifetime; care must be taken to design and implement transport layer algorithms that allow maximum network lifetime. In this paper we present current and future challenges in the design of transport layers for sensor networks. Current transport layer protocols are compared based on how they implement reliable message delivery, congestion control, and energy efficiency.",
    "keywords": [
      "wireless",
      "networking",
      "wireless sensor network",
      "wsn",
      "transport layer",
      "layer 4",
      "end-to-end reliability"
    ]
  },
  {
    "id": "1001",
    "title": "Optimal braking and estimation of tyre friction in automotive vehicles using sliding modes",
    "abstract": "This article proposes a sliding-mode-based scheme for optimal deceleration in an automotive braking maneuvre. The scheme is model-based and seeks to maintain the longitudinal slip value associated with the tyre road contact patch at an optimum value - the point at which the friction coefficient-slip curve reaches a maximum. The scheme assumes only wheel angular velocity is measured, and uses a sliding mode observer to reconstruct the states and a parameter relating to road conditions for use in the controller. The sliding mode controller then seeks to maintain the vehicle at this optimal slip value through an appropriate choice of sliding surface.",
    "keywords": [
      "sliding modes",
      "observers",
      "nonlinear systems",
      "friction estimation"
    ]
  },
  {
    "id": "1002",
    "title": "Representing parametric probabilistic models tainted with imprecision",
    "abstract": "Numerical possibility theory, belief functions have been suggested as useful tools to represent imprecise, vague or incomplete information. They are particularly appropriate in uncertainty analysis where information is typically tainted with imprecision or incompleteness. Based on their experience or their knowledge about a random phenomenon, experts can sometimes provide a class of distributions without being able to precisely specify the parameters of a probability model. Frequentists use two-dimensional Monte-Carlo simulation to account for imprecision associated with the parameters of probability models. They hence hope to discover how variability and imprecision interact. This paper presents the limitations and disadvantages of this approach and propose a fuzzy random variable approach to treat this kind of knowledge.  ",
    "keywords": [
      "imprecise probabilities",
      "possibility",
      "belief functions",
      "probability-boxes",
      "monte-carlo 2d",
      "fuzzy random variable"
    ]
  },
  {
    "id": "1003",
    "title": "Designing hypermedia tools for solving problems in mathematics",
    "abstract": "This article presents the design and preparation using hypermedia tools of an interactive CD-ROM for the active teaching and learning of diverse problem-solving strategies in Mathematics for secondary school students. The use of the CD-ROM allows the students to learn, interactively, the heuristic style of solving problems. A range of problems has been used, each of which requires different solving strategies. A complementary section for consulting the theoretical foundations for the process of solving problems and other related information is also included on the CD-ROM. This section provides both theoretical and curriculum support for teachers.  ",
    "keywords": [
      "computer mediated communication",
      "interactive learning environments",
      "multimedia/hypermedia systems",
      "secondary education",
      "teaching/learning strategies"
    ]
  },
  {
    "id": "1004",
    "title": "a service framework for carrier grade multimedia services using parplay apis over a sip system",
    "abstract": "The implementation of new mobile communication technologies developed in the third generation partnership project (3GPP) will allow to access the Internet not only from a PC but also via mobile phones, palmtops and other devices. New applications will emerge, combining several basic services like voice telephony, e-mail, voice over IP, mobility or web-browsing, and thus wiping out the borders between the fixed telephone network, mobile radio and the Internet. Offering those value-added services will be the key factor for success of network and service providers in an increasingly competitive market. In 3GPP's service framework the use of the PARLAY APIs is proposed that allow application development by third parties in order to speed up service creation and deployment. 3GPP has also adopted SIP for session control of multimedia communications in an IP network. This paper proposes a mapping of SIP functionality to PARLAY services and describes a prototype implementation using the SIP Servlet API. Furthermore an architecture of a Service Platform is presented that offers a framework for the creation, execution and management of carrier grade multimedia services in heterogeneous networks.",
    "keywords": [
      "carrier grade services",
      "network-independent services",
      "sir-parlay mapping",
      "caller preferences",
      "service platform"
    ]
  },
  {
    "id": "1005",
    "title": "Condition for relaxed Monte Carlo method of solving systems of linear equations",
    "abstract": "In this paper, we point out the limitation of the paper entitled \"Solving Systems of Linear Equations with Relaxed Monte Carlo Method\" published in this journal (Tan in J. Supercomput. 22:113-123, 2002). We argue that the relaxed Monte Carlo method presented in Sect. 7 of the paper is only correct under the condition that the coefficient matrix A must be diagonal dominate. However, for nondiagonal dominate case; the corresponding Neumann series may diverge, which would lead to infinite loop when simulating the iterative Monte Carlo algorithm. In this paper, we first prove that only for the diagonal dominate matrix, the corresponding von Neumann series can converge, and the Monte Carlo algorithm can be relaxed. Therefore, it is not true for nondiagonal dominate matrix, no matter the relaxed parameter gamma is a single value or a set of values. We then present and analyze the numerical experiment results to verify our arguments.",
    "keywords": [
      "monte carlo methods",
      "relaxed monte carlo method",
      "diagonal dominate matrix"
    ]
  },
  {
    "id": "1006",
    "title": "A computational fluid dynamics study of inspiratory flow in orotracheal geometries",
    "abstract": "Computational fluid dynamics (CFD) has been used to investigate the flow of air through the human orotracheal system. Results from an idealised geometry, and from a patient-specific geometry created from MRI scans were compared. The results showed a significant difference in the flow structures between the two geometries. Inert particles with diameters in the range 19?m were tracked through the two geometries. Particle diameter has proved to be an important factor in defining the eventual destinations of inhaled particles. Results from our calculations match other experimental and computational results in the literature, and differences between the idealised and patient-specific geometries are less significant.",
    "keywords": [
      "cfd",
      "image based meshing",
      "respiration"
    ]
  },
  {
    "id": "1007",
    "title": "modelling and evolutionary multi-objective evaluation of interdependencies and work processes in airport operations",
    "abstract": "An airport is a multi-stakeholders environment, with work processes and operations cutting across a number of organizations. Airport landside operations involve a variety of services and entities that interact and depend on each others. In this paper, we introduce the Landside Modelling and Analysis of Services (LAMAS) tool, which is a multi-agent system, to simulate, analyze and evaluate the interdependencies of services in airport operations. A genetic algorithm is used to distribute resources among the different entities in an airport such that the level of service is maintained. The problem is modelled as a multi-objective constrained resource allocation problem with the objective functions being the maximization of quality of service while reducing the total cost.",
    "keywords": [
      "quality of service",
      "genetic algorithm",
      "multi-agent system",
      "airport landside",
      "work-processes"
    ]
  },
  {
    "id": "1008",
    "title": "A quantitative and qualitative assessment of aspectual feature modules for evolving software product lines",
    "abstract": "Variability mechanisms are systematically evaluated in the evolution of SPLs. FOP and AFM have shown better adherence to the Open-Closed Principle than CC. When crosscutting concerns are present, AFM are recommended over FOP. Refactoring at component level has important impact in AFM and FOP. CC compilation should be avoided when modular design is an important requirement.",
    "keywords": [
      "software product lines",
      "feature-oriented programming",
      "aspect-oriented programming",
      "aspectual feature modules",
      "variability mechanisms"
    ]
  },
  {
    "id": "1009",
    "title": "Circular Cone: A novel approach for protein ligand shape matching using modified PCA",
    "abstract": "Nowadays in modern medicine, computer modeling has already become one of key methods toward the discovery of new pharmaceuticals. And virtual screening is a necessary process for this discovery. In the procedure of virtual screening, shape matching is the first step to select ligands for binding protein. In the era of HTS (high throughput screening), a fast algorithm with good result is in demand. Many methods have been discovered to fulfill the requirement. Our method, called Circular Cone, by finding principal axis, gives another way toward this problem. We use modified PCA (principal component analysis) to get the principal axis, around which the rotation is like whirling a cone. By using this method, the speed of giving score to a pocket and a ligand is very fast, while the accuracy is ordinary. So, the good speed and the general accuracy of our method present a good choice for HTS.",
    "keywords": [
      "shape matching",
      "pocket",
      "ligand",
      "new pharmaceuticals",
      "virtual screen",
      "circular cone"
    ]
  },
  {
    "id": "1010",
    "title": "XML-based agent communication, migration and computation in mobile agent systems",
    "abstract": "This article presents the research work that exploits using XML (Extensible Markup Language) to represent different types of information in mobile agent systems, including agent communication messages, mobile agent messages, and other system information. The goal of the research is to build a programmable information base in mobile agent systems through XML representations. The research not only studies using XML in binary agent system space such as representing agent communication messages and mobile agent messages, but also explores interpretive XML data processing to avoid the need of an interface layer between script mobile agents and system data represented in XML. These XML-based information representations have been implemented in Mobile-C, a FIPA (The Foundation for Intelligent Physical Agents) compliant mobile agent platform. Mobile-C uses FIPA ACL (Agent Communication Language) messages for both inter-agent communication and inter-platform migration. Using FIPA ACL messages for agent migration in FIPA compliant agent systems simplifies agent platform, reduces development effort, and easily achieves inter-platform migration through well-designed communication mechanisms provided in the system. The ability of interpretive XML data processing allows mobile agents in Mobile-C directly accessing XML data information without the need of an extra interface layer.",
    "keywords": [
      "mobile agents",
      "agent communication",
      "mobility",
      "xml"
    ]
  },
  {
    "id": "1011",
    "title": "Automatic proxy-based watermarking for WWW",
    "abstract": "With the appearance of digital libraries and information archive centers on the Internet, ",
    "keywords": [
      "internet",
      "digital libraries",
      "copyright",
      "watermarking",
      "proxy server",
      "content transformation"
    ]
  },
  {
    "id": "1012",
    "title": "On k-convex polygons",
    "abstract": "We introduce a notion of k-convexity and explore polygons in the plane that have this property. Polygons which are k-convex can be triangulated with fast yet simple algorithms. However, recognizing them in general is a 3SUM-hard problem. We give a characterization of 2-convex polygons, a particularly interesting class, and show how to recognize them in O ( n log n ) time. A description of their shape is given as well, which leads to Erd?sSzekeres type results regarding subconfigurations of their vertex sets. Finally, we introduce the concept of generalized geometric permutations, and show that their number can be exponential in the number of 2-convex objects considered.",
    "keywords": [
      "convexity",
      "visibility",
      "transversal theory"
    ]
  },
  {
    "id": "1013",
    "title": "Robot formation motion planning using Fast Marching",
    "abstract": "This paper presents the application of the Voronoi Fast Marching (VFM) method to path planning of mobile formation robots. The VFM method uses the propagation of a wave (Fast Marching) operating on the world model to determine a motion plan over a viscosity  map (similar to the refraction index in optics) extracted from the updated map model. The computational efficiency of the method allows the planner to operate at high rate sensor frequencies. This method allows us to maintain good response time and smooth and safe planned trajectories. The navigation function can be classified as a type of potential field, but it has no local minima, it is complete (it finds the solution path if it exists) and it has a complexity of order n(O(n)) n ( O ( n ) ) , where n is the number of cells in the environment map. The results presented in this paper show how the proposed method behaves with mobile robot formations and generates trajectories of good quality without problems of local minima when the formation encounters non-convex obstacles.",
    "keywords": [
      "robot formation motion planning",
      "formation control",
      "fast marching"
    ]
  },
  {
    "id": "1014",
    "title": "Optimization of CAMD techniques 3. Virtual screening enrichment studies: a help or hindrance in tool selection",
    "abstract": "Over the last few years many articles have been published in an attempt to provide performance benchmarks for virtual screening tools. While this research has imparted useful insights, the myriad variables controlling said studies place significant limits on results interpretability. Here we investigate the effects of these variables, including analysis of calculation setup variation, the effect of target choice, active/decoy set selection (with particular emphasis on the effect of analogue bias) and enrichment data interpretation. In addition the optimization of the publicly available DUD benchmark sets through analogue bias removal is discussed, as is their augmentation through the addition of large diverse data sets collated using WOMBAT.",
    "keywords": [
      "virtual screening",
      "enrichment",
      "validation",
      "analogue bias",
      "chemotypes",
      "dud",
      "wombat"
    ]
  },
  {
    "id": "1015",
    "title": "Identification of stock market forces in the system adaptation framework",
    "abstract": "Based on the system adaptation framework which has been proposed in our previous work, this paper focuses on the input selection of this framework to identify crucial market influential factors. We first carry out an empirical research to preselect influential factors from economic and sentimental aspects. The causal relationship between each of them and the internal residue of the market is then tested. Lastly, a multicollinearity test is applied to those factors that show significant causality to the internal residue of the market to exclude the redundant indicators. As the causal relationship plays an essential role in this method, both linear time-varying and nonlinear causality tests are employed based on the predictive ability of our framework. This double selection method is applied to the US and China stock markets, and it is shown to be efficient in identifying market influential factors. We also find that these influential factors are market-dependent and frequency-dependent. Some well-tested factors in the developed market and literature may not work in the emerging market.",
    "keywords": [
      "financial system modeling",
      "system adaptation",
      "market input selection",
      "causality test"
    ]
  },
  {
    "id": "1016",
    "title": "mixed-integer quadrangulation",
    "abstract": "We present a novel method for quadrangulating a given triangle mesh. After constructing an as smooth as possible symmetric cross field satisfying a sparse set of directional constraints (to capture the geometric structure of the surface), the mesh is cut open in order to enable a low distortion unfolding. Then a seamless globally smooth parametrization is computed whose iso-parameter lines follow the cross field directions. In contrast to previous methods, sparsely distributed directional constraints are sufficient to automatically determine the appropriate number, type and position of singularities in the quadrangulation. Both steps of the algorithm (cross field and parametrization) can be formulated as a mixed-integer problem which we solve very efficiently by an adaptive greedy solver. We show several complex examples where high quality quad meshes are generated in a fully automatic manner.",
    "keywords": [
      "singularities",
      "parametrization",
      "mixed-integer",
      "remeshing",
      "direction field",
      "quadrangulation"
    ]
  },
  {
    "id": "1017",
    "title": "Holographic reduction, interpolation and hardness",
    "abstract": "We introduce the method of proving complexity dichotomy theorems by holographic reductions. Combined with interpolation, we present a unified strategy to prove #P-hardness. Specifically, we prove a complexity dichotomy theorem for a class of counting problems on 2-3 regular graphs expressible by Boolean signatures. For these problems, whenever a holographic reduction followed by interpolation fails to prove #P-hardness, we can show that the problem is solvable in polynomial time.",
    "keywords": [
      "holographic reduction",
      "polynomial interpolation",
      "#p-hard",
      "counting complexity"
    ]
  },
  {
    "id": "1018",
    "title": "Intention Learning From Human Demonstration",
    "abstract": "Equipped with better sensing and learning capabilities, robots nowadays are meant to perform versatile tasks. To remove the load of detailed analysis and programming from the engineer, a concept has been proposed that the robot may learn how to execute the task from human demonstration by itself Following the idea, in this paper, we propose an approach for the robot to learn the intention of the demonstrator from the resultant trajectory during task execution. The proposed approach, identifies the portions of the trajectory that correspond to delicate and skillful maneuvering. Those portions, referred to as motion features, may implicate the intention of the demonstrator. As the trajectory may result from so many possible intentions, it poses a severe challenge on finding the correct one's. We first formulate the problem into a realizable mathematical form and then employ the method of dynamic programming for the search. Experiments based on the pouring and also fruit jam tasks are performed to demonstrate the proposed approach, in which the derived intention is used to execute the same task under different experimental settings.",
    "keywords": [
      "intention learning",
      "human demonstration",
      "motion feature",
      "robot imitation",
      "skill transfer"
    ]
  },
  {
    "id": "1019",
    "title": "Port partitioning and dynamic queueing for IP forwarding",
    "abstract": "With the increase of internet protocol (IP) packets the performance of routers became an important issue in internet/working. In this paper we examine the matching algorithm in gigabit router which has input queue with virtual output queueing. Dynamic queue scheduling is also proposed to reduce the packet delay and packet loss probability. Port partitioning is employed to reduce the computational burden of the scheduler in a switch which matches the input and output ports for fast packet switching. Each port is divided into two groups such that the matching algorithm is implemented within each pair of groups in parallel. The matching is performed by exchanging the pair of groups at every time slot. Two algorithms, maximal weight matching by port partitioning (MPP) and modified maximal weight matching by port partitioning (MMPP) are presented. In dynamic queue scheduling, a popup decision rule for each delay critical packet is made to reduce both the delay of the delay critical packet and the loss probability of loss critical packet. Computational results show that MMPP has the lowest delay and requires the least buffer size. The throughput is illustrated to be linear to the packet arrival rate, which can be achieved under highly efficient matching algorithm. The dynamic queue scheduling is illustrated to be highly effective when the occupancy of the input buffer is relatively high. To cope with the increasing internet traffic, it is necessary to improve the performance of routers. To accelerate the switching from input ports to output in the router partitioning of ports and dynamic queueing are proposed. Input and output ports are partitioned into two groups A/B and a/b, respectively. The matching for the packet switching is performed between group pairs (A, a) and (B, b) in parallel at one time slot and (A, b) and (B, a) at the next time slot. Dynamic queueing is proposed at each input port to reduce the packet delay and packet loss probability by employing the popup decision rule and applying it to each delay critical packet. The partitioning of ports is illustrated to be highly effective in view of delay, required buffer size and throughput. The dynamic queueing also demonstrates good performance when the traffic volume is high.",
    "keywords": [
      "ip-forwarding",
      "scheduling",
      "switch",
      "dynamic-queueing"
    ]
  },
  {
    "id": "1020",
    "title": "Learning and diagnosis of individual and class conceptual perspectives: an intelligent systems approach using clustering techniques",
    "abstract": "In a classroom, a teacher attempts to convey his or her knowledge to the students. and thus it is important for the teacher to obtain formative feedback about how well students are understanding the new material. By gaining insight into the students' understanding and possible misconceptions. the teacher will be able to adjust the teaching and to supply more useful learning materials as necessary. Therefore. the diagnosis of formative student evaluations is critical for teachers and learners, as is the diagnosis of patterns in the overall learning by a class in order to inform a teacher about the efficacy of his or her teaching. This paper investigates what might be called the \"class learning diagnosis problem\" by embedding important concepts in a test and analyzing the results with a hierarchical coding scheme. Based on previous research. the part-of and type-of relationships among concepts are used to construct a concept hierarchy that may then be coded hierarchically. All concepts embedded in the test items then can be formulated into concept matrices, and the answer sheets of the learners in a class are then analyzed to indicate particular types of concept errors. The trajectories of concept errors are studied to identify both individual misconceptions students might have as well as patterns of misunderstanding in the overall class. In particular, a clustering algorithm is employed to distinguish student groups who might share similar misconceptions. These approaches are implemented as an integrated module in a previously developed system and applied to two real classroom data sets, the results of which show the practicability of this proposed method.  ",
    "keywords": [
      "concept map",
      "misconception",
      "learning diagnosis",
      "community",
      "clustering"
    ]
  },
  {
    "id": "1021",
    "title": "A modified ABC algorithm for the stage shop scheduling problem",
    "abstract": "A modified artificial bee colony algorithm is proposed for the stage shop scheduling problem. The stage shop is a new extension for the mixed shop problem and as a result, for job shop and open shop problems. In employed bee phase of ABC, a potent neighborhood of the stage shop is used and a tabu search manner is substituted for greedy selection. In onlooker bee phase of ABC, particle swarm optimization idea is applied instead of completely random search. The proposed algorithm obtained new optimal solutions and upper bounds for benchmark problems.",
    "keywords": [
      "scheduling",
      "stage shop",
      "artificial bee colony",
      "cma-es",
      "particle swarm optimization"
    ]
  },
  {
    "id": "1022",
    "title": "demonstrating remote operation of industrial devices using mobile phones",
    "abstract": "In the SmartFactory KL , the intelligent factory of the future, a consortium of companies and research facilities explores new, intelligent technologies. Being a development and demonstration center for industrial applications, the SmartFactory KL is arbitrarily modifiable and expandable (flexible), connects components from multiple manufacturers (networked), enables its components to perform context-related tasks autonomously (self-organizing), and emphasizes user-friendliness (user-oriented). In this paper, we present a prototypical system that enables commercial mobile phones to monitor, diagnose, and remotely control plant components via Bluetooth.",
    "keywords": [
      "remote operation",
      "mobile interaction",
      "flexible automation",
      "wireless system integration",
      "agile control",
      "smartphone"
    ]
  },
  {
    "id": "1023",
    "title": "Modulation diversity for frequency-selective fading channels",
    "abstract": "In this correspondence, modulation diversity (MD) for frequency-selective fading channels is proposed. The achievable performance with MD is analyzed and a simple design criterion for MD codes for Rayleigh-fading channels is deduced from an upper bound on the pairwise error probability (PEP) for single-symbol transmission. This design rule is similar to the well-known design rule for MD codes for flat fading and does not depend on the power-delay profile of the fading channel. Several examples for MD codes with prescribed properties are given and compared. Besides the computationally costly optimum receiver, efficient low-complexity linear equalization (LE) and decision-feedback equalization (DFE) schemes for MD codes are also introduced. Simulations for the widely accepted COST fading models show that performance gains of several decibels can be achieved by MD combined with LE or DFE at bit-error rates (BERs) of practical interest. In addition, MD also enables the suppression of cochannel interference.",
    "keywords": [
      "code design",
      "equalization",
      "modulation diversity",
      "performance bounds"
    ]
  },
  {
    "id": "1024",
    "title": "team work in software development student projects",
    "abstract": "To enhance students' capabilities regarding team work in real software development projects, should be a major objective for any Computer Science department within a University. Starting from the authors' experience in coordinating student teams for the development of complex projects, this paper outlines a set of considerations in this regard.",
    "keywords": [
      "software life cycle",
      "team work",
      "student project",
      "software development"
    ]
  },
  {
    "id": "1025",
    "title": "Gas desorption electron stimulated during operation of field emitter phosphor screen pairs",
    "abstract": "Gas desorption from field emitter array (FEA) cathode and phosphor screen anode in a flat panel display during lifetime operation can affect cathode electron emission and degrade display performance and uniformity. We have measured the outgassing products from selected FEA-phosphor pairs in an ultrahigh vacuum system equipped with a calibrated quadrople residual gas analyzer. Different low voltage phosphors and blank anodes were studied. A Spindt-type FEA was used as the electron source. A unique carousel was used so the desorption from all these different anodes could be measured without intervening vacuum breaks. this allowed the desorption from the different anodes to be directly compared to each other. Quantitative outgassing rates are given and the implications of the results for the pumping of the flat panel and emission from the FEAs an discussed.  ",
    "keywords": [
      "field emission displays",
      "field emitter array",
      "phosphor"
    ]
  },
  {
    "id": "1026",
    "title": "Application of the sznajd sociophysics model to small-world networks",
    "abstract": "The Sznajd model for the opinion formation is generalized to small-world networks. This generalization destroyed the stalemate fixed point, Then a simple definition of leaders is included. No fixed points are observed. This model displays some interesting aspects in sociology. The model is investigated using time series analysis.",
    "keywords": [
      "ising model",
      "opinion formation models",
      "small-world networks",
      "leaders"
    ]
  },
  {
    "id": "1027",
    "title": "interactive modular programming in scheme",
    "abstract": "This paper presents a module system and a programming environment designed to support interactive program development in Scheme. The module system extends lexical scoping while maintainig its flavor and benefits and supports mutually recursive modules. The programming environment supports dynamic linking, separate compilation, production code compilation, and a window-based user interface with multiple read-eval-print contexts.",
    "keywords": [
      "interaction",
      "printing",
      "program",
      "product",
      "systems",
      "recursion",
      "developer",
      "code",
      "dynamic",
      "context",
      "module",
      "user interface",
      "compilation",
      "modular",
      "support",
      "separate compilation",
      "windows",
      "paper",
      "read",
      "programming environment",
      "linking",
      "scheme"
    ]
  },
  {
    "id": "1028",
    "title": "Subword balance, position indices and power sums",
    "abstract": "In this paper, we investigate various ways of characterizing words, mainly over a binary alphabet, using information about the positions of occurrences of letters in words. We introduce two new measures associated with words, the position index and sum of position indices. We establish some characterizations, connections with Parikh matrices, and connections with power sums. One particular emphasis concerns the effect of morphisms and iterated morphisms on words.  ",
    "keywords": [
      "position of letter",
      "subword",
      "parikh matrix",
      "power sum",
      "iterated morphism",
      "thue morphism",
      "fibonacci morphism"
    ]
  },
  {
    "id": "1029",
    "title": "An intelligent approach to integration and control of textile processes",
    "abstract": "This paper introduces a methodology to integrate and control effectively major plant processes with strong couplings between them. The proposed integration philosophy consists of causeeffect relationships and decides upon control setpoints for the individual processes by optimizing a global objective function which aims at improving process yield. A neuro-fuzzy model and a fuzzy objective function are employed to address the integration and control tasks. Such models and objective functions are defined and developed using experimental data or an operator's experience. The objective is to maximize productivity and at the same time, reduce defects in each of the subsequent operations. A textile plant is considered as a testbed and three major processes  warping, slashing and weaving  are employed to illustrate the feasibility of the approach. The supervisory level of the control architecture is intended to continuously improve the control setpoints depending upon feedback information from the weave room, slasher operator, and warping data.",
    "keywords": [
      "polynomial fuzzy neural networks",
      "fuzzy logic control",
      "integration",
      "causeeffect relation",
      "genetic algorithms",
      "hybrid genetic optimization"
    ]
  },
  {
    "id": "1030",
    "title": "Wavelet based multiresolution expectation maximization image reconstruction algorithm for positron emission tomography",
    "abstract": "Maximum Likelihood (ML) estimation based Expectation Maximization (EM) [IEEE Trans Med Imag, MI-1 (2) (1982) 113] reconstruction algorithm has shown to provide good quality reconstruction for positron emission tomography (PET). Our previous work [IEEE Trans Med Imag, 7(4) (1988) 273; Proc IEEE EMBS Conf, 20(2/6) (1998) 759] introduced the multigrid (MG) and multiresolution (MR) concept for PET image reconstruction using EM. This work transforms the MGEM and MREM algorithm to a Wavelet based Multiresolution EM (WMREM) algorithm by extending the concept of switching resolutions in both image and data spaces. The MR data space is generated by performing a 2D-wavelet transform on the acquired tube data that is used to reconstruct images at different spatial resolutions. Wavelet transform is used for MR reconstruction as well as adapted in the criterion for switching resolution levels. The advantage of the wavelet transform is that it provides very good frequency and spatial (time) localization and allows the use of these coarse resolution data spaces in the EM estimation process. The MR algorithm recovers low-frequency components of the reconstructed image at coarser resolutions in fewer iterations, reducing the number of iterations required at finer resolution to recover high-frequency components. This paper also presents the design of customized biorthogonal wavelet filters using the lifting method that are used for data decomposition and image reconstruction and compares them to other commonly known wavelets.",
    "keywords": [
      "multiresolution reconstruction",
      "wavelets",
      "expectation maximization",
      "positron emission tomography",
      "lifting scheme"
    ]
  },
  {
    "id": "1031",
    "title": "Process models for agent-based development",
    "abstract": "A great deal of research in the area of agent-oriented software engineering (AOSE) focuses on proposing methodologies for agent systems, i.e., on identifying the guidelines to drive the various phases of agent-based software development and the abstractions to be exploited in these phases. However, very little attention has been paid so far to the engineering process subjacent to the development activity, disciplining the execution of the different phases involved in the software development. In this paper, we focus on process models for software development and put these in relation with current researches in AOSE. First, we introduce the key concepts and issues related to software processes and present the various software process models currently adopted in mainstream software engineering. Then, we survey the characteristics of a number of agent-oriented methodologies, as they pertain to software processes. In particular, for each methodology, we analyze which software process model it (often implicitly) underlies and which phases of the process are covered by it, thus enabling us to identify some key limitations of currently methodology-centered researches. On this basis, we eventually identify and analyze several open issues in the area of software process models for agent-based development, calling for further researches and experiences.",
    "keywords": [
      "agent-based computing",
      "software engineering",
      "methodologies",
      "process models",
      "agent software development"
    ]
  },
  {
    "id": "1032",
    "title": "decentralized computation",
    "abstract": "For a decentralized computing system of many computing elements (whether geographically distributed mainframe computers, or miniature computing elements within a single board or even chip), a naturally decentralized model is prerequisite for the organization of computation. This will allow a large number of computer elements to cooperate in the execution of a program. Computational models used in parallel computers (data flow, control flow, and reduction) help to identify the attributes of such a decentralized and general-purpose model. This paper examines data flow, control flow, and reduction models and presents a classification for their underlying concepts. In addition, it describes a computational model called recursive control flow, which is a synthesis of these concepts and which directly supports data flow, control flow, and reduction computation.",
    "keywords": [
      "help",
      "organization",
      "systems",
      "recursion",
      "computer modeling",
      "computation",
      "concept",
      "reduction",
      "data",
      "flow control",
      "flow",
      "control flow",
      "general",
      "attributes",
      "synthesis",
      "parallel computation",
      "paper",
      "distributed",
      "model",
      "decentralization",
      "classification"
    ]
  },
  {
    "id": "1033",
    "title": "Fuzzy hybrid simulated annealing algorithms for topology design of switched local area networks",
    "abstract": "Topology design of switched local area networks (SLAN) is classified as an NP-hard problem since a number of objectives, such as monetary cost, network delay, hop count between communicating pairs, and reliability need to be simultaneously optimized under a set of constraints. This paper presents a multiobjective heuristic based on a simulated annealing (SA) algorithm for topology design of SLAN. Fuzzy logic has been incorporated in the SA algorithm to handle the imprecise multiobjective nature of the SLAN topology design problem, since the logic provides a suitable mathematical framework to address the multiobjective aspects of the problem. To enhance the performance of the proposed fuzzy simulated annealing (FSA) algorithm, two variants of FSA are also proposed. These variants incorporate characteristics of tabu search (TS) and simulated evolution (SimE) algorithms. The three proposed fuzzy heuristics are mutually compared with each other. Furthermore, two fuzzy operators, namely, ordered weighted average (OWA) and unified AND-OR (UAO) are also applied in certain steps of these algorithms. Results show that in general, the variant which embeds characteristics of SimE and TS into the fuzzy SA algorithm exhibits more intelligent search of the solution subspace and was able to find better solutions than the other two variants of the fuzzy SA. Also, the OWA and UAO operators exhibited relatively similar performance.",
    "keywords": [
      "network topology",
      "fuzzy logic",
      "distributed networks",
      "simulated annealing",
      "simulated evolution"
    ]
  },
  {
    "id": "1034",
    "title": "applicative functors and fully transparent higher-order modules",
    "abstract": "we present a variety of the Standard ML module system where parameterized abstract types (i.e. functors returning generative types) map provably equal arguments to compatible abstract types, instead of generating distinct types at each applications as in Standard ML. This extension solves the full transparency problem (how to give syntactic signatures for higher-order functors that express exactly their propagation of type equations), and also provides better support for non-closed code fragments.",
    "keywords": [
      "fragmentation",
      " ml ",
      "order",
      "applications",
      "generation",
      "systems",
      "abstraction",
      "transparency",
      "code",
      "types",
      "module",
      "standardization",
      "support",
      "extensibility",
      "signature",
      "argument",
      "propagation"
    ]
  },
  {
    "id": "1035",
    "title": "ImageAdmixture: Putting Together Dissimilar Objects from Groups",
    "abstract": "We present a semiautomatic image editing framework dedicated to individual structured object replacement from groups. The major technical difficulty is element separation with irregular spatial distribution, hampering previous texture, and image synthesis methods from easily producing visually compelling results. Our method uses the object-level operations and finds grouped elements based on appearance similarity and curvilinear features. This framework enables a number of image editing applications, including natural image mixing, structure preserving appearance transfer, and texture mixing.",
    "keywords": [
      "natural image",
      "structure analysis",
      "texture",
      "image processing"
    ]
  },
  {
    "id": "1036",
    "title": "Pel recursive motion estimation and compensation in subbands",
    "abstract": "In this correspondence, the problem of recursive motion estimation and compensation in image subbands is considered. A pel recursive algorithm is presented for this purpose and it is shown experimentally that the motion can be compensated almost as well as in the original fields. Based on this algorithm, a scalable and recursive video coding scheme is outlined which is compared successfully to a hybrid coding scheme based on block matching.",
    "keywords": [
      "subband decomposition",
      "motion estimation",
      "motion compensation",
      "pel-recursive"
    ]
  },
  {
    "id": "1037",
    "title": "an approach based on metrics for monitoring web accessibility in brazilian municipalities web sites",
    "abstract": "Monitoring and measuring the accessibility of government Web sites is an important challenge for regulators and policy makers. Moreover, over the next few years, e-government (e-gov) services are expected to expand and it is necessary to ensure access for everyone. In this paper, we present a metric based approach for evaluating municipalities Web pages using automatic accessibility evaluation tools. The sampling of the pages was done by the tool E-GOVMeter, and the accessibility evaluation and generation of the metrics was done by means of an adaptation of the tool Hera. The results show that much work should be done to improve the accessibility of Brazilian municipalities Web sites. Although it has limitations, the use of automatically generated accessibility metrics is a powerful tool for helping measuring and monitoring the accessibility of e-gov Web sites.",
    "keywords": [
      "web accessibility evaluation",
      "web accessibility",
      "web metrics",
      "e-government"
    ]
  },
  {
    "id": "1038",
    "title": "Humanlike conversation with gestures and verbal cues based on a three-layer attention-drawing model",
    "abstract": "When describing a physical object, we indicate which object by pointing and using reference terms, such as 'this' and 'that', to inform the listener quickly of an indicated object's location. Therefore, this research proposes using a three-layer attention-drawing model for humanoid robots that incorporates such gestures and verbal cues. The proposed three-layer model consists of three sub-models: the Reference Term Model (RTM); the Limit Distance Model (LDM); and the Object Property Model (OPM). The RTM selects an appropriate reference term for distance, based on a quantitative analysis of human behaviour. The LDM decides whether to use a property of the object, such as colour, as an additional term for distinguishing the object from its neighbours. The OPM determines which property should be used for this additional reference. Based on this concept, an attention-drawing system was developed for a communication robot named 'Robovie', and its effectiveness was tested.",
    "keywords": [
      "human-robot interface",
      "human-robot interaction",
      "deictic gestures"
    ]
  },
  {
    "id": "1039",
    "title": "A Low-Voltage and Low-Power 3-GHz CMOS LC VCO for S-Band Wireless Applications",
    "abstract": "A fully integrated 0.18-(upmu hbox {m}) CMOS LC-tank voltage-controlled oscillator (VCO) suitable for low-voltage and low-power S-band wireless applications is proposed in this paper. In order to meet the requirement of low voltage applications, a differential configuration with two cross-coupled pairs by adopting admittance-transforming technique is employed. By using forward-body-biased metal oxide semiconductor field effect transistors, the proposed VCO can operate at 0.4V supply voltage. Despite the low power supply near threshold voltage, the VCO achieves wide tuning range by using a voltage-boosting circuit and the standard mode PMOS varactors in the proposed oscillator architecture. The simulation results show that the proposed VCO achieves phase noise of (-)120.1dBc/Hz at 1MHz offset and 39.3% tuning range while consuming only (594~upmu hbox {W}) in 0.4V supply. Figure-of-merit with tuning range of the proposed VCO is (-)192.1dB at 3GHz.",
    "keywords": [
      "voltage-controlled oscillator ",
      "forward-body-biased",
      "admittance-transforming",
      "voltage-boosting",
      "low voltage",
      "low power"
    ]
  },
  {
    "id": "1040",
    "title": "Identifying Web Spam with the Wisdom of the Crowds",
    "abstract": "Combating Web spam has become one of the top challenges for Web search engines. State-of-the-art spam-detection techniques are usually designed for specific, known types of Web spam and are incapable of dealing with newly appearing spam types efficiently. With user-behavior analyses from Web access logs, a spam page-detection algorithm is proposed based on a learning scheme. The main contributions are the following. (1) User-visiting patterns of spam pages are studied, and a number of user-behavior features are proposed for separating Web spam pages from ordinary pages. (2) A novel spam-detection framework is proposed that can detect various kinds of Web spam, including newly appearing ones, with the help of the user-behavior analysis. Experiments on large-scale practical Web access log data show the effectiveness of the proposed features and the detection framework.",
    "keywords": [
      "measurement",
      "experimentation",
      "human factors",
      "spam detection",
      "web search engine",
      "user behavior analysis"
    ]
  },
  {
    "id": "1041",
    "title": "image schemata in animated metaphors for insight problem solving",
    "abstract": "While most of the work on metaphors has focused on conceptual ones, less attention has been paid to the visual metaphors for insight problems. This paper investigates the role of dynamism and realism in visual metaphors for cueing the insight problem solving process. To match the visual-kinesthetic feature of the eight-coin insight problem, the developed metaphors represented the insight cues, both kinetically and kinesthetically. An experimental study showed the superiority of metaphors as realistic and continuous animations over schematic and discrete animations.",
    "keywords": [
      "visual insight problem",
      "animation",
      "visual metaphors",
      "image schemata.",
      "realism"
    ]
  },
  {
    "id": "1042",
    "title": "English-Thai structure-based machine translation",
    "abstract": "We propose an alternative method of machine-aided translation: Structure-Based Machine Translation (SBMT). SBMT uses language structure matching techniques to reduce complicated grammar rules and provide efficient and feasible translation results. SBMT comprises the following four features: (1) source language input sentence analysis; (2) source language sentence transformation into target language structured (3) dictionary lookup: and (4) semantic disambiguation or word sense disambiguation (WSD) for correct output Selection, SBMT has been designed and a prototype system has been implemented that generates satisfactory translations.",
    "keywords": [
      "machine translation"
    ]
  },
  {
    "id": "1043",
    "title": "Global existence and uniform decay rates for the semi-linear wave equation with damping and source terms",
    "abstract": "In this paper, we consider a semi-linear wave equation with damping and source terms. Using a potential well method, we prove the existence and uniqueness of global solutions of the wave equation and investigate uniform decay rates of solutions. Moreover, an example is given to illustrate our results.",
    "keywords": [
      "existence of solution",
      "energy decay",
      "source term",
      "numerical result"
    ]
  },
  {
    "id": "1044",
    "title": "ARGOS: Automatically extracting repeating objects from multimedia streams",
    "abstract": "Many media streams consist of distinct objects that repeat. For example, broadcast television and radio signals contain advertisements, call sign jingles, songs, and even whole programs that repeat. The problem we address is to explicitly identify the underlying structure in repetitive streams and de-construct them into their component objects. Our algorithm exploits dimension reduction techniques on the audio portion of a multimedia stream to make search and buffering feasible. Our architecture assumes no a priori knowledge of the streams, and does not require that the repeating objects (ROs) be known. Everything the system needs, including the position and duration of the ROs, is learned on the fly. We demonstrate that it is perfectly feasible to identify in real-time ROs that occur days or even weeks apart in audio or video streams. Both the compute and buffering requirements are comfortably within reach for a basic desktop computer. We outline the algorithms, enumerate several applications and present results from real broadcast streams.",
    "keywords": [
      "audio fingerprint",
      "low-dimension representation",
      "multimedia",
      "repeats",
      "segmentation"
    ]
  },
  {
    "id": "1045",
    "title": "A matched interface and boundary method for solving multi-flow NavierStokes equations with applications to geodynamics",
    "abstract": "We have developed a second-order numerical method, based on the matched interface and boundary (MIB) approach, to solve the NavierStokes equations with discontinuous viscosity and density on non-staggered Cartesian grids. We have derived for the first time the interface conditions for the intermediate velocity field and the pressure potential function that are introduced in the projection method. Differentiation of the velocity components on stencils across the interface is aided by the coupled fictitious velocity values, whose representations are solved by using the coupled velocity interface conditions. These fictitious values and the non-staggered grid allow a convenient and accurate approximation of the pressure and potential jump conditions. A compact finite difference method was adopted to explicitly compute the pressure derivatives at regular nodes to avoid the pressurevelocity decoupling. Numerical experiments verified the desired accuracy of the numerical method. Applications to geophysical problems demonstrated that the sharp pressure jumps on the clast-Newtonian matrix are accurately captured for various shear conditions, moderate viscosity contrasts and a wide range of density contrasts. We showed that large transfer errors will be introduced to the jumps of the pressure and the potential function in case of a large absolute difference of the viscosity across the interface; these errors will cause simulations to become unstable.",
    "keywords": [
      "navierstokes equations",
      "geophysics",
      "multi-flow",
      "jump conditions",
      "interface method",
      "non-staggered grid",
      "projection method",
      "stability"
    ]
  },
  {
    "id": "1046",
    "title": "Eliciting Web application requirements  an industrial case study",
    "abstract": "A small variety of methods and techniques are presented in the literature as solutions to manage requirements elicitation for Web applications. However, the existing state of the art is lacking research regarding practical functioning solutions that would match Web application characteristics. The main concern for this paper is how requirements for Web applications can be elicited. The Viewpoint-Oriented Requirements Definition method (VORD) is chosen for eliciting and formulating Web application requirements in an industrial case study. VORD is helpful because it allows structuring of requirements around viewpoints and formulating very detailed requirements specifications. Requirements were understandable to the client with minimal explanation but failed to capture the business vision, strategy, and daily business operations, and could not anticipate the changes in the business process as a consequence of introducing the Web application within the organisation. The paper concludes by a discussion of how to adapt and extend VORD to suit Web applications.",
    "keywords": [
      "viewpoint-oriented requirements definition ",
      "web applications ",
      "web requirements engineering ",
      "business strategy"
    ]
  },
  {
    "id": "1047",
    "title": "Differential approximation results for the traveling salesman and related problems",
    "abstract": "This paper deals with the problem of constructing a Hamiltonian cycle of optimal weight, called TSP. We show that TSP is 2/3-differential approximable and cannot be differential approximable greater than 649/650. Next, we demonstrate that, when dealing with edge-costs 1 and 2, the same algorithm idea improves this ratio to 3/4 and we obtain a differential non-approximation threshold equal to 741/742. Remark that the 3/4-differential approximation result has been recently proved by a way more specific to the 1-, 2-case and with another algorithm in the recent conference, Symposium on Fundamentals of Computation Theory, 2001. Based upon these results, we establish new bounds for standard ratio: 5/6 for Mar TSP[a, 2a] and 7/8 for MaxTSP[1, 2]. We also derive some approximation results on partition graph problems by paths.  ",
    "keywords": [
      "approximation algorithms",
      "differential ratio",
      "performance ratio",
      "analysis of algorithms"
    ]
  },
  {
    "id": "1048",
    "title": "Quantum Bayesian game with symmetric and asymmetric information",
    "abstract": "We use one of the influential quantum game models, the MarinattoWeber model, to investigate quantum Bayesian game. We show that in a quantum Bayesian game which has more than one Nash equilibrium, one equilibrium stands out as the compelling solution, whereas two Nash equilibria seem equally compelling in the classical Bayesian game.",
    "keywords": [
      "game theory",
      "bayesian game",
      "quantum game",
      "nash equilibrium"
    ]
  },
  {
    "id": "1049",
    "title": "A framework for flexible summarization of racquet sports video using multiple modalities",
    "abstract": "While most existing sports video research focuses on detecting event from soccer and baseball etc., little work has been contributed to flexible content summarization on racquet sports video, e.g. tennis, table tennis etc. By taking advantages of the periodicity of video shot content and audio keywords in the racquet sports video, we propose a novel flexible video content summarization framework. Our approach combines the structure event detection method with the highlight ranking algorithm. Firstly, unsupervised shot clustering and supervised audio classification are performed to obtain the visual and audio mid-level patterns respectively. Then, a temporal voting scheme for structure event detection is proposed by utilizing the correspondence between audio and video content. Finally, by using the affective features extracted from the detected events, a linear highlight model is adopted to rank the detected events in terms of their exciting degrees. Experimental results show that the proposed approach is effective.  ",
    "keywords": [
      "sports video summarization",
      "scene segmentation",
      "temporal voting strategy",
      "highlight ranking"
    ]
  },
  {
    "id": "1050",
    "title": "Unambiguous stereo matching using reliability-based dynamic programming",
    "abstract": "An efficient unambiguous stereo matching technique is presented in this paper. Our main contribution is to introduce a new reliability measure to dynamic programming approaches in general. For stereo vision application, the reliability of a proposed match on a scanline is defined as the cost difference between the globally best disparity assignment that includes the match and the globally best assignment that does not include the match. A reliability-based dynamic programming algorithm is derived accordingly, which can selectively assign disparities to pixels when the corresponding reliabilities exceed a given threshold. The experimental results show that the new approach can produce dense (>70 percent of the unoccluded pixels) and reliable (error rate <0.5 percent) matches efficiently (<0.2 sec on a 2GHz P4) for the four Middlebury stereo data sets.",
    "keywords": [
      "stereo",
      "dynamic programming."
    ]
  },
  {
    "id": "1051",
    "title": "Contribution of fuzzy reasoning method to knowledge integration in a defect recognition system",
    "abstract": "This article presents the improvement of a defect recognition system for wooden boards by using knowledge integration from two expert fields. These two kinds of knowledge to integrate respectively concern wood expertise and industrial vision expertise. First of all, extraction, modelling and integration of knowledge use the Natural Language Information Analysis method (NIAM) to be formalized from their natural language expression. Then, to improve a classical industrial vision system, we propose to use the resulting symbolic model of knowledge to partially build a numeric model of wood defect recognition. This model is created according to a tree structure where each inference engine is a fuzzy rule based inference system. The expert knowledge model previously obtained is used to configure each node of the resulting hierarchical structure. The practical results we obtained in industrial conditions show the efficiency of such an approach.",
    "keywords": [
      "knowledge integration",
      "niam method",
      "orm model",
      "pattern recognition",
      "fuzzy logic"
    ]
  },
  {
    "id": "1052",
    "title": "The free surface of a coupled ocean-atmosphere model due to forcing effects",
    "abstract": "In this work, we consider external effects on a coupled shallow water ocean-atmosphere model. The forced shallow water equation due to the wind action is studied through a decomposition technique. Induced free perturbations of the permanent response are identified by using a spectral operator basis that is generated by a dynamical Green function. This later was determined through a spectral technique. By using the semi-lagrangian method, we compute the non-linear response of the full model due to a shear stress that comes from the action of the wind at the ocean surface.  ",
    "keywords": [
      "forced response",
      "decomposition",
      "semi-lagrangian method",
      "dynamic green function",
      "geophysical modelling"
    ]
  },
  {
    "id": "1053",
    "title": "Research on Inventory Control Policies for Nonstationary Demand based on TOC",
    "abstract": "An effective inventory replenishment method employed in the supply chain is one of the key factors to achieving low inventory while maintaining high customer delivery performance. The state of demand process is often not directly observed by the decision maker. Thus, in many literatures, the inventory control problem is a composite-state, partially observed Markov decision process (POMDP), which is an appropriate model for a number of dynamic demand problems. In practice, managers often use certainty equivalent control (CEC) policies to solve such a problem. However, in reality, Theory of Constraints (TOC) has brought a practical control policy that almost always provides much better solutions for this problem than the CEC policies commonly used in practice. In this paper, we proposed three different inventory control policies based on TOC buffer management framework, and use simulation approach to compare them with traditional adaptive (s,S,T) policy. The computational results indicate how specific problem characteristics influence the performance of whole system and demonstrate the efficiency of the proposed control policy.",
    "keywords": [
      "inventory control policy",
      "nonstationary demand",
      "theory of constraints",
      "buffer management"
    ]
  },
  {
    "id": "1054",
    "title": "boosting topic-based publish-subscribe systems with dynamic clustering",
    "abstract": "We consider in this paper a class of Publish-Subscribe (pub-sub) systems called topic-based systems, where users subscribe to topics and are notified on events that belong to those subscribed topics. With the recent flourishing of RSS news syndication, these systems are regaining popularity and are raising new challenging problems. In most of the modern topics-based systems, the events in each topic are delivered to the subscribers via a supporting, distributed, data structure (typically a multicast tree). Since peers in the network may come and go frequently, this supporting structure must be continuously maintained so that \"holes\" do not disrupt the events delivery. The dissemination of events in each topic thus incurs two main costs: (1) the actual transmission cost for the topic events,and (2) the maintenance cost for its supporting structure. This maintenance overhead becomes particularly dominating when a pub-sub system supports a large number of topics with moderate event frequency; a typical scenario in nowadays news syndication scene. The goal of this paper is to devise a method for reducing this maintenance overhead to the minimum. Our aim is not to invent yet another topic-based pub-sub system, but rather to develop a generic technique for better utilization of existing platforms. Our solution is based on a novel distributed clustering algorithm that utilizes correlations between user subscriptions to dynamically group topics together, into virtual topics (called topic-clusters ), andt hereby unifies their supporting structures and reduces costs. Our technique continuously adapts the topic-clusters and the user subscriptions to the system state, and incurs only very minimal overhead. We have implemented our solution in the Tamara pub-sub system. Our experimental study shows this approach to be extremely effective, improving the performance by an order of magnitude.",
    "keywords": [
      "publish-subscribe",
      "dynamic clustering",
      "peer-to-peer"
    ]
  },
  {
    "id": "1055",
    "title": "3dtv and 3d video communications",
    "abstract": "With wider availability of low cost multi-view cameras, 3D displays, and broadband communication options, 3D media is destined to move from the movie theater to home and mobile platforms. In the near term, popular 3D media will most likely be in the form of stereoscopic video with associated spatial audio. Recent trials indicate that consumers are willing to watch stereoscopic 3D media on their TVs, laptops, and mobile phones. While it is possible to broadcast 3D stereoscopic media (two-views) over digital TV platforms today, streaming over IP will provide a more flexible approach for distribution of 3D media to users with different connection bandwidths and different 3D displays. In the intermediate term, free-view 3D video and 3DTV with multi-view capture are next steps in the evolution of 3D media technology. Recent free-view 3D auto-stereoscopic displays can display multi-view video, ranging from 5 to 200 views. Transmission of multi-view 3D media, via broadcast or on-demand, to end users with varying 3D display terminals and bandwidths is one of the biggest challenges to realize the vision of bringing 3D media experience to the home and mobile devices. This requires flexible rate-scalable, resolution-scalable, view-scalable, view-selective, and packet-loss resilient transport methods. In this talk, first I will briefly review the state of the art in 3D video formats, coding methods, IP streaming protocols and streaming architectures. We will then take a look at 3D video transport options. There are two main platforms for 3D broadcasting: standard digital television (DTV) platforms and the IP platform. I will summarize the approach of European project DIOMEDES which is developing novel methods for adaptive streaming of multi-view video over a combination of DVB and IP platforms. I will also summarize additional challenges associated with real-time interactive 3D video communications for applications such as 3D telepresence. Finally, open research challenges for the long term vision of haptic video and holographic 3D video will be presented.",
    "keywords": [
      "media streaming",
      "3dtv",
      "video communication"
    ]
  },
  {
    "id": "1056",
    "title": "Association between objective and subjective measurements of comfort and discomfort in hand tools",
    "abstract": "In the current study, the relationship between objective measurements and subjective experienced comfort and discomfort in using handsaws was examined. Twelve carpenters evaluated five different handsaws. Objective measures of contact pressure (average pressure, pressure area and pressuretime (Pt) integral) in static and dynamic conditions, muscle activity (electromyography) of five muscles of the upper extremity, and productivity were obtained during a sawing task. Subjective comfort and discomfort were assessed using the comfort questionnaire for hand tools and a scale for local perceived discomfort (LPD). We did not find any relationship between muscle activity and comfort or discomfort. The Pt integral during the static measurement (beta=?0.24, p<0.01) was the best predictor of comfort and the pressure area during static measurement was the best predictor of LPD (beta=0.45, p<0.01). Additionally, productivity was highly correlated to comfort (beta=0.31, p<0.01) and discomfort (beta=?0.49, p<0.01).",
    "keywords": [
      "comfort/discomfort",
      "hand tools",
      "objective measurements"
    ]
  },
  {
    "id": "1057",
    "title": "Using pore space 3D geometrical modelling to simulate biological activity: Impact of soil structure",
    "abstract": "This study is the follow-up to a previous one devoted to soil pore space modelling. In the previous study, we proposed algorithms to represent soil pore space by means of optimal piecewise approximation using simple 3D geometrical primitives: balls, cylinders, cones, etc. In the present study, we use the ball-based piecewise approximation to simulate biological activity. The basic idea for modelling pore space consists in representing pore space using a minimal set of maximal balls (Delaunay spheres) recovering the shape skeleton. In this representation, each ball is considered as a maximal local cavity corresponding to the intuitive notion of a pore as described in the literature. The space segmentation induced by the network of balls (pores) is then used to spatialise biological dynamics. Organic matter and microbial decomposers are distributed within the balls (pores). A valuated graph representing the pore network, organic matter and microorganism distribution is then defined. Microbial soil organic matter decomposition is simulated by updating this valuated graph. The method has been implemented and tested on real data. As far as we know, this approach is the first one to formally link pore space geometry and biological dynamics. The long-term goal is to define geometrical typologies of pore space shape that can be attached to specific biological dynamic properties. This paper is a first attempt to achieve this goal.",
    "keywords": [
      "3d computer vision",
      "biological dynamics simulation",
      "computed tomography",
      "computational geometry",
      "microbial decomposition",
      "pore space modelling"
    ]
  },
  {
    "id": "1058",
    "title": "Design of a Comb Generator for High Capacity Coherent-WDM Systems",
    "abstract": "This paper presents a theoretical model developed for estimating the power, the optical signal to noise ratio and the number of generated carriers in a comb generator, having as a reference the minimum optical signal do noise ratio at the receiver input, for a given fiber link. Based on the recirculating frequency shifting technique, the generator relies on the use of coherent and orthogonal multi-carriers (Coherent-WDM) that makes use of a single laser source (seed) for feeding high capacity (above 100 Gb/s) systems. The theoretical model has been validated by an experimental demonstration, where 23 comb lines with an optical signal to noise ratio ranging from 25 to 33 dB, in a spectral window of similar to 3.5 nm, are obtained.",
    "keywords": [
      "coherent-wdm",
      "comb generator",
      "energy efficiency",
      "high capacity optical fiber transport",
      "orthogonal frequency division multiplexing",
      "recirculating frequency shifting",
      "spectral efficiency"
    ]
  },
  {
    "id": "1059",
    "title": "safety verification in murphy using fault tree analysis",
    "abstract": "MURPHY is a language-independent, experimental methodology for building safety-critical, real time software, which will include an integrated tool set. Using Ada as an example, this paper presents a technique for verifying the safety of complex, real-time software using Software Fault Tree Analysis. The templates for Ada are presented along with an example of applying the technique to an Ada program. The tools in the MURPHY tool set to aid in this type of analysis are described.",
    "keywords": [
      "tree",
      "software",
      "examples",
      "experimentation",
      "methodology",
      "analysis",
      "language",
      "verification",
      "tool",
      "fault",
      "complexity",
      "paper",
      "safety critical",
      "template",
      "real-time",
      "tools",
      "integrability"
    ]
  },
  {
    "id": "1060",
    "title": "News Recommendations from Social Media Opinion Leaders: Effects on Media Trust and Information Seeking",
    "abstract": "Polls show a strong decline in public trust of traditional news outlets; however, social media offers new avenues for receiving news content. This experiment used the Facebook API to manipulate whether a news story appeared to have been posted on Facebook by one of the respondent's real-life Facebook friends. Results show that social media recommendations improve levels of media trust, and also make people want to follow more news from that particular media outlet in the future. Moreover, these effects are amplified when the real-life friend sharing the story on social media is perceived as an opinion leader. Implications for democracy and the news business are discussed.",
    "keywords": [
      "social media",
      "news media effects",
      "experiment",
      "opinion leader",
      "media trust",
      "two-step flow",
      "interpersonal communication"
    ]
  },
  {
    "id": "1061",
    "title": "Numerical methods for instability mitigation in the modeling of laser wakefield accelerators in a Lorentz-boosted frame",
    "abstract": "Modeling of laser-plasma wakefield accelerators in an optimal frame of reference [1] has been shown to produce orders of magnitude speed-up of calculations from first principles. Obtaining these speedups required mitigation of a high-frequency instability that otherwise limits effectiveness. In this paper, methods are presented which mitigated the observed instability, including an electromagnetic solver with tunable coefficients, its extension to accommodate Perfectly Matched Layers and Friedmans damping algorithms, as well as an efficient large bandwidth digital filter. It is observed that choosing the frame of the wake as the frame of reference allows for higher levels of filtering or damping than is possible in other frames for the same accuracy. Detailed testing also revealed the existence of a singular time step at which the instability level is minimized, independently of numerical dispersion. A combination of the techniques presented in this paper prove to be very efficient at controlling the instability, allowing for efficient direct modeling of 10GeV class laser plasma accelerator stages. The methods developed in this paper may have broader application, to other Lorentz-boosted simulations and Particle-In-Cell simulations in general.",
    "keywords": [
      "laser wakefield acceleration",
      "particle-in-cell",
      "plasma simulation",
      "special relativity",
      "boosted frame",
      "numerical instability"
    ]
  },
  {
    "id": "1062",
    "title": "Online scheduling on batching machines to minimise the total weighted completion time of jobs with precedence constraints and identical processing times",
    "abstract": "This article studies online scheduling of equal length jobs with precedence constraints on m parallel batching machines. The jobs arrive over time. The objective is to minimise the total weighted completion time of jobs. Denote the size of each batch by b with b = in the unbounded batching and b in the bounded batching. For the unbounded batching version, we provide an online algorithm with a best possible competitive ratio of m, where m is the positive solution of m+1 - = 1. The algorithm is also best possible when the jobs have identical weights. For the bounded batching version with identical weights of jobs, we provide an online algorithm with a competitive ratio of 2.",
    "keywords": [
      "online scheduling",
      "precedence constraints",
      "parallel batch"
    ]
  },
  {
    "id": "1063",
    "title": "a study of multiaccess computer communications",
    "abstract": "The communications characteristics of multiaccess computing are generating new needs for communications. The results of a study of multiaccess computer communications are the topic of this paper. The analyses made are based on a model of the user-computer interactive process that is described and on data that were collected from operating computer systems. Insight into the performance of multiaccess computer systems can be gleaned from these analyses. In this paper emphasis is placed on communications considerations . For this reason, the conclusions presented deal with the characteristics of communications systems and services appropriate for multiaccess computer systems.",
    "keywords": [
      "interaction",
      "communication",
      "operability",
      "data",
      "process",
      "service",
      "communication systems",
      "systems",
      "model",
      "paper",
      "user",
      "performance",
      "computation"
    ]
  },
  {
    "id": "1064",
    "title": "Trends in built environment semantic Web applications: Where are we today",
    "abstract": "BIM, Semantic Web and Linked Data are key technologies for construction information. Progress is being made from often too-common ontological concepts to Linked Data. Semantic Web sustainable construction applications are now emerging.",
    "keywords": [
      "built environment",
      "climate change",
      "linked open data",
      "semantic web"
    ]
  },
  {
    "id": "1065",
    "title": "management of keyword variation with frequency based generation of word forms in ir",
    "abstract": "This paper presents a new management method for morphological variation of keywords. The method is called FCG, Frequent Case Generation. It is based on the skewed distributions of word forms in natural languages and is suitable for languages that have either fair amount of morphological variation or are morphologically very rich. The proposed method has been evaluated so far with four languages, Finnish, Swedish, German and Russian, which show varying degrees of morphological complexity.",
    "keywords": [
      "evaluation",
      "word form generation",
      "monolingual information retrieval",
      "management of morphological variation"
    ]
  },
  {
    "id": "1066",
    "title": "The extension of the van der Pauw method to anisotropic media",
    "abstract": "The method for calculating the specific conductivity tensor of an anisotropically conductive medium, proposed in this paper, distinguishes itself by the simplicity of physical measurements: it suffices to make an equally thick rectangle-shaped sample with four electrodes fixed on its sides and to take various measurements of current intensity and differences of potentials. The necessary mathematical calculations can be promptly performed, even without using a complex computing technique. The accuracy of the results obtained depends on the dimensions of the sample and on the ratios of the conductivity tensor components.",
    "keywords": [
      "modelling",
      "anisotropic media",
      "electrical conductivity",
      "current flow",
      "numerical methods",
      "measurements"
    ]
  },
  {
    "id": "1067",
    "title": "Analysis of uniform circular arrays for adaptive beamforming applications using particle swarm optimization algorithm",
    "abstract": "In this article, the particle swarm optimization algorithm is used to calculate the complex excitations, amplitudes and phases, of the adaptive circular array elements. To illustrate the performance of this method for steering a signal in the desired direction and imposing nulls in the direction of interfering signals by controlling the complex excitation of each array element, two types of arrays are considered. A uniform circular array (UCA) and a planar uniform circular array (PUCA) with 16 elements of half-wave dipoles are examined. Also, the performance of an adaptive array using 3-bit amplitude and 4-bit phase shifters are studied. In our analysis, the method of moments is used to estimate the response of the dipole UCAs in a mutual coupling environment.",
    "keywords": [
      "smart antennas",
      "adaptive beamforming",
      "method of moments",
      "mutual coupling",
      "uniform circular arrays",
      "particle swarm optimization algorithm",
      "adaptive array"
    ]
  },
  {
    "id": "1068",
    "title": "Parallel algorithms to solve two-stage stochastic linear programs with robustness constraints",
    "abstract": "In this paper we present a parallel method for solving two-stage stochastic linear programs with restricted recourse. The mathematical model considered here can be used to represent several real-world applications, including financial and production planning problems, for which significant changes in the recourse solutions should be avoided because of their difficulty to be implemented. Our parallel method is based on a primal-dual path-following interior point algorithm, and exploits fruitfully the dual block-angular structure of the constraint matrix and the special block structure of the matrices involved in the restricted recourse model. We describe and discuss both message-passing and shared-memory implementations and we present the numerical results collected on the Origin2000.",
    "keywords": [
      "stochastic programming",
      "restricted recourse",
      "interior point methods",
      "numa multiprocessor system",
      "pvm",
      "openmp"
    ]
  },
  {
    "id": "1069",
    "title": "SCOMDY models based on pair-copula constructions with application to exchange rates",
    "abstract": "Vine pair-copula constructions (PCCs) provide an important milestone for the usage of multivariate copulas to model dependence. At present time PCCs are recognized to be the most flexible class of multivariate copulas. Vine PCCs and semiparametric copula-based dynamic (SCOMDY) models with ARMA-GARCH margins are combined. As building blocks of the PCCs, bivariate t-copulas are used. Exchange rates are considered as an application and their dependence structure is modelled using regular and canonical vines. A non-nested model comparison of the above SCOMDY models is performed using the adapted Voungs test.",
    "keywords": [
      "multivariate copula",
      "garch-arma margins",
      "exchange rates",
      "pair-copula construction",
      "vines"
    ]
  },
  {
    "id": "1070",
    "title": "Routing strategy in disconnected mobile ad hoc networks with group mobility",
    "abstract": "Most of the current proposed routing protocols in delay-tolerant network (DTN) are designed based on the entity mobility. In this article, we consider the routing in DTN with group mobility, which is useful in modeling those cooperative activities. The new proposed routing scheme is called group-epidemic routing (G-ER). G-ER is designed on the basis of one DTN protocol called epidemic routing (ER). In G-ER, two strategies related to the unique characteristics of the group mobility have been proposed to greatly improve ER. The first is to treat each group as a single node and exchange packets between groups instead of individual nodes. Thus, the resource-consuming problem of ER could be much alleviated. In the meantime, exchanging packets between two groups could speed up the packet delivery. The second is the buffer sharing inside a group, which is supported by the cooperative nature in group mobility. Moreover, we specifically propose a group dynamic model for group mobility to realize group splitting and merging. The performance of G-ER is studied by extensive simulations and compared with ER and dynamic source routing (DSR). Results show that G-ER outperforms ER and DSR in different network scenarios even with group dynamics.",
    "keywords": [
      "delay-tolerant network",
      "group mobility",
      "group-epidemic routing",
      "epidemic routing",
      "group dynamic model"
    ]
  },
  {
    "id": "1071",
    "title": "Efficient and Quality Contouring Algorithms on the GPU",
    "abstract": "Interactive isosurface extraction has recently become possible through successful efforts to map algorithms such as Marching Cubes (MC) and Marching Tetrahedra (MT) to modern Graphics Processing Unit (GPU) architectures. Other isosurfacing algorithms, however, are not so easily portable to GPUs, either because they involve more complex operations or because they are not based on discrete case tables, as is the case with most marching techniques. In this paper, we revisit the Dual Contouring (MC) and Macet isosurface extraction algorithms and propose, respectively: (i) a novel, efficient and parallelizable version of Dual Contouring and (ii) a set of GPU modules which extend the original Marching Cubes algorithm. Similar to marching methods, our novel technique is based on a case table, which allows for a very efficient GPU implementation. In addition, we enumerate and evaluate several alternatives to implement efficient contouring algorithms on the GPU, and present trade-offs among all approaches. Finally, we validate the efficiency and quality of the tessellations produced in all these alternatives.",
    "keywords": [
      "isosurfacing",
      "marching cubes",
      "gpu"
    ]
  },
  {
    "id": "1072",
    "title": "Cellular Automata Pattern Recognition and Rule Evolution Through a Neuro-Genetic Approach",
    "abstract": "Cellular Automata rules often produce spatial patterns which make them recognizable by human observers. Nevertheless, it is generally difficult, if not impossible, to identify the characteristic(s) that make a rule produce a particular pattern. Discovering rules that produce spatial patterns that a human being would find \"similar\" to another given pattern is a very important task, given its numerous possible applications in many complex systems models. In this paper, we propose a general framework to accomplish this task, based on a combination of Machine Learning strategies including Genetic Algorithms and Artificial Neural Networks. This framework is tested on a 3-values, 6-neighbors, k-totalistic cellular automata rule called the \"burning paper\" rule. Results are encouraging and should pave the way for the use of our framework in real-life complex systems models.",
    "keywords": [
      "spatial patterns",
      "pattern recognition",
      "rule evolution",
      "machine learning",
      "hybrid learning systems",
      "neural networks",
      "genetic algorithms"
    ]
  },
  {
    "id": "1073",
    "title": "Graphical Gaussian modeling for gene association structures based on expression deviation patterns induced by various chemical stimuli",
    "abstract": "Activity patterns of metabolic subnetworks, each of which can be regarded as a biological function module, were focused on in order to clarify biological meanings of observed deviation patterns of gene expressions induced by various chemical stimuli. We tried to infer association structures of genes by applying the multivariate statistical method called graphical Gaussian modeling to the gene expression data in a subnetwork-wise manner. It can be expected that the obtained graphical models will provide reasonable relationships between gene expressions and macroscopic biological functions. In this study, the gene expression patterns in nematodes under various conditions (stresses by chemicals such as heavy metals and endocrine disrupters) were observed using DNA microarrays. The graphical models for metabolic subnetworks were obtained from these expression data. The obtained models (independence graph) represent gene association structures of cooperativities of genes. We compared each independence graph with a corresponding metabolic subnetwork. Then we obtained a pattern that is a set of characteristic values for these graphs, and found that the pattern of heavy metals differs considerably from that of endocrine disrupters. This implies that a set of characteristic values of the graphs can representative a macroscopic biological meaning.",
    "keywords": [
      "gene expression pattern",
      "graphical gaussian modeling",
      "association structure",
      "metabolic network"
    ]
  },
  {
    "id": "1074",
    "title": "story driven testing - sdt",
    "abstract": "In the last years, SCESM community has studied a number of synthesis approaches that turn scenario descriptions into some kind of state machine. In our story driven modeling approach, the statechart synthesis is done manually. Many other approaches rely on human interaction, too. Frequently, the resulting state machines are just the starting point for further system development. The manual steps and the human interaction and the subsequent development steps are subject to the introduction of errors. Thus, it is not guaranteed that the final implementation still covers the initial scenarios. Therefore, this paper proposes the exploitation of scenarios for the derivation of automatic tests. These tests may be used to force the implementation to implement at least the behavior outlined in the requirements scenarios. In addition, this approach raises the value of formal scenarios for requirements elicitation and analysis since such scenarios are turned into automatic tests that may be used to drive iterative development processes according to test-first principles.",
    "keywords": [
      "test-first principle",
      "code generation",
      "scenarios"
    ]
  },
  {
    "id": "1075",
    "title": "ON THE CONSTRUCTION OF BENT FUNCTIONS OF n+2 VARIABLES FROM BENT FUNCTIONS OF n VARIABLES",
    "abstract": "In this paper we present a method to construct iteratively new bent functions of n+2 variables from bent functions of n variables using minterms of n variables and minterms of two variables. Also, we provide the number of bent functions of n+2 variables that we can obtain with the method here presented.",
    "keywords": [
      "boolean function",
      "bent function",
      "linear function",
      "balanced function",
      "nonlinearity",
      "truth table",
      "hamming weight",
      "minterm"
    ]
  },
  {
    "id": "1076",
    "title": "computational complexity versus virtual worlds",
    "abstract": "The ability to simulate complex physical situations in real-time is a critical element of any \"virtual world\" scenario, as well as being key for many engineering and robotics applications. Unfortunately the computation cost of standard physical simulation methods increases rapidly as the situation becomes more complex. The result is that even when using the fastest supercomputers we are still able to interactively simulate only small, toy worlds. To solve this problem I propose changing the way we represent and simulate physics in order to reduce the computational complexity of physical simulation, thus making possible interactive simulation of complex situations.",
    "keywords": [
      "interaction",
      "situated",
      "scenario",
      "order",
      "applications",
      "method",
      "simulation",
      "engine",
      "computation",
      "supercomputer",
      "physical simulation",
      "standardization",
      "physical",
      "robotics",
      "complexity",
      "critic",
      "real-time",
      "cost",
      "computational complexity",
      "virtual world"
    ]
  },
  {
    "id": "1077",
    "title": "Efficient and fair scheduling for two-level information broadcasting systems",
    "abstract": "In a ubiquitous environment, there are many applications where a server disseminates information of common interest to pervasive clients and devices. For an example, an advertisement server sends information from a broadcast server to display devices. We propose an efficient information scheduling scheme for information broadcast systems to reduce average waiting time for information access while maintaining fairness between information items. Our scheme allocates information items adaptively according to relative popularity for each local server. Simulation results show that our scheme van reduce the waiting time up to 30% compared with the round robin scheme while maintaining cost-effective fairness. ",
    "keywords": [
      "two-level broadcasting",
      "fairness",
      "data popularity"
    ]
  },
  {
    "id": "1078",
    "title": "Separating touching objects in remote sensing imagery: The restricted growing concept and implementations",
    "abstract": "This paper defines the restricted growing concept (RGC) for object separation and provides an algorithmic analysis of its implementations. Our concept decomposes the problem of object separation into two stages. First, separation is achieved by shrinking the objects to their cores while keeping track of their originals as masks, Then the core is grown within the masks obeying the guidelines of a restricted growing algorithm. In this paper, we apply RGC to the remote sensing domain, particularly the synthetic aperture radar (SAR) sea ice images.",
    "keywords": [
      "morphology",
      "object separation",
      "remote sensing imagery",
      "restricted growing"
    ]
  },
  {
    "id": "1079",
    "title": "Digital Distribution of Academic Journals and its Impact on Scholarly Communication: Looking Back After 20Years",
    "abstract": "It has been approximately 20years since distributing scholarly journals digitally became feasible. This article discusses the broad implications of the transition to digital distributed scholarship from a historical perspective and focuses on the development of open access (OA) and the various models for funding OA in the context of the roles scholarly journals play in scientific communities.",
    "keywords": [
      "open access",
      "history",
      "serials"
    ]
  },
  {
    "id": "1080",
    "title": "an evaluation of product review modalities for mobile phones",
    "abstract": "Research has shown that product reviews on the Internet not only support consumers when shopping, but also lead to increased sales for retailers. Recent approaches successfully use smart phones to directly relate products (e.g. via barcode or RFID) to corresponding reviews, making these available to consumers on the go. However, it is unknown what modality (star ratings/text/video) users consider useful for creating reviews and using reviews on their mobile phone, and how the preferred modalities are different from those on the Web. To shed light on this we conduct two experiments, one of them in a quasi-realistic shopping environment. The results indicate that, in contrast to the known approaches, stars and pre-structured text blocks should be implemented on mobile phones rather than long texts and videos. Users prefer less and rather well-aggregated product information while on the go. This accounts both for entering and, surprisingly, also for using product reviews.",
    "keywords": [
      "product recommendations",
      "mobile interaction",
      "product ratings",
      "user interfaces",
      "product reviews",
      "mobile applications"
    ]
  },
  {
    "id": "1081",
    "title": "A distributed collision resolution scheme for improving the performance in wireless LANs",
    "abstract": "The IEEE 802.11 distributed coordination function (DCF) provides a contention-based distribution channel access mechanism for stations to share the wireless medium. However, performance of the DCF drops dramatically due to high collision probability as the number of active stations becomes larger. In this paper, we propose a simple and effective collision resolution scheme for improving the performance of the DCF mechanism. Our idea is based on the estimation of the channels contention level, by measuring duration of busy and idle periods observed on the channel at each station. In order to reduce collision probability, the proposed scheme limits the number of contending stations at the same time according to the channel contention level. Performance of the proposed scheme is investigated by numerical analysis and simulation. Our results show that the proposed scheme is very effective and improves the performance under a wide range of contention levels.",
    "keywords": [
      "backoff algorithm",
      "collision resolution",
      "dcf",
      "mac",
      "wireless lan"
    ]
  },
  {
    "id": "1082",
    "title": "cryptographic link signatures for spectrum usage authentication in cognitive radio",
    "abstract": "It was shown that most of the radio frequency spectrum was inefficiently utilized. To fully use these spectrums, cognitive radio networks have been proposed. The idea is to allow secondary users to use a spectrum if the primary user (i.e., the legitimate owner of the spectrum) is not using it. To achieve this, secondary users should constantly monitor the usage of the spectrum to avoid interference with the primary user. However, achieving a trustworthy monitoring is not easy. A malicious secondary user who wants to gain an unfair use of a spectrum can emulate the primary user, and can thus trick the other secondary users into believing that the primary user is using the spectrum when it is not. This attack is called the Primary User Emulation (PUE) attack. To prevent this attack, there should be a way to authenticate primary users' spectrum usage. We propose a method that allows primary users to add a cryptographic link signature to its signal so the spectrum usage by primary users can be authenticated. This signature is added to the signal in a transparent way, such that the receivers (who do not care about the signature) still function as usual, while the cognitive radio receivers can retrieve the signature from the signal. We describe two schemes to add a signature, one using modulation, and the other using coding. We have analyzed the performance of both schemes.",
    "keywords": [
      "primary user emulation attack",
      "cognitive radio networks",
      "physical-layer authentication"
    ]
  },
  {
    "id": "1083",
    "title": "an end-user oriented graph-based visualization for spreadsheets",
    "abstract": "One of the difficulties in understanding and debugging spreadsheets is due to the invisibility of the data flow structure which is associated with cell formulas. In this paper, we present a spreadsheet visualization approach that is mainly based on the Markov Clustering (MCL) algorithm in an attempt to help spreadsheet users understand and debug their spreadsheets. The MCL algorithm helps in visualizing large graphs by generating clusters of cells. In our visualization approach, we also use compound fisheye views and treemaps to help in the navigation of the generated clusters. Compound fish eye views help to view members of a particular cluster while showing their linkages with other clusters. Treemaps help to visualize the depth we are at while navigating a cluster tree. Our initial experiments show that graph-based spreadsheet visualization using the MCL algorithm generates clusters which match with the corresponding logical areas of a given spreadsheet. Our experiments also show that analysis of the clusters helps us to identify some errors in the spreadsheets.",
    "keywords": [
      "visualization",
      "visual programming",
      "spreadsheets",
      "mcl algorithm",
      "end-user software engineering"
    ]
  },
  {
    "id": "1084",
    "title": "Pallet operation sequencing based on network part program logic",
    "abstract": "CAPP systems play a relevant role in aiding planners during setup planning, operation sequencing and pallet configuration activities. The support and automation granted by these techniques, together with the use of non-linear process planning logic, lead to a reduction in the planning time and costs, thus making manufacturers more competitive. This paper presents an approach that integrating process and production planning leads to the definition at the shop-floor level of the optimal operation sequence to machine all of the workpieces on a pallet using a four-axis machine tool. Part programs of non-production movements for each possible sequence of two operations are automatically generated at the shop-floor level and are simulated to obtain the non-production time. The complete sequence of operations is then defined on the basis of the minimisation of the estimated non-production time. This minimisation is performed using a mathematical model that defines a good sequence of operations. Four algorithms are adopted to analyse the proposed solution and to reduce the gap from optimality. The approach is tested on some cases taken from literature and on a real case. The real case was provided by a company that produces mechanical components. The obtained results underline a reduction on production and planning time, and consequently an increment in the company profit.",
    "keywords": [
      "computer aided process planning",
      "operation sequencing",
      "network part program"
    ]
  },
  {
    "id": "1085",
    "title": "Robust Capon beamforming against large DOA mismatch",
    "abstract": "In the presence of significant direction-of-arrival (DOA) mismatch, existing robust Capon beamformers based on the uncertainty set of the steering vector require a large size of uncertainty set for providing sufficient robustness against the increased mismatch. Under such circumstance, however, their output signal-to-interference-plus-noise ratios (SINRs) degrade. In this paper, a new robust Capon beamformer is proposed to achieve robustness against large DOA mismatch. The basic idea of the proposed method is to express the estimate of the desired steering vector corresponding to the signal of interest (SOI) as a linear combination of the basis vectors of an orthogonal subspace, then we can easily obtain the estimate of the desired steering vector by rotating this subspace. Different from the uncertainty set based methods, the proposed method does not make any assumptions on the size of the uncertainty set. Thus, compared to the uncertainty set based robust beamformers, the proposed method achieves a higher output SINR performance by preserving its interference-plus-noise suppression abilities in the presence of large DOA mismatch. In addition, computationally efficient online implementation of the proposed method has also been developed. Computer simulations demonstrate the effectiveness and validity of the proposed method.",
    "keywords": [
      "capon beamformer",
      "robust adaptive beamformer",
      "doa mismatch",
      "robustness"
    ]
  },
  {
    "id": "1086",
    "title": "Order Selection of the Linear Mixing Model for Complex-Valued FMRI Data",
    "abstract": "Functional magnetic resonance imaging (fMRI) data are originally acquired as complex-valued images, which motivates the use of complex-valued data analysis methods. Due to the high dimension and high noise level of fMRI data, order selection and dimension reduction are important procedures for multivariate analysis methods such as independent component analysis (ICA). In this work, we develop a complex-valued order selection method to estimate the dimension of signal subspace using information-theoretic criteria. To correct the effect of sample dependence to information-theoretic criteria, we develop a general entropy rate measure for complex Gaussian random process to calibrate the independent and identically distributed (i.i.d.) sampling scheme in the complex domain. We show the effectiveness of the approach for order selection on both simulated and actual fMRI data. A comparison between the results of order selection and ICA on real-valued and complex-valued fMRI data demonstrates that a fully complex analysis extracts more meaningful components about brain activation.",
    "keywords": [
      "order selection",
      "complex-valued fmri",
      "linear mixing model",
      "i.i.d. sampling",
      "entropy rate"
    ]
  },
  {
    "id": "1087",
    "title": "Optimal approximation of elliptic problems by linear and nonlinear mappings III: Frames",
    "abstract": "We study the optimal approximation of the solution of an operator equation A(u) = f by certain n-term approximations with respect to specific classes of frames. We consider worst case errors, where f is an element of the unit ball of a Sobolev or Besov space B-q(1)(L-p(Omega)) and Omega subset of R-d is abounded Lipschitz domain; the error is always measured in the H-s-norm. We study the order of convergence of the corresponding nonlinear frame widths and compare it with several other approximation schemes. Our main result is that the approximation order is the same as for the nonlinear widths associated with Riesz bases, the Gelfand widths, and the manifold widths. This order is better than the order of the linear widths iff p < 2. The main advantage of frames compared to Riesz bases, which were studied in our earlier papers, is the fact that we can now handle arbitrary bounded Lipschitz domains-also for the upper bounds.  ",
    "keywords": [
      "elliptic operator equation",
      "worst case error",
      "frames",
      "nonlinear approximation methods",
      "best n-term approximation",
      "manifold width",
      "besov spaces on lipschitz domains"
    ]
  },
  {
    "id": "1088",
    "title": "A hybrid heuristicgenetic algorithm for task scheduling in heterogeneous processor networks",
    "abstract": "Efficient task scheduling on heterogeneous distributed computing systems (HeDCSs) requires the consideration of the heterogeneity of processors and the inter-processor communication. This paper presents a two-phase algorithm, called H2GS, for task scheduling on HeDCSs. The first phase implements a heuristic list-based algorithm, called LDCP, to generate a high quality schedule. In the second phase, the LDCP-generated schedule is injected into the initial population of a customized genetic algorithm, called GAS, which proceeds to evolve shorter schedules. GAS employs a simple genome composed of a two-dimensional chromosome. A mapping procedure is developed which maps every possible genome to a valid schedule. Moreover, GAS uses customized operators that are designed for the scheduling problem to enable an efficient stochastic search. The performance of each phase of H2GS is compared to two leading scheduling algorithms, and H2GS outperforms both algorithms. The improvement in performance obtained by H2GS increases as the inter-task communication cost increases.",
    "keywords": [
      "genetic algorithms",
      "task scheduling",
      "list-based scheduling heuristics",
      "directed acyclic graph",
      "parallel and distributed processing",
      "heterogeneous systems"
    ]
  },
  {
    "id": "1089",
    "title": "A color image retrieval method based on color moment and color variance of adjacent pixels",
    "abstract": "This paper first introduces three simple and effective image features - the color moment (CM), the color variance of adjacent pixels (CVAP) and CM-CVAP. The CM feature delineates the color-spatial information of images, and the CVAP feature describes the color variance of pixels in an image. However, these two features can only characterize the content of images in different ways. This paper hence provides another feature CM-CVAP, which combines both, to raise the quality of similarity measure. The experimental results show that the image retrieval method based on the CM-CVAP feature gives quite an impressive performance.",
    "keywords": [
      "color-based image retrieval",
      "color histogram",
      "mass moment preserving"
    ]
  },
  {
    "id": "1090",
    "title": "Designing and optimizing a healthcare kiosk for the community",
    "abstract": "We outline the development of an interactive self-service healthcare kiosk. We apply a formal methodology to guarantee the measurement accuracy. The formal, generalizable and rigorous approach shows its practical efficiency. We know of no other studies that apply such methods in designing interaction. There is globally an increasing need for the health technologies outlined herein.",
    "keywords": [
      "self-service healthcare kiosk",
      "measurement accuracy",
      "parameter identification"
    ]
  },
  {
    "id": "1091",
    "title": "Hypergraph regularized autoencoder for image-based 3D human pose recovery",
    "abstract": "Pose recovery with autoencoder is imposed locality reservation with Laplacian matrix. The construction of Laplacian matrix is improved by using hypergraph optimization.",
    "keywords": [
      "human pose recovery",
      "deep learning",
      "manifold regularization",
      "hypergraph",
      "patch alignment framework"
    ]
  },
  {
    "id": "1092",
    "title": "Watermarking for HDR Image Robust to Tone Mapping",
    "abstract": "High Dynamic Range (HDR) images have been widely applied in daily applications. However, HDR image is a special format, which needs to be pre-processed known as tone mapping operators for display. Since the visual quality of HDR images is very sensitive to luminance value variations, conventional watermarking methods for low dynamic range (LDR) images are not suitable and may even cause catastrophic visible distortion. Currently, few methods for HDR image watermarking are proposed. In this paper, two watermarking schemes targeting HDR images are proposed, which are based on p-Law and bilateral filtering, respectively. Both of the subjective and objective qualities of watermarked images are greatly improved by the two methods. What's more, these proposed methods also show higher robustness against tone mapping operations.",
    "keywords": [
      "hdr image",
      "watermarking",
      "tone mapping",
      "mu-law",
      "bilateral filtering"
    ]
  },
  {
    "id": "1093",
    "title": "A GRA-based intuitionistic fuzzy multi-criteria group decision making method for personnel selection",
    "abstract": "Due to the increasing competition of globalization, selection of the most appropriate personnel is one of the key factors for an organizations success.The importance and complexity of the personnel selection problem call for the method combining both subjective and objective assessments rather than just subjective decisions. The aim of this paper is to develop a new method for solving the decision making process. An intuitionistic fuzzy multi-criteria group decision making method with grey relational analysis (GRA) is proposed. Intuitionistic fuzzy weighted averaging (IFWA) operator is utilized to aggregate individual opinions of decision makers into a group opinion. Intuitionistic fuzzy entropy is used to obtain the entropy weights of the criteria. GRA is applied to the ranking and selection of alternatives. A numerical example for personnel selection is given to illustrate the proposed method finally.",
    "keywords": [
      "personnel selection",
      "grey relational analysis ",
      "multi-criteria group decision making",
      "intuitionistic fuzzy set "
    ]
  },
  {
    "id": "1094",
    "title": "Design of a PC-based multimedia telemedicine system for brain function teleconsultation",
    "abstract": "During time-critical brain surgery, the detection of developing cerebral ischemia is particularly important because early therapeutic intervention may reduce the mortality of the patient. The purpose of this system is to provide an efficient means of remote teleconsultation for the early detection of ischemia, particularly when subspecialists are unavailable. The hardware and software design architecture for the multimedia brain function teleconsultation system including the dedicated brain function monitoring system is described. In order to comprehensively support remote teleconsultation, multi-media resources needed for ischemia interpretation were included: EEG signals, CSA, CD-CSA, radiological images, surgical microscope video images and video conferencing. PC-based system integration with standard interfaces and the operability over the Ethernet meet the cost-effectiveness while the modular software was customized with a diverse range of data manipulations and control functions necessary for shared workspace and standard interfaces.",
    "keywords": [
      "brain function monitoring",
      "teleconsultation",
      "multimedia"
    ]
  },
  {
    "id": "1095",
    "title": "Thermal characterization of LDMOS transistors for accelerating stress testing",
    "abstract": "The time to market is a major concern in the high-technology industry and when designing new products, the development cycle time becomes critical. Indeed, when a delay occurs in the development schedule, the potential market share of the designed product can be drastically decreased. In this context, developing accelerated stress testing (AST) in order to assess quickly the long-term behavior of a semiconductor becomes extremely useful. In this paper we show an example of how thermal characterization including simulation can be used to define a consistent AST for power ICs.",
    "keywords": [
      "semiconductor industry",
      "multi-pulse testing",
      "energy pulse characterization"
    ]
  },
  {
    "id": "1096",
    "title": "Steady flow and heat transfer of a magnetohydrodynamic Sisko fluid through porous medium in annular pipe",
    "abstract": "In this paper, the steady flow and heat transfer of a magnetohydrodynamic fluid is studied. The fluid is assumed to be electrically conducting in the presence of a uniform magnetic field and occupies the porous space in annular pipe. The governing nonlinear equations are modeled by introducing the modified Darcy's law obeying the Sisko model. The system is solved using the homotopy analysis method (HAM), which yields analytical solutions in the form of a rapidly convergent infinite series. Also, HAM is used to obtain analytical solutions of the problem for noninteger values of the power index. The resulting problem for velocity field is then numerically solved using an iterative method to show the accuracy of the analytic solutions. The obtained solutions for the velocity and temperature fields are graphically sketched and the salient features of these solutions are discussed for various values of the power index parameter. We also present a comparison between Sisko and Newtonian fluids. ",
    "keywords": [
      "mhd sisko fluid",
      "heat transfer",
      "porous medium"
    ]
  },
  {
    "id": "1097",
    "title": "identifying table boundaries in digital documents via sparse line detection",
    "abstract": "Most prior work on information extraction has focused on extracting information from text in digital documents. However, often, the most important information being reported in an article is presented in tabular form in a digital document. If the data reported in tables can be extracted and stored in a database, the data can be queried and joined with other data using database management systems. In order to prepare the data source for table search, accurately detecting the table boundary plays a crucial role for the later table structure decomposition. Table boundary detection and content extraction is a challenging problem because tabular formats are not standardized across all documents. In this paper, we propose a simple but effective preprocessing method to improve the table boundary detection performance by considering the sparse-line property of table rows. Our method easily simplifies the table boundary detection problem into the sparse line analysis problem with much less noise. We design eight line label types and apply two machine learning techniques, Conditional Random Field (CRF) and Support Vector Machines (SVM), on the table boundary detection field. The experimental results not only compare the performances between the machine learning methods and the heuristics-based method, but also demonstrate the effectiveness of the sparse line analysis in the table boundary detection.",
    "keywords": [
      "sparse line property",
      "table boundary detection",
      "support vector machine",
      "table data collection",
      "table labeling",
      "conditional random field"
    ]
  },
  {
    "id": "1098",
    "title": "An efficient document classification model using an improved back propagation neural network and singular value decomposition",
    "abstract": "This paper proposed a new improved method for back propagation neural network, and used an efficient method to reduce the dimension and improve the performance. The traditional back propagation neural network (BPNN) has the drawbacks of slow learning and is easy to trap into a local minimum, and it will lead to a poor performance and efficiency. In this paper, we propose the learning phase evaluation back propagation neural network (LPEBP) to improve the traditional BPNN. We adopt a singular value decomposition (SVD) technique to reduce the dimension and construct the latent semantics between terms. Experimental results show that the LPEBP is much faster than the traditional BPNN. It also enhances the performance of the traditional BPNN. The SVD technique cannot only greatly reduce the high dimensionality but also enhance the performance. So SVD is to further improve the document classification systems precisely and efficiently.",
    "keywords": [
      "document classification",
      "singular value decomposition",
      "bpnn",
      "lpebp"
    ]
  },
  {
    "id": "1099",
    "title": "browsing in a digital library collecting linearly arranged documents",
    "abstract": "A method of assisting a user in finding the required documents effectively is proposed. A user being informed which documents are worth examining can browse in a digital library (DL) in a linear fashion. Computational evaluations were carried out, and a DL and its navigator are designed and constructed.",
    "keywords": [
      "learning",
      "relevant information",
      "browsing assistant"
    ]
  },
  {
    "id": "1100",
    "title": "a framework for building haptic interactions for teleoperation systems",
    "abstract": "From the nano/micro-manipulation domain to the intervention in the nuclear field, haptics has become essential in actual teleoperation systems. The active property of this modality makes gestures more reliable and accurate. Nowadays, there is much research concerning the integration of haptic modality. The proposed solutions give, according to the adopted approaches, various efficiency levels. However, this work doesn't integrate generally and systematically psychophysics studies and ergonomics considerations. These elements are very important if we want to include effectively the human operator in any teleoperation system. Otherwise, various new applications present to the human operator several unfamiliar phenomena (e.g., nano-environments, underwater environments and the outer space environments). It's thus necessary in this type of application to transform virtually the remote environment to present to the operator nature-like haptic interactions. In this paper, we present a framework for building haptic interactions for teleoperation systems. This framework integrates the psychophysics studies and ergonomics elements, and presents a method to project the remote environment in the intuitive perception space.",
    "keywords": [
      "teleoperation",
      "haptic interaction"
    ]
  },
  {
    "id": "1101",
    "title": "Convex normal functions revisited",
    "abstract": "The lattice L of upper semicontinuous convex normal functions with convolution ordering arises in studies of type-2 fuzzy sets. In 2002, Kawaguchi and Miyakoshi [Extended t-norms as logical connectives of fuzzy truth values, Multiple-Valued Logic 8(1) (2002) 53-69] showed that this lattice is a complete Heyting algebra. Later. Harding et al. [Lattices of convex, normal functions, Fuzzy Sets and Systems 159 (2008) 1061-1071] gave an improved description of this lattice and showed it was a continuous lattice in the sense of Gierz et al. [A Compendium of Continuous Lattices, Springer, Berlin, 1980]. In this note we show the lattice L(u) is isomorphic to the lattice of decreasing functions from the real unit interval [0, 1] to the interval [0,2] under pointwise ordering, modulo equivalence almost everywhere. This allows development of further properties of L(u). It is shown that L(u) is completely distributive, is a compact Hausdorff topological lattice whose topology is induced by a metric, and is self-dual via a period two antiautomorphism. We also show the lattice L(u) has another realization of natural interest in studies of type-2 fuzzy sets. It is isomorphic to a quotient of the lattice L of all convex normal functions under the convolution ordering. This quotient identifies two convex normal functions if they agree almost everywhere and their intervals of increase and decrease agree almost everywhere.  ",
    "keywords": [
      "type-2 fuzzy set",
      "uniquely complemented lattice",
      "complete lattice",
      "continuous lattice",
      "metric topology"
    ]
  },
  {
    "id": "1102",
    "title": "On detecting spatial outliers",
    "abstract": "The ever-increasing volume of spatial data has greatly challenged our ability to extract useful but implicit knowledge from them. As an important branch of spatial data mining, spatial outlier detection aims to discover the objects whose non-spatial attribute values are significantly different from the values of their spatial neighbors. These objects, called spatial outliers, may reveal important phenomena in a number of applications including traffic control, satellite image analysis, weather forecast, and medical diagnosis. Most of the existing spatial outlier detection algorithms mainly focus on identifying single attribute outliers and could potentially misclassify normal objects as outliers when their neighborhoods contain real spatial outliers with very large or small attribute values. In addition, many spatial applications contain multiple non-spatial attributes which should be processed altogether to identify outliers. To address these two issues, we formulate the spatial outlier detection problem in a general way, design two robust detection algorithms, one for single attribute and the other for multiple attributes, and analyze their computational complexities. Experiments were conducted on a real-world data set, West Nile virus data, to validate the effectiveness of the proposed algorithms.",
    "keywords": [
      "algorithm",
      "outlier detection",
      "spatial data mining"
    ]
  },
  {
    "id": "1103",
    "title": "From blurry numbers to clear preferences: A mechanism to extract reputation in social networks",
    "abstract": "We explore the problems derived from aggregating quantitative opinions in reputation. We propose a mechanism that aggregates opinions based on pairwise comparisons. We show the suitability of the mechanism with an evaluation on real data sets.",
    "keywords": [
      "reputation",
      "pairwise elicitation",
      "tournaments",
      "social networks"
    ]
  },
  {
    "id": "1104",
    "title": "On the correlation distribution of the Coulter-Matthews decimation",
    "abstract": "The distribution of the cross correlation between the ternary m-sequence {s(t)} of period n = 3(m) - 1 and the decimated sequences {s(dt)} and {s(dt+1)} ofperiod (3(m) - 1)/2, where d = (2)/(3k+1) with k odd and gcd (k, m) = 1 is determined. The method to find this distribution is related to the result by Coulter and Matthews that f(x) = x(d) is a planar function over GF(3(m)).",
    "keywords": [
      "cross-correlation function",
      "m-sequences",
      "planar functions"
    ]
  },
  {
    "id": "1105",
    "title": "A Graphical Technique and Penalized Likelihood Method for Identifying and Estimating Infant Failures",
    "abstract": "Field failure data often exhibit extra heterogeneity as early failure data may have quite different distribution characteristics from later failure data. These infant failures may come from a defective subpopulation instead of the normal product population. Many exiting methods for field failure analyses focus only on the estimation for a hypothesized mixture model, while the model identification is ignored. This paper aims to develop efficient, accurate methods for both detecting data heterogeneity, and estimating mixture model parameters. Mixture distribution detection is achieved by applying a mixture detection plot (MDP) on field failure observations. The penalized likelihood method, and the expectation-maximization (EM) algorithm are then used for estimating the components in the mixture model. Two field datasets are employed to demonstrate and validate the proposed approach.",
    "keywords": [
      "expectation maximization",
      "infant mortality",
      "mixture detection plot",
      "mixture distribution"
    ]
  },
  {
    "id": "1106",
    "title": "Distributed multimedia service composition with statistical QoS assurances",
    "abstract": "Service composition allows multimedia services to be automatically composed from atomic service components based on dynamic service requirements. Previous work falls short for distributed multimedia service composition in terms of scalability, flexibility and quality-of-service (QoS) management. In this paper, we present a fully decentralized service composition framework, called SpiderNet, to address the challenges. SpiderNet provides statistical multiconstrained QoS assurances and load balancing for service composition. Moreover, SpiderNet supports directed acyclic graph composition topologies and exchangeable composition orders. We have implemented a prototype of SpiderNet and conducted experiments on both wide-area networks and a simulation testbed. Our experimental results show the feasibility and efficiency of the SpiderNet service composition framework.",
    "keywords": [
      "middleware",
      "quality-of-service ",
      "service composition",
      "service overlay network"
    ]
  },
  {
    "id": "1107",
    "title": "A classifier learning system using a coevolution method for deflection yoke misconvergence pattern classification problem",
    "abstract": "Deflection yoke (DY) is one of the core components of a cathode ray tube (CRT) in a computer monitor or a television that determines the image quality. Once a DY anomaly is found from beam patterns on a display in the production line of CRTs, the remedy process should be performed through three steps: identifying misconvergence types from the anomalous display pattern, adjusting manufacturing process parameters, and fine tuning. This study focuses on discovering a classifier for the identification of DY misconvergence patterns by applying a coevolutionary classification method. The DY misconvergence classification problems may be decomposed into two subproblems, which are feature selection and classifier adaptation. A coevolutionary classification method is designed by coordinating the two subproblems, whose performances are affected by each other. The proposed method establishes a group of partial sub-regions, defined by regional feature set, and then fits a finite number of classifiers to the data pattern by using a genetic algorithm in every sub-region. A cycle of the cooperation loop is completed by evolving the sub-regions based on the evaluation results of the fitted classifiers located in the corresponding sub-regions. The classifier system has been tested with real-field data acquired from the production line of a computer monitor manufacturer in Korea, showing superior performance to other methods such as k-nearest neighbors, decision trees, and neural networks.",
    "keywords": [
      "deflection yoke",
      "pattern classification",
      "feature selection"
    ]
  },
  {
    "id": "1108",
    "title": "Design of an energy function based fuzzy tuning controller for HVDC links",
    "abstract": "The paper introduces an energy function based fuzzy tuning method for the controller parameters of an HVDC transmission link. The test system, a point to point DC link, was subjected to various small and large disturbances to examine the effectiveness of the proposed method. The DC current error and its derivative are taken as the two principal signals to generate the change in the proportional and the integral gains of the rectifier current regulator according to a fuzzy rule base. Computer simulation results confirm the superiority of the proposed adaptive fuzzy controllers over the conventional fixed gain controllers in damping out the transient oscillations in HVDC links connected to weak AC systems.",
    "keywords": [
      "fuzzy tuning controller",
      "computer simulation",
      "fixed gain controller"
    ]
  },
  {
    "id": "1109",
    "title": "Influence of the Anx7 (+/?) Knockout Mutation and Fasting Stress on the Genomics of the Mouse Adrenal Gland",
    "abstract": "The Anx7 gene codes for a Ca2+/GTPase with calcium channel and membrane fusion properties that has been proposed to regulate exocytotic secretion in chromaffin and other cell types. We have previously reported that the homozygous Anx7 (+/?) knockout mouse has an embryonically lethal phenotype. However, the viable heterozygous Anx7 (+/?) mouse displays a complex phenotype that includes adrenal gland hypertrophy, chromaffin cell hyperplasia, and defective IP3 receptor (IP3R) expression. To search for a molecular basis for this phenotype, we have used cDNA microarray technology and have challenged control and mutant mice with fed or fasting conditions. We report that in the absence of the Anx7/IP3R signaling system, the cells in the adrenal gland are unable to discriminate between the fed and fasted states, in vivo. In control chromaffin cells, fasting is accompanied by an increased expression of structural genes for chromaffin cell contents, including chromogranin A and B, and D?H. There are also genes whose expression is specifically reduced. However, the Anx7 (+/?) mutation results in sustained expression of these nutritionally sensitive genes. We hypothesize that the calcium signaling defect due to the missing IP3R may be responsible for the global effects of the mutation on nutritionally sensitive genes. We further hypothesize that the tonically elevated expression of chromogranin A, a reportedly master control switch for dense core granule formation, may contribute to the process driving glandular hypertrophy and chromaffin cell hyperplasia in the Anx7 (+/?) mutant mouse.",
    "keywords": [
      "annexin 7",
      "synexin",
      "adrenal medulla",
      "mutation",
      "chromaffin cell",
      "nutriomics"
    ]
  },
  {
    "id": "1110",
    "title": "pruning long documents for distributed information retrieval",
    "abstract": "Query-based sampling is a method of discovering the contents of a text database by submitting queries to a search engine and observing the documents returned. In prior research sampled documents were used to build resource descriptions for automatic database selection, and to build a centralized sample database for query expansion and result merging. An unstated assumption was that the associated storage costs were acceptable.When sampled documents are long, storage costs can be large. This paper investigates methods of pruning long documents to reduce storage costs. The experimental results demonstrate that building resource descriptions and centralized sample databases from the pruned contents of sampled documents can reduce storage costs by 54-93% while causing only minor losses in the accuracy of distributed information retrieval.",
    "keywords": [
      "distributed information retrieval",
      "merging",
      "association",
      "centrality",
      "document pruning",
      "paper",
      "sampling",
      "prune",
      "select",
      "contention",
      "research",
      "method",
      "accuracy",
      "experimentation",
      "search engine",
      "query-expansion",
      "storage",
      "text",
      "demonstrate",
      "queries",
      "distributed",
      "documentation",
      "database",
      "resource",
      "query",
      "information retrieval"
    ]
  },
  {
    "id": "1111",
    "title": "predictive thread-to-core assignment on a heterogeneous multi-core processor",
    "abstract": "As multi-core processors are becoming common, vendors are starting to explore trade offs between the die size and the number of cores on a die, leading to heterogeneity among cores on a single chip. For efficient utilization of these processors, application threads must be assigned to cores such that the resource needs of a thread closely matches resource availability at the assigned core. Current methods of thread-to-core assignment often require an application's execution trace to determine its runtime properties. These traces are obtained by running the application on some representative input. A problem is that developing these representative input sets is time consuming, and requires expertise that the user of a general-purpose processor may not have. We propose an approach for automatic thread-to-core assignment for heterogeneous multicore processors to address this problem. The key insight behind our approach is simple - if two phases of a program are similar, then the data obtained by dynamic monitoring of one phase can be used to make scheduling decisions about other similar phases. The technical underpinnings of our approach include: a preliminary static analysis-based approach for determining similarity among program sections, and a thread-to-core assignment algorithm that utilizes the statically generated information as well as execution information obtained from monitoring a small fraction of the program to make scheduling decisions.",
    "keywords": [
      "thread-to-core assignment",
      "static program analysis",
      "heterogeneous multi-core processors",
      "phase behavior"
    ]
  },
  {
    "id": "1112",
    "title": "Fitting very large sparse Gaussian graphical models",
    "abstract": "In this paper we consider some methods for the maximum likelihood estimation of sparse Gaussian graphical (covariance selection) models when the number of variables is very large (tens of thousands or more). We present a procedure for determining the pattern of zeros in the model and we discuss the use of limited memory quasi-Newton algorithms and truncated Newton algorithms to fit the model by maximum likelihood. We present efficient ways of computing the gradients and likelihood function values for such models suitable for a desktop computer. For the truncated Newton method we also present an efficient way of computing the action of the Hessian matrix on an arbitrary vector which does not require the computation and storage of the Hessian matrix. The methods are illustrated and compared on simulated data and applied to a real microarray data set. The limited memory quasi-Newton method is recommended for practical use.",
    "keywords": [
      "covariance selection",
      "gene networks",
      "graphical models",
      "high dimensional",
      "large scale optimisation",
      "limited memory quasi-newton"
    ]
  },
  {
    "id": "1113",
    "title": "Segmentation of tiny objects in very poor-quality angiogenesis images",
    "abstract": "This paper deals with a straightforward and effective solution that isolates tiny objects from very poor-quality angiogenesis images. The objects of interest consist of the cross-section of blood vessels present in histological cuts of malign tumors that grow in soft parts of the human body through a natural process known as angiogenesis. The proposed strategy applies a conditional morphological closing operator using a structuring element based on criteria resulting from local statistical properties. This approach gives in all cases a lower percent of false target count (FTC) and false non-target count (FNTC) errors, with respect to the error equally calculated for two other strategies discussed briefly in this paper, when the results are compared with images segmented manually by pathologists.",
    "keywords": [
      "angiogenesis",
      "blood vessel segmentation",
      "conditional closing",
      "noise filtering",
      "morphology"
    ]
  },
  {
    "id": "1114",
    "title": "Varieties of Properties",
    "abstract": "The traditional distinction between primary (observation independent) and secondary (observation dependent) qualities is not based on a difference that can be sustained in the full light of contemporary scientific understanding. An alternative division of physical and chemical properties is proposed. Like the traditional division of qualities, the alternative system has two main categories. Properties of compound particulars that result from simple combination (e.g., addition) of the properties of their component parts constitute the first class",
    "keywords": [
      "properties",
      "qualities",
      "mereology",
      "primary qualities",
      "chemical combination",
      "mixis",
      "cooperative interactions",
      "autocatalysis",
      "dissipative structures",
      "mixed valence",
      "ivct"
    ]
  },
  {
    "id": "1115",
    "title": "lower bound for scalable byzantine agreement",
    "abstract": "We consider the problem of computing Byzantine Agreement in a synchronous network with n processors each with a private random string, where each pair of processors is connected by a private communication line. The adversary is malicious and non-adaptive, i.e., it must choose the processors to corrupt at the start of the algorithm. Byzantine Agreement is known to be computable in this model in an expected constant number of rounds.We consider a scalable model where in each round each uncorrupted processor can send to any set of log n other processors and listen to any set of log n processors. We define the loss of a computation to be the number of uncorrupted processors whose output does not agree with the output of the majority of uncorrupted processors. We show that if there are t corrupted processors, then any protocol which has probability at least 1/2 +1/log n of loss less than t 2/3 32 fn 1/3 log 5/3 n requires at least f rounds.",
    "keywords": [
      "probabilistic",
      "scalable",
      "byzantine agreement",
      "malicious adversary",
      "non-adaptive adversary",
      "randomized",
      "distributed computing",
      "lower bounds"
    ]
  },
  {
    "id": "1116",
    "title": "A variable compliance, soft gripper",
    "abstract": "Autonomous grasping is an important but challenging task and has therefore been intensively addressed by the robotics community. One of the important issues is the ability of the grasping device to accommodate varying object shapes in order to form a stable, multi-point grasp. Particularly in the human environment, where robots are faced with a vast set of objects varying in shape and size, a versatile grasping device is highly desirable. Solutions to this problem have often involved discrete continuum structures that typically comprise of compliant sections interconnected with mechanically rigid parts. Such devices require a more complex control and planning of the grasping action than intrinsically compliant structures which passively adapt to complex shapes objects. In this paper, we present a low-cost, soft cable-driven gripper, featuring no stiff sections, which is able to adapt to a wide range of objects due to its entirely soft structure. Its versatility is demonstrated in several experiments. In addition, we also show how its compliance can be passively varied to ensure a compliant but also stable and safe grasp.",
    "keywords": [
      "grasping",
      "soft robotics",
      "continuum robot",
      "variable compliance",
      "shape invariant grasping"
    ]
  },
  {
    "id": "1117",
    "title": "Extended Overview Techniques for Outdoor Augmented Reality",
    "abstract": "In this paper, we explore techniques that aim to improve site understanding for outdoor Augmented Reality (AR) applications. While the first person perspective in AR is a direct way of filtering and zooming on a portion of the data set, it severely narrows overview of the situation, particularly over large areas. We present two interactive techniques to overcome this problem: multi-view AR and variable perspective view. We describe in details the conceptual, visualization and interaction aspects of these techniques and their evaluation through a comparative user study. The results we have obtained strengthen the validity of our approach and the applicability of our methods to a large range of application domains.",
    "keywords": [
      "information interfaces and presentation",
      "mobile augmented reality",
      "multi-perspective views",
      "situation awareness",
      "navigation"
    ]
  },
  {
    "id": "1118",
    "title": "Influence of transition metals on halogen-bonded complexes of MCCBr center dot center dot center dot NCH and HCCBra center dot center dot center dot NCM ' (M, M ' = Cu, Ag, and Au)",
    "abstract": "We have performed quantum chemical calculations for the MCCBra (TM) a (TM) a (TM) NCH and HCCBra (TM) a (TM) a (TM) NCM' (M, M' = Cu, Ag, and Au) halogen-bonded complexes at the MP2 level. The results showed that the transition metals have different influences on the halogen bond donor and the electron donor. The transition metal atom in the former makes the halogen bond weaker, and that in the latter causes it to enhance. Molecular electrostatic potential and natural bond orbital analysis were carried out to reveal the nature of the substitution.",
    "keywords": [
      "electron donor",
      "halogen bond donor",
      "molecular electrostatic potential",
      "nbo",
      "transition metal"
    ]
  },
  {
    "id": "1119",
    "title": "Cost, Power Consumption and Performance Evaluation of Metro Networks",
    "abstract": "We provide models for evaluating the performance, cost and power consumption of different architectures suitable for a metropolitan area network (MAN). We then apply these models to compare today's synchronous optical network/synchronous digital hierarchy metro rings with different alternatives envisaged for next-generation MAN: an Ethernet carrier grade ring, an optical hub-based architecture and an optical time-slotted wavelength division multiplexing (WDM) ring. Our results indicate that the optical architectures are likely to decrease power consumption by up to 75% when compared with present day MANs. Moreover, by allowing the capacity of each wavelength to be dynamically shared among all nodes, a transparent slotted WDM yields throughput performance that is practically equivalent to that of today's electronic architectures, for equal capacity.",
    "keywords": [
      "metropolitan area networks",
      "optical networks",
      "power consumption",
      "performance evaluation"
    ]
  },
  {
    "id": "1120",
    "title": "Equity in access to fortified maize flour and corn meal",
    "abstract": "Mass fortification of maize flour and corn meal with a single or multiple micronutrients is a public health intervention that aims to improve vitamin and mineral intake, micronutrient nutritional status, health, and development of the general population. Micronutrient malnutrition is unevenly distributed among population groups and is importantly determined by social factors, such as living conditions, socioeconomic position, gender, cultural norms, health systems, and the socioeconomic and political context in which people access food. Efforts trying to make fortified foods accessible to the population groups that most need them require acknowledgment of the role of these determinants. Using a perspective of social determinants of health, this article presents a conceptual framework to approach equity in access to fortified maize flour and corn meal, and provides nonexhaustive examples that illustrate the different levels included in the framework. Key monitoring areas and issues to consider in order to expand and guarantee a more equitable access to maize flour and corn meal are described.",
    "keywords": [
      "fortified maize flour",
      "social determinants of health",
      "equity",
      "accessibility",
      "fortified corn meal"
    ]
  },
  {
    "id": "1121",
    "title": "Design of a printed MIMO/diversity monopole antenna for future generation handheld devices",
    "abstract": "This article presents a printed crescent-shaped monopole MIMO diversity antenna for wireless communications. The port-to-port isolation is increased by introducing an I-shaped conductor symmetrically between the two antenna elements and shaping the ground plane. Both the computed and experimental results confirm that the antenna possesses a wide impedance bandwidth of 54.5% across 1.62.8 GHz, with a reflection coefficient and mutual coupling better than ?10 and ?14 dB, respectively. By further validating the simulated and the measured radiation and MIMO characteristics including far-field, gain, envelope correlation and channel capacity loss, the results show that the antenna can offer effective MIMO/diversity operation to alleviate multipath environments.  ",
    "keywords": [
      "monopole antenna",
      "port-to-port isolation",
      "reflection coefficient",
      "mutual coupling",
      "mimo"
    ]
  },
  {
    "id": "1122",
    "title": "a rich traceability model for social interactions",
    "abstract": "In 1993, Goguen published a research note addressing the social issues in Requirements Engineering. He identified in the requirements process three major social groups: the client organization; the requirements team; and the development team. However, nowadays there is a lack of technological support that traces requirements to social issues on the requirements team or development team. From early published traceability metamodels to current requirements traceability literature, the client organization and the stakeholders are first-class citizens, but the software engineers and the interactions between these groups are not. In this paper we present a partially formalized RichPicture traceability model to fill this gap. ITrace is a flexible model to weave together the social network graph, the information sources graph, the social interactions graph, and the Requirements Engineering artifacts evolution graph. We empirically developed our traceability model tracking a Transparency catalogue evolution. We also compare our model structure to Contribution Structures.",
    "keywords": [
      "rich picture",
      "software evolution",
      "requirements traceability",
      "graph-based traceability",
      "social issues"
    ]
  },
  {
    "id": "1123",
    "title": "Neuro-fuzzy closed-loop control of depth of anaesthesia",
    "abstract": "The utility of the auditory evoked potential (AEP) is under investigation as a feedback signal for the automatic closed-loop control of general anaesthesia using neural networks and fuzzy logic. The AEP is a signal derived from the electroencephalogram (EEG) in response to auditory stimulation, which may be useful as an index of the depth of anaesthesia. A simple back-propagation neural network can learn the AEP and provides a satisfactory input to a fuzzy logic infusion controller for the administration of anaesthetic drugs, but the problem remains that of reliable signal acquisition.",
    "keywords": [
      "anaesthesia",
      "closed-loop control",
      "auditory evoked potential",
      "neural network",
      "fuzzy controller"
    ]
  },
  {
    "id": "1124",
    "title": "Computability and realizability for interactive computations",
    "abstract": "This paper deals with computability of interactive computations. It aims at the characterization and analysis of a general concept of interactive computation as a basis for the extension and generalization of the notion of computability to interactive computations. We extend the notion of computability to interactive computations. Instead of partial functions on natural numbers or on finite strings we work with functions and relations on infinite input and output streams. As part of the computability of such functions and relations on streams we treat the following aspects of interactive computations: causality between input and output streams realizability of single output streams for given input streams the role of non-realizable output relating non-realizable behaviors to state machines the concept of interactive computation and computability for interactive systems the role of time in computability.",
    "keywords": [
      "computability",
      "interaction",
      "realizability"
    ]
  },
  {
    "id": "1125",
    "title": "Probability density of the differential phase difference in applications to passive wireless surface acoustic wave sensing",
    "abstract": "The probability density function (pdf) is discussed of the differential phase difference (DPD) in the radio frequency (RF) pulse-burst perturbed by Gaussian noise at the coherent receiver. Statistical properties of the DPD are of importance for error estimation in coherent systems such as remote passive wireless surface acoustic wave (SAW) sensing with multiple differential phase measurement. The rigorous probability density of the DPD is derived and its particular functions, all having no closed forms, are given for different signal-to-noise ratios (SNRs) in the RF pulses. Employing the von Mises/Tikhonov distribution, an efficient approximation is proposed via the modified Bessel functions of the first kind and zeroth order. Engineering features and small errors of the approximation are demonstrated. Applications are given for the phase difference drift rate and error probability for the drift rate to exceed a threshold.",
    "keywords": [
      "differential phase difference",
      "probability density",
      "passive saw sensing",
      "drift rate",
      "error probability"
    ]
  },
  {
    "id": "1126",
    "title": "An improved algorithm for the distance constrained p-center location problem with mutual communication on tree networks",
    "abstract": "In a recent article Averbakh and Berman present an O(p(3)root(log log p)(log p) + n(2)) serial algorithm to solve the distance constrained p-center location problem with mutual communication on a tree network with n nodes. In this note we suggest two simple modifications leading to the improved (subquadratic in n), O(p(3)root(log log p)(log p) + p(n + p)log(2)(n + p)) complexity bound. We also present a new O(p(2)n log n log(n + p)) algorithm for the discrete version of this problem. ",
    "keywords": [
      "multifacility location",
      "p-center",
      "tree networks"
    ]
  },
  {
    "id": "1127",
    "title": "CADRE: The CArma Data REduction pipeline",
    "abstract": "The Combined Array for Millimeter-wave Astronomy (CARMA) data reduction pipeline (CADRE) has been developed to give investigators a first look at a fully reduced set of their data. It runs automatically on all data produced by the telescope as they arrive in the CARMA data archive. CADRE is written in Python and uses Python wrappers for MIRIAD subroutines for direct access to the data. It goes through the typical reduction procedures for radio telescope array data and produces a set of continuum and spectral line maps in both MIRIAD and FITS format. CADRE has been in production for nearly two years and this paper presents the current capabilities and planned development.",
    "keywords": [
      "data reduction pipeline"
    ]
  },
  {
    "id": "1128",
    "title": "A novel IP3 boosting technique using feedforward distortion cancellation method for 5 GHz CMOS LNA",
    "abstract": "Low noise amplifier (LNA) in many wireless and wireline communication systems must have low noise, sufficient gain and high linearity performance. This paper presents a novel IP3 boosting technique using Feedforward Distortion Cancellation (FDC) method, that is, use an additional path to generate distortion and then cancel with the original LNA's distortion at its output. Through this technique, the IIP3 of LNA can be boosted from about 0 dBm, which is reported in most public literature to date, to +21 dBm, which is firstly reported to this day, with negligible noise degradation.",
    "keywords": [
      "ip3 boosting",
      "cmos lna",
      "noise",
      "wlan"
    ]
  },
  {
    "id": "1129",
    "title": "ISSDC DIGRAM CODING BASED LOSSLESS DATA COMPRESSION ALGORITHM",
    "abstract": "In this paper, a new lossless data compression method that is based on digram coding is introduced This data compression method uses semi-static dictionaries All of the used characters and most frequently used two character blocks (digrams) in the source are found and inserted into a dictionary in the first pass, compression is performed in the second pass This two-pass structure is repeated several times and in every iteration particular number of elements is inserted in the dictionary until the dictionary is filled This algorithm (ISSDC Iterative Semi-Static Digram Coding) also includes some mechanisms that can decide about total number of iterations and dictionary size whenever these values are not given by the user Our experiments show that ISSDC is better than LZW/GIF and BPE in compression ratio It is worse than DEFLATE in compression of text and binary data, but better than PNG (which uses DEFLATE compression) in lossless compression of simple images",
    "keywords": [
      "lossless data compression",
      "dictionary-based compression",
      "semi-static dictionary",
      "digram coding"
    ]
  },
  {
    "id": "1130",
    "title": "An embedded system course using JavaME and android",
    "abstract": "This paper describes the objectives and contents of a cost-effective curriculum for embedded system course in our university. The main aim of the course is learning to solve a real problem by developing a real system. The students learn skills to adapt this system to new scenarios. The system consists of a wireless module, a microcontroller and an application for smartphones to control lights by wireless communications. In order to motivate students, JavaME and Android, the important technologies for today's smartphones were chosen. As a result of the course, following the Bologna guidelines, the students worked cooperatively, like in a real scenario. Each group member worked in a complementary manner by analyzing the division of tasks for each student. We followed project-based methodology has provided an incremental learning to students. According to results, students responded to the course survey that they are knowledgeable on how embedded systems work after taking the course.  ",
    "keywords": [
      "engineering education",
      "embedded system",
      "project-based learning",
      "educational innovation",
      "education technology",
      "radio communication"
    ]
  },
  {
    "id": "1131",
    "title": "Positive solutions of a focal problem for one-dimensional p-Laplacian equations",
    "abstract": "This paper mainly deals with the existence and multiplicity of positive solutions for the focal problem involving both the p-Laplacian and the first order derivative: {((u')(p-1))' + f (t, u, u') = 0, t is an element of(0, 1), u(0) = u'(1) = 0. The main tool in the proofs is the fixed point index theory, based on a priori estimates achieved by using Jensen's inequality and a new inequality. Finally the main results are applied to establish the existence of positive symmetric solutions to the Dirichlet problem: {(|u'|(p-2)u')' + f (u, u') = 0, t is an element of (-1, 0) boolean OR (0, 1), u(-1) = u(1) = 0.  ",
    "keywords": [
      "p-laplacian equation",
      "positive solution",
      "focal problem",
      "fixed point index",
      "dirichlet problem",
      "jensen's inequality"
    ]
  },
  {
    "id": "1132",
    "title": "A gas-kinetic BGK scheme for gaswater flow",
    "abstract": "A new one-dimensional gas-kinetic BGK scheme for gaswater flow is developed with the inclusion of the stiffened equation of state for water. The mixture model is considered, where the gas and water inside a computational cell achieve the equilibrium state, with equal pressure, velocity and temperature, within a time step. The splitting method is adopted to calculate the flux of each component at a cell interface individually. The preliminary application of the present newly developed method in different types of shock tube problems, including gasgas shock tube and gaswater shock tube problems, validates its good performance for gaswater flow.",
    "keywords": [
      "gas-kinetic scheme",
      "gaswater two-phase flow",
      "mixture model"
    ]
  },
  {
    "id": "1133",
    "title": "Simulations of spacing of localized zones in reinforced concrete beams using elasto-plasticity and damage mechanics with non-local softening",
    "abstract": "The paper presents quasi-static plane strain FE-simulations of strain localization in reinforced concrete beams without stirrups. The material was modeled with two different isotropic continuum crack models: an elasto-plastic and a damage one. In case of elasto-plasticity, linear Drucker-Prager criterion with a non-associated flow rule was defined in the compressive regime and a Rankine criterion with an associated flow rule was adopted in the tensile regime. In the case of a damage model, the degradation of the material due to micro-cracking was described with a single scalar damage parameter. To ensure the mesh-independence and to capture size effects, both criteria were enhanced in a softening regime by non-local terms. Thus, a characteristic length of micro-structure was included. The effect of a characteristic length, reinforcement ratio, bond-slip stiffness, fracture energy and beam size on strain localization was investigated. The numerical results with reinforced concrete beams were quantitatively compared with corresponding laboratory tests by Walraven (1978).",
    "keywords": [
      "bond-slip",
      "concrete",
      "characteristic length",
      "damage mechanics",
      "elasto-plasticity",
      "nonlocal theory",
      "reinforcement",
      "strain localization"
    ]
  },
  {
    "id": "1134",
    "title": "a lab implementation of syn flood attack and defense",
    "abstract": "A \"denial-of-service\" attack is characterized by an explicit attempt by attackers to prevent legitimate users of a service from using that service. SYN flood attack is one of the most common types of DoS. In this lab, we model and simulate a real world network, and we launch a SYN attack against our web server. Through this, we study the nature of the attack and investigate the effectiveness of several approaches in defending against SYN attack. This lab will allow students to anatomize the SYN flooding attack and defense in the lab environment and obtain data and parameters of DoS resilience capability.",
    "keywords": [
      "curriculum",
      "security",
      "laboratory"
    ]
  },
  {
    "id": "1135",
    "title": "Student-Project Allocation with preferences over Projects",
    "abstract": "We study the problem of allocating students to projects, where both students and lecturers have preferences over projects, and both projects and lecturers have capacities. In this context we seek a stable matching of students to projects, which respects these preference and capacity constraints. Here, the stability definition generalises the corresponding notion in the context of the classical Hospitals/Residents problem. We show that stable matchings can have different sizes, which motivates max-spa-p, the problem of finding maximum cardinality stable matching. We prove that max-spa-p is NP-hard and not approximable within ? , for some ?>1 ? > 1 , unless P=NP P = NP . On the other hand, we give an approximation algorithm with a performance guarantee of 2 for max-spa-p.",
    "keywords": [
      "matching problem",
      "stable matching",
      "np-hardness",
      "approximation hardness",
      "approximation algorithm"
    ]
  },
  {
    "id": "1136",
    "title": "A novel SIP based procedure for congestion aware handover in heterogeneous networks",
    "abstract": "One of the next steps in the current rapid evolution of wireless technologies will be that operators will need to enable users to use communication' services independently of access technologies, so they will have to support seamless handovers in heterogeneous networks. In this paper we present a novel SIP based procedure for congestion aware handover in heterogeneous networks. With newly defined SIP messages the handover decision is based not only on signal strength, but also on target network status. Using the SIP protocol, this approach is completely independent of access technology and the underlying protocol used, and thus can be easily deployed in operator's environments. The proposed procedure was evaluated with a purpose-built simulation model. The results show that, using the proposed procedure, the QoE of VoIP users can be maintained during session by eliminating handovers to target network, which could cause degradation of service in the target network and by triggering handovers to another network, when there could come to degradation of service in the current network.  ",
    "keywords": [
      "seamless handover",
      "sip",
      "heterogeneous networks",
      "performance evaluation",
      "congestion awareness"
    ]
  },
  {
    "id": "1137",
    "title": "use of active rfid and environment-embedded sensors for indoor object location estimation",
    "abstract": "This paper describes a method for localizing objects in an actual living environment. We have developed this method by using a complementary combination of 1) received signal strength indicators (RSSIs) and vibration data acquired from active RFID tags, and 2) human behavior detected from various types of sensors embedded in the environment. Regarding the former, we use a pattern recognition method to select a feature appeared in SSIs received by several radio frequency (RF) readers at different places and to classify them into a particular location. In our work, we regard the estimated location as the most probable location where the object is placed. As for the latter, we use the detected human behavior to support the estimation based on the analysis of RSSIs. Experiment results showed that the proposed method improved the estimation performance from about 50 to 95% compared with using only RSSIs to localize objects. Moreover, the results also suggested that we can estimate object location indoors without sensors for detecting human position. This indoor object localization method can contribute for constructing an indoor object management system that improves living comfort.",
    "keywords": [
      "rssi",
      "environment-embedded sensor",
      "indoor localization",
      "active rfid"
    ]
  },
  {
    "id": "1138",
    "title": "Noise-Tuning-Based Hysteretic Noisy Chaotic Neural Network for Broadcast Scheduling Problem in Wireless Multihop Networks",
    "abstract": "Compared with noisy chaotic neural networks (NCNNs), hysteretic noisy chaotic neural networks (HNCNNs) are more likely to exhibit better optimization performance at higher noise levels, but behave worse at lower noise levels. In order to improve the optimization performance of HNCNNs, this paper presents a novel noise-tuning-based hysteretic noisy chaotic neural network (NHNCNN). Using a noise tuning factor to modulate the level of stochastic noises, the proposed NHNCNN not only balances stochastic wandering and chaotic searching, but also exhibits stronger hysteretic dynamics, thereby improving the optimization performance at both lower and higher noise levels. The aim of the broadcast scheduling problem (BSP) in wireless multihop networks (WMNs) is to design an optimal time-division multiple-access frame structure with minimal frame length and maximal channel utilization. A gradual NHNCNN (G-NHNCNN), which combines the NHNCNN with the gradual expansion scheme, is applied to solve BSP in WMNs to demonstrate the performance of the NHNCNN. Simulation results show that the proposed NHNCNN has a larger probability of finding better solutions compared to both the NCNN and the HNCNN regardless of whether noise amplitudes are lower or higher.",
    "keywords": [
      "broadcast scheduling problem",
      "hysteresis",
      "noise tuning",
      "noisy chaotic neural network",
      "wireless multihop networks"
    ]
  },
  {
    "id": "1139",
    "title": "probabilistic multi-path vs. deterministic single-path protocols for dynamic ad-hoc network scenarios",
    "abstract": "We investigate the performance of different protocol stacks under various application scenarios. Our method of choice is a full-fledged simulation in QualNet, testing the complete protocol stack over fairly large-scale networks. We find that the relative ranking of protocols strongly depends on the network scenario, the session load, the mobility level, and the choice of protocol parameters. We show that the Parametric Probabilistic Protocols, which we generalize from their original definition, can outperform standard routing protocols, such as AODV or Gossiping or Shortest-Path, in a variety of realistic scenarios.",
    "keywords": [
      "wireless networks",
      "probabilistic routing",
      "scenarios",
      "mobility"
    ]
  },
  {
    "id": "1140",
    "title": "Geophysics and NIH Image1",
    "abstract": "It is possible to interpret 2D and 3D seismic data using NIH Image, a free medical imaging software product developed by the US National Institutes of Health. Using Image, or one of several developing spin-offs, the basic flow of seismic interpretation can be accomplished. It is also capable of some advanced methods such as volume visualization, amplitude calibration to well control, cube displays and reservoir area/volume estimates. All figures in this paper were generated using this free product. At the least, Image is a marvelous data viewer which compliments workstation class systems. However, for many users, it may be sufficient for the entire interpretation process.",
    "keywords": [
      "seismic interpretation",
      "software",
      "internet"
    ]
  },
  {
    "id": "1141",
    "title": "Coal-fired generation in a privatised electricity supply industry",
    "abstract": "Since privatisation of the UK Electricity Supply Industry, the merit-order for the dispatch of generating plant has undergone radical changes. In Scotland, where the electricity companies have a broad-based generation portfolio, base load is now usually carried by a combination of nuclear and gas-fired generation. Hydroelectricity has now moved to a much less well-defined position for generation, partly because of its seasonal variability. However, the major consequence of this arrangement has been that coal-fired generation has slipped in the merit-order from the days of Nationalisation and now occupies a mid-merit position. This changed role for coal has imposed new requirements on engineers involved in the operation of, and generation planning for, coal-fired power stations.",
    "keywords": [
      "coal-fired generation",
      "base load",
      "mid-merit position"
    ]
  },
  {
    "id": "1142",
    "title": "Provable algorithms for parallel generalized sweep scheduling",
    "abstract": "We present provably efficient parallel algorithms for sweep scheduling, which is a commonly used technique in radiation transport problems, and involves inverting an operator by iteratively sweeping across a mesh from multiple directions. Each sweep involves solving the operator locally at each cell. However. each direction induces a partial order in which this computation can proceed. On a distributed computing system, the goal is to schedule the computation, so that the length of the schedule is minimized. Due to efficiency and coupling considerations, we have an additional constraint, namely. a mesh cell must be processed on the same processor along each direction. Problems similar in nature to sweep scheduling arise in several other applications, and here we formulate a combinatorial generalization of this problem that captures the sweep scheduling constraints and call it the generalized sweep scheduling problem. Several heuristics have been proposed for this problem; see [S. Pautz, An algorithm for parallel S-n, sweeps on unstructured meshes, Nucl. Sci. Eng. 140 (2002) 111-136; S. Plimpton, B. Hendrickson, S. Burns, W. McLendon, Parallel algorithms for radiation transport on unstructured grids, Super Comput. (2001)] and the references therein; but none of these have provable worst case performance guarantees. Here we present a simple, almost linear time randomized algorithm for the generalized sweep scheduling problem that (provably) gives a schedule of length at most O(log(2)n) times the optimal schedule for instances with n cells, when the communication cost is not considered, and a slight variant, which coupled with a Much more careful analysis, gives a schedule of (expected) length 0(log m log log log m) times the optimal schedule for in processors. These are the first such provable guarantees for this problem. The algorithm can be extended with an additional multiplicative factor in the case when we have inter-processor communication latency, in the models of Rayward-Smth [UET scheduling with inter-processor communication delays, Discrete Appl. Math. 18 (1) (1987) 55-71] and Hwang et al. [Scheduling precedence graphs in systems with inter-processor communication times. SIAM J. Comput. 18(2) (1989) 244-257]. Our algorithms are extremely simple, and use no geometric information about the mesh; therefore, these techniques are likely to be applicable in more general settings. We also design a priority based list schedule using these ideas, with the same theoretical guarantee, but much better performance in practice; combining this algorithm with a simple block decomposition also lowers the overall communication cost significantly. Finally, we perform a detailed experimental analysis of our algorithm. Our results indicate that the algorithm compares favorably with the length of the schedule produced by other natural and efficient parallel algorithms proposed in the literature [S. Pautz, An Algorithm for parallel S-n sweeps on unstructured meshes, Nucl. Sci. Eng. 140 (2002) 111-136: S. Plimpton, B. Hendrickson, S. Burns, W. McLendon, Parallel algorithms for radiation transport on unstructured grids, Super Comput. (2001)].  ",
    "keywords": [
      "parallel algorithms",
      "minimum makespan scheduling",
      "approximation algorithms",
      "sweep scheduling on meshes"
    ]
  },
  {
    "id": "1143",
    "title": "RDTC optimized compression of image-based scene representations (Part I): Modeling and theoretical analysis",
    "abstract": "Rendering of virtual views in interactive streaming of compressed image-based scene representations requires random access to arbitrary parts of the reference image data. The degree of interframe dependencies exploited during encoding has an impact on the transmission and decoding time and, at the same time, delimits the (storage) rate-distortion (RD) tradeoff that can be achieved. In this work, we extend the classical RD optimization approach using hybrid video coding concepts to a tradeoff between the storage rate (R), distortion (D), transmission data rate (T), and decoding complexity (Q. We present a theoretical model for this RDTC space with a focus on the decoding complexity and, in addition, the impact of client side caching on the RDTC measures is considered and evaluated. Experimental results qualitatively match those predicted by our theoretical models and show that an adaptation of the encoding process to scenario specific parameters like computational power of the receiver and channel throughput can significantly reduce the user-perceived delay or required storage for RDTC optimized streams compared to RD optimized or independently encoded scene representations.",
    "keywords": [
      "image-based rendering ",
      "interactive streaming",
      "rdtc optimization"
    ]
  },
  {
    "id": "1144",
    "title": "Forecasting accuracy of behavioural models for participation in the arts",
    "abstract": "This paper assesses the forecasting performance of count data models applied to arts attendance. We estimate participation models for two artistic activities that differ in their degree of popularity  museums and jazz concerts  with data derived from the 2002 release of the Survey of Public Participation in the Arts for the United States. We estimate a finite mixture model  a zero-inflated negative binomial model  that allows us to distinguish between true non-attendants and goers and their respective behaviour regarding participation in the arts. We evaluate the predictive (in-sample) and forecasting (out-of-sample) accuracy of the estimated model using bootstrapping techniques to compute the Brier score. Overall, the results indicate the model performs well in terms of forecasting. Finally, we draw certain policy implications from the models forecasting capacity, thereby allowing the identification of target populations.",
    "keywords": [
      "forecasting",
      "count data",
      "brier scores",
      "bootstrapping",
      "cultural participation"
    ]
  },
  {
    "id": "1145",
    "title": "Effect of intervertebral disc degeneration on disc cell viability: a numerical investigation",
    "abstract": "Degeneration of the intervertebral disc may be initiated and supported by impairment of the nutrition processes of the disc cells. The effects of degenerative changes on cell nutrition are, however, only partially understood. In this work, a finite volume model was used to investigate the effect of endplate calcification, water loss, reduction of disc height and cyclic mechanical loading on the sustainability of the disc cell population. Oxygen, lactate and glucose diffusion, production and consumption were modelled with non-linear coupled partial differential equations. Oxygen and glucose consumption and lactate production were expressed as a function of local oxygen concentration, pH and cell density. The cell viability criteria were based on local glucose concentration and pH. Considering a disc with normal water content, cell death was initiated in the centre of the nucleus for oxygen, glucose, and lactate diffusivities in the cartilaginous endplate below 20% of the physiological values. The initial cell population could not be sustained even in the non-calcified endplates when a reduction of diffusion inside the disc due to water loss was modelled. Alterations in the disc shape such as height loss, which shortens the transport route between the nutrient sources and the cells, and cyclic mechanical loads, could enhance cell nutrition processes.",
    "keywords": [
      "cell nutrition",
      "cell metabolism",
      "finite volume",
      "cell viability",
      "intervertebral disc"
    ]
  },
  {
    "id": "1146",
    "title": "RESOURCE AWARE RUN-TIME ADAPTATION SUPPORT FOR RECOVERY STRATEGIES",
    "abstract": "The selection of recovery strategies is often based only on the types and circumstances of the failures. However, also changes in the euvironment such as fewer resources at node levels or degradation of quality-of-service Should be considered before allocating a new process/task to another host or before taking re-configuration decisions. In this paper we present why and how resource availability information should be considered for recovery strategies adaptation. Such resource aware run-time adaptation of recovery improves the availability and survivability of a system.",
    "keywords": [
      "recovery strategy",
      "fault tolerance",
      "adaptation",
      "resource monitoring",
      "availability"
    ]
  },
  {
    "id": "1147",
    "title": "Optimal retailers replenishment decisions in the EPQ model under two levels of trade credit policy",
    "abstract": "The main purpose of this paper is to investigate the optimal retailers replenishment decisions under two levels of trade credit policy within the economic production quantity (EPQ) framework. We assume that the supplier would offer the retailer a delay period and the retailer also adopts the trade credit policy to stimulate his/her customer demand to develop the retailers replenishment model under the replenishment rate is finite. Furthermore, we assume that the retailers trade credit period offered by supplier M is not shorter than the customers trade credit period offered by retailer N (M?N). Since the retailer cannot earn any interest in this situation, M<N. Based upon the above arguments, this paper incorporates both Chung and Huang [K.J. Chung, Y.F. Huang, The optimal cycle time for EPQ inventory model under permissible delay in payments, International Journal of Production Economics 84 (2003) 307318] and Huang [Y.F. Huang, Optimal retailers ordering policies in the EOQ model under trade credit financing, Journal of the Operational Research Society 54 (2003) 10111015] under above conditions. In addition, we model the retailers inventory system as a cost minimization problem to determine the retailers optimal replenishment decisions. Then three theorems are developed to efficiently determine the optimal replenishment decisions for the retailer. We deduce some previously published results of other authors as special cases. Finally, numerical examples are given to illustrate the theorems obtained in this paper. Then, as well as, we obtain a lot of managerial insights from numerical examples.",
    "keywords": [
      "epq",
      "inventory",
      "two levels of trade credit",
      "permissible delay in payments"
    ]
  },
  {
    "id": "1148",
    "title": "Focused shape models for hip joint segmentation in 3D magnetic resonance images",
    "abstract": "? We introduce a weighted shape learning approach applied to the human hip joint. Weights may be set to focus the shape representation energy to important areas. The highly weighted areas become the dominant representations within the model. Lower reconstruction errors and higher accuracy can be obtained in those areas. Validation was done on 35 3T unilateral small field of view MR scans.",
    "keywords": [
      "hip joint",
      "wpca",
      "bone segmentation",
      "shape models",
      "mri"
    ]
  },
  {
    "id": "1149",
    "title": "Top-k k followee recommendation over microblogging systems by exploiting diverse information sources",
    "abstract": "A novel CF approach for recommending high quality top-k k microblogging followees. Latent factor model to exploit both tweet content and social relations. The comprehensive experiments on large-scale traces and Amazon Mechanical Turk.",
    "keywords": [
      "microblogging",
      "followee recommendation",
      "multiple source information"
    ]
  },
  {
    "id": "1150",
    "title": "Orientation-dependent stress build-up during the formation of epitaxial CoSi2",
    "abstract": "The epitaxial CoSi2 is formed by using a Ti interlayer with variable thickness on a (100) Si substrate (TIME: Ti interlayer mediated epitaxy). These experiments confirm previous work where for an increasing amount of Ti in between the Si substrate and the Co layer, a transition from polycrystalline CoSi2 over a preferential (220) orientation towards (400) epitaxial CoSi2 is observed. This work emphasises the impact of the volume fraction of the epitaxial (400) CoSi2 grains on the mechanical stress induced during the Si/Ti/Co silicidation. It is shown that the mechanical stress increases for increasing epitaxial (400) orientation of the CoSi2 film. The stress caused by the epitaxial CoSi2 can have a detrimental impact on the device performance.",
    "keywords": [
      "epitaxial cosi2",
      "mechanical stress",
      "stress relaxation mechanisms"
    ]
  },
  {
    "id": "1151",
    "title": "Multi-view action recognition based on action volumes, fuzzy distances and cluster discriminant analysis",
    "abstract": "In this paper, we present a view-independent action recognition method exploiting a low computational-cost volumetric action representation. Binary images depicting the human body during action execution are accumulated in order to produce the so-called action volumes. A novel time-invariant action representation is obtained by exploiting the circular shift invariance property of the magnitudes of the Discrete Fourier Transform coefficients. The similarity of an action volume with representative action volumes is exploited in order to map it to a lower-dimensional feature space that preserves the action class properties. Discriminant learning is, subsequently, employed for further dimensionality reduction and action class discrimination. By using such an action representation, the proposed approach performs fast action recognition. By combining action recognition results coming from different view angles, high recognition rates are obtained. The proposed method is extended to interaction recognition, i.e., to human action recognition involving two persons. The proposed approach is evaluated on a publicly available action recognition database using experimental settings simulating situations that may appear in real-life applications, as well as on a new nutrition support action recognition database.",
    "keywords": [
      "action recognition",
      "action volumes",
      "fuzzy vector quantization",
      "cluster discriminant analysis"
    ]
  },
  {
    "id": "1152",
    "title": "A hierarchical model for object-oriented design quality assessment",
    "abstract": "This paper describes an improved hierarchical model for the assessment of high-level design quality attributes in object-oriented designs. In this model, structural and behavioral design properties of classes, objects, and their relationships are evaluated using a suite of object-oriented design metrics. This model relates design properties such as encapsulation, modularity, coupling, and cohesion to high-level quality attributes such as reusability, flexibility, and complexity using empirical and anecdotal information, The relationship, or links, from design properties to quality attributes are weighted in accordance with their influence and importance. The model is validated by using empirical and expert opinion to compare with the model results on several large commercial object-oriented systems. A key attribute of the model is that it can be easily modified to include different relationships and weights, thus providing a practical quality assessment too adaptable to a variety of demands.",
    "keywords": [
      "quality model",
      "quality attributes",
      "design metrics",
      "product metrics",
      "object-oriented metrics"
    ]
  },
  {
    "id": "1153",
    "title": "Support System for Novice Researchers (SSNR): Usability Evaluation of the First Use",
    "abstract": "Scholars make use of research output in the form of conference proceedings, journal and. theses as references as guideline in generating new knowledge for the use of future generations. Support in the early stage of study is crucial for novice researchers as it will give them some insights of where to seek for extra information on relevant literature, institutions, people and research trend without having to go through tedious process of identifying this information all by themselves. The result of the implementation of SSNR shows significant information that can be utilized by novice researchers in accelerating research process. Thus, this paper will discuss on the evaluation of SSNR by novice researchers, in terms of its usability. The results are promising which indicated that SSNR work as it should.",
    "keywords": [
      "ssnr",
      "novice researchers",
      "usability",
      "human-machine system"
    ]
  },
  {
    "id": "1154",
    "title": "An automated process for compiling dataflow graphs into reconfigurable hardware",
    "abstract": "We describe a system, developed as part of the Cameron project, which compiles programs written in a single-assignment subset of C called SA-C into dataflow graphs and then into VHDL, The primary application domain is image processing, The system consists of an optimizing compiler which produces dataflow graphs and a dataflow graph to VHDL translator. The method used for the translation is described here, along with some results on an application. The objective is not to produce yet another design entry tool, but rather to shift the programming paradigm from HDLs to an algorithmic level, thereby extending the realm of hardware design to the application programmer.",
    "keywords": [
      "adaptive computing",
      "configurable",
      "image processing",
      "reconfigurable components",
      "reconfigurable computing",
      "reconfigurable systems"
    ]
  },
  {
    "id": "1155",
    "title": "Dialogue on Reverse-Engineering Assessment and Methods",
    "abstract": "The biotechnological advances of the last decade have confronted us with an explosion of genetics, genomics, transcriptomics, proteomics, and metabolomics data. These data need to be organized and structured before they may provide a coherent biological picture. To accomplish this formidable task, the availability of an accurate map of the physical interactions in the cell that are responsible for cellular behavior and function would be exceedingly helpful, as these data are ultimately the result of such molecular interactions. However, all we have at this time is, at best, a fragmentary and only partially correct representation of the interactions between genes, their byproducts, and other cellular entities. If we want to succeed in our quest for understanding the biological whole as more than the sum of the individual parts, we need to build more comprehensive and cell-contextspecific maps of the biological interaction networks. DREAM, the Dialogue on Reverse Engineering Assessment and Methods, is fostering a concerted effort by computational and experimental biologists to understand the limitations and to enhance the strengths of the efforts to reverse engineer cellular networks from high-throughput data. In this chapterwe will discuss the salient arguments of the first DREAM conference. We will highlight both the state of the art in the field of reverse engineering as well as some of its challenges and opportunities",
    "keywords": [
      "reverse engineering",
      "pathway inference",
      "dream conference"
    ]
  },
  {
    "id": "1156",
    "title": "Design of a Java spatial extension for relational databases",
    "abstract": "Jaspa (Java Spatial) is a novel spatial extension for relational database management systems (RDBMSs). It is the result of a research project that aims to accomplish two goals: firstly, to fill the absence in the Free and Open Source Software (FOSS) world of a solid Java-based alternative to PostGIS, the leading spatial extension written in C. Secondly, to exploit the advantages of Java and the Java geospatial libraries over C in terms of portability and easiness to extend. Java programming for geospatial purposes is a flowering field and similar solutions to Jaspa have recently appeared, but none of them can equate with PostGIS due to lack of functionalities. Jaspa currently implements almost all PostGIS functionality. The next step will be the enrichment of the software with more sophisticated features: storage of spatial data in a topological structure within the RDBMS, cluster tolerance and geodetic functionalities. Jaspa is being developed at the Department of Cartographic Engineering, Geodesy and Photogrammetry of the Universidad Politcnica de Valencia and it has been published under an Open Source license on the OSOR.eu repository. This paper has been written by its creators with the aim of introducing users to its main capabilities.",
    "keywords": [
      "spatial database",
      "postgis",
      "foss",
      "java",
      "gis",
      "ogc"
    ]
  },
  {
    "id": "1157",
    "title": "Modelling and estimating heterogeneous variances in threshold models for ordinal discrete data via Winbugs/Openbugs",
    "abstract": "Analysis of discrete repeated outcomes is an important issue in biomedical studies. The aim of this paper is to propose a flexible and parsimonious model to account for heterogeneous variances for discrete outcomes. The proposed method is based on the use of a linear mixed model on the log of the standard deviation parameters. It is also shown how parameter estimation in this model can be performed with an exact procedure based on a Gibbs sampling algorithm implemented with the Winbugs/Openbugs software. A model comparison study is presented to illustrate the efficiency of this procedure using a well known example from the clinical trial literature. It was found that the proposed methodology considerably improved the predictive ability of the model while remaining very parsimonious. In particular, it was found that adding a random subject effect in the variance model significantly improved the posterior predictive p-value criterion of the model.",
    "keywords": [
      "ordered categorical data",
      "mixed models",
      "heteroscedasticity",
      "glmm",
      "winbugs",
      "openbugs"
    ]
  },
  {
    "id": "1158",
    "title": "Software engineering technology innovation - Turning research results into industrial success",
    "abstract": "This paper deals with the innovation of software engineering technologies. These technologies are methods and tools for conducting software development and maintenance. We consider innovation as a process consisting of two phases, being technology creation and technology transfer. In this paper, we focus mainly on the transfer phase. Technology transfer is of mutual interest to both academia and industry. Transfer is important for academia, because it shows the industrial relevance of their research results to, for example, their sponsoring authorities. Showing the industrial applicability of research results is sometimes referred to as valorization of research. Nowadays, valorization is often required by research funding bodies. The transfer is important for industries, because innovating their development processes and their products is supportive in gaining a competitive edge or remaining competitive in their business. We describe the technology transfer phase by means of three activities: technology evaluation, technology engineering, and technology embedding. The technology evaluation activity is perceived as the main gate between the technology creation phase and the technology transfer phase. With two case studies. originating from the Dutch high-tech systems industry, we will illustrate the activities in the transfer phase. In addition to the process we will also define the main roles in a software engineering technology innovation, namely: the technology provider (academic research, industrial research and technology vendor) and the technology receiver (industrial development). With those roles we also address the issues concerning the ownership of technologies.  ",
    "keywords": [
      "technology transfer",
      "process",
      "technology engineering",
      "technology embedding"
    ]
  },
  {
    "id": "1159",
    "title": "Recovering the missing components in a large noisy low-rank matrix: Application to SFM",
    "abstract": "In computer vision, it is common to require operations on matrices with \"missing data,\" for example, because of occlusion or tracking failures in the Structure from Motion (SFM) problem. Such a problem can be tackled, allowing the recovery of the missing values, if the matrix should be of low rank ( when noise free). The filling in of missing values is known as imputation. Imputation can also be applied in the various subspace techniques for face and shape classification, online \"recommender\" systems, and a wide variety of other applications. However, iterative imputation can lead to the \"recovery\" of data that is seriously in error. In this paper, we provide a method to recover the most reliable imputation, in terms of deciding when the inclusion of extra rows or columns, containing significant numbers of missing entries, is likely to lead to poor recovery of the missing parts. Although the proposed approach can be equally applied to a wide range of imputation methods, this paper addresses only the SFM problem. The performance of the proposed method is compared with Jacobs' and Shum's methods for SFM.",
    "keywords": [
      "imputation",
      "missing-data problem",
      "rank constraint",
      "singular value decomposition",
      "denoising capacity",
      "structure from motion",
      "affine sfm",
      "linear subspace"
    ]
  },
  {
    "id": "1160",
    "title": "DTPA: A reliable datagram transport protocol over ad hoc networks",
    "abstract": "As a prevalent reliable transport protocol in the Internet, TCP uses two key functions: the additive- increase multiplicative-decrease (AIMD) congestion control and a cumulative ACK technique for guaranteeing delivery. However, these two functions lead to the inefficiency of TCP in ad hoc networks where the TCP connections have very small bandwidth-delay products (BDPs) and there are frequent packet losses in the network due to various reasons such as route breakages and radio interference. In this paper, we show that if the BDP of a path is as low as several packets and is known before the connection establishment, any AIMD-style congestion control is costly and is hence not necessary for ad hoc networks. On the contrary, a technique for guaranteeing reliable transmission and recovering packet losses plays a more critical role in the design of a transport protocol over ad hoc networks. With this basis, we propose a novel effective datagram-oriented end-to-end reliable transport protocol for ad hoc networks, which we call Datagram Transport Protocol for Ad hoc networks (DTPA). The proposed scheme incorporates two techniques: a fixed-sizewindow-based flow-control algorithm and a cumulative bit-vector-based selective ACK strategy. We then develop a mathematical model for evaluating the performance of DTPA based on these two techniques. An optimum transmission window is determined for an n-hop chain and is computed to be the BDP of the path plus 3. The protocol is verified using GloMoSim, and simulation results show that our proposal substantially improves the network performance in terms of throughput, round-trip time, number of retransmissions, and IP queue size.",
    "keywords": [
      "tcp",
      "reliable transport protocol",
      "congestion control",
      "ad hoc networks",
      "ieee 802.11",
      "renewal process"
    ]
  },
  {
    "id": "1161",
    "title": "Weakly Supervised Photo Cropping",
    "abstract": "Photo cropping is widely used in the printing industry, photography, and cinematography. Conventional photo cropping methods suffer from three drawbacks: 1) the semantics used to describe photo aesthetics are determined by the experience of model designers and specific data sets, 2) image global configurations, an essential cue to capture photos aesthetics, are not well preserved in the cropped photo, and 3) multi-channel visual features from an image region contribute differently to human aesthetics, but state-of-the-art photo cropping methods cannot automatically weight them. Owing to the recent progress in image retrieval community, image-level semantics, i.e., photo labels obtained without much human supervision, can be efficiently and effectively acquired. Thus, we propose weakly supervised photo cropping, where a manifold embedding algorithm is developed to incorporate image-level semantics and image global configurations with graphlets, or, small-sized connected subgraph. After manifold embedding, a Bayesian Network (BN) is proposed. It incorporates the testing photo into the framework derived from the multi-channel post-embedding graphlets of the training data, the importance of which is determined automatically. Based on the BN, photo cropping can be casted as searching the candidate cropped photo that maximally preserves graphlets from the training photos, and the optimal cropping parameter is inferred by Gibbs sampling. Subjective evaluations demonstrate that: 1) our approach outperforms several representative photo cropping methods, including our previous cropping model that is guided by semantics-free graphlets, and 2) the visualized graphlets explicitly capture photo semantics and global spatial configurations.",
    "keywords": [
      "bayesian network",
      "image-level semantics",
      "photo cropping",
      "weakly supervised"
    ]
  },
  {
    "id": "1162",
    "title": "Fast and loose reasoning is morally correct",
    "abstract": "Functional programmers often reason about programs as if they were written in a total language, expecting the results to carry over to non-total (partial) languages. We justify such reasoning. Two languages are defined, one total and one partial, with identical syntax. The semantics of the partial language includes partial and infinite values, and all types are lifted, including the function spaces. A partial equivalence relation (PER) is then defined, the domain of which is the total subset of the partial language. For types not containing function spaces the PER relates equal values, and functions are related if they map related values to related values. It is proved that if two closed terms have the same semantics in the total language, then they have related semantics in the partial language. It is also shown that the PER gives rise to a bicartesian closed category which can be used to reason about values in the domain of the relation.",
    "keywords": [
      "languages",
      "theory",
      "verification",
      "equational reasoning",
      "partial and total languages",
      "non-strict and strict languages",
      "partial and infinite values",
      "lifted types",
      "inductive and coinductive types"
    ]
  },
  {
    "id": "1163",
    "title": "a heuristic routing mechanism using a new addressing scheme",
    "abstract": "Current methods of routing are based on network information in the form of routing tables, in which routing protocols determine how to update the tables according to the network changes. Despite the variability of data in routing tables, node addresses are constant. In this paper, we first introduce the new concept of variable addresses, which results in a novel framework to cope with routing problems using heuristic solutions. Then we propose a heuristic routing mechanism based on the application of genes for determination of network addresses in a variable address network and describe how this method flexibly solves different problems and induces new ideas in providing integral solutions for variety of problems. The case of ad-hoc networks is where simulation results are more supportive and original solutions have been proposed for issues like mobility.",
    "keywords": [
      "pricing",
      "mobile ad-hoc networks",
      "addressing scheme",
      "heuristic routing"
    ]
  },
  {
    "id": "1164",
    "title": "Deal of the Day-Plattformen: Was treibt die Kundenloyalitt",
    "abstract": "Die Beliebtheit von Deal of the Day (DoD)-Plattformen ist rasant angestiegen, da sie Einsparungen fr lokale Dienstleistungen, Produkte und Urlaube anbieten. Fr Hndler stellen diese Plattformen einen neuen Marketingkanal dar, um ihre Produkte und Services zu bewerben und neue Kunden zu gewinnen. Die Betreiber von DoD-Plattformen kmpfen jedoch um einen stabilen Marktanteil und Profitabilitt, da Eintritts- und Wechselkosten gering sind. Um eine wettbewerbsfhige Marktposition zu halten, suchen DoD-Betreiber daher nach Wegen, um einen loyalen Kundenstamm aufzubauen. Jedoch gibt es bisher kaum Forschung, welche die Determinanten von Kundenloyalitt in diesem neuen Kontext untersucht. Um diese Lcke zu fllen, nutzt diese Studie die Grounded-Theory-Methodologie, um ein konzeptionelles Modell der Kundenloyalitt fr DoD-Betreiber zu entwickeln. Im nchsten Schritt werden diese qualitativen Erkenntnisse erweitert und validiert unter Verwendung quantitativer Daten, die im Rahmen einer Umfrage unter 202 DoD-Nutzern erhoben wurden. Die Autoren haben herausgefunden, dass Kundenloyalitt zu einem groen Teil von monetren Anreizen beeinflusst wird, welche jedoch untergraben werden, wenn die Begegnung mit dem Hndler unter den Erwartungen bleibt. Zustzlich stellen die Erweiterung des Anteils relevanter Deals fr Kunden (d.h. das Signal-Rausch-Verhltnis) und die Verringerung des wahrgenommenen Risikos einer Transaktion eine Herausforderung dar. Neben ihrem theoretischen Wert bieten die Erkenntnisse praktische Einblicke darber, wie die Kundenloyalitt zu DoD-Betreibern verbessert werden kann.",
    "keywords": [
      "deal of the day",
      "loyalitt",
      "grounded theory",
      "strukturgleichungsmodell",
      "deal of the day",
      "loyalty",
      "grounded theory",
      "structural equation modeling"
    ]
  },
  {
    "id": "1165",
    "title": "Combination of multiple classifiers for the customer's purchase behavior prediction",
    "abstract": "In these days, EC companies are eager to learn about their customers using data mining technologies. But the diverse situations of such companies make it difficult to know which is the most effective algorithm for the given problems. Recently, a movement towards combining multiple classifiers has emerged to improve classification results. In this paper, we propose a method for the prediction of the EC customer's purchase behavior by combining multiple classifiers based on genetic algorithm. The method was tested and evaluated using Web data from a leading EC company. We also tested the validity of our approach in general classification problems using handwritten numerals. In both cases, our method shows better performance than individual classifiers and other known combining methods we tried.",
    "keywords": [
      "purchase behavior prediction",
      "multiple classifiers",
      "combination",
      "genetic algorithm"
    ]
  },
  {
    "id": "1166",
    "title": "LZ trie and dictionary compression",
    "abstract": "An efficient algorithm for trie compression has already been described. Here we present its practical value and demonstrate its superiority in terms of space savings to other methods of lexicon compression. Apart from simple lexicons, a compressed trie can, with some additional processing, be used as a component in the compact representation of simple static databases. We present the potential of the algorithm in compressing natural language dictionaries. ",
    "keywords": [
      "lz trie",
      "lexicon compression",
      "dictionary compression",
      "index compression",
      "static database compression"
    ]
  },
  {
    "id": "1167",
    "title": "A DSL for modeling application-specific functionalities of business applications",
    "abstract": "A DSL for specifying application-specific functionalities of business applications. Application-specific functionalities are specified at the PIM level. Complete program code is generated. We present the language editor.",
    "keywords": [
      "domain-specific languages",
      "iis?cfunclang",
      "application-specific functionalities",
      "model transformations",
      "iis?case"
    ]
  },
  {
    "id": "1168",
    "title": "Sourcing decisions with capacity reservation contracts",
    "abstract": "By committing to long-term supply contracts, buyers seek to lower their purchasing costs, and have products delivered without interruption. When a long-term contract is available, suppliers are less pressured to find new customers, and can afford to charge a price lower than the prevailing spot market price. We examine sourcing decisions of a firm in the presence of a capacity reservation contract that this firm makes with its long-term supplier in addition to the spot market alternative. This contract entails delivery of any desired portion of a reserved fixed capacity in exchange for a guaranteed payment by the buyer. We investigate rational actions of the two parties under two different types of periodic review inventory control policies used by the buyer: the two-number policy, and the base stock policy. When typical demand probability distributions are considered, inclusion of the spot market source in the buyers procurement plan significantly reduces the capacity commitments from the long-term supplier.",
    "keywords": [
      "supply chain management",
      "long-term contracts",
      "inventory",
      "capacity reservation"
    ]
  },
  {
    "id": "1169",
    "title": "Studies of structures elaborated by focused ion beam induced deposition",
    "abstract": "In this contribution, we present characterizations of tungsten wires and silicon oxide layers elaborated by focused ion beam induced deposition (FIBID). The deposits are performed in a cross-beam station equipped with a three channel gas injection system. Tungsten wires have been deposited from tungsten hexacarbonyl precursor. We have studied their electrical properties in situ by following the evolution of the wire resistance during the deposition process. These submicronic wires display an ohmic behaviour, a low resistivity (only 20 times higher than the bulk) and a good stability. As for insulator deposition, silicon oxide films from dissociation of penta methyl cyclo penta siloxane (PMCPS) precursor have been patterned. We present our results concerning electrical and chemical analyses carried out on test structure entirely elaborated by FIBID.",
    "keywords": [
      "fib induced deposition",
      "interconnections",
      "tungsten",
      "pmcps"
    ]
  },
  {
    "id": "1170",
    "title": "the effect of crossover on evolution ability of population",
    "abstract": "Currently most of the analysis about the running mechanism of GA focuses on the convergence problem while few focus on population characteristics after the single generation descendiblity. This paper presents the concept of evolution ability of population and discusses the ability of finding the optimal solution for the population after one-generation selection, crossover and mutation. Based on the analysis of effect of crossover on evolution ability of population, this paper presents some important conclusions. The important method to improve evolution ability of population is to include larger crossover optimal solution area in a smaller crossover family area. If the crossover optimal solution area isn't included in any crossover family area of population, the population either converges to the optimal solution, or evolution will be trapped in the premature of convergence. These conclusions above do not only help improve the GA, but also provide the basis for later research work.",
    "keywords": [
      "genetic algorithm",
      "crossover operators",
      "evolution ability of population"
    ]
  },
  {
    "id": "1171",
    "title": "Saliency detection based on singular value decomposition",
    "abstract": "Saliency detection based on the human-perception mechanism is proposed. The relationship of singular values and salient regions is investigated. Singular values are divided into large, intermediate, and small. Salient regions appear after regularizing the singular values.",
    "keywords": [
      "gaussian filter",
      "saliency detection",
      "singular value decomposition",
      "human-perception mechanism",
      "learning-based",
      "human-eye fixations",
      "salient-object detection",
      "visual attention"
    ]
  },
  {
    "id": "1172",
    "title": "Solution of unit commitment problem using quasi-oppositional teaching learning based algorithm",
    "abstract": "This paper presents QOTLBO based algorithm to solve UC problems. Presents quasi oppositional based learning strategy to improve TLBO algorithm. Valve-point effects, ramp rate, spinning reserve and up/down time constraints are considered. The robustness of the QOTLBO is demonstrated in five standard UC problems. Performance of QOTLBO is found to be better than other techniques.",
    "keywords": [
      "unit commitment",
      "generation scheduling",
      "spinning reserve",
      "ramp rate",
      "teaching learning based algorithm ",
      "quasi-oppositional based learning "
    ]
  },
  {
    "id": "1173",
    "title": "Fine grained load balancing in multi-hop wireless networks",
    "abstract": "In this paper we address the problem of local balancing in multi-hop wireless networks. We introduce the notion of proactive routing: after a short pre-processing phase in which nodes build their routing tables by exchanging messages with neighbors, we require that nodes decide the relay of each message without any further interaction with other nodes. Besides delivering very low communication overhead, proactive routing protocols are robust against some well known active attacks to network routing. In this framework, we develop a proactive routing protocol that is able to balance the local load. Experiments show that our protocol improves network lifetime up to 98% and that it delivers a network that is more robust against attacks that have the goal of getting control over a large part of the network traffic.",
    "keywords": [
      "multi-hop wireless networks",
      "sensor networks",
      "routing",
      "load balancing",
      "security",
      "energy efficient"
    ]
  },
  {
    "id": "1174",
    "title": "Mixed finite elements for electromagnetic analysis",
    "abstract": "The occurrence of spurious solutions is a well-known limitation of the standard nodal finite element method when applied to electromagnetic problems. The two commonly used remedies that are used to address this problem are (i) The addition of a penalty term with the penalty factor based on the local dielectric constant, and which reduces to a Helmholtz form on homogeneous domains (regularized formulation); (ii) A formulation based on a vector and a scalar potential. Both these strategies have some shortcomings. The penalty method does not completely get rid of the spurious modes, and both methods are incapable of predicting singular eigenvalues in non-convex domains. Some non-zero spurious eigenvalues are also predicted by these methods on non-convex domains. In this work, we develop mixed finite element formulations which predict the eigenfrequencies (including their multiplicities) accurately, even for nonconvex domains. The main feature of the proposed mixed finite element formulation is that no ad-hoc terms are added to the formulation as in the penalty formulation, and the improvement is achieved purely by an appropriate choice of finite element spaces for the different variables. We show that the formulation works even for inhomogeneous domains where double noding is used to enforce the appropriate continuity requirements at an interface. For two-dimensional problems, the shape of the domain can be arbitrary, while for the three-dimensional ones, with our current formulation, only regular domains (which can be nonconvex) can be modeled. Since eigenfrequencies are modeled accurately, these elements also yield accurate results for driven problems.",
    "keywords": [
      "maxwell equations",
      "nodal finite elements",
      "spurious modes",
      "mixed finite element formulation"
    ]
  },
  {
    "id": "1175",
    "title": "A Monte Carlo Enhanced PSO Algorithm for Optimal QoM in Multi-Channel Wireless Networks",
    "abstract": "In wireless monitoring networks, wireless sniffers are distributed in a region to monitor the activities of users. It can be used for fault diagnosis, resource management and critical path analysis. Due to hardware limitations, wireless sniffers typically can only collect information on one channel at a time. Therefore, it is a key topic to optimize the channel selection for sniffers to maximize the information collected, so as to maximize the quality of monitoring (QoM) of the network. In this paper, a particle swarm optimization (PSO)-based solution is proposed to achieve the optimal channel selection. A 2D mapping particle coding and its moving scheme are devised. Monte Carlo method is incorporated to revise the solution and significantly improve the convergence of the algorithm. The extensive simulations demonstrate that the Monte Carlo enhanced PSO (MC-PSO) algorithm outperforms the related algorithms evidently with higher monitoring quality, lower computation complexity, and faster convergence. The practical experiment also shows the feasibility of this algorithm.",
    "keywords": [
      "multi-channel wireless network",
      "channel selection",
      "quality of monitoring",
      "monte carlo",
      "particle swarm optimization"
    ]
  },
  {
    "id": "1176",
    "title": "EIBAS EIBAS : An efficient identity-based broadcast authentication scheme in wireless sensor networks",
    "abstract": "In this paper, we propose an efficient identity-based broadcast authentication scheme, EIBAS EIBAS , to achieve security requirements in wireless sensor networks. To minimize communication and computational costs, we use a pairing-optimal identity-based signature scheme with message recovery, where the original message of the signature is not required to be transmitted together with the signature, as it can be recovered according to the verification/message recovery process. The EIBAS EIBAS scheme achieves a minimization of communication overhead, allowing the total energy consumption to be reduced by up to 48.5% compared to previous identity-based broadcast authentication schemes.",
    "keywords": [
      "identity-based system",
      "digital signature with message recovery",
      "broadcast authentication",
      "message integrity",
      "bilinear pairing"
    ]
  },
  {
    "id": "1177",
    "title": "cad-based security, cryptography, and digital rights management",
    "abstract": "Manufacturing variability is inherent to many silicon and nano-scale technologies and can be manifested in many different ways and modalities (e.g. power and delay). We propose a flow that starts with gate-level integrated circuit (IC) characterization which results in unique identification (ID). The ID's are an integrated part of the design functionality and software and provide a basis for conceptually new CAD-based security protocols. As an examples, we present a new IC metering schemes that ensure very low overhead and digital right management in horizontally integrated IC market. Therefore, after many years of CAD importing and benefiting from many other areas such as numerical analysis, theoretical CS, VLSI design, computer architectures, and compilers, CAD has its historical chance to impact many fields of computer science and engineering through manufacturing variability-based security and right management.",
    "keywords": [
      "computer-aided design",
      "security",
      "intellectual property protection",
      "hardware metering",
      "digital rights management"
    ]
  },
  {
    "id": "1178",
    "title": "timing closure for low-fo4 microprocessor design",
    "abstract": "In this paper, we discuss timing closure for high performance microprocessor designs. Aggressive cycle time and deep sub-micron technology scaling introduce a myriad of problems that are not present in the ASIC domain. The impact of these problems on floorplanning, placement, clocking and logic synthesis is described. We present ideas and potential solutions for tackling these problems.",
    "keywords": [
      "fo4",
      "high performance",
      "placement",
      "synthesis"
    ]
  },
  {
    "id": "1179",
    "title": "delay-optimal technology mapping for fpgas with heterogeneous luts",
    "abstract": "Recent generation of FPGAs take advantage of speed and density benefits resulted from heterogeneous FPGAs, which provide either an array of homogeneous programmable logic blocks (PLBs), each configured to implement circuits with lookup tables (LUTs) of different sizes, or an array of physically heterogeneous LUTs. LUTs with different sizes usually have different delays. This paper presents the first polynomial-time optimal technology mapping algorithm, named HeteroMap, for delay minimization in heterogeneous FPGA designs. For a heterogeneous FPGA consisting of K 1 -LUTs, K 2 -LUTs, , and K c-LUTs, HeteroMap computes the minimum delay mapping solution in O (? c i =1 K i  n  m log n ) time for a circuit netlist with n gates and m edges. The HeteroMap algorithm generates favorable results for Xilinx XC4000 series FPGAs and Lucent ORCA2C series FPGAs. Furthermore, the optimality of the HeteroMap algorithm enables us to quantitatively evaluate various heterogeneous architectures without the bias of mapping heuristics.",
    "keywords": [
      "fpgas",
      "generation",
      "polynomial",
      "pla-style logic blocks",
      "optimality",
      "technology mapping",
      "architecture",
      "design",
      "programmable logic devices",
      "delay",
      "arrays",
      "programmable logic",
      "tables",
      "map",
      "timing",
      "heuristics",
      "bias",
      "algorithm",
      "paper",
      "heterogeneity",
      "evaluation",
      "minimal",
      "circuits"
    ]
  },
  {
    "id": "1180",
    "title": "adaptive shifting of auxiliary strategies over three formulations of multicast routing problem",
    "abstract": "Multicast routing algorithms have recently been intensively investigated due to the increment over the last years in the use of new point-to-multipoint applications. In this work, three formulations for the routing problem are investigated, considering 3, 4 and 5 objectives related to Quality of Service and Traffic Engineering requirements. A multiobjective evolutionary model is proposed to tackle this problem, using the well-known SPEA2 scheme as the underlying search. The key investigation performed here is about the incorporation of two strategies to help SPEA2 convergence to Pareto solutions, namely, filtering to reduce repeated individuals, and a mating selection based on neighborhood crossover. Results indicate that the adequacy of the strategies depends on the dynamics of currently non-dominated set over the generations. A new adaptive environment is proposed in which this information is considered periodically to decide what kind of strategy will be used in each situation.",
    "keywords": [
      "multicast routing",
      "neighborhood",
      "genetic algorithm"
    ]
  },
  {
    "id": "1181",
    "title": "Optimal instruction set design through adaptive database generation",
    "abstract": "This paper proposes a new method to design an optimal pipelined instruction set processor for ASIP development using a formal HW/SW codesign methodology. First, a HW/SW partitioning algorithm for selecting an optimal pipelined architecture is outlined. Then, an adaptive database approach is presented that enables to enhance the optimality of the design through very accurate estimation of the performance of a pipelined ASIP in the HW/SW partitioning process. The experimental results show that the proposed method is effective and efficient.",
    "keywords": [
      "asip",
      "hw/sw partitioning",
      "performance estimation",
      "adaptive database generation"
    ]
  },
  {
    "id": "1182",
    "title": "Evaluation of mechanical stresses in silicon substrates due to leadtin solder bumps via synchrotron X-ray topography and finite element modeling",
    "abstract": "Solder-based flip-chip packaging has prompted interest in integrated circuit (IC) packaging applications due to its many advantages in terms of cost, package size, electrical performance, input/output density, etc. The ball grid array (BGA) is one of the most common flip-chip packaging techniques used for microprocessor applications. However, mechanical stresses induced by the flip-chip process can impact adversely on the reliability of products. Synchrotron X-ray topography (SXRT), a non-destructive technique, has been employed to investigate the spatial extent of strain fields imposed on the underlying silicon substrate for IntelPentiumIII microprocessors due to the leadtin solder bump process for BGA packaging. Large area and section back-reflection SXRT images were taken before and after a simulation of the reflow process at 350 C in atmosphere. The presence of induced strain fields in the Si substrate due to the overlying bump structures has been observed via the extinction contrast effect in these X-ray topographs. In addition, orientational contrast effects have also been found after the reflow process due to the severe stresses in the underlying silicon beneath the lead bumps. The estimated magnitudes of stress,, imposed on the underlying silicon were calculated to be of the order of 100 MPa. The spatial strains in the underlying silicon were relieved dramatically after the lead bumps were removed from the wafer, which confirms that the bumps are indeed a major source of strain in the underlying Si. Finite element modeling (FEM) has also been performed in two-dimensional (2-D) plane strain mode. The magnitudes and spatial distribution of the stresses after the reflow process are in good agreement with the SXRT results.",
    "keywords": [
      "x-ray topography",
      "flip-chip",
      "finite element modeling",
      "ball grid array",
      "stress"
    ]
  },
  {
    "id": "1183",
    "title": "A note on Taylor series approach to fuzzy multiple objective linear fractional programming",
    "abstract": "Toksari, in his paper Taylor series approach to fuzzy multiobjective linear fractional programming, published in Information Sciences 178 (2008), proposed a new method to solve multiple objective linear fractional programming problems and declared that it is more effective when compare to previous methods. He constructed fuzzy goals and then used first order Taylor-polynomials to approximate the corresponding linear fractional membership functions by linear functions. Aggregating the linear functions Toksari obtained a crisp linear programming problem and claimed that it is equal to the fuzzy fractional one. In this paper, we indicate the fallacy that arises by using Taylor approximation and propose an improved method that guarantees the efficiency of the solution it provides. Both practical applications from Toksaris paper are recalled to show that the improvements we suggest are effective.",
    "keywords": [
      "fuzzy mathematical programming",
      "linear fractional programming",
      "multi-objective optimization"
    ]
  },
  {
    "id": "1184",
    "title": "Fractal descriptors based on the probability dimension: A texture analysis and classification approach",
    "abstract": "A texture descriptor is proposed based on the fractal descriptors concept. Fractal dimension estimation is performed by the probability (Voss) method. The descriptor are obtained applying a multi-scale transform into the fractal curve. The method is compared and outperforms fractal and non-fractal texture stat-of-art methods.",
    "keywords": [
      "pattern recognition",
      "fractal dimension",
      "fractal descriptors",
      "probability dimension",
      "texture analysis"
    ]
  },
  {
    "id": "1185",
    "title": "Pansystems-based fuzzy systems relations and clustering",
    "abstract": "Purpose - Melting universality, quantification and relative computability into a meta-synthesis, pansystems theory develops ail investigation on W-fuzziness and 0*-fuzziness connected with generalized conceptions such as derivative, equation, variational principle and OR. The purpose of this paper is to unify various mathematical structures, fuzziness categories, definitions of systems are unified within a general framework. Design/methodology/approach - The paper includes topics: pansystems approach to fuzzy systems and relations, pansystems variational principle and Zadeh's extension principle, pansystems clustering and its fuzzy embodiment, pansystems topology and approximation to fuzziness, relative unification of fuzziness and roughness. Findings - Zadeh extension principle about fuzziness transmission can be considered as a specific model of pansystems extremum principle, and so the more modes can be developed. Based on them a further investigation is present on pansystems clustering, which is a W-fuzzy clustering, an extension or sublation of traditional one and fuzzy one. Originality/value - Pansystems clustering embodies mutuality of many logoi of different subbraches with classification-styled OR, including related interpromotions of the principles among knowledge rediscovery, data mining, mathematical reasoning and the investigations of fuzzy systems. W-fuzziness and 0*-fuzziness realize a relative unification for many logoi and principles.",
    "keywords": [
      "fuzzy logic",
      "approximation theory",
      "cybernetics",
      "cluster analysis",
      "topology",
      "systems theory"
    ]
  },
  {
    "id": "1186",
    "title": "hand motion gestural oscillations and multimodal discourse",
    "abstract": "To develop multimodal interfaces, one needs to understand the constraints underlying human communicative gesticulation and the kinds of features one may compute based on these underlying human characteristics.In this paper we address hand motion oscillatory gesture detection in natural speech and conversation. First, the hand motion trajectory signals are extracted from video. Second, a wavelet analysis based approach is presented to process the signals. In this approach, wavelet ridges are extracted from the responses of wavelet analysis for the hand motion trajectory signals, which can be used to characterize frequency properties of the hand motion signals. The hand motion oscillatory gestures can be extracted from these frequency properties. Finally, we relate the hand motion oscillatory gestures to the phases of speech and multimodal discourse analysis.We demonstrate the efficacy of the system on a real discourse dataset in which a subject described her action plan to an interlocutor. We extracted the oscillatory gestures from the x , y and z motion traces of both hands. We further demonstrate the power of gestural oscillation detection as a key to unlock the structure of the underlying discourse.",
    "keywords": [
      "speech analysis",
      "multimodal",
      "hand gesture",
      "multimodal discourse structure",
      "hand motion trajectory",
      "interaction",
      "gesture symmetry"
    ]
  },
  {
    "id": "1187",
    "title": "The relationship between quality management practices and their effects on quality outcomes",
    "abstract": "Recent research into quality management has examined the relationship between quality management and performance. The purpose of this study is to identify the relationships between quality management practices, and to examine the direct and indirect effects of these practices on quality outcomes by means of replication research. The paper uses a path analysis and a research model is tested using cross-section data collected from 106 certified firms in Spain. The findings support the relationships between quality management practices and the positive impact of these practices on quality outcomes. Evidence is also found confirming previous research showing that a firm could transfer the organizational forms and behaviours underlying quality management to other countries with similar cultures. However, as minor differences emerge, managers should consider the cultural issues. The contribution of the paper is that it provides empirical support for direct and indirect effects of quality management on performance in Spain compared to previous studies carried out in USA and Korea.",
    "keywords": [
      "quality management",
      "continuous quality improvement",
      "iso 9000",
      "path analysis",
      "spain"
    ]
  },
  {
    "id": "1188",
    "title": "An experience report: porting the MG-RAST rapid metagenomics analysis pipeline to the cloud",
    "abstract": "Existing applications in computational biology typically favor a local cluster based integrated computational platform. We present a lessons learned type report for scaling up an existing metagenomics application that outgrew the available local cluster hardware. In our example, removing a number of assumptions linked to tight integration allowed to expand beyond one administrative domain, increase the number and type of machines available for the application, and also improved scaling properties of the application. The assumptions made in designing the computational client make it well suitable for deployment as a virtual machine inside a cloud. This paper discusses the decision process and describes the suitability of deploying various bioinformatics computations to distributed heterogeneous machines. ",
    "keywords": [
      "cloud computing",
      "bioinformatics",
      "metagenomics",
      "genomics"
    ]
  },
  {
    "id": "1189",
    "title": "A high-precision unstructured adaptive mesh technique for gas-liquid two-phase flows",
    "abstract": "Adaptive mesh techniques are used widely in the numerical simulations of fluid flows, and the simulation results with high accuracies are obtained by appropriate mesh adaptations. However, gasliquid two-phase flows are still difficult to be simulated on adaptive meshes, especially on unstructured adaptive meshes, because the physical phenomena near gasliquid interfaces are highly complicated and in general, not modeled appropriately on adaptive meshes. In this paper, a high-precision unstructured adaptive mesh technique for gasliquid two-phase flows is developed and verified/validated. In the unstructured adaptive mesh technique, the PLIC algorithm is employed to simulate interfacial dynamic behaviors and, therefore, the reconstruction method for the interfaces in refined cells is developed, which satisfies the gas and liquid volume conservations and geometrical conservations of interfaces. In addition, the physics-based consideration is performed on the momentum calculations near interfaces, and the calculation method with gas and liquid momentum conservations is developed. For verification, the slotted-disk revolution problem is solved. As a result, the unstructured adaptive mesh technique succeeds in reproducing the slotted-disk shape accurately and well maintaining the shape after one full-revolution. The dam-break problem is also simulated and the momentum conservative calculation method succeeds in providing physically appropriate results, which show good agreements with experimental data. Therefore, it is confirmed that the developed unstructured adaptive mesh technique is very efficient to simulate gasliquid two-phase flows accurately. ",
    "keywords": [
      "numerical simulation",
      "gas-liquid two-phase flow",
      "volume-of-fluid",
      "plic",
      "unstructured adaptive mesh"
    ]
  },
  {
    "id": "1190",
    "title": "A reduced variable neighborhood search algorithm for uncapacitated multilevel lot-sizing problems",
    "abstract": "Multilevel lot-sizing (MLLS) problems, which involve complicated product structures with interdependence among the items, play an important role in the material requirement planning (MRP) system of modern manufacturing/assembling lines. In this paper, we present a reduced variable neighborhood search (RVNS) algorithm and several implemental techniques for solving uncapacitated MLLS problems. Computational experiments are carried out on three classes of benchmark instances under different scales (small, medium, and large). Compared with the existing literature, RVNS shows good performance and robustness on a total of 176 tested instances. For the 96 small-sized instances, the RVNS algorithm can find 100% of the optimal solutions in less computational time; for the 40 medium-sized and the 40 large-sized instances, the RVNS algorithm is competitive against other methods, enjoying good effectiveness as well as high computational efficiency. In the calculations, RVNS updated 7 (17.5%) best known solutions for the medium-sized instances and 16 (40%) best known solutions for the large-sized instances.",
    "keywords": [
      "meta-heuristics",
      "uncapacitated multilevel lot-sizing  problem",
      "material requirement planning ",
      "reduced variable neighborhood search  algorithm",
      "production planning"
    ]
  },
  {
    "id": "1191",
    "title": "An outer bound to the capacity region of the broadcast channel",
    "abstract": "An outer bound to the capacity region of the two-receiver discrete memoryless broadcast channel is given. The outer bound is tight for all cases where the capacity region is known. When specialized to the case of no common information, this outer bound is contained in the Korner-Marton outer bound. This containment is shown to be strict for the binary skew-symmetric broadcast channel. Thus, this outer bound is in general tighter than all other known outer bounds.",
    "keywords": [
      "broadcast channel",
      "capacity",
      "outer bound"
    ]
  },
  {
    "id": "1192",
    "title": "task-first or context-first? tool integration revisited",
    "abstract": "If software engineering tools are not \"properly integrated\", they can reduce engineers' productivity. Associating and retrieving information scattered across the tools become unsystematic and inefficient. Our work provides empirical evidence on what is a \"poor\" and a \"proper\" tool integration, focusing on practitioners' perspectives. We interviewed 62 engineers and analyzed the content of their project artifacts. We identified problem situations and practices related to tool integration. Engineers agreed that tool integration approaches must support change, heterogeneity and automatic linking of change to context. To quantify our results, we conducted a field experiment with 27 and a survey with 782 subjects. We found a strong correlation between change frequency and preferred integration approaches. Particularly in projects with short release cycles, tasks should be used to link information handled by different tools. We also found that half of engineers' work is not defined as tasks. Therefore, a context-based tool integration approach is more effective than a task-based one.",
    "keywords": [
      "task management",
      "emperical research",
      "information need",
      "context awareness",
      "tool integration"
    ]
  },
  {
    "id": "1193",
    "title": "An improved greedy search algorithm for the development of a phonetically rich speech corpus",
    "abstract": "An efficient way to develop large scale speech corpora is to collect phonetically rich ones that have high coverage of phonetic contextual units. The sentence set, usually called as the minimum set, should have small text size in order to reduce the collection cost. It can be selected by a greedy search algorithm from a large mother text corpus. With the inclusion of more and more phonetic contextual effects, the number of different phonetic contextual units increased dramatically, making the search not a trivial issue. In order to improve the search efficiency, we previously proposed a so-called least-to-most-ordered greedy search based on the conventional algorithms. This paper evaluated these algorithms in order to show their different characteristics. The experimental results showed that the least-to-most-ordered methods successfully achieved smaller objective sets at significantly less computation time, when compared with the conventional ones. This algorithm has already been applied to the development a number of speech corpora, including a large scale phonetically rich Chinese speech corpus ATRPTH which played an important role in developing our multi-language translation system.",
    "keywords": [
      "greedy search",
      "minimum sentence set",
      "speech recognition",
      "speech corpus"
    ]
  },
  {
    "id": "1194",
    "title": "Cycle Double Covers in Cubic Graphs having Special Structures",
    "abstract": "In the first part of this article, we employ Thomason's Lollipop Lemma [25] to prove that bridgeless cubic graphs containing a spanning lollipop admit a cycle double cover (CDC) containing the circuit in the lollipop; this implies, in particular, that bridgeless cubic graphs with a 2-factor F having two components admit CDCs containing any of the components in the 2-factor, although it need not have a CDC containing all of F. As another example consider a cubic bridgeless graph containing a 2-factor with three components, all induced circuits. In this case, two of the components may separately be used to start a CDC although it is uncertain whether the third component may be part of some CDC. Numerous other corollaries shall be given as well. In the second part of the article, we consider special types of bridgeless cubic graphs for which a prominent circuit can be shown to be included in a CDC. The interest here is the proof technique and therefore we only give the simplest case of the theorem. Notably, we show that a cubic graph that consists of an induced 2k-circuit C together with an induced 4k-circuit T and an independent set of 2k vertices, each joined by one edge to C and two edges to T, has a CDC starting with T.",
    "keywords": [
      "cdc",
      "lollipop"
    ]
  },
  {
    "id": "1195",
    "title": "Wave propagation in functionally graded and layered materials",
    "abstract": "In this study, wave propagation in functionally graded materials (FGMs) and layered materials is analyzed using spacetime discontinuous Galerkin method introduced earlier by the authors [H.G. Aksoy, E. Senocak, Spacetime discontinuous Galerkin method for dynamics of solids, Communications in Numerical Methods in Engineering 24 (2008) 18871907]. The numerical performance of the method is presented via two dimensional example problems of wave propagations. The effect of loading (quasi-static and dynamic) on the analysis of displacements and stresses is demonstrated. The results show that spacetime discontinuous Galerkin method is successful in modelling wave propagation in FGMs and layered materials.",
    "keywords": [
      "wave propagation",
      "fgm",
      "layered media",
      "discontinuous galerkin method"
    ]
  },
  {
    "id": "1196",
    "title": "Directed self-assembly of block copolymers for nanocircuitry fabrication",
    "abstract": "A perspective on the possible insertion of block copolymer lithography into integrated circuit manufacture is provided. Review of the fundamentals of block copolymer self-assembly and substrate interactions. Overview of progress in defining well-defined structural arrangements provided. Key challenges for integration of block copolymer technology are assessed.",
    "keywords": [
      "block copolymer lithography",
      "chemical epitaxy",
      "physical epitaxy",
      "pattern transfer",
      "process flows",
      "defectivity"
    ]
  },
  {
    "id": "1197",
    "title": "how do robotic agents' appearances affect people's interpretations of the agents' attitudes",
    "abstract": "An experimental investigation of how the appearance of robotic agents affects interpretations people make of the agents.' attitudes is described. We conducted a psychological experiment where participants were presented artificial sounds that can make people estimate specific agents' primitive attitudes from three kinds of agents, e.g., a Mindstorms robot, AIBO robot, and normal laptop PC. They were also asked to select the correct attitudes based on the sounds expressed by these three agents. The results showed that the participants had higher interpretation rates when a PC presented the sounds, while they had lower rates when Mindstorms and AIBO robots presented the sounds, even though the artificial sounds expressed by these agents were completely the same.",
    "keywords": [
      "subtle expressions",
      "appearance of agents",
      "agents' attitudes",
      "human-agent interaction"
    ]
  },
  {
    "id": "1198",
    "title": "A high order ADI method for separable generalized Helmholtz equations",
    "abstract": "We present a multilevel high order ADI method for separable generalized Helmholtz equations. The discretization method we use is a one-dimensional fourth order compact finite difference applied to each directional component of the Laplace operator, resulting in a discrete system efficiently solvable by ADI methods. We apply this high order difference scheme to all levels of grids, and then starting from the coarsest grid, solve the discretized equation with an ADI method at each grid level, with the solution from the previous grid level as the initial guess. The multilevel procedure stops as the ADI finishes its iterations on the finest grid. Analytical and experimental results show that the proposed method is highly accurate and efficient while remaining as algorithmically and data-structurally simple as the single grid ADI method.",
    "keywords": [
      "generalized helmholtz equation",
      "adi method",
      "multilevel method",
      "high order discretization"
    ]
  },
  {
    "id": "1199",
    "title": "associating assertions with business processes and monitoring their execution",
    "abstract": "Business processes that span organizational borders describe the interaction between multiple parties working towards a common objective. They also express business rules that govern the behavior of the process and account for expressing changes reflecting new business objectives and new market situations. In our previous work we developed a service request language and support framework that allow users to formulate their requests against standard business processes. In this paper we extend this approach by presenting a framework capable of automatically associating business rules with relevant processes involved in a user request. This framework plans and monitors the execution of the request against services underlying these processes. Definitions and classifications of business rules (named assertions in the paper) are given together with an assertion language for expressing th. The framework is able to handle the non-determinism typical for service-oriented computing environments and it is based on the interleaving of planning and execution.",
    "keywords": [
      "service delivery",
      "service and ai computing",
      "quality",
      "management",
      "monitoring"
    ]
  },
  {
    "id": "1200",
    "title": "Efficient reconfiguration algorithms for communication-aware three-dimensional processor arrays",
    "abstract": "Homogeneous processor arrays are emerging in tera-scale computation and effective fault tolerance techniques are essential to improving the reliability of such complex integrated circuits. We study the degradable processor arrays to achieve fault tolerance by employing reconfiguration. Three bypass schemes and three rerouting schemes are proposed to reconfigure three-dimensional processor arrays with defective processors to achieve target arrays without faults. A heuristic algorithm is proposed to construct a target array on the selected rows and columns. It is also proved that the proposed greedy plane rerouting algorithm (GPR) produces maximum target array. In addition, the problem of constructing the communication efficient array is considered in this paper. An algorithm is proposed to refine the communication among processors within the target array constructed by GPR. Experimental study shows that the proposed algorithm GPR produces target arrays with higher harvest and lower degradation on the host arrays with fault density no more than 5%. In addition, the communication performance is significantly optimized by reducing the number of long interconnects, and the average improvement is about 34% for all cases considered in this paper.  ",
    "keywords": [
      "3d processor array",
      "reconfiguration",
      "fault tolerance",
      "communication aware",
      "interconnection networks",
      "algorithm"
    ]
  },
  {
    "id": "1201",
    "title": "combining group compromised price with bundle search strategies",
    "abstract": "This paper proposes combining the buyer coalition strategy, called Group Compromised Price, with bundle search strategy to obtain the greater discounts from forming a coalition that gives the number of buyers as large as possible.",
    "keywords": [
      "group buying",
      "bundle search",
      "e-commerce",
      "coalition formation"
    ]
  },
  {
    "id": "1202",
    "title": "A systematic review of code generation proposals from state machine specifications",
    "abstract": "Model Driven Development (MDD) encourages the use of models for developing complex software systems. Following a MDD approach, modelling languages are used to diagrammatically model the structure and behaviour of object-oriented software, among which state-based languages (including UML state machines, finite state machines and Harel statecharts) constitute the most widely used to specify the dynamic behaviour of a system. However, generating code from state machine models as part of the final system constitutes one of the most challenging tasks due to its dynamic nature and because many state machine concepts are not supported by the object-oriented programming languages. Therefore, it is not surprising that such code generation has received great attention over the years. The overall objective of this paper is to plot the landscape of published proposals in the field of object oriented code generation from state machine specifications, restricting the search neither to a specific context nor to a particular programming language. We perform a systematic, accurate literature review of published studies focusing on the object oriented implementation of state machine specifications. The systematic review is based on a comprehensive set of 53 resources in all, which we have classified into two groups: pattern-based and not pattern-based. For each proposal, we have analysed both the state machine specification elements they support and the means the authors propose for their implementation. Additionally, the review investigates which proposals take into account desirable features to be considered in software development such as maintenance or reusability. One of the conclusions drawn from the review is that most of the analysed works are based on a software design pattern. Another key finding is that many papers neither support several of the main components of the expressive richness of state machine specifications nor provide an implementation strategy that considers relevant qualitative aspects in software development.",
    "keywords": [
      "uml state machines",
      "finite state machines",
      "statecharts",
      "code generation",
      "systematic review"
    ]
  },
  {
    "id": "1203",
    "title": "a thin client for networked access to a central register and electronic voting terminal",
    "abstract": "Networked terminals for marking the electoral register at poll places has been trialed at a number of sites (most recently [5]) allowing immediate detection of attempted multiple voting even in truly anonymous voting systems. We describe new technology piloted in 2007 in a binding local government election. Our commercial remote voting product eLect [1] was extended to provide new services. Firstly, networked register terminals replaced paper registers. Secondly, the networked register formed the basis of enabling single-vote access for poll-place electronic voting. Finally, poll place electronic voting machines were provided as stateless thin clients who were networked for real-time central aggregation of votes. A central server was charged with the coordination of 400 such voting and register machines at 64 sites. Register terminals also formed the basis of recording the issue of paper votes if the voter so chose, as these were provided to allow voting in the traditional manner. In concert with these systems, votes were also collected via telephone and web-based remote electronic voting services. The pilot has been judged a success by the central government.",
    "keywords": [
      "systems pilots",
      "multi-modal voting",
      "multi-channel voting",
      "elections",
      "internet voting",
      "voter rolls",
      "voter registers",
      "electronic voting"
    ]
  },
  {
    "id": "1204",
    "title": "optimization of high-performance superscalar architectures for energy efficiency",
    "abstract": "In recent years reducing power has become a critical design goal for high-performance microprocessors. This work attempts to bring the power issue to the earliest phase of high-performance microprocessor development. We propose a methodology for power-optimization at the micro-architectural level. First, major targets for power reduction are identified within superscalar microarchitecture, then an optimization of a superscalar micro-architecture is performed that generates a set of energy-efficient configurations forming a convex hull in the power-performance space. The energy-efficient families are then compared to find configurations that dissipate the lowest power given a performance target, or, conversely, deliver the highest performance given a power budget. Application of the developed methodology to a superscalar micro-architecture shows that at the architectural level there is a potential for reducing power up to 50%, given a performance requirement, and for up to 15% performance improvement, given a power budget.",
    "keywords": [
      "requirements",
      "applications",
      "efficiency",
      "methodology",
      "optimality",
      "microprocessor",
      "space",
      "convex hull",
      "developer",
      "architecture",
      "performance",
      "high-performance",
      "design",
      "reduction",
      "configurability",
      "budget",
      "power",
      "microarchitecture",
      "critic",
      "energy efficiency",
      "power optimization"
    ]
  },
  {
    "id": "1205",
    "title": "context modeling for ranking and tagging bursty features in text streams",
    "abstract": "Bursty features in text streams are very useful in many text mining applications. Most existing studies detect bursty features based purely on term frequency changes without taking into account the semantic contexts of terms, and as a result the detected bursty features may not always be interesting or easy to interpret. In this paper we propose to model the contexts of bursty features using a language modeling approach. We then propose a novel topic diversity-based metric using the context models to find newsworthy bursty features. We also propose to use the context models to automatically assign meaningful tags to bursty features. Using a large corpus of a stream of news articles, we quantitatively show that the proposed context language models for bursty features can effectively help rank bursty features based on their newsworthiness and to assign meaningful tags to annotate bursty features.",
    "keywords": [
      "bursty feature tagging",
      "context modeling",
      "bursty features",
      "bursty features ranking"
    ]
  },
  {
    "id": "1206",
    "title": "comparative evaluation of visualization and experimental results using image comparison metrics",
    "abstract": "Comparative evaluation of visualization and experiment results is a critical step in computational steering. In this paper, we present a study of image comparison metrics for quantifying the magnitude of difference between a visualization of a computer simulation and a photographic image captured from an experiment. We examined eleven metrics, including three spatial domain, four spatial-frequency domain and four HVS (human-vision system) metrics. Among these metrics, a spatial-frequency domain metric called 2nd-order Fourier comparison was proposed specifically for this work. Our study consisted of two stages: base cases and field trials. The former is a general study on a controlled comparison space using purposely selected data, and the latter involves imagery results from computational fluid dynamics and a rheological experiment. This study has introduced a methodological framework for analyzing image-level methods used in comparative visualization. For the eleven metrics considered, it has offered a set of informative indicators as to the strengths and weaknesses of each metric. In particular, we have identified three image comparison metrics that are effective in separating \"similar\" and \"different\" image groups. Our 2nd-order Fourier comparison metric has compared favorably with others in two of the three tests, and has shown its potential to be used for steering computer simulation quantitatively.",
    "keywords": [
      "rheology",
      "error metrics",
      "human vision systems",
      "image comparison",
      "comparative visualization",
      "scientific visualization"
    ]
  },
  {
    "id": "1207",
    "title": "An empirical study of object-oriented system evolution",
    "abstract": "Software metrics have been used to measure software artifacts staticallymeasurements are taken after the artifacts are created. In this study, three metricsSystem Design Instability (SDI), Class Implementation Instability (CII), and System Implementation Instability (SII)are used for the purpose of measuring object-oriented (OO) software evolution. The metrics are used to track the evolution of an OO system in an empirical study. We found that once an OO project starts, the metrics can give good indications of project progress, e.g. how mature the design and implementation is. This information can be used to adjust the project plan in real time. We also performed a study of design instability that examines how the implementation of a class can affect its design. This study determines that some aspects of OO design are independent of implementation, while other aspects are dependent on implementation.",
    "keywords": [
      "object-oriented metrics",
      "object-oriented design evolution",
      "object-oriented implementation evolution"
    ]
  },
  {
    "id": "1208",
    "title": "Analysis of a delayed predator-prey model with ratio-dependent functional response and quadratic harvesting",
    "abstract": "In this paper, we investigate the dynamics of a ratio dependent predator-prey model with quadratic harvesting. We examine the existence of the positive equilibria, the related dynamical behaviors of the model, as well as the boundedness and permanence property of the system. We also study the global stability of the interior equilibrium without time delay. Finally some bifurcation analysis is carried out for the system with delay and the results are illustrated numerically.",
    "keywords": [
      "time delay",
      "ratio dependent",
      "permanence",
      "hopf bifurcation",
      ""
    ]
  },
  {
    "id": "1209",
    "title": "Consumer value of camera-based mobile interaction with the real world",
    "abstract": "Camera-based mobile interaction with the real world allows consumers to connect digital information with the real-world environment, and furthermore, to interact with real-world objects and places. To explore and understand the types of consumer value in the context of such consumer-level applications, we applied the critical incident technique to reflect actual use experiences from 107 application users with a recognized consumer value framework by Holbrook. The findings of the study suggest that at the current state value is heavily based on utilitarian efficiency and excellence. Although the applications enable a diverse value, they have yet to fulfill their potential in providing hedonic and other-oriented value.",
    "keywords": [
      "consumer value",
      "utilitarian",
      "hedonic",
      "mobile applications"
    ]
  },
  {
    "id": "1210",
    "title": "Integrating exploration, localization, navigation and planning with a common representation",
    "abstract": "Two major themes of our research include the creation of mobile robot systems that are robust and adaptive in rapidly changing environments, and the view of integration as a basic research issue. Where reasonable, we try to use the same representations to allow different components to work more readily together and to allow better and more natural integration of and communication between these components. In this paper, we describe our most recent work in integrating mobile robot exploration, localization, navigation, and planning through the use of a common representation, evidence grids.",
    "keywords": [
      "mobile robots",
      "localization",
      "planning",
      "navigation",
      "exploration",
      "evidence grids",
      "integration"
    ]
  },
  {
    "id": "1211",
    "title": "Exact value of ex(n;{C3,,Cs}) e x ( n ; { C 3 ,  , C s } ) for n25 ( s ? 1 ) 8",
    "abstract": "For integers s?8 s ? 8 and s + 1 ? n25 ( s ? 1 ) 8 ? , we determine the exact value of the function ex(n;{C3,,Cs}) e x ( n ; { C 3 ,  , C s } ) , that represents the maximum number of edges in a {C3,,Cs} { C 3 ,  , C s } -free graph of order n . This result was already known when 3?s?7 3 ? s ? 7 . To do that, for 1?k?5 1 ? k ? 5 , we provide a family of graphs H s k such that e ( H s k ) ? n ( H s k ) = k and with the property that H s k reaches girth s+1 s + 1 with the minimum number of vertices. Also, we determine an infinity family of solutions of the problem ex(n;{C3,,Cs})=n+6 e x ( n ; { C 3 ,  , C s } ) = n + 6 .",
    "keywords": [
      "extremal function",
      "extremal graphs",
      "forbidden cycles",
      "girth"
    ]
  },
  {
    "id": "1212",
    "title": "Automated detection of unusual events on stairs",
    "abstract": "This paper presents a method for automatically detecting unusual human events on stairs from video data. The motivation is to provide a tool for biomedical researchers to rapidly find the events of interest within large quantities of video data. Our system identifies potential sequences containing anomalies, and reduces the amount of data that needs to be searched by a human. We compute two sets of features from a video of a person descending a stairwell. The first set of features are the foot positions and velocities. We track both feet using a mixed state particle filter with an appearance model based on histograms of oriented gradients. We compute expected (most likely) foot positions given the state of the filter at each frame. The second set of features are the parameters of the mean optical flow over a foreground region. Our final classification system inputs these two sets of features into a hidden Markov model (HMM) to analyse the spatio-temporal progression of the stair descent. A single HMM is trained on sequences of normal stair use, and a threshold on sequence likelihoods is used to detect unusual events in new data. We demonstrate our system on a data set with five people descending a set of stairs in a laboratory environment. We show how our system can successfully detect nearly all anomalous events, with a low false positive rate. We discuss limitations and suggest improvements to the system.",
    "keywords": [
      "human motion analysis",
      "hidden markov model",
      "gait analysis",
      "anomaly detection",
      "event recognition",
      "bayesian tracking",
      "particle filter",
      "automated video analysis",
      "machine learning",
      "biomedical analysis",
      "stairs"
    ]
  },
  {
    "id": "1213",
    "title": "Novel framework for registration of pedobarographic image data",
    "abstract": "This article presents a framework to register (or align) plantar pressure images based on a hybrid registration approach, which first establishes an initial registration that is subsequently improved by the optimization of a selected image (dis)similarity measure. The initial registration has two different solutions: one based on image contour matching and the other on image cross-correlation. In the final registration, a multidimensional optimization algorithm is applied to one of the following (dis)similarity measures: the mean squared error (MSE), the mutual information, and the exclusive or (XOR). The framework has been applied to intra- and inter-subject registration. In the former, the framework has proven to be extremely accurate and fast (<70ms on a normal PC notebook), and obtained superior XOR and identical MSE values compared to the best values reported in previous studies. Regarding the inter-subject registration, by using rigid, similarity, affine, projective, and polynomial (up to the fourth degree) transformations, the framework significantly optimized the image (dis)similarity measures. Thus, it is considered to be very accurate, fast, and robust in terms of noise, as well as being extremely versatile, all of which are regarded as essential features for near-real-time applications.",
    "keywords": [
      "biomechanics",
      "plantar pressure data",
      "image registration",
      "optimization"
    ]
  },
  {
    "id": "1214",
    "title": "concepts for usable patterns of groupware applications",
    "abstract": "Patterns, which are based on in-depth practical experience, can be instructing for the design of groupware applications as socio-technical systems. On the basis of a summary of the concept of patterns - as elaborated by the architect Christopher Alexander - its adoptions within computer science are retraced and relationships to the area of groupware are described. General principles for patterns within this domain are formulated and supported by examples from a wide range of experience with knowledge management systems. The analysis reveals that every pattern of a groupware application has to combine the description of social as well as technical structures, and that a single pattern can only be understood in the context of a pattern language. It also shows that such a language has to integrate patterns of socio-technical solutions with measures and procedures for introducing them, and that the language not only has to express one type of directed relationship between the patterns but a variety of different types which have to be deliberately assigned to the patterns.",
    "keywords": [
      "pattern",
      "pattern language",
      "socio-technical systems",
      "cscw",
      "knowledge management"
    ]
  },
  {
    "id": "1215",
    "title": "A direct-forcing immersed boundary method for the thermal lattice Boltzmann method",
    "abstract": "In this study, a direct-forcing immersed boundary method (IBM) for thermal lattice Boltzmann method (TLBM) is proposed to simulate the non-isothermal flows. The direct-forcing IBM formulas for thermal equations are derived based on two TLBM models: a double-population model with a simplified thermal lattice Boltzmann equation (Model 1) and a hybrid model with an advection-diffusion equation of temperature (Model 2). As an interface scheme, which is required due to a mismatch between boundary and computational grids in the IBM, the sharp interface scheme based on second-order bilinear and linear interpolations (instead of the diffuse interface scheme, which uses discrete delta functions) is adopted to obtain the more accurate results. The proposed methods are validated through convective heat transfer problems with not only stationary but also moving boundaries - the natural convection in a square cavity with an eccentrically located cylinder and a cold particle sedimentation in an infinite channel. In terms of accuracy, the results from the IBM based on both models are comparable and show a good agreement with those from other numerical methods. In contrast. the IBM based on Model 2 is more numerically efficient than the IBM based on Model 1.  ",
    "keywords": [
      "direct-forcing immersed boundary method",
      "thermal lattice boltzmann method",
      "double-population model",
      "hybrid thermal model",
      "natural convection",
      "moving particle"
    ]
  },
  {
    "id": "1216",
    "title": "Na/Ca Exchange in Function, Growth, and Demise of ?-Cells",
    "abstract": "Recent knowledge concerning the Na/Ca exchanger (NCX) in the pancreatic ?-cell is reviewed. The ?-cell expresses various NCX1 splice variants in a species-specific pattern (NCX1.3 and 1.7 in the rat; NCX1.2, 1.3, and 1.7 in the mouse) and in variable and different proportions. In the rat ?-cell, the exchanger displays a high capacity, accounts for about 70% of Ca2+ extrusion, and participates in Ca2+ inflow during membrane depolarization. In the mouse, however, the contribution of the exchanger to Ca2+ extrusion is more modest, and to Ca2+ inflow, less evident. The exchanger has a stoichiometry of 3 Na+ for 1 Ca2+, is electrogenic, and displays a reversal potential at ?20 mV. Although being of low magnitude, the current generated by the exchanger shapes glucose-induced ?-cell electrical activity and intracellular Ca2+ oscillations. Intracellular Ca2+ may also trigger apoptosis. For instance, overexpression of the exchanger increases Ca2+-dependent and Ca2+-independent ?-cell death by apoptosis, a phenomenon resulting from the depletion of ER Ca2+ stores with subsequent activation of caspase-12. Na/Ca exchange overexpression also reduces ?-cell growth. Hence, the Na/Ca exchanger is a versatile system that appears to play an important role in the function, growth, and demise of the ?-cell.",
    "keywords": [
      "na/ca exchange",
      "pancreatic ?-cell",
      "ca2+ oscillations",
      "apoptosis",
      "caspase-12",
      "endoplasmic reticulum stress"
    ]
  },
  {
    "id": "1217",
    "title": "CVBEM for solving De Saint-Venant solid under shear forces",
    "abstract": "Evaluation of shear stresses distribution due to external shear forces applied to De Saint-Venant beams has been solved through Complex Variable Boundary Element Method properly extended, to benefit from advantages of this method, so far widely used for twisted solids. Extending the above method, further simplifications have been introduced such as those of performing line integrals only, instead of domain integrals. Numerical applications confirm accuracy and efficiency of the proposed extended version of the method, since the good agreement with results proposed in literature.",
    "keywords": [
      "de saint-venant beam",
      "cvbem",
      "complex potential function",
      "shear stresses"
    ]
  },
  {
    "id": "1218",
    "title": "A Model for Capturing and Managing Software Engineering Knowledge and Experience",
    "abstract": "During software development projects there is always a particular working \"product\" that is generated but rarely managed: the knowledge and experience that team members acquire. This knowledge and experience, if conveniently managed, can be reused in future software projects and be the basis for process improvement initiatives. In this paper we present a model for managing the knowledge and experience team members acquire during software development projects in a non-disruptive way, by integrating its management into daily project activities. The purpose of the model is to identify and capture this knowledge and experience in order to derive lessons learned and proposals for best practices that enable an organization to preserve them for future use, and support software process improvement activities. The main contribution of the model is that it enables an organization to consider knowledge and experience management activities as an integral part of its software projects, instead of being considered, as it was until now, as a follow-up activity that is (infrequently) carried out after the end of the projects.",
    "keywords": [
      "knowledge management",
      "software engineering",
      "experience capture"
    ]
  },
  {
    "id": "1219",
    "title": "Determinants of maximal oxygen uptake (VO2 max) in fire fighter testing",
    "abstract": "We evaluate the current daily practice of aerobic capacity testing in Belgian fire fighters. The impact of personal and test-related parameters on the outcome has been evaluated. Test-related parameters have to be taken into account when interpreting VO2 max results. Standardization of aerobic capacity testing is needed in the medical evaluation of fire fighters.",
    "keywords": [
      "aerobic capacity",
      "maximal oxygen uptake",
      "fire fighters",
      "medical evaluation"
    ]
  },
  {
    "id": "1220",
    "title": "social presence with video and application sharing",
    "abstract": "We present two experimental studies examining the effects of videoconferencing and application sharing on task performance. We studied performance on a cognitive reasoning task while subjects were observed via two-way video, one-way video and application sharing. Results demonstrate that performance is impaired when subjects are observed via media compared to when they are not observed. Surprisingly, we found no significant difference in awareness of the observer's presence between the application sharing and the two-way video conditions. This is surprising because application sharing lacks visual feedback of the observer. This finding contradicts social presence theory which claims that media which provides visual feedback of others produce the greatest sense of social presence. Our data also show that media use heightens the perception of task difficulty. We extend social presence theory and argue that these social effects need to be considered in the design and deployment of video and application sharing technologies for use in the workplace.",
    "keywords": [
      "media",
      "task",
      "applications",
      "video",
      "use",
      "social affordance",
      "design",
      "performance",
      "presence",
      "theory",
      "social",
      "application sharing",
      "sharing",
      "observability",
      "reasoning",
      "visualization",
      "technologies",
      "experimentation",
      "cognition",
      "distance collaboration",
      "social presence",
      "perception",
      "deployment",
      "data",
      "demonstrate",
      "awareness",
      "video conferencing",
      "feedback",
      "videoconferencing",
      "effect"
    ]
  },
  {
    "id": "1221",
    "title": "on the logic of argumentation theory",
    "abstract": "The paper applies modal logic to formalize fragments of argumentation theory. Such formalization allows to import, for free, a wealth of new notions (e.g., argument equivalence), new techniques (e.g., calculi, model-checking games, bisimulation games), and results (e.g., completeness of calculi, adequacy of games, complexity of model-checking) from logic to argumentation.",
    "keywords": [
      "modal logic",
      "argumentation theory"
    ]
  },
  {
    "id": "1222",
    "title": "Combinatorial Resampling Particle Filter: An Effective and Efficient Method for Articulated Object Tracking",
    "abstract": "Particle filter (PF) is a method dedicated to posterior density estimations using weighted samples whose elements are called particles. In particular, this approach can be applied to object tracking in video sequences in complex situations and, in this paper, we focus on articulated object tracking, i.e., objects that can be decomposed as a set of subparts. One of PFs crucial step is a resampling step in which particles are resampled to avoid degeneracy problems. In this paper, we propose to exploit mathematical properties of articulated objects to swap conditionally independent subparts of the particles in order to generate new particle sets. We then introduce a new resampling method called Combinatorial Resampling that resamples over the particle set resulting from all the admissible swappings, the so-called combinatorial set. In essence, combinatorial resampling (CR) is quite similar to the combination of a crossover operator and a usual resampling, but there exists a fundamental difference between CR and the use of crossover operators: we prove that CR is sound, i.e., in a Bayesian framework, it is guaranteed to represent without any bias the posterior densities of the states over time. By construction, the particle sets produced by CR better represent the density to estimate over the whole state space than the original set and, therefore, CR produces higher quality samples. Unfortunately, the combinatorial set is generally of an exponential size and, therefore, to be scalable, we show how it can be implicitly constructed and resampled from, thus resulting in both an efficient and effective resampling scheme. Finally, through experimentations both on challenging synthetic and real video sequences, we also show that our resampling method outperforms all classical resampling methods both in terms of the quality of its results and in terms of computation times.",
    "keywords": [
      "particle filter",
      "resampling",
      "dynamic bayesian networks",
      "articulated object tracking"
    ]
  },
  {
    "id": "1223",
    "title": "NationTelescope: Monitoring and visualizing large-scale collective behavior in LBSNs",
    "abstract": "The research of collective behavior has attracted a lot of attention in recent years, which can empower various applications, such as recommendation systems and intelligent transportation systems. However, in traditional social science, it is practically difficult to collect large-scale user behavior data. Fortunately, with the ubiquity of smartphones and Location Based Social Networks (LBSNs), users continuously report their activities online, which massively reflect their collective behavior. In this paper, we propose NationTelescope, a platform that monitors, compares and visualizes large-scale nation-wide user behavior in LBSNs. First, it continuously collects user behavior data from LBSNs. Second, it automatically generates behavior data summary and integrates an interactive map interface for data visualization. Third, in order to compare and visualize the behavioral differences across countries, it detects the discriminative activities according to the related traffic patterns in different countries. By implementing a prototype of NationTelescope platform, we evaluate its effectiveness and usability via two case studies and a system usability scale survey. The results show that the platform can not only efficiently capture, compare and visualize nation-wide collective behavior, but also achieve good usability and user experience.",
    "keywords": [
      "participatory sensing",
      "collective behavior",
      "location based social networks",
      "system usability scale"
    ]
  },
  {
    "id": "1224",
    "title": "The quality of group tacit knowledge",
    "abstract": "Organizational knowledge creation theory explains the process of making available and amplifying knowledge created by individuals as well as crystallizing and connecting it to an organizations knowledge system. What individuals get to know in their (working) lives benefits their colleagues and, eventually, the wider organization. In this article, we briefly review central elements in organizational knowledge creation theory and show a research gap related to the quality of tacit knowledge in a group. We advance organizational knowledge creation theory by developing the concept of quality of group tacit knowledge. Based on this concept, we further develop a comprehensive model explaining different levels of tacit knowledge quality that a group can achieve. Finally, we discuss managerial implications resulting from our model and outline imperatives for future theory building and empirical research.",
    "keywords": [
      "tacit knowledge",
      "group tacit knowledge",
      "quality of group tacit knowledge",
      "improvisation",
      "phronesis",
      "collective action"
    ]
  },
  {
    "id": "1225",
    "title": "Monolithic z-axis CMOS MEMS accelerometer",
    "abstract": "We propose a monolithic z-axis CMOS MEMS accelerometer with low power consumption. The sensor is integrated with circuits in a wafer-level 0.18-?m CMOS MEMS process. The bending displacements in the microstructures can be controlled well. The use of CMOS-compatible MEMS process can reduce the cost of chip fabrication.",
    "keywords": [
      "accelerometer",
      "cmos mems",
      "monolithic",
      "z-axis"
    ]
  },
  {
    "id": "1226",
    "title": "Realization of a four parameter family of generalized one-dimensional contact interactions by three nearby delta potentials with renormalized strengths",
    "abstract": "We propose a new method to construct a four parameter family of quantum-mechanical point interactions in one dimension, which is known as all possible self-adjoint extensions of the symmetric operator T = Delta inverted right perpendicularC(0)(infinity)(R{0}). It is achieved in the small distance limit of equally spaced three neighboring Dirac's delta potentials. The strength for each delta is appropriately renormalized according to the distance and it diverges, in general, in the small distance limit. The validity of our method is ensured by numerical calculations. In general cases except for usual delta, the wave function discontinuity appears around the interaction and one can observe such a tendency even at a finite distance level.",
    "keywords": [
      "quantum mechanics",
      "one dimension",
      "generalized point interaction",
      "wave function discontinuity",
      "functional analysis"
    ]
  },
  {
    "id": "1227",
    "title": "Highly reliable LDPC coded data transfer in home networks by using Canetes PLC channel model",
    "abstract": "A power line communication system for smart grids is investigated in this study. Canetes PLC channel model is employed in this study. Several channel codes and decoder types are compared for performance evaluation. LDPC codes can be efficiently utilized for channel coding purposes in smart grids.",
    "keywords": [
      "power line communication",
      "low-density parity-check codes",
      "bit error rate performance simulation"
    ]
  },
  {
    "id": "1228",
    "title": "Seascorr: A MATLAB program for identifying the seasonal climate signal in an annual tree-ring time series",
    "abstract": "A common research task in dendroclimatology is identification of the monthly or seasonal climate signal in an annual time series of indices of ring width. A MATLAB function, seascorr, is introduced as a general statistical tool for identifying the signal. Monthly time series of primary and secondary climate variables are input to the function along with a tree-ring time series and specifications for seasonal groupings. The relationship of the tree-ring series with the seasonalized primary climate variable is summarized by simple correlations. The relationship with the secondary climate variable is summarized by partial correlations, controlling for the influence of the primary climate variable. Confidence intervals on sample correlations and partial correlations are estimated with the help of Monte Carlo simulation of the tree-ring series by exact simulation, which preserves the spectral properties of the observed series. Results are summarized in graphical and statistical output. The function is illustrated with examples from Tunisia and Russia.  ",
    "keywords": [
      "dendroclimatology",
      "tree growth",
      "exact simulation",
      "paleoclimatology",
      "tunisia"
    ]
  },
  {
    "id": "1229",
    "title": "Implementation of a direct procedure for critical point computations using preconditioned iterative solvers",
    "abstract": "Computation of critical points on an equilibrium path requires the solution of a non-linear eigenvalue problem. These critical points could be either bifurcation or limit points. When the external load is parametrized by a single parameter, the non-linear stability eigenvalue problem consists of solving the equilibrium equations along the criticality condition. Several techniques exist for solution of such a system. Their algorithmic treatment is usually focused for direct linear solvers and thus use the block elimination strategy. In this paper special emphasis is given for a strategy which can be used also with iterative linear solvers. Comparison to the block elimination strategy with direct linear solvers is given. Due to the non-uniqueness of the critical eigenmode a normalizing condition is required. In addition, for bifurcation points, the Jacobian matrix of the augmented system is singular at the critical point and additional stabilization is required in order to maintain the quadratic convergence of the Newtons method. Depending on the normalizing condition, convergence to a critical point with negative load parameter value can happen. The form of the normalizing equation is critically discussed. Due to the slenderness of the buckling sensitive structures the resulting matrices are ill-conditioned and a good preconditioner is mandatory for efficient solution.",
    "keywords": [
      "non-linear eigenvalue problem",
      "equilibrium equations",
      "critical points",
      "preconditioned iterations"
    ]
  },
  {
    "id": "1230",
    "title": "ubiquitous ims emergency services over cooperative heterogeneous networks",
    "abstract": "There are various emergency services based on wireless sensor network being proposed recently. However, the ability of these services/networks is inherently limited by geographical restrictions and need to be deployed in advance. This paper proposes an application level approach to enhance the service coverage and availability of emergency services. Specifically, we augment these services with All-IP network infrastructure based on IP Multimedia Subsystem (IMS). Furthermore, we integrate the IMS Emergency Services architecture with Cooperative Network technology to provide ubiquitous emergency services. We also investigate the prime problems of cooperation between heterogeneous networks and IMS. Finally, we present and discuss the experimental results of performance in our Cooperative Emergency IMS Testbed.",
    "keywords": [
      "cooperative networks",
      "ip multimedia subsystems",
      "emergency service"
    ]
  },
  {
    "id": "1231",
    "title": "Gaussian Process Models of Dynamic PET for Functional Volume Definition in Radiation Oncology",
    "abstract": "In routine oncologic positron emission tomography (PET), dynamic information is discarded by time-averaging the signal to produce static images of the \"standardised uptake value\" (SUV). Defining functional volumes of interest (VOIs) in terms of SUV is flawed, as values are affected by confounding factors and the chosen time window, and SUV images are not sensitive to functional heterogeneity of pathological tissues. Also, SUV iso-contours are highly affected by the choice of threshold and no threshold, or other SUV-based segmentation method, is universally accepted for a given VOI type. Gaussian Process (GP) time series models describe macro-scale dynamic behavior arising from countless interacting micro-scale processes, as is the case for PET signals from heterogeneous tissue. We use GPs to model time-activity curves (TACs) from dynamic PET and to define functional volumes for PET oncology. Probabilistic methods of tissue discrimination are presented along with novel contouring methods for functional VOI segmentation. We demonstrate the value of GP models for voxel classification and VOI contouring of diseased and metastatic tissues with functional heterogeneity in prostate PET. Classification experiments reveal superior sensitivity and specificity over SUV calculation and a TAC-based method proposed in recent literature. Contouring experiments reveal differences in shape between gold-standard and GP VOIs and correlation with kinetic models shows that the novel VOIs contain extra clinically relevant information compared to SUVs alone. We conclude that the proposed models offer a principled data analysis technique that improves on SUVs for oncologic VOI definition. Continuing research will generalize GP models for different oncology tracers and imaging protocols with the ultimate goal of clinical use including treatment planning.",
    "keywords": [
      "gaussian processes",
      "image classification",
      "nonlinear dynamical systems",
      "object segmentation",
      "positron emission tomography "
    ]
  },
  {
    "id": "1232",
    "title": "The effects of exploration strategies and communication models on the performance of cooperative exploration",
    "abstract": "Exploration with mobile robots is utilized in a wide range of applications including search and rescue missions, planetary exploration, homeland security, surveillance, and reconnaissance. Cooperative exploration offers the potential of exploring an unknown zone more quickly and robustly than single-robot case. However, coordinating multiple robots is a challenging task due to heterogeneous processing and communication requirements, and the complexities of exploration algorithms. This paper presents a comparison of different cooperative exploration strategies, such as frontier-based exploration, market-driven exploration, and role-based exploration, based on their exploration performances and processing time requirements. To show the effect of CPU power on the processing time of the exploration algorithms, two notebooks and a netbook with different specifications have been extensively used. Comparative simulation results of our own application developed in Java show that the processing time requirements are consistent with the computational complexities of the exploration strategies. The results we obtained are consistent with the CPU power tests of independent organizations, and show that higher processing power reduces processing time accordingly.",
    "keywords": [
      "cooperative exploration strategies",
      "communication models",
      "processing time requirements",
      "exploration performances",
      "netbeans ide"
    ]
  },
  {
    "id": "1233",
    "title": "parabolic cable element for static analysis of cable structures",
    "abstract": "Purpose - The purpose of this paper is to present a finite element formulation of enhanced two-node parabolic cable element for the static analysis of cable structures. Design/methodology/approach - Unlike the assumed polynomial displacement interpolation functions, the present approach uses the analytical cable dynamic stiffness matrix to obtain the explicit expression of the static stiffness matrix of an inclined sagging cable by setting the frequency at zero. The Newton-Raphson-based iterative method is used to obtain the solution. Findings - It is demonstrated that the present results agree well with those obtained from the nonlinear analytical theory of a parabolic cable and previous reported methods in the literature. Originality/value - This paper proposes a two-node parabolic cable element. For comparable accuracy with the truss element method, fewer numbers of such cable elements are needed.",
    "keywords": [
      "finite element analysis",
      "newton method",
      "physical properties of materials",
      "strength of materials",
      "structural analysis",
      "design and theory"
    ]
  },
  {
    "id": "1234",
    "title": "Dominating Induced Matchings for P 7-Free Graphs in Linear Time",
    "abstract": "LetG be a finite undirected graph with edge set E. An edge set EE is an induced matching inG if the pairwise distance of the edges of E? inG is at least two; E? is dominating inG if every edge e?E?E? intersects some edge in E?. The Dominating Induced Matching Problem (DIM, for short) asks for the existence of an induced matching E? which is also dominating inG; this problem is also known as the Efficient Edge Domination Problem.",
    "keywords": [
      "dominating induced matching",
      "efficient edge domination",
      "p 7-free graphs",
      "linear time algorithm",
      "robust algorithm"
    ]
  },
  {
    "id": "1235",
    "title": "energy-aware dual-mode voltage scaling for weakly hard real-time systems",
    "abstract": "In this paper we study the problem of minimizing energy while ensuring the designated QoS requirements for weakly hard realtime systems on a dual-mode variable voltage processor. The QoS is quantified with the (m, k)-constraints. We proposed a dynamic scheduling strategy, based on pattern variation and the enhanced dual priority scheduling, to guarantee the (m, k)-constraints while optimizing the energy consumption. The simulation results demonstrate that our proposed techniques can achieve significant energy saving performance while ensuring the (m, k)-guarantee.",
    "keywords": [
      "dual priority scheduling",
      "dual-mode voltage scaling",
      "qos"
    ]
  },
  {
    "id": "1236",
    "title": "Laplace mixture of linear experts",
    "abstract": "Mixture of Linear Experts (MoLE) models provide a popular framework for modeling nonlinear regression data. The majority of applications of MoLE models utilizes a Gaussian distribution for regression error. Such assumptions are known to be sensitive to outliers. The use of a Laplace distributed error is investigated. This model is named the Laplace MoLE (LMoLE). Links are drawn between the Laplace error model and the least absolute deviations regression criterion, which is known to be robust among a wide class of criteria. Through application of the minorizationmaximization algorithm framework, an algorithm is derived that monotonically increases the likelihood in the estimation of the LMoLE model parameters. It is proven that the maximum likelihood estimator (MLE) for the parameter vector of the LMoLE is consistent. Through simulation studies, the robustness of the LMoLE model over the Gaussian MOLE model is demonstrated, and support for the consistency of the MLE is provided. An application of the LMoLE model to the analysis of a climate science data set is described.",
    "keywords": [
      "laplace distribution",
      "minorizationmaximization algorithm",
      "mixture of experts",
      "robust regression"
    ]
  },
  {
    "id": "1237",
    "title": "Compact low-power time-based conversion with noise immunity similar to digital conversion",
    "abstract": "An analog signal representation based on the inter-pulse-interval (IPI) time is presented. Voltage-to-IPI and IPI-to-voltage conversion circuits based on the representation are described. The circuits have been fabricated using a 0.35 mu m mixed-signal CMOS process. Simulation and test results agree with the theory. Voltage-to-IPI conversion needs significantly less area and power than ADC and is significantly more immune to noise and other problems than using analog voltage/current signals.",
    "keywords": [
      "inter-pulse interval",
      "analog signal",
      "mixed signal",
      "low power",
      "noise immunity",
      "integrated circuits",
      "cmos",
      "adc",
      "soc"
    ]
  },
  {
    "id": "1238",
    "title": "Using a composite grid approach in a complex coastal domain to estimate estuarine residence time",
    "abstract": "We investigate the processes that influence residence time in a partially mixed estuary using a three-dimensional circulation model. The complex geometry of the study region is not optimal for a structured grid model and so we developed a new method of grid connectivity. This involves a novel approach that allows an unlimited number of individual grids to be combined in an efficient manner to produce a composite grid. We then implemented this new method into the numerical Regional Ocean Modeling System (ROMS) and developed a composite grid of the Hudson River estuary region to investigate the residence time of a passive tracer. Results show that the residence time is a strong function of the time of release (spring vs. neap tide), the along-channel location, and the initial vertical placement. During neap tides there is a maximum in residence time near the bottom of the estuary at the mid-salt intrusion length. During spring tides the residence time is primarily a function of along-channel location and does not exhibit a strong vertical variability. This model study of residence time illustrates the utility of the grid connectivity method for circulation and dispersion studies in regions of complex geometry.",
    "keywords": [
      "residence time",
      "estuary",
      "three-dimensional model",
      "hudson river estuary",
      "roms",
      "composite grid"
    ]
  },
  {
    "id": "1239",
    "title": "DISTRIBUTED VERIFICATION AND HARDNESS OF DISTRIBUTED APPROXIMATION",
    "abstract": "We study the verification problem in distributed networks, stated as follows. Let H be a subgraph of a network G where each vertex of G knows which edges incident on it are in H. We would like to verify whether H has some properties, e. g., if it is a tree or if it is connected (every node knows at the end of the process whether H has the specified property or not). We would like to perform this verification in a decentralized fashion via a distributed algorithm. The time complexity of verification is measured as the number of rounds of distributed communication. In this paper we initiate a systematic study of distributed verification and give almost tight lower bounds on the running time of distributed verification algorithms for many fundamental problems such as connectivity, spanning connected subgraph, and s-t cut verification. We then show applications of these results in deriving strong unconditional time lower bounds on the hardness of distributed approximation for many classical optimization problems including minimum spanning tree (MST), shortest paths, and minimum cut. Many of these results are the first nontrivial lower bounds for both exact and approximate distributed computation, and they resolve previous open questions. Moreover, our unconditional lower bound of approximating MST subsumes and improves upon the previous hardness of approximation bound of Elkin [M. Elkin, SIAM J. Comput., 36 (2006), pp. 433-456] as well as the lower bound for (exact) MST computation of Peleg and Rubinovich [D. Peleg and V. Rubinovich, SIAM J. Comput., 30 (2000), pp. 1427-1442]. Our result implies that there can be no distributed approximation algorithm for MST that is significantly faster than the current exact algorithm for any approximation factor. Our lower bound proofs show an interesting connection between communication complexity and distributed computing which turns out to be useful in establishing the time complexity of exact and approximate distributed computation of many problems.",
    "keywords": [
      "distributed algorithms",
      "graph optimization problems",
      "lower bounds",
      "hardness of approximation",
      "communication complexity"
    ]
  },
  {
    "id": "1240",
    "title": "Biomechanical study of different plate configurations for distal humerus osteosynthesis",
    "abstract": "Fractures of the distal humerus are most commonly fixed by open reduction and internal fixation, using plates and screws, either in a locking or in a non-locking construct. Three different plating systems are commonly used in practice. The most important differences between them are in plate orientation, which affects both the rigidity of the osteosynthesis and invasiveness of the surgical procedure. Unfortunately, there is no common agreement between surgeons about which plate configuration brings the best clinical outcome. In this study, we investigate the theoretical rigidity of plate osteosyntheses considering two types of AO/ASIF configurations (90 angle between plates), Mayo clinic (Acumed) configuration (180 between plates) and dorsal fixation of both plates. We also compared the results for cases with and without contact between the bone fragments. In the case of no bone contact, the Mayo clinic plate configuration is found to be the most rigid, followed by both AO/ASIF plate configurations, and the least rigid system is the Korosec plate configuration. On the other hand, no significant differences between all types of fixation configurations are found in cases with contact in-between the bone fragments. Our findings show that this contact is very important and can compensate for the lack of load carrying capacity of the implants. This could therefore incite other implant fixation solutions, leading to less invasive surgical procedures and consequently improved clinical outcome.",
    "keywords": [
      "distal humerus",
      "fracture",
      "osteosynthesis",
      "minimally invasive surgery",
      "finite element analysis"
    ]
  },
  {
    "id": "1241",
    "title": "Stochastic Traffic Engineering in Multihop Cognitive Wireless Mesh Networks",
    "abstract": "In this work, the stochastic traffic engineering problem in multihop cognitive wireless mesh networks is addressed. The challenges induced by the random behaviors of the primary users are investigated in a stochastic network utility maximization framework. For the convex stochastic traffic engineering problem, we propose a fully distributed algorithmic solution which provably converges to the global optimum with probability one. We next extend our framework to the cognitive wireless mesh networks with nonconvex utility functions, where a decentralized algorithmic solution, based on learning automata techniques, is proposed. We show that the decentralized solution converges to the global optimum solution asymptotically.",
    "keywords": [
      "cognitive networks",
      "network utility maximization",
      "learning algorithms"
    ]
  },
  {
    "id": "1242",
    "title": "Mid-air display experiments to create novel user interfaces",
    "abstract": "Displays are the most visible part of most computer applications. Novel display technologies strongly influence and inspire new forms of computer use and interaction. We are particularly interested in the interplay of novel displays and interaction for ubiquitous computing or ambient media environments, as emerging display technologies may become game-changers in how we define and use computers, possibly changing the context of computing fundamentally.",
    "keywords": [
      "fogscreen",
      "display",
      "interaction",
      "ambient media"
    ]
  },
  {
    "id": "1243",
    "title": "Automatic tag expansion using visual similarity for photo sharing websites",
    "abstract": "In this paper we present an automatic photo tag expansion method designed for photo sharing websites. The purpose of the method is to suggest tags that are relevant to the visual content of a given photo at upload time. Both textual and visual cues are used in the process of tag expansion. When a photo is to be uploaded, the system asks for a couple of initial tags from the user. The initial tags are used to retrieve relevant photos together with their tags. These photos are assumed to be potentially content related to the uploaded target photo. The tag sets of the relevant photos are used to form the candidate tag list, and visual similarities between the target photo and relevant photos are used to give weights to these candidate tags. Tags with the highest weights are suggested to the user. The method is applied on Flickr (http://?www.?flickr.?com). Results show that including visual information in the process of photo tagging increases accuracy with respect to text-based methods.",
    "keywords": [
      "tagging",
      "photo-annotation",
      "visual similarity",
      "folksonomy",
      "flickr"
    ]
  },
  {
    "id": "1244",
    "title": "PASTA: splice junction identification from RNA-Sequencing data",
    "abstract": "Next generation transcriptome sequencing (RNA-Seq) is emerging as a powerful experimental tool for the study of alternative splicing and its regulation, but requires ad-hoc analysis methods and tools. PASTA (Patterned Alignments for Splicing and Transcriptome Analysis) is a splice junction detection algorithm specifically designed for RNA-Seq data, relying on a highly accurate alignment strategy and on a combination of heuristic and statistical methods to identify exon-intron junctions with high accuracy.",
    "keywords": [
      "rna-seq",
      "next-generation sequencing",
      "alternative splicing",
      "computational analysis of alternative splicing"
    ]
  },
  {
    "id": "1245",
    "title": "ray tracing deformed surfaces",
    "abstract": "A collection of new methods for ray tracing differentiable surfaces is developed. The methods are general, and extend the set of \"ray-traceable\" surfaces suitable for use in geometric modeling. We intersect a ray l = at + b , t > 0 with a parametric surface x = f (u, v) , and with implicit surfaces f(x,y,z) = 0. A smooth surface is treated as a deformation of a flat sheet; the intersection problem is converted to a new coordinate system in which the surfaces are flat, and the rays are bent. We develop methods for providing good initial estimates of the parametric intersection values, and a \"closeness criterion,\" to reduce computation. These same criteria help us substitute a set of simpler surfaces for the more complex surface. The parametric method produces the intersection values of u, v , and t . These are suitable for shading calculations and for mapping textures onto the surface; they can also produce the local coordinate frame values, suitable for anisotropic lighting models.",
    "keywords": [
      "help",
      "coordination",
      "intersection",
      "method",
      "use",
      "texture",
      "systems",
      "values",
      "ray tracing",
      "implicit surfaces",
      "computation",
      "collect",
      "parametric",
      "map",
      "general",
      "complexity",
      "smooth",
      "model",
      "lighting",
      "locality",
      "deformation",
      "surface"
    ]
  },
  {
    "id": "1246",
    "title": "Probability density of the differential phase difference in applications to passive wireless surface acoustic wave sensing",
    "abstract": "The probability density function (pdf) is discussed of the differential phase difference (DPD) in the radio frequency (RF) pulse-burst perturbed by Gaussian noise at the coherent receiver. Statistical properties of the DPD are of importance for error estimation in coherent systems such as remote passive wireless surface acoustic wave (SAW) sensing with multiple differential phase measurement. The rigorous probability density of the DPD is derived and its particular functions, all having no closed forms, are given for different signal-to-noise ratios (SNRs) in the RF pulses. Employing the von Mises/Tikhonov distribution, an efficient approximation is proposed via the modified Bessel functions of the first kind and zeroth order. Engineering features and small errors of the approximation are demonstrated. Applications are given for the phase difference drift rate and error probability for the drift rate to exceed a threshold.",
    "keywords": [
      "differential phase difference",
      "probability density",
      "passive saw sensing",
      "drift rate",
      "error probability"
    ]
  },
  {
    "id": "1247",
    "title": "Ultra-deep sequencing enables high-fidelity recovery of biodiversity for bulk arthropod samples without PCR amplification",
    "abstract": "Next-generation-sequencing (NGS) technologies combined with a classic DNA barcoding approach have enabled fast and credible measurement for biodiversity of mixed environmental samples. However, the PCR amplification involved in nearly all existing NGS protocols inevitably introduces taxonomic biases. In the present study, we developed new Illumina pipelines without PCR amplifications to analyze terrestrial arthropod communities.",
    "keywords": [
      "next-generation-sequencing",
      "species richness",
      "abundance",
      "biomonitoring",
      "insect biodiversity",
      "mitochondria",
      "pcr-independent",
      "metabarcoding"
    ]
  },
  {
    "id": "1248",
    "title": "PHAST: Hardware-accelerated shortest path trees",
    "abstract": "We present a novel algorithm to solve the non-negative single-source shortest path problem on road networks and graphs with low highway dimension. After a quick preprocessing phase, we can compute all distances from a given source in the graph with essentially a linear sweep over all vertices. Because this sweep is independent of the source, we are able to reorder vertices in advance to exploit locality. Moreover, our algorithm takes advantage of features of modern CPU architectures, such as SSE and multiple cores. Compared to Dijkstras algorithm, our method needs fewer operations, has better locality, and is better able to exploit parallelism at multi-core and instruction levels. We gain additional speedup when implementing our algorithm on a GPU, where it is up to three orders of magnitude faster than Dijkstras algorithm on a high-end CPU. This makes applications based on all-pairs shortest-paths practical for continental-sized road networks. Several algorithms, such as computing the graph diameter, arc flags, or exact reaches, can be greatly accelerated by our method.",
    "keywords": [
      "shortest paths",
      "gpu",
      "route planning",
      "high performance computing"
    ]
  },
  {
    "id": "1249",
    "title": "towards secure dataflow processing in open distributed systems",
    "abstract": "Open distributed systems such as service oriented architecture and cloud computing have emerged as promising platforms to deliver software as a service to users. However, for many security sensitive applications such as critical data processing, trust management poses significant challenges for migrating those critical applications into open distributed systems. In this paper, we present the design and implementation of a new secure dataflow processing system that aims at providing trustworthy continuous data processing in multi-party open distributed systems. We identify a set of major security attacks that can compromise the integrity of dataflow processing and provide effective protection mechanisms to counter those attacks. We have implemented a prototype of the secure dataflow processing framework and tested it on the PlanetLab testbed. Our experimental results show that our protection schemes are effective and impose low performance impact for dataflow processing in large-scale open distributed systems.",
    "keywords": [
      "service oriented architecture",
      "secure data processing",
      "secure component composition",
      "cloud computing"
    ]
  },
  {
    "id": "1250",
    "title": "a case study on interactive exploration and guidance aids for visualizing historical data",
    "abstract": "In this paper, we address the problem of historical data visualization. We describe the data acquisition, preparation, and visualization. Since the data contain four dimensions, the standard 3D exploration techniques have to be extended or appropriately adapted in order to enable interactive exploration. We discuss in detail two interaction concepts: (1) navigation with one fixed dimension, and (2) quasi 4D navigation allowing to simultaneously explore the four-dimensional space. In addition, we also present a picture-in-picture display mode, enabling the user to interactively view the data, while \"flying with\" a particular event, tracking its motion in time and space. Finally, we present a technique for guided exploration and animation generation, allowing for a vivid gain of insight into the historical data.",
    "keywords": [
      "data visualization",
      "order",
      "generation",
      "event",
      "interaction",
      "concept",
      "addressing",
      "standardization",
      "timing",
      "paper",
      "navigation",
      "motion",
      "user",
      "exploration",
      "visualization",
      "historical data",
      "case studies",
      "space",
      "data acquisition",
      "data",
      "3d",
      "tracking",
      "time-dependent data",
      "animation",
      "visualization techniques"
    ]
  },
  {
    "id": "1251",
    "title": "Video-to-Shot Tag Propagation by Graph Sparse Group Lasso",
    "abstract": "Traditional approaches to video tagging are designed to propagate tags at the same level, such as assigning the tags of training videos (or shots) to the test videos (or shots), such as generating tags for the test video when the training videos are associated with the tags at the video-level or assigning tags to the test shot when given a collection of annotated shots. This paper focuses on automatical shot tagging given a collection of videos with the tags at the video-level. In other words, we aim to assign specific tags from the training videos to the test shot. The paper solves the V2S issue by assigning the test shot with the tags deriving from parts of the tags in a part of training videos. To achieve the goal, the paper first proposes a novel Graph Sparse Group Lasso (shorted for GSGL) model to linearly reconstruct the visual feature of the test shot with the visual features of the training videos, i.e., finding the correlation between the test shot and the training videos. The paper then proposes a new tagging propagation rule to assign the video-level tags to the test shot by the learnt correlations. Moreover, to effectively build the reconstruction model, the proposed GSGL simultaneously takes several constraints into account, such as the inter-group sparsity, the intra-group sparsity, the temporal-spatial prior knowledge in the training videos and the local structure of the test shot. Extensive experiments on public video datasets are conducted, which clearly demonstrate the effectiveness of the proposed method for dealing with the video-to-shot tag propagation.",
    "keywords": [
      "manifold learning",
      "sparse coding",
      "sparse group lasso",
      "structure sparsity",
      "video annotation",
      "video tagging"
    ]
  },
  {
    "id": "1252",
    "title": "Computational and statistical study on the molecular interaction between antigen and antibody",
    "abstract": "Simulations were performed to clarify the factors in antigenantibody interaction. Appearance rate of each kind of amino acid residue at CDR was counted. The residues strongly involved in the antigenantibody binding were suggested. The binding free energy and its energetic components were examined.",
    "keywords": [
      "antibody",
      "antigen",
      "complex structure",
      "binding energy",
      "molecular dynamics simulation"
    ]
  },
  {
    "id": "1253",
    "title": "The receptor-dependent LQTA-QSAR: application to a set of trypanothione reductase inhibitors",
    "abstract": "A new Receptor-Dependent LQTA-QSAR approach, RD-LQTA-QSAR, is proposed as a new 4D-QSAR method. It is an evolution of receptor independent LQTA-QSAR. This approach uses the free GROMACS package to carry out molecular dynamics simulations and generates a conformational ensemble profile for each compound. Such an ensemble is used to build molecular interaction field-based QSAR models, as in CoMFA. To show the potential of this methodology, a set of 38 phenothiazine derivatives that are specific competitive T. cruzi trypanothione reductase inhibitors, was chosen. Using a combination of molecular docking and molecular dynamics simulations, the binding mode of the phenotiazine derivatives was evaluated in a simulated induced fit approach. The ligands alignments were performed using both ligand and binding site atoms, enabling unbiased alignment. The models obtained were extensively validated by leave-N-out cross-validation and y-randomization techniques to test for their robustness and absence of chance correlation. The final model presented Q (2) LOO of 0.87 and RA(2) of 0.92 and a suitable external prediction of = 0.78. The adapted binding site obtained is useful to perform virtual screening and ligand structure-based design and the descriptors in the final model can aid in the design new inhibitors.",
    "keywords": [
      "lqta-qsar",
      "t. cruzi",
      "docking",
      "molecular dynamics simulations"
    ]
  },
  {
    "id": "1254",
    "title": "adaptive configuration of wpans and wlans communications using multi-scale statistical process control",
    "abstract": "Quality of communication in wireless systems is heavily dependent on several factors including interference on the radio signal and mobility of the communicating devices. However, it is very difficult to assess the transmitted signal and predict when a bad quality signal is being caused by interference or mobility. This work presents a new method for monitoring and analyzing wireless communications on two types of networks: IEEE 802.11 and Bluetooth. This technique uses a multi-scale control chart approach known as MCEWMA - Moving Centerline Exponential Weighted Moving Average - associated with multi-resolution analysis. The implementation of this control chart allows the description of the transmitted signal on each wireless channel with low cost and, more important, it can be used on on-line applications. To prove the efficiency of this method, a simple application was developed and tested and the results proved the efficiency of the proposed method.",
    "keywords": [
      "wifi",
      "adaptive middleware",
      "statistical control process",
      "bluetooth"
    ]
  },
  {
    "id": "1255",
    "title": "A method for simulating Atomic Force Microscope nanolithography in the Level Set framework",
    "abstract": "During the last decades it has been shown that the Atomic Force Microscope (AFM) can be used in non-contact mode as an efficient lithographic technique capable of manufacturing nanometer sized devices on the surface of a silicon wafer. The AFM nanooxidation approach is based on generating a potential difference between a cantilever needle tip and a silicon wafer. A water meniscus builds up between the tip and the wafer, resulting in a medium for oxyions to move due to the high electric field in the region. A simulator for nanooxidation with a non-contact AFM, implemented in a Level Set environment, was developed. The presented simulator implements the growth of thicker oxides by analyzing the potential, electric field, and ion concentrations at the ambient/oxide and oxide/silicon interfaces, while the growth of thin oxides assumes a single liquid/silicon interface, which is modeled as an infinitely long conducting plane. The nanodot shapes have been shown to follow the electric field and hence the surface charge distribution shape; therefore, a Monte Carlo particle distribution for the surface charge density is generated for two-dimensional and three-dimensional topography simulations in a Level Set framework.",
    "keywords": [
      "topography simulation",
      "level set",
      "monte carlo",
      "local anodic oxidation",
      "atomic force microscope",
      "oxide nanodots"
    ]
  },
  {
    "id": "1256",
    "title": "the gardener's problem for web information monitoring",
    "abstract": "We introduce and theoretically study the Gardener's problem that well models many web information monitoring scenarios, where numerous dynamically changing web sources are monitored and local information needs to be periodically updated under communication and computation capacity constraints. Typical such examples include maintenance of inverted indexes for search engines and maintenance of extracted structures for unstructured data management systems. We formulate a corresponding multicriteria optimization problem and propose heuristic solutions.",
    "keywords": [
      "olap",
      "web information monitoring",
      "multicriteria maintenance scheduling",
      "np-hard",
      "gardener's problem"
    ]
  },
  {
    "id": "1257",
    "title": "An Eulerian method for computation of multimaterial impact with ENO shock-capturing and sharp interfaces",
    "abstract": "A technique is presented for the numerical simulation of high-speed multimaterial impact. Of particular interest is the interaction of solid impactors with targets. The computations are performed on a fixed Cartesian mesh by casting the equations governing material deformation in Eulerian conservation law form. The advantage of the Eulerian setting is the disconnection of the mesh from the boundary deformation allowing for large distortions of the interfaces. Eigenvalue analysis reveals that the system of equations is hyperbolic for the range of materials and impact velocities of interest. High-order accurate ENO shock-capturing schemes are used along with interface tracking techniques to evolve sharp immersed boundaries. The numerical technique is designed to tackle the following physical phenomena encountered during impact: (1) high velocities of impact leading to large deformations of the impactor as well as targets; (2) nonlinear wave-propagation and the development of shocks in the materials; (3) modeling of the constitutive properties of materials under intense impact conditions and accurate numerical calculation of the elasto-plastic behavior described by the models; (4) phenomena at multiple interfaces (such as impactortarget, targetambient and impactorambient), i.e. both free surface and surfacesurface dynamics. Comparison with Lagrangian calculations is made for the elasto-plastic deformation of solid material. The accuracy of convex ENO scheme for shock capturing, with the MieGruneisen equation of state for pressure, is closely examined. Good agreement of the present finite difference fixed grid results is obtained with exact solutions in 1D and benchmarked moving finite element solutions for axisymmetric Taylor impact.",
    "keywords": [
      "moving boundaries",
      "impact",
      "multimaterial flows",
      "high-strain rate"
    ]
  },
  {
    "id": "1258",
    "title": "policy modeling in risk-driven environment",
    "abstract": "In this paper, the main challenges of applying ICT in policy modeling are described and a solution is proposed, which emphasizes policy impact exploration, monitoring and risk management. State of the art of policy modeling is given, with the summary of those features of our solution, which goes beyond the available approaches. The paper will be structured as follows: First, the challenges of the ICT utilization for policy modeling are detailed. Next theoretical background of policy modeling is discussed, followed by research overview. The proposed solution -- policy modeling cycle and the corresponding system is presented in the following section. Finally, conclusion and future work are shown.",
    "keywords": [
      "ict",
      "semantic technology",
      "policy modeling",
      "policy impact",
      "risk management",
      "e-government"
    ]
  },
  {
    "id": "1259",
    "title": "Decision support for Web service adaptation",
    "abstract": "With the Internet of Services, Web services from all areas of life and business will be offered to service consumers. Even though Web service technologies make it easy to consume services on arbitrary devices due to their platform independence, service messaging is heavyweight. This may cause problems if services are invoked using devices with limited resources, e.g., smartphones. To overcome this issue, several adaptation mechanisms to decrease service messaging have been proposed. However, none of these are the best-performing under all possible system contexts. In this paper, we present a decision support system that aims at helping an operator to apply appropriate adaptation mechanisms based on the system context. We formulate the corresponding decision problem and present two scoring algorithmsone Quality of Service-based and one Quality of Experience-based. Missing data and, thus, an incomplete system context is a serious challenge for scoring algorithms. Regarding the problem at hand, missing data may lead to errors with respect to the recommended adaptation mechanisms. To address this challenge, we apply the statistical concept of imputation, i.e., substituting missing data. Based on the evaluation of different imputation algorithms used for one of our scoring algorithms, we show which imputation algorithms significantly decrease the error imposed by the missing data and decide whether imputation algorithms tailored to our scenario should be investigated.",
    "keywords": [
      "web services",
      "pervasive computing",
      "qos",
      "qoe"
    ]
  },
  {
    "id": "1260",
    "title": "Micromechanics-based progressive failure analysis of fibre-reinforced composites with non-iterative element-failure method",
    "abstract": "This paper proposes a micromechanics-based progressive failure analysis strategy for fibre-reinforced composites. A non-iterative element-failure method (NEFM) is developed and integrated with a micromechanics-based failure criterion. The micromechanics of failure (MMF) is further investigated and modified to improve predictions. The NEFM is focused on implementation of damaged elements with direct solution and implicit nodal forces. Progressive failure of fibre-reinforced laminates is analysed with the micromechanics-based NEFM and the results are compared with those from the traditional finite element method or experiments. The comparison demonstrates computational efficiency and validity of the micromechanics-based NEFM in progressive failure analysis of composites.",
    "keywords": [
      "fibre-reinforced composite",
      "progressive failure analysis",
      "micromechanics of failure  criterion",
      "non-iterative element-failure method "
    ]
  },
  {
    "id": "1261",
    "title": "Optimal control of service rates and arrivals in Jackson networks",
    "abstract": "We can represent each dynamic job shop system as a network of queues, in which each service station indicates a machine or a production department. Now, assume that we can control the service rates of these service stations and also the arrival rates to the service stations, in which the arrival rate to each service station corresponds to the total rates of demands for the products which are processed by this service station. In this paper, we develop a new model for optimal control of service rates of all service stations and also the arrival rates to these service stations in a class of Jackson networks, in which the expected value of shortest path of the network and also the total operating costs of all service stations of the network per period are minimized. The expected value of shortest path of such networks of queues is equal to the expected value of the completion time of the first product, which is an important factor in production systems. The networks of queues, which we analyze in this paper, have all specifications of Jackson networks, except they do not include M/M/C queueing systems.",
    "keywords": [
      "queueing",
      "control",
      "nonlinear programming"
    ]
  },
  {
    "id": "1262",
    "title": "A note on differentiable palindromes",
    "abstract": "We give a characterization of the palindromes in a class of infinite words over Sigma = {1,2} related to the Kolakoski word K. This characterization, based on the left palindromic closure of all prefixes of K, is obtained by using a bijection between the class of right infinite words over Sigma and a class of words over the same alphabet, and reveals the first link between the existence of some palindromes and the recurrence of K. Indeed, the existence of arbitrarily long palindromes implies the recurrence of K, and a stronger assumption implies the closure of the set of its factors by permutation of the letters in Sigma.  ",
    "keywords": [
      "combinatorics on words",
      "palindrome complexity",
      "kolakoski word",
      "recurrence"
    ]
  },
  {
    "id": "1263",
    "title": "Postprandial performance of Dexcom SEVEN PLUS and Medtronic Paradigm Veo: Modeling and statistical analysis",
    "abstract": "Minimizing excessive postprandial glucose excursion is still an unmet need in treating diabetes. This work addresses the analysis and modeling in the postprandial state of two commercial CGM devices. Twelve patients with type 1 diabetes were studied in the postprandial state on four different occasions under controlled conditions. Each time, we performed simultaneous glucose monitoring using the Dexcom SEVEN PLUS (47 datasets) and the Medtronic Paradigm Veo (42 datasets). The following statistical properties of the error signal were analyzed and modeled sequentially for the two devices: the lag time, the error stationarity, the error probability distribution and the time correlation. Finally, models were built for sensor simulation in silico studies. Lag time followed an exponential probability distribution for both monitors (?sevenplus=108, ?veo=1.69). Standard deviation and mean of the error signal, calculated as time-dependent signals across the population of sensors, were time-varying and correlated with the reference value and its rate of change, respectively. In both cases, a high variability of postprandial behaviors was observed. After non-stationarity compensation, a logistic distribution was obtained for the SEVEN PLUS error (close to normal distribution). Regarding the Paradigm Veo, a multimodal distribution was obtained, which turned into normal after elimination of five unstable sensors. Finally, a first order autoregressive model fitted the SEVEN PLUS error time-series while a third-order filter was necessary for the Paradigm Veo. The Paradigm Veo device exhibited greater delay variability with higher delay time and higher probability of abnormal sensor behaviors as compared to the SEVEN PLUS device. In both cases, the observed variability may have important clinical implications in postprandial performance. Therefore, further improvements are needed in calibration algorithms to reduce this variability.",
    "keywords": [
      "continuous glucose monitoring",
      "statistical modeling",
      "error analysis",
      "simulation",
      "modeling errors",
      "biomedical systems"
    ]
  },
  {
    "id": "1264",
    "title": "Spin Kostka polynomials",
    "abstract": "We introduce a spin analogue of Kostka polynomials and show that these polynomials enjoy favorable properties parallel to the Kostka polynomials. Further connections of spin Kostka polynomials with representation theory are established.",
    "keywords": [
      "kostka polynomials",
      "symmetric groups",
      "schur q-functions",
      "halllittlewood functions",
      "q-weight multiplicity",
      "heckeclifford algebra"
    ]
  },
  {
    "id": "1265",
    "title": "A state of the art on ADC modelling",
    "abstract": "The state of the art of the research on modelling of analog-to-digital converter (ADC)-based measuring devices is surveyed. Main topics of modelling are reviewed according to the fields of prevailing scientific interest in metrological research such as quantization models, error models, and correction-aimed models. In these fields, recent developments are analysed with the aim of focusing both the contemporary situation and the imminent trends.",
    "keywords": [
      "adc",
      "modelling",
      "error models",
      "quantization models",
      "error correction"
    ]
  },
  {
    "id": "1266",
    "title": "Efficient descriptor tree growing for fast action recognition",
    "abstract": "Our method significantly reduces the time complexity of a popular classifier. Our method achieves state-of-the-art classification accuracy. We present an efficient algorithm to organize the training data used by our method. An important parameter of the data organization algorithm is analyzed.",
    "keywords": [
      "action recognition",
      "nearest neighbor",
      "instance-to-class distance"
    ]
  },
  {
    "id": "1267",
    "title": "Glomerular microcircuits in the olfactory bulb",
    "abstract": "Microcircuits in the olfactory bulb have long received particular attention from both experimentalists and theoreticians, due in part to an abundance of dendrodendritic interactions and other specialized modifications to the canonical cortical circuit architecture. Recent experimental and theoretical results have elucidated the mechanisms and function of these circuits and their presumed contributions to olfactory stimulus processing and odor perception. We here review the architecture and functionality of a prominent olfactory bulb microcircuit: the glomerular network.",
    "keywords": [
      "olfaction",
      "computational modeling",
      "glomerulus",
      "decorrelation",
      "normalization"
    ]
  },
  {
    "id": "1268",
    "title": "An Euler-Monte Carlo algorithm assessing Moment Lyapunov Exponents for stochastic bridge flutter predictions",
    "abstract": "Wind-induced coupled flutter may lead to the collapse of cable-supported bridges. This study investigates several issues, associated with the assessment of bridge flutter. These include: influence of turbulence modeling, simulated by random perturbation to the span-wise correlation length of buffeting loads; effects of errors in the motion-induced or aeroelastic loads. The study makes use of random differential equations and numerically evaluates two-mode flutter by means of stochastic stability beyond mean squares through Moment Lyapunov Exponents. The development and implementation of a numerical algorithm are presented. Results, obtained for two simplified bridge examples, are discussed.  ",
    "keywords": [
      "long-span bridge aeroelasticity",
      "flutter",
      "uncertainty propagation",
      "turbulence",
      "stochastic stability",
      "moment lyapunov exponents"
    ]
  },
  {
    "id": "1269",
    "title": "CORM: A reference model for future computer networks",
    "abstract": "This paper acknowledges the need for revolutionary designs to devise the Future Internet by presenting a clean-slate Concern-Oriented Reference Model (CORM) for architecting future networks. CORM is derived in accordance to the FunctionBehaviorStructure engineering framework, conceiving computer networks as a distributed software-dependent complex system. CORM networks are designed along two main dimensions: a vertical dimension addressing structure and configuration of network building blocks; and a horizontal dimension addressing communication and interaction among the previously formulated building blocks. For each network dimension, CORM factors the design space into function, structure, and behavior, applying to each the principle of separation of concerns for further systematic decomposition. Perceiving the network as a complex system, CORM constructs the network recursively in a bottomup approach starting by the network building block, whose structure and behavior are inspired by an evolutionary bacterium cell. Hence, CORM is bio-inspired, it refutes the long-endorsed concept of layering, it accounts intrinsically for emergent behavior fostering network integrity and stability. We conjecture that networks designed according to CORM-based architectures can adapt and further evolve to better fit their contexts. To justify our conjecture, we derive and simulate a CORM-based architecture for ad hoc networks.",
    "keywords": [
      "complex adaptive systems",
      "computer network architecture",
      "computer network reference model",
      "protocol design"
    ]
  },
  {
    "id": "1270",
    "title": "agent programming via planning programs",
    "abstract": "We imagine agent \"planning\" programs as programs built from achievement and maintenance goals. Their executions require the ability to meet such goals while respecting the programs' control flow. The question then is: can we always guarantee the execution of such programs? In this paper, we define this novel planning-programming problem formally, and propose a sound, complete and optimal wrt computational complexity technique to actually generate a solution by appealing to recent results in LTL-based synthesis of reactive systems.",
    "keywords": [
      "ltl",
      "agent programming",
      "planning",
      "synthesis",
      "model checking"
    ]
  },
  {
    "id": "1271",
    "title": "Computational and experimental analyses on impact reduction when a humanoid robot lands on the ground",
    "abstract": "The purpose of this study is to reduce the impact force when a humanoid robot lands on the ground or the floor. First, the landing postures of a human subject were analyzed while measuring the impact forces when the subject lands on the floor. Through the experimentally observed relations between the landing postures and the impact forces, it was hypothesized that a human being may reduce the impact force by motion control of his or her center of gravity, for example, to lengthen the time of the landing motion. Then, a landing robot was developed, and an experiment to measure the impact forces was conducted to prove the hypothesis.",
    "keywords": [
      "humanoid robot",
      "impact reduction",
      "passive landing",
      "active landing",
      "center of gravity"
    ]
  },
  {
    "id": "1272",
    "title": "efficient editing of aged object textures",
    "abstract": "Real objects present an enormous amount of detail, including aging effects. Artists need an intuitive control when they iteratively review and redesign their work to achieve a specific aging effect pattern but physically based and empirical simulations rarely provide an appropriate control. Our motivation comes from simplifying the redesign step by providing appropriate tools. In our system the user interactively identifies aging effects in a source image or photograph. The user then designs a target aging mask presenting the wanted aging effects pattern. Our system then synthesizes the output texture within a few seconds using a texture synthesis approach adapted to aged object texture editing. Thus, the user can quickly redesign the aging mask to achieve better results or test new configurations.",
    "keywords": [
      "weathering",
      "aging effects",
      "realistic rendering",
      "deterioration",
      "texture",
      "blemishes",
      "texture synthesis"
    ]
  },
  {
    "id": "1273",
    "title": "Searching for Zimin patterns",
    "abstract": "In the area of pattern avoidability the central role is played by special words called Zimin patterns. The symbols of these patterns are treated as variables and the rank of the pattern is its number of variables. Zimin type of a word x is introduced here as the maximum rank of a Zimin pattern matching x. We show how to compute Zimin type of a word on-line in linear time. Consequently we get a quadratic time, linear-space algorithm for searching Zimin patterns in words. Then we demonstrate how the Zimin type of the length n prefix of the infinite Fibonacci word is related to the representation of n  in the Fibonacci numeration system. Using this relation, we prove that Zimin types of such prefixes and Zimin patterns inside them can be found in logarithmic time. Finally, we give some upper bounds on the function f(n,k) f ( n , k ) such that every k-ary word of length at least f(n,k) f ( n , k ) has a factor that matches the rank n Zimin pattern.",
    "keywords": [
      "zimin word",
      "unavoidable pattern",
      "on-line algorithm",
      "fibonacci word"
    ]
  },
  {
    "id": "1274",
    "title": "Local and Global Information Exchange for Enhancing Object Detection and Tracking",
    "abstract": "Object detection and tracking using visual sensors is a critical component of surveillance systems, which presents many challenges. This paper addresses the enhancement of object detection and tracking via the combination of multiple visual sensors. The enhancement method we introduce compensates for missed object detection based on the partial detection of objects by multiple visual sensors. When one detects an object or more visual sensors, the detected object's local positions transformed into a global object position. Local and global information exchange allows a missed local object's position to recover. However, the exchange of the information may degrade the detection and tracking performance by incorrectly recovering the local object position, which propagated by false object detection. Furthermore, local object positions corresponding to an identical object can transformed into nonequivalent global object positions because of detection uncertainty such as shadows or other artifacts. We improved the performance by preventing the propagation of false object detection. In addition, we present an evaluation method for the final global object position. The proposed method analyzed and evaluated using case studies.",
    "keywords": [
      "visual sensor",
      "localization",
      "distributed detection system",
      "sensor network",
      "data combination",
      "information exchange"
    ]
  },
  {
    "id": "1275",
    "title": "DtiStudio: Resource program for diffusion tensor computation and fiber bundle tracking",
    "abstract": "A versatile resource program was developed for diffusion tensor image (DTI) computation and fiber tracking. The software can read data formats from a variety of MR scanners. Tensor calculation is performed by solving an over-determined linear equation system using least square fitting. Various types of map data, such as tensor elements, eigenvalues, eigenvectors, diffusion anisotropy, diffusion constants, and color-coded orientations can be calculated. The results are visualized interactively in orthogonal views and in three-dimensional mode. Three-dimensional tract reconstruction is based on the Fiber Assignment by Continuous Tracking (FACT) algorithm and a brute-force reconstruction approach. To improve the time and memory efficiency, a rapid algorithm to perform the FACT is adopted. An index matrix for the fiber data is introduced to facilitate various types of fiber bundles selection based on approaches employing multiple regions of interest (ROIs). The program is developed using C++ and OpenGL on a Windows platform.",
    "keywords": [
      "diffusion tensor imaging",
      "fiber tractography",
      "magnetic resonance imaging ",
      "medical image visualization"
    ]
  },
  {
    "id": "1276",
    "title": "Impact of gate insulator on the dc and dynamic performance of AlGaN/GaN MIS-HEMTs",
    "abstract": "The impact of SiN gate insulator on the performance of AlGaN/GaN MIS-HEMTs is studied. Lower gate leakage and trapping effects are measured in PEALD SiN devices. Measurable threshold voltage shift is measured when positive gate bias is applied. Threshold voltage dynamic shift is well correlated with gate leakage current. Trapping effects studied are mainly due to electrons injected in the gate insulator.",
    "keywords": [
      "gate insulator",
      "gan",
      "mis-hemts",
      "trapping effects"
    ]
  },
  {
    "id": "1277",
    "title": "An ontology-based approach for the reconstruction and analysis of digital incidents timelines",
    "abstract": "Due to the democratisation of new technologies, computer forensics investigators have to deal with volumes of data which are becoming increasingly large and heterogeneous. Indeed, in a single machine, hundred of events occur per minute, produced and logged by the operating system and various software. Therefore, the identification of evidence, and more generally, the reconstruction of past events is a tedious and time-consuming task for the investigators. Our work aims at reconstructing and analysing automatically the events related to a digital incident, while respecting legal requirements. To tackle those three main problems (volume, heterogeneity and legal requirements), we identify seven necessary criteria that an efficient reconstruction tool must meet to address these challenges. This paper introduces an approach based on a three-layered ontology, called ORD2I, to represent any digital events. ORD2I is associated with a set of operators to analyse the resulting timeline and to ensure the reproducibility of the investigation.",
    "keywords": [
      "digital forensics",
      "event reconstruction",
      "forensic ontology",
      "knowledge extraction",
      "ontology population",
      "timeline analysis"
    ]
  },
  {
    "id": "1278",
    "title": "Exploring the relationship of a files history and its fault-proneness: An empirical method and its application to open source programs",
    "abstract": "The knowledge about particular characteristics of software that are indicators for defects is very valuable for testers because it helps them to focus the testing effort and to allocate their limited resources appropriately. In this paper, we explore the relationship between several historical characteristics of files and their defect count. For this purpose, we propose an empirical approach that uses statistical procedures and visual representations of the data in order to determine indicators for a files defect count. We apply this approach to nine open source Java projects across different versions. Only 4 of 9 programs show moderate correlations between a files defects in previous and in current releases in more than half of the analysed releases. In contrast to our expectations, the oldest files represent the most fault-prone files. Additionally, late changes correlate with a files defect count only partly. The number of changes, the number of distinct authors performing changes to a file as well as the files age are good indicators for a files defect count in all projects. Our results show that a softwares history is a good indicator for ist quality. We did not find one indicator that persists across all projects in an equal manner. Nevertheless, there are several indicators that show significant strong correlations in nearly all projects: DA (number of distinct authors) and FC (frequency of change). In practice, for each software, statistical analyses have to be performed in order to evaluate the best indicator(s) for a files defect count.",
    "keywords": [
      "empirical study",
      "software testing",
      "software history/evolution"
    ]
  },
  {
    "id": "1279",
    "title": "Motion constraint",
    "abstract": "In this paper, we propose a hybrid postural control approach taking advantage of data-driven and goal-oriented methods while overcoming their limitations. In particular, we take advantage of the latent space characterizing a given motion database. We introduce a motion constraint operating in the latent space to benefit from its much smaller dimension compared to the joint space. This allows its transparent integration into a Prioritized Inverse Kinematics framework. If its priority is high the constraint may restrict the solution to lie within the motion database space. We are more interested in the alternate case of an intermediate priority level that channels the postural control through a spatiotemporal pattern representative of the motion database while achieving a broader range of goals. We illustrate this concept with a sparse database of large range full-body reach motions.",
    "keywords": [
      "inverse kinematics",
      "motion editing",
      "posture control"
    ]
  },
  {
    "id": "1280",
    "title": "CPO semantics of timed interactive actor networks",
    "abstract": "We give a denotational framework for composing interactive components into closed or open systems and show how to adapt classical domain-theoretic approaches to open systems and to timed systems. For timed systems, prior approaches are based on temporal logics, automata theory, or metric spaces. In this paper, we base the semantics on a CPO with a prefix order, as has been done previously for untimed systems. We show that existence and uniqueness of behaviors are ensured by continuity with respect to this prefix order. Existence and uniqueness of behaviors, however, do not imply that a composition of components yields a useful behavior. The unique behavior could be empty or smaller than expected. We define liveness and show that appropriately defined causality conditions ensure liveness and freedom from Zeno conditions. In our formulation, causality does not require a metric and can embrace a wide variety of models of time.  ",
    "keywords": [
      "semantics",
      "cpos",
      "posets",
      "interaction",
      "actors",
      "agents",
      "timed systems",
      "process networks",
      "discrete events",
      "dataflow"
    ]
  },
  {
    "id": "1281",
    "title": "Analytical solution and entanglement swapping of a double JaynesCummings model in non-Markovian environments",
    "abstract": "Analytical solution and entanglement swapping of a double JaynesCummings model in non-Markovian environments are investigated by the time convolution less master equation method. We obtain the analytical solution of this model and discuss in detail the influence of atomcavity coupling, non-Markovian effect and initial state purity on entanglement dynamics. The results show that in the non-Markovian environments, the entanglement between two cavities can be swapped to other bipartite subsystems by interaction between an atom and its own cavity. Due to the dissipation of environment, the entanglements of all bipartite subsystems will eventually decay to zero when the atom couples weakly to its cavity and the non-Markovian effect is also weak. All bipartite subsystems can tend to steady entanglement states if and only if there is the strong atomcavity coupling or the strong non-Markovian effect. The steady state of the subsystem composed of an atom and its own cavity is independent on the purity, but the steady states of other bipartite subsystems are dependent on the purity.",
    "keywords": [
      "analytical solution",
      "entanglement swapping",
      "double jaynescummings model",
      "non-markovian environment"
    ]
  },
  {
    "id": "1282",
    "title": "New bin packing fast lower bounds",
    "abstract": "In this paper, we address the issue of computing fast lower bounds for the Bin Packing problem, i.e., bounds that have a computational complexity dominated by the complexity of ordering the items by non-increasing values of their volume. We introduce new classes of fast lower bounds with improved asymptotic worst-case performance compared to well-known results for similar computational effort. Experimental results on a large set of problem instances indicate that the proposed bounds reduce both the deviation from the optimum and the computational effort.",
    "keywords": [
      "bin packing",
      "lower bounds"
    ]
  },
  {
    "id": "1283",
    "title": "A Type-2 Fuzzy Ontology and Its Application to Personal Diabetic-Diet Recommendation",
    "abstract": "It has been widely pointed out that classical ontology is not sufficient to deal with imprecise and vague knowledge for some real-world applications like personal diabetic-diet recommendation. On the other hand, fuzzy ontology can effectively help to handle and process uncertain data and knowledge. This paper proposes a novel ontology model, which is based on interval type-2 fuzzy sets (T2FSs), called type-2 fuzzy ontology (T2FO), with applications to knowledge representation in the field of personal diabetic-diet recommendation. The T2FO is composed of 1) a type-2 fuzzy personal profile ontology (type-2 FPPO); 2) a type-2 fuzzy food ontology (type-2 FFO); and 3) a type-2 fuzzy-personal food ontology (type-2 FPFO). In addition, the paper also presents a T2FS-based intelligent diet-recommendation agent (IDRA), including 1) T2FS construction; 2) a T2FS-based personal ontology filter; 3) a T2FS-based fuzzy inference mechanism; 4) a T2FS-based diet-planning mechanism; 5) a T2FS-based menu-recommendation mechanism; and 6) a T2FS-based semantic-description mechanism. In the proposed approach, first, the domain experts plan the diet goal for the involved diabetes and create the nutrition facts of common Taiwanese food. Second, the involved diabetics are requested to routinely input eaten items. Third, the ontology-creating mechanism constructs a T2FO, including a type-2 FPPO, a type-2 FFO, and a set of type-2 FPFOs. Finally, the T2FS-based IDRA retrieves the built T2FO to recommend a personal diabetic meal plan. The experimental results show that the proposed approach can work effectively and that the menu can be provided as a reference for the involved diabetes after diet validation by domain experts.",
    "keywords": [
      "diabetes",
      "diet recommendation",
      "intelligent agents",
      "interval type-2 fuzzy sets ",
      "type-2 fuzzy ontology "
    ]
  },
  {
    "id": "1284",
    "title": "An advanced queueing model to analyze appointment-driven service systems",
    "abstract": "Many service systems are appointment-driven. In such systems, customers make an appointment and join an external queue (also referred to as the waiting list). At the appointed date, the customer arrives at the service facility and receives service. Important measures of interest include the size of the waiting list as well as the time spent in the waiting list. We develop a model to assess these performance measures. The model may be used to support strategic decisions concerning server capacity (e.g. how often should a server be online, how many customers should be served during each service session, etc.). The model is a vacation model that uses efficient algorithms and matrix analytical techniques to obtain waiting list performance measures.",
    "keywords": [
      "appointment system",
      "vacation model",
      "waiting list",
      "queueing system",
      "algorithm"
    ]
  },
  {
    "id": "1285",
    "title": "technique for eliminating irrelevant terms in term rewriting for annotated media retrieval",
    "abstract": "In this paper, we present an efficient term rewriting technique that computes a degree of term to domain relevance. The proposed method resolves the problems in ontology integrated concept search. Those problems are (i) Pre-defined concept classes in ontology are not relevant to users (no proper concept class for a target annotation has not found). (ii) Too many similar concept classes are provided to a user therefore, a user may fail to choose a correct semantic class for a target annotation (ordinary users are not an expert in concept classification). The method uses sense disambiguation task for finding relevant terms for a given domain. Sense disambiguation requires term-to-term similarity measurement and term frequency measurement. For fair modeling of not observed term frequencies, discounting and redistribution model is applied. The proposed method is a compliment to our previous work presented in [13][14]. Robustness of our method is demonstrated through human judgment test that shows our method allows prediction of precise term list (overall 75% of correct prediction) that are relevant to a given domain.",
    "keywords": [
      "precise",
      "measurement",
      "media",
      "task",
      "efficiency",
      "human",
      "concept",
      "relevance",
      "paper",
      "model",
      "user",
      "search",
      "robust",
      "predict",
      "ontologies",
      "classification",
      "method",
      "term rewriting",
      "users",
      "semantic",
      "test",
      "annotation",
      "retrieval",
      "similarity measure",
      "class",
      "integrability"
    ]
  },
  {
    "id": "1286",
    "title": "A mobility constraint model to infer sensor behaviour in forest fire risk monitoring",
    "abstract": "Wireless sensor networks (WSNs) play an important role in forest fire risk monitoring. Various applications are in operation. However, the use of mobile sensors in forest risk monitoring remains largely unexplored. Our research contributes to fill this gap by designing a model which abstracts mobility constraints within different types of contexts for the inference of mobile sensor behaviour. This behaviour is focused on achieving a suitable spatial coverage of the WSN when monitoring forest fire risk. The proposed mobility constraint model makes use of a Bayesian network approach and consists of three components: (1) a context typology describing different contexts in which a WSN monitors a dynamic phenomenon; (2) a context graph encoding probabilistic dependencies among variables of interest; and (3) contextual rules encoding expert knowledge and application requirements needed for the inference of sensor behaviour. As an illustration, the model is used to simulate the behaviour of a mobile WSN to obtain a suitable spatial coverage in low and high fire risk scenarios. It is shown that the implemented Bayesian network within the mobility constraint model can successfully infer behaviour such as sleeping sensors, moving sensors, or deploying more sensors to enhance spatial coverage. Furthermore, the mobility constraint model contributes towards mobile sensing in which the mobile sensor behaviour is driven by constraints on the state of the phenomenon and the sensing system.",
    "keywords": [
      "forest fire risk monitoring",
      "mobile wireless sensor network",
      "bayesian networks",
      "mobility constraints",
      "sensor behaviour",
      "spatial coverage"
    ]
  },
  {
    "id": "1287",
    "title": "VLSI architecture for real-time fractal image coding processors",
    "abstract": "This paper proposes an efficient architecture for fractal image coding processors. The proposed architecture achieves high-speed image coding comparable to conventional JPEG processing. This architecture achieves less than 33.3 msec fractal image compression coding against a 512 x 512 pixel image and enables full-motion fractal image coding. The circuit size of the proposed architecture design is comparable to those of JPEG processors and much smaller than those of previously proposed fractal processors.",
    "keywords": [
      "vlsi architecture",
      "image coding",
      "fractal compression"
    ]
  },
  {
    "id": "1288",
    "title": "New heuristics in feature selection for high dimensional data",
    "abstract": "Massive data sets have become common in many applications making the task of finding an optimum subset of attributes extremely difficult. Traditional feature selection techniques can be very inefficient in high dimensional data, especially when the subset evaluation is obtained through a learning algorithm. We describe a method based on the statistical significance of adding a feature from a ranked-list to the final subset. To measure individual feature, we propose a new simple and fast criterion based on the projections of data set elements onto each attribute.",
    "keywords": [
      "machine learning",
      "preprocessing",
      "feature ranking",
      "feature selection"
    ]
  },
  {
    "id": "1289",
    "title": "Clusterfile: a flexible physical layout parallel file system",
    "abstract": "This paper presents Clusterfile, a parallel file system that provides parallel file access on a cluster of computers. We introduce a file partitioning model that has been used in the design of Clusterfile. The model uses a data representation that is optimized for multidimensional array partitioning while allowing arbitrary partitions. The paper shows how the file model can be employed for file partitioning into both physical subfiles and logical views. We also present how the conversion between two partitions of the same file is implemented using a general memory redistribution algorithm. We show how we use the algorithm to optimize non-contiguous read and write operations. The experimental results include performance comparisons with the Parallel Virtual File System (PVFS) and an MPI-IO implementation for PVFS. ",
    "keywords": [
      "parallel file system",
      "parallel i/o",
      "physical and logical partitioning",
      "non-contiguous i/o",
      "data redistribution",
      "mapping functions"
    ]
  },
  {
    "id": "1290",
    "title": "latency effects on reachability in large-scale peer-to-peer networks",
    "abstract": "In this paper we study the latency effects introduced in large scale internet applications. In particular, we study the effects of heterogeneous latency on reachability in decentralized, distributed networks operating under flooding protocols. We show that the standard protocol mechanisms of time-to-live (TTL) and unique message identification (UID), used to govern flooding message transmissions, can combine to potentially devastating effect on the reachability of message broadcast. We call this combined effect short-circuiting , and we investigate consequences of this phenomenon. We show that in the worstcase, short-circuiting resulting from heterogeneous latencies can near-completely eliminate the reach of broadcast messages, even with an ability to place k &gne; 1 broadcast servers optimally. This dramatic negative effect on reachability shows that application protocol designs need to be sensitive to latency models. In addition, we show that short-circuiting can have a negative effect on well known approximation algorithms for maximizing reachability. We show that with respect to distance measures that account for short-circuiting, the standard k-center problem can not be approximated within n 1-e , unless P = NP . Our theoretical results suggest that it may be quite difficult to find near optimal solutions to certain internet server selection problems. We consider the significance of our results on a large-scale peer-to-peer searching application, that relies on related flooding protocols. We report measurements through experimental studies with both simulated networks and a large network application known as Gnutella. Our empirical results, using statistics obtained from both the simulations and real applications, support the conclusion that, on average, the real effects of short-circuiting are significant, but not devastating to the performance of an overall large-scale system.",
    "keywords": [
      "measurement",
      "network",
      "sensitive",
      "statistics",
      "applications",
      "approximation algorithms",
      "simulation",
      "peer-to-peer",
      "large-scale",
      "performance",
      "design",
      "standardization",
      "account",
      "timing",
      "model",
      "paper",
      "heterogeneity",
      "select",
      "distance",
      "place",
      "flood",
      "live",
      "systems",
      "experimentation",
      "latency",
      "optimality",
      "message",
      "broadcast",
      "identification",
      "operability",
      "support",
      "internet",
      "distributed",
      "decentralization",
      "effect",
      "server",
      "reachability"
    ]
  },
  {
    "id": "1291",
    "title": "INDIRECT QUANTUM CONTROL FOR FINITE-DIMENSIONAL COUPLED SYSTEMS",
    "abstract": "We present a new analysis on the quantum control for a quantum system coupled to a quantum probe. This analysis is based on the coherent control for the quantum system and a hypothesis that the probe can be prepared in specified initial states. The results show that a quantum system can be manipulated by probe state-dependent coherent control. In this sense, the present analysis provides a new control scheme which combines the coherent control and state preparation technology.",
    "keywords": [
      "quantum information",
      "quantum control"
    ]
  },
  {
    "id": "1292",
    "title": "Quantifying Success Factors for IT Projects-An Expert-Based Bayesian Model",
    "abstract": "Large investments are made annually to develop and maintain IT systems. Successful outcome of IT projects is therefore crucial for the economy. Yet, many IT projects fail completely or are delayed or over budget, or they end up with less functionality than planned. This article describes a Bayesian decision-support model. The model is based on expert elicited data from 51 experts. Using this model, the effect management decisions have upon projects can be estimated beforehand, thus providing decision support for the improvement of IT project performance.",
    "keywords": [
      "it project success factors",
      "decision support",
      "bayesian networks",
      "noisy-or",
      "expert elicitation"
    ]
  },
  {
    "id": "1293",
    "title": "progressive transmission of scientific data using biorthogonal wavelet transform",
    "abstract": "An important issue in scientific visualization systems is the management of data sets. Most data sets in scientific visualization, whether created by measurement or simulation, are usually voluminous. The goal of data management is to reduce the storage space and the access time of these data sets to speed up the visualization process. A new progressive transmission scheme using spline biorthogonal wavelet bases is proposed in this paper. By exploiting the properties of this set of wavelet bases, a fast algorithm involving only additions and subtractions is developed. Due to the multiresolutional nature of the wavelet transform, this scheme is compatible with hierarchical-structured rendering algorithms. The formula for reconstructing the functional values in a continuous volume space is given in a simple polynomial form. Lossless compression is possible, even when using floating-point numbers. This algorithm has been applied to data from a global ocean model. The lossless compression ratio is about 1.5:1. With a compression ratio of 50:1, the reconstructed data is still of good quality. Several other wavelet bases are compared with the spline biorthogonal wavelet bases. Finally, the reconstructed data is visualized using various algorithms and the results are demonstrated.",
    "keywords": [
      "measurement",
      "quality",
      "simulation",
      "polynomial",
      "floating point",
      "timing",
      "model",
      "access",
      "data management",
      "paper",
      "management",
      "hierarchic",
      "visualization",
      "structure",
      "systems",
      "values",
      "space",
      "rendering",
      "wavelet",
      "storage",
      "functional",
      "wavelet transform",
      "data",
      "process",
      "algorithm",
      "scientific visualiztion",
      "continuation",
      "compression",
      "global",
      "scheme"
    ]
  },
  {
    "id": "1294",
    "title": "bci control protocol pseudocode",
    "abstract": "Brain computer interface (BCI) is a device which allows the people to communicate without using their mouths or hands. The information about the subject's intention is issued by his brain and exists in his electroencephalogram (EEG), recorded from the scalp. In this paper a pseudo code of a protocol for BCIs control of three environment functions is described. The protocol is based on the phenomenal pattern of the mental task \"Hyperventilation\".",
    "keywords": [
      "power spectrum",
      "hyperventilation",
      "eeg analysis",
      "bci"
    ]
  },
  {
    "id": "1295",
    "title": "An effective quasi-human based heuristic for solving the rectangle packing problem",
    "abstract": "In this paper, we introduce an effective deterministic heuristic, Less Flexibility First, for solving the classical NP-complete rectangle packing problem. Many effective heuristics implemented for this problem are CPU-intensive and non-deterministic in nature. Others, including the polynomial approximation methodology [J. Assoc. Comput. Mach. 32 (1) (1985) 130] are too laborious for practical problem sizes. The technique we propose is inspired and developed by enhancing some rule-of-thumb guidelines resulting from the generation-long work experience of human professionals in ancient days. Although the Less Flexibility First heuristic is a deterministic algorithm, the results are very encouraging. This algorithm can consistently produce packing densities of around 99% on most randomly generated large examples. As compared with the recent results of a well known simulated annealing based Rectangle Packing (RP) algorithm [IEEE Trans. Computer-aided Design Integrated Circuits Systems 17 (1) (1998) 60], the results are much better both in less dead space2 (4% vs 6.7%) and much less CPU time (9.57 vs 331.78 seconds). Experimenting our heuristics on a public rectangle packing data set covering instances of 1697 rectangles, the average unpack ratio is quite satisfactory (0.92% for bounding boxes limited to be optimum and 2.68% for the completed packing), while most cases spend only a few minutes in CPU time.",
    "keywords": [
      "two-dimensional packing and cutting",
      "vlsi floor planning"
    ]
  },
  {
    "id": "1296",
    "title": "Hierarchical clustering of XML documents focused on structural components",
    "abstract": "Clustering XML documents by structure is the task of grouping them by common structural components. Hitherto, this has been accomplished by looking at the occurrence of one preestablished type of structural components in the structures of the XML documents. However, the a-priori chosen structural components may not be the most appropriate for effective clustering. Moreover, it is likely that the resulting clusters exhibit a certain extent of inner structural inhomogeneity, because of uncaught differences in the structures of the XML documents, due to further neglected forms of structural components. To overcome these limitations, a new hierarchical approach is proposed, that allows to consider (if necessary) multiple forms of structural components to isolate structurally-homogeneous clusters of XML documents. At each level of the resulting hierarchy, clusters are divided by considering some type of structural components (unaddressed at the preceding levels), that still differentiate the structures of the XML documents. Each cluster in the hierarchy is summarized through a novel technique, that provides a clear and differentiated understanding of its structural properties. A comparative evaluation over both real and synthetic XML data proves that the devised approach outperforms established competitors in effectiveness and scalability. Cluster summarization is also shown to be very representative.",
    "keywords": [
      "data mining",
      "semi-structured data and xml",
      "xml clustering",
      "xml transactional representation",
      "xml cluster summarization and representative"
    ]
  },
  {
    "id": "1297",
    "title": "On the Convergence of a Full Discretization Scheme for the Stochastic Navier-Stokes Equations",
    "abstract": "We mainly investigate the error approximation of stochastic Navier-Stokes equations driven by white noise. Herein the discretization about space is studied by finite element method, and in time it holds the backward Euler scheme. To obtain the optimal error estimations, the main error is divided into three parts. The proofs of the first two parts are based on the corresponding deterministic case. The third part which contains stochastic error is researched by the relevant norms and integrals.",
    "keywords": [
      "stochastic navier-stokes equations",
      "discrete scheme",
      "error approximation",
      "numerical analysis"
    ]
  },
  {
    "id": "1298",
    "title": "A robust adaptive beamformer with a blocking matrix using coefficient-constrained adaptive filters",
    "abstract": "This paper proposes a new robust adaptive beamformer applicable to microphone arrays. The proposed beamformer is a generalized sidelobe canceller (GSC) with a variable blocking matrix using coefficient-constrained adaptive filters (CCAFs). The CCAFs, whose common input signal is the output of a fixed beamformer, minimize leakage of the target signal into the interference path of the GSC. Each coefficient of the CCAFs is constrained to avoid mistracking. In the multiple-input canceller, leaky adaptive filters are used to decrease undesirable target-signal cancellation. The proposed beamformer can allow large look-direction error with almost no degradation in interference-reduction performance and can be implemented with a small number of microphones. The maximum allowable look-direction error can be specified by the user. Simulation results show that the proposed beamformer, when designed to allow about 20 degrees of look-direction error, can suppress interference by more than 17 dB.",
    "keywords": [
      "beamforming",
      "microphone array",
      "adaptive signal processing",
      "noise reduction"
    ]
  },
  {
    "id": "1299",
    "title": "Oil spill response planning with consideration of physicochemical evolution of the oil slick: A multiobjective optimization approach",
    "abstract": "This paper addresses the optimal planning of oil spill response operations under economic and responsive criteria, with consideration of oil weathering process. The economic criterion is measured by total cost, while the measure of responsiveness is the time span of the entire response operations. A bi-criterion, multiperiod mixed-integer linear programming (MILP) model is developed that simultaneously predicts the optimal time trajectories of oil volume and slick area, transportation profile, response resource utilization levels, cleanup schedule, and coastal protection plan. The MILP model integrates with the prediction of an oil weathering model that accounts for oil physicochemical properties, spilled amount, hydrodynamics, and weather conditions. The multi-objective optimization model is solved with the epsilon-constraint method and produces a Pareto optimal curve that reveals how the optimal total cost and response operations change under different specifications of responsiveness. We present two illustrative examples for oil spill incidents in the Gulf of Mexico and New England.  ",
    "keywords": [
      "planning",
      "oil spill response",
      "milp",
      "multi-objective optimization",
      "ode"
    ]
  },
  {
    "id": "1300",
    "title": "fast dual-vdd buffering based on interconnect prediction and sampling",
    "abstract": "This paper presents fast algorithms for power-optimal interconnect synthesis based on interconnect prediction and sampling considering dual V dd buffers. We present three pruning techniques including interconnect prediction based pruning ( pre-buffer slack pruning and predictive min-delay pruning ) and sampling ( 3D sampling ), of which 3D sampling is effective but the other two improve both efficiency and accuracy of sampling. We also show that the key to runtime reduction is to reduce the number of propagated options, while the sophisticated data-structures which have good amortized complexity do not necessarily reduce runtime. We obtain an empirically linear time algorithm with less than 1% of delay and power increase but over 50x speedup compared with the most efficient algorithm for dual V dd buffer insertion. In addition, we further enhance the power-optimal buffered tree construction by introducing routing grid reduction. We apply our speedup techniques to buffered tree construction algorithm. Experimental results show that we obtain over 100x speedup compared with the most efficient existing algorithms for dual V dd buffered tree construction.",
    "keywords": [
      "routing",
      "buffer insertion",
      "dual-v dd ",
      "low power",
      "interconnect"
    ]
  },
  {
    "id": "1301",
    "title": "resolution enhancement of facial image based on top-down learning",
    "abstract": "This paper proposes a new method of synthesizing a highresolution facial image from a low-resolution facial image based on top-down learning. A face is represented by a linear combination of prototypes of shape and texture. With the shape and texture information about the pixels in a given low-resolution facial image, we can estimate optimal coefficients for a linear combination of prototypes of shape and those of texture by solving least square minimization. Then high-resolution facial image can be synthesized by using the optimal coeffcients for linear combination of the high-resolution prototypes.The encouraging results of the proposed method show that our method can be used to increase the performance of the face recognition by applying our method to enhance the low-resolution facial images captured at surveillance systems.",
    "keywords": [
      "low-resolution",
      "image reconstruction",
      "pattern recognition",
      "face reconstruction",
      "image enhancement",
      "morphable face model",
      "face recognition"
    ]
  },
  {
    "id": "1302",
    "title": "Explicit error bounds for lazy reversible Markov chain Monte Carlo",
    "abstract": "We prove explicit, i.e., non-asymptotic, error bounds for Markov Chain Monte Carlo methods, Such as the Metropolis algorithm, The problem is to compute the expectation (or integral) off with respect to a measure pi which can be given by a density rho with respect to another measure. A straight simulation of the desired distribution by a random number generator is in general not possible. Thus it is reasonable to use Markov chain sampling with a burnin. We study such an algorithm and extend the analysis of Lovasz and Simonovits [L Lovasz, M. Simonovits, Random walks in a convex body and an improved Volume algorithm, Random Structures Algorithms 4 (4) (1993) 359-412] to obtain an explicit error bound.  ",
    "keywords": [
      "markov chain monte carlo",
      "metropolis algorithm",
      "conductance",
      "explicit error bounds",
      "burn-in",
      "ball walk",
      "reversible",
      "lazy"
    ]
  },
  {
    "id": "1303",
    "title": "Some numerical problems in discrete geometry",
    "abstract": "What is the smallest square which contains ten pairwise disjoint congruent open disks of unit diameter? It is conjectured that the minimum side is 3.373.... This paper proves that the side is at least 3.334.  ",
    "keywords": [
      "discrete geometry",
      "packing of disks",
      "lower bounds"
    ]
  },
  {
    "id": "1304",
    "title": "Urban activity pattern classification using topic models from online geo-location data",
    "abstract": "Geo-location data from social media has potential for activity-travel analysis. Identifies the limitations of geo-location data for activity-based modeling. Adopts a data-driven approach (topic model) for activity pattern classification. Extends the topic model to capture user-specific activity patterns. Extends to account for missing activities  a major issue of social media data.",
    "keywords": [
      "activity pattern classification",
      "activity-based modeling",
      "social computing",
      "location-based data",
      "big data",
      "social media",
      "topic modeling",
      "machine learning"
    ]
  },
  {
    "id": "1305",
    "title": "NMR and NQR study of Si-doped (6,0) zigzag single-walled aluminum nitride nanotube as n or P-semiconductors",
    "abstract": "Density functional theory (DFT) calculations were performed to investigate the electronic structure properties of pristine and Si-doped aluminum nitride nanotubes as n or P-semiconductors at the B3LYP/6-31G* level of theory in order to evaluate the influence of Si-doped in the (6,0) zigzag AlNNTs. We extended the DFT calculation to predict the electronic structure properties of Si-doped aluminum nitride nanotubes, which are very important for production of solid-state devices and other applications. To this aim, pristine and Si-doped AlNNT structures in two models (Si-N and Si-Al) were optimized, and then the electronic properties, the isotropic (CSI) and anisotropic (CSA) chemical shielding parameters for the sites of various Al-27 and N-14 atoms, NQR parameters for the sites of various of Al-27 and N-14 atoms, and quantum molecular descriptors were calculated in the optimized structures. The optimized structures, the electronic properties, NMR and NQR parameters, and quantum molecular descriptors for the Si-N and Si-Al models show that the Si-N model is a more reactive material than the pristine or Si-Al model.",
    "keywords": [
      "aluminum nitride nanotubes",
      "quantum molecular descriptors",
      "dft",
      "nmr",
      "nqr"
    ]
  },
  {
    "id": "1306",
    "title": "On Nakhleh's Metric for Reduced Phylogenetic Networks",
    "abstract": "We prove that Nakhleh's metric for reduced phylogenetic networks is also a metric on the classes of tree-child phylogenetic networks, semibinary tree-sibling time consistent phylogenetic networks, and multilabeled phylogenetic trees. We also prove that it separates distinguishable phylogenetic networks. In this way, it becomes the strongest dissimilarity measure for phylogenetic networks available so far. Furthermore, we propose a generalization of that metric that separates arbitrary phylogenetic networks.",
    "keywords": [
      "biology and genetics",
      "graph algorithms",
      "network problems"
    ]
  },
  {
    "id": "1307",
    "title": "The EVE approach: View synchronization in dynamic distributed environments",
    "abstract": "The construction and maintenance of data warehouses (views) in large-scale environments composed of numerous distributed and evolving information sources (ISs) such as the WWW has received great attention recently. Such environments are plagued with changing information because ISs tend to continuously evolve by modifying not only their content but also their query capabilities and interface and by joining or leaving the environment at any time. We are the first to introduce and address the problem of schema changes of ISs, while previous work in this area, such as incremental view maintenance, has mainly dealt with data changes at ISs. In this paper, we outline our solution approach to this challenging new problem of how to adapt views in such evolving environments. We identify a new view adaptation problem for view evolution in the context of ISs schema changes, which we call View Synchronization. We also outline the Evolvable View Environment (EVE) approach that we propose as framework for solving the view synchronization problem, along with our decisions concerning the key design issues surrounding EVE. The main contributions of this paper are: 1) we provide an E-SQL view definition language with which the view definer can direct the view evolution process, 2) we introduce a model for information source description which allows a large class of ISs to participate in our system dynamically, 3) we formally define what constitutes a legal view rewriting, 4) we develop replacement strategies for affected view components which are designed to meet the preferences expressed by E-SQL, 5) we prove the correctness of the replacement strategies, and 6) we provide a set of view synchronization algorithms based on those strategies. A prototype of our EVE system has successfully been built using Java, JDBC, Oracle, and MS Access.",
    "keywords": [
      "data warehouses",
      "view maintenance",
      "query rewriting",
      "view adaptation",
      "view synchronization",
      "view definition language distributed",
      "evolving information sources",
      "and source evolution."
    ]
  },
  {
    "id": "1308",
    "title": "Solving the likelihood equations",
    "abstract": "Given a model in algebraic statistics and data, the likelihood function is a rational function on a projective variety. Algebraic algorithms are presented for computing all critical points of this function, with the aim of identifying the local maxima in the probability simplex. Applications include models specified by rank conditions on matrices and the Jukes-Cantor models of phylogenetics. The maximum likelihood degree of a generic complete intersection is also determined.",
    "keywords": [
      "maximum likelihood",
      "maximum likelihood degree",
      "syzygies",
      "phylogenetic trees"
    ]
  },
  {
    "id": "1309",
    "title": "crossed lines",
    "abstract": "This paper provides a brief description of the interactive film Crossed Lines .",
    "keywords": [
      "telephone interface",
      "interactive story",
      "interactive film",
      "crossed lines",
      "interactive narrative",
      "interactive video",
      "interactive drama"
    ]
  },
  {
    "id": "1310",
    "title": "An improved approach to find membership functions and multiple minimum supports in fuzzy data mining",
    "abstract": "Fuzzy mining approaches have recently been discussed for deriving fuzzy knowledge. Since items may have their own characteristics, different minimum supports and membership functions may be specified for different items. In the past, we proposed a genetic-fuzzy data-mining algorithm for extracting minimum supports and membership functions for items from quantitative transactions. In that paper. minimum supports and membership functions of all items are encoded in a chromosome Such that it may be not easy to converge. In this paper, an enhanced approach is proposed, which processes the items in a divide-and-conquer strategy. The approach is called divide-and-conquer genetic-fuzzy mining algorithm for items with Multiple Minimum Supports (DCFMMS), and is designed for finding minimum supports, membership functions, and fuzzy association rules. Possible solutions are evaluated by their requirement satisfaction divided by their suitability of derived membership functions. The proposed GA framework maintains multiple populations, each for one item's minimum support and membership functions. The final best minimum supports and membership functions in all the populations are then gathered together to be used for mining fuzzy association rules. Experimental results also show the effectiveness of the proposed approach.  ",
    "keywords": [
      "data mining",
      "fuzzy set",
      "genetic algorithm",
      "genetic-fuzzy mining",
      "multiple minimum supports",
      "membership functions"
    ]
  },
  {
    "id": "1311",
    "title": "Encoding and Constructing 1-Nested Phylogenetic Networks with Trinets",
    "abstract": "Phylogenetic networks are a generalization of phylogenetic trees that are used in biology to represent reticulate or non-treelike evolution. Recently, several algorithms have been developed which aim to construct phylogenetic networks from biological data using triplets, i.e. binary phylogenetic trees on 3-element subsets of a given set of species. However, a fundamental problem with this approach is that the triplets displayed by a phylogenetic network do not necessarily uniquely determine or encode the network. Here we propose an alternative approach to encoding and constructing phylogenetic networks, which uses phylogenetic networks on 3-element subsets of a set, or trinets, rather than triplets. More specifically, we show that for a special, well-studied type of phylogenetic network called a 1-nested network, the trinets displayed by a 1-nested network always encode the network. We also present an efficient algorithm for deciding whether a dense set of trinets (i.e. one that contains a trinet on every 3-element subset of a set) can be displayed by a 1-nested network or not and, if so, constructs that network. In addition, we discuss some potential new directions that this new approach opens up for constructing and comparing phylogenetic networks.",
    "keywords": [
      "phylogenetic network",
      "triplets",
      "trinets",
      "reticulate evolution"
    ]
  },
  {
    "id": "1312",
    "title": "Automatic 3D model reconstruction of cutting tools from a single camera",
    "abstract": "Collision detection by machining simulation requires the 3D models of rotating cutters. However, the 3D models of a cutter and holder are not always available. In this paper, a new method is proposed to design an automatic vision-based 3D modeling system, which is able to quickly reconstruct the 3D model of a cutter and holder when they are installed onto the spindle. Only a single camera is mounted on the machine tool to capture the image of the rotating cutter and holder. By viewing the rotating cutter and holder as an object of surface of revolution, the contour of the imaged cutter and holder can be used to reconstruct the 3D model as a stack of circular cross-sections. Then the complete generating function of the cutter and holder can be recovered from the cross-sections. Finally, the 3D model of the cutter is built by rotating the generating function around the spindle axis. The effectiveness and accuracy of the proposed method are verified by experiments on-machine using 12 kinds of cutters and holders, which can satisfy the requirement of collision detection.  ",
    "keywords": [
      "cutting tools",
      "surface of revolution",
      "single view reconstruction"
    ]
  },
  {
    "id": "1313",
    "title": "Short-term planning and scheduling in multipurpose batch chemical plants: a multi-level approach",
    "abstract": "This paper deals with short-term planning/scheduling problems when the product demands are driven by customer orders. This type of problem is, in general difficult to be solved, once the orders are subject to changes in short periods. In this paper is proposed a multi-level decomposition procedure, containing at least two levels. At the planning level, demands are adjusted, a raw material delivery plan is defined and a capacity analysis is performed. Therefore, time windows for each operation are defined. At the scheduling level, an MILP model is developed. In order to reduce scheduling problem dimension, it is performed an analysis of equipment contention profile, enabling to model only situations where a decision variable is requested.  ",
    "keywords": [
      "short-term planning",
      "scheduling of batch plants",
      "capacity analysis"
    ]
  },
  {
    "id": "1314",
    "title": "Traveling salesman problem of segments",
    "abstract": "In this paper, we present a polynomial time approximation scheme (PTAS) for a variant of the traveling salesman problem (called segment TSP) in which a traveling salesman tour is sought to traverse a set of n epsilon-separated segments in two dimensional space. Our results are based on an interesting combinatorial result which bounds the total number of entry points in an optimal TSP tour and a generalization of Arora's technique(5) for Euclidean TSP (of a set of points). The randomized version of our algorithm takes O(n(2)(log n)(O(1/e2))) time to compute a (1 + epsilon)-approximation with probability greater than or equal to 1/2, and can be derandomized with an additional factor of O(n(2)).",
    "keywords": [
      "algorithm",
      "traveling salesman problem",
      "polynomial time approximation scheme"
    ]
  },
  {
    "id": "1315",
    "title": "Local meshing plane analysis as a source of information about the gear quality",
    "abstract": "In the paper the application of the local meshing plane concept is discussed and applied for detecting of tooth degradation due to fatigue, and for overall gear quality assessment. Knowing the kinematic properties of the machine (i.e. gear tooth numbers) it is possible to modify the diagnostic signal in such a manner that its fragments will be linked to different rotating parts. This allows for presentation of either raw or processed gearbox signal in a form of three dimensional map on the plane pinion teethgear teeth, called local meshing plane. The meshing plane in Cartesian coordinates z1z2 allows for precise location and assessment of gear faults in terms of meshing quality of consecutive tooth pairs. Although the method was applied to simulated signals generated by the gearbox model, similar results were obtained for the measurement signals recorded during the back-to-back test stand experiment. The described method could be used for assessing the manufacturing quality of gears, the assembly quality as well as for the gear failure evaluation during normal exploitation.",
    "keywords": [
      "gears",
      "gearbox diagnosis",
      "local meshing plane analysis",
      "nonstationary signal",
      "fault diagnosis",
      "gearbox modelling"
    ]
  },
  {
    "id": "1316",
    "title": "Improved Cross Authentication Model Based on Threshold Method in Wireless Network",
    "abstract": "A novel cross authentication model is proposed based on (t, n) threshold method in Layered and Grouped Structure (LGS) wireless multi-hop network, the issues of cross authentication and secure communication between nodes in different trusted domains are settled. With no CA, new scheme of cross certificate being issued and validated is designed, by which, the node identification can be authenticated in different trusted domains.",
    "keywords": [
      "wireless network",
      "cross authentication",
      "threshold method"
    ]
  },
  {
    "id": "1317",
    "title": "On optimal solution error covariances in variational data assimilation problems",
    "abstract": "The problem of variational data assimilation for a nonlinear evolution model is formulated as an optimal control problem to find unknown parameters such as distributed model coefficients or boundary conditions. The equation for the optimal solution error is derived through the errors of the input data (background and observation errors), and the optimal solution error covariance operator through the input data error covariance operators, respectively. The quasi-Newton BFGS algorithm is adapted to construct the covariance matrix of the optimal solution error using the inverse Hessian of an auxiliary data assimilation problem based on the tangent linear model constraints. Preconditioning is applied to reduce the number of iterations required by the BFGS algorithm to build a quasi-Newton approximation of the inverse Hessian. Numerical examples are presented for the one-dimensional convectiondiffusion model.",
    "keywords": [
      "variational data assimilation",
      "parameter estimation",
      "optimal solution error covariances",
      "hessian preconditioning"
    ]
  },
  {
    "id": "1318",
    "title": "Asymptotics for the number of walks in a Weyl chamber of type B",
    "abstract": "We consider lattice walks in confined to the region with fixed (but arbitrary) starting and end points. These walks are assumed to be such that their number can be counted using a reflection principle argument. The main results are asymptotic formulas for the total number of walks of length n with either a fixed or a free end point for a general class of walks as n tends to infinity. As applications, we find the asymptotics for the number of k-non-crossing tangled diagrams as well as asymptotics for two k-vicious walkers models subject to a wall restriction.  ",
    "keywords": [
      "lattice walks",
      "weyl chamber",
      "asymptotics",
      "determinants",
      "saddle point method"
    ]
  },
  {
    "id": "1319",
    "title": "efficient meta-level control in bounded-rational agents",
    "abstract": "An important issue for complex agents operating in open real-time environments is how to sequence execution and computation actions without consuming too many resources. An empirical approach to this meta-level control problem is presented. We show that explicit meta-level control leads to significant improvement in agent performance.",
    "keywords": [
      "action selection and planning",
      "agent architectures",
      "rational agency"
    ]
  },
  {
    "id": "1320",
    "title": "Tactile Synthesis and Perceptual Inverse Problems Seen from the Viewpoint of Contact Mechanics",
    "abstract": "A contact-mechanics analysis was used to explain a tactile illusion engendered by straining the fingertip skin tangentially in a progressive wave pattern resulting in the perception of a moving undulating surface. We derived the strain tensor field induced by a sinusoidal surface sliding on a finger as well as the field created by a tactile transducer array deforming the fingerpad skin by lateral traction. We found that the first field could be well approximated by the second. Our results have several implications. First, tactile displays using lateral skin deformation can generate tactile sensations similar to those using normal skin deformation. Second, a synthesis approach can achieve this result if some constraints on the design of tactile stimulators are met. Third, the mechanoreceptors embedded in the skin must respond to the deviatoric part of the strain tensor field and not to its volumetric part. Finally, many tactile stimuli might represent, for the brain, an inverse problem to be solved, such specific examples of \"tactile metameres\" are given.",
    "keywords": [
      "human factors",
      "theory",
      "tactile synthesis",
      "computational tactile perception",
      "tactile transducers arrays",
      "contact mechanics",
      "tactile sensing",
      "lateral skin deformation",
      "haptics"
    ]
  },
  {
    "id": "1321",
    "title": "A simplified variable step-size LMS algorithm for Fourier analysis and its statistical properties",
    "abstract": "A simplified VSS-LMS (SVSS-LMS) algorithm is proposed for Fourier analysis. Its performance is comparable to most of the seven typical VSS-LMS algorithms. In-depth performance analysis is provided for the proposed algorithm. Simulations are conducted to confirm the performance and the analytical results.",
    "keywords": [
      "adaptive fourier analyzer",
      "discrete fourier coefficient ",
      "variable step-size  lms",
      "convergence",
      "performance analysis"
    ]
  },
  {
    "id": "1322",
    "title": "Streamlining long latency instructions for seamlessly combined out-of-order and in-order execution",
    "abstract": "In the current day wide-issue processors, the size of the instruction scheduling window (also called Issue Queue (IQ)) is limited mainly by the hardware complexity to design the logic, and thus limits the number of instructions scanned every cycle to extract instruction level parallelism (ILP). To exacerbate the problems, instructions depending on long latency load operations continue to reside in the IQ until their source operands are ready. Thus, such delayed instructions block any new instructions from entering the IQ even if potentially they are ready for execution. The growing disparity in processor and memory speeds is further aggravating the delay in dislodging instructions from IQ. To alleviate the problem, in this paper we propose a novel technique to streamline instructions in separate buffers according to the chain of dependencies. Each instruction is streamlined behind a parent instruction while it waits for the source operand to be supplied by the long latency memory operations. These instructions are segregated from the IQ and thus the pressure on IQ is relieved which enables flow of potentially executable instructions in the pipeline. Our analysis of SPEC2000 programs reveals that instructions dependent on load cache misses or their dependents, typically have their first source operand ready within 515% of their total wait time in the IQ. Based on the observations, the long latency memory dependent instructions are streamlined into in-order buffers when their first operand is ready. In the proposed architecture, instructions from both the conventional IQ and the heads of the streamline buffers can be selected for execution, while the wakeup logic complexity remains same as in the conventional design. Our results show that the performance speedup of 32-entry IQ supplemented by 32 in-order buffers is 15.7% and 2% for FP and integer benchmark respectively, which is very much comparable to that of a conventional 64-entry IQ. A 64-entry IQ design can gain performance over a 32-entry IQ, albeit with a large overhead in circuit delay complexity of wakeup logic, while streamline buffers can gain performance over 32-entry IQ without any such overhead.",
    "keywords": [
      "wide-issue processors",
      "dynamic scheduling",
      "out-of-order processing",
      "long latency instructions",
      "streamline buffers"
    ]
  },
  {
    "id": "1323",
    "title": "On topological properties of the Choquet weak convergence of capacity functionals of random sets",
    "abstract": "In view of the recent interests in random sets in information technology, such as models for imprecise data in intelligent systems, morphological analysis in image processing, we present, in this paper, some contributions to the foundation of random set theory, namely, a complete study of topological properties of capacity functionals of random sets, generalizing weak convergence of probability measures. These results are useful for investigating the concept of Choquet weak convergence of capacity functionals leading to tractable criteria for convergence in distribution of random sets. The weak topology is defined on the space of all capacity functionals on R. We show that this topological space is separable and metrizable.  ",
    "keywords": [
      "random closed set",
      "capacity functional",
      "choquet integral",
      "choquet weak convergence"
    ]
  },
  {
    "id": "1324",
    "title": "Improving the characterization of the alternative hypothesis via minimum verification error training with applications to speaker verification",
    "abstract": "Speaker verification is usually formulated as a statistical hypothesis testing problem and solved by a log-likelihood ratio (LLR) test. A speaker verification system's performance is highly dependent on modeling the target speaker's voice (the null hypothesis) and characterizing non-target speakers voices (the alternative hypothesis). However, since the alternative hypothesis involves unknown impostors, it is usually difficult to characterize a priori. In this paper, we propose a framework to better characterize the alternative hypothesis with the goal of optimally distinguishing the target speaker from impostors. The proposed framework is built on a weighted arithmetic combination (WAC) or a weighted geometric combination (WGC) of useful information extracted from a set of pre-trained background models. The parameters associated with WAC or WGC are then optimized using two discriminative training methods, namely, the minimum verification error (MVE) training method and the proposed evolutionary MVE (EMVE) training method, such that both the false acceptance probability and the false rejection probability are minimized. Our experiment results show that the proposed framework outperforms conventional LLR-based approaches.",
    "keywords": [
      "genetic algorithm",
      "hypothesis testing",
      "log-likelihood ratio",
      "minimum verification error training",
      "speaker verification"
    ]
  },
  {
    "id": "1325",
    "title": "Derivation of a simplified relation for assessing aortic root pressure drop incorporating wall compliance",
    "abstract": "Aging and some pathologies such as arterial hypertension, diabetes, hyperglycemia, and hyperinsulinemia cause some geometrical and mechanical changes in the aortic valve microstructure which contribute to the development of aortic stenosis (AS). Because of the high rate of mortality and morbidity, assessing the impact and progression of this disease is essential. Systolic transvalvular pressure gradient (TPG) and the effective orifice area are commonly used to grade the severity of valvular dysfunction. In this study, a theoretical model of the transient viscous blood flow across the AS is derived by taking into account the aorta compliance. The derived relation of the new TPG is expressed in terms of clinically available surrogate variables (anatomical and hemodynamic data). The proposed relation includes empirical constants which need to be empirically determined. We used a numerical model including an anatomically 3D geometrical model of the aortic root including the sinuses of Valsalva for their identification. The relation was evaluated using clinical values of pressure drops for cases for which the modified Gorlin equation is problematic (low flow, low gradient AS).",
    "keywords": [
      "pressure gradient relation",
      "aortic stenosis",
      "aortic valve",
      "pathologies",
      "three-dimensional global model",
      "fluidstructure interaction"
    ]
  },
  {
    "id": "1326",
    "title": "application and statistic testing of the 5-adic summation-shrinking generator",
    "abstract": "The paper describes the software application of a scheme of stream cipher named N-adic Summation-Shrinking Generator (NSumSG). The statistic testing proves the pseudo-randomness of the output bits of the NSumSG and applicability of the NSumSG as a real stream cipher.",
    "keywords": [
      "pseudorandom generator",
      "statistic testing",
      "encryption and decryption"
    ]
  },
  {
    "id": "1327",
    "title": "Shared buffer implementations of signal processing systems using lifetime analysis techniques",
    "abstract": "There has been a proliferation of block-diagram environments fur specifying and prototyping digital signal processing (DSP) systems. These include tools from academia such as Ptolemy and commercial tools such as DSPCanvas from Angeles Design Systems, signal processing work system (SPW) from Cadence, and COSSAP from Synopsys, The block diagram languages used in these environments are usually based on dataflow semantics because various subsets of dataflow have proven to be good matches for expressing and modeling signal processing systems. In particular, synchronous dataflow (SDF) has been found to be a particularly good match for expressing multirate signal processing systems. One of the key problems that arises during synthesis from an SDF specification is scheduling. Past work on scheduling from SDF has focused on optimization of program memory and buffer memory under a model that did not exploit sharing opportunities. In this paper, we build on our previously developed analysis and optimization framework for looped schedules to formally tackle the problem of generating optimally compact schedules for SDF graphs. We develop techniques for computing these optimally compact schedules in a manner that also attempt to minimize buffering memory under the assumption that buffers will be shared. This results in schedules whose data memory usage is drastically lower than methods in the past have achieved. The method we use is that of lifetime analysis; we develop a model for buffer lifetimes in SDF graphs and develop scheduling algorithms that attempt to generate schedules that minimize the maximum number of live tokens under the particular buffer lifetime model. We develop several efficient algorithms for extracting the relevant lifetimes from the SDF schedule. We then use the well-known firstfit heuristic for packing arrays efficiently into memory. We report extensive experimental results on applying these techniques to several practical SDF systems and show improvements that average 50% over previous techniques, with some systems exhibiting up to an 53% improvement over previous techniques.",
    "keywords": [
      "block diagram compiler",
      "dsp software synthesis",
      "dynamic programming dynamic storage allocation",
      "lifetime analysis",
      "loop fusion",
      "memory allocation",
      "static scheduling",
      "synchronous dataflow",
      "weighted interval graph coloring"
    ]
  },
  {
    "id": "1328",
    "title": "Graphical User Interfaces for Business Information Systems",
    "abstract": "Proponents of computer graphics foresee them having a dramatic impact on decision maker productivity. The availability of inexpensive graphic computer technology permits organizations to rely extensively on graphics for standard information presentations. Despite the claims of the proponents, however, there is little substantial evidence linking the use of graphics with increased management productivity and/or higher quality decisions. Moreover, the use of powerful graphics capabilities by end users, unskilled in graphics design, presents the likelihood that information presentations will suffer rather than improve as computer generated graphics grow in popularity. This article discusses typical and atypical applications of computer graphics for presenting business information. Existing evidence relating the use of graphics with improvements in user productivity is discussed. Much of the article is focused on computer graphics design within the organization including: who should do design, the conceptual foundations of good graphics design, and a set of guidelines and cautions applicable to the design of quality graphics. The article concludes with a list of suggested research topics.",
    "keywords": [
      "user interface",
      "interface",
      "graphics",
      "human information processing",
      "spatial management",
      "icons",
      "color",
      "graphic design",
      "dual brain",
      "mis",
      "graphics research",
      "business charts"
    ]
  },
  {
    "id": "1329",
    "title": "a program stability measure",
    "abstract": "This paper contributes to the understanding of program structures in terms of its stability and reliability in a quantitative sense. Distinctions are made between the logical structure of a program and the information structure of a program. The general characteristics of a good program will not be discussed in this paper other than citing relevent references. The term stability is defined as the resistance to the amplification of changes that has been made to a given program. The information structure of a program is based on the sharing of information between the components of the program.",
    "keywords": [
      "sharing",
      "measurement",
      "understandability",
      "stability",
      "general",
      "structure",
      "paper",
      "informal",
      "reliability",
      "change",
      "component"
    ]
  },
  {
    "id": "1330",
    "title": "Real-time recognition of cyclic strings by one-way and two-way cellular automata",
    "abstract": "This paper discusses real-time language recognition by 1-dimensional one-way cellular automata (OCAs) and two-way cellular automata (CAs), focusing on limitations of the parallel computation power. To clarify the limitations, we investigate real-time recognition of cyclic strings of the form u(k) with u is an element of {0, 1}(+) and k greater than or equal to 2. We show a version of pumping lemma for recognizing cyclic strings by OCAs, which can be used for proving that several languages are not recognizable by OCAs in real time. The paper also discusses the real-time language recognition of CAs by prefix and postfix computation, in which every prefix or postfix of an input string is also accepted, if the prefix or postfix is in the language. It is shown that there are languages L subset of or equal to Sigma(+) such that L is not recognizable by OCA in real-time and the reversal of L and the concatenation LSigma* are recognizable by CA in real-time.",
    "keywords": [
      "cellular automata",
      "oca",
      "parallel language recognition",
      "pumping lemma",
      "prefix recognition"
    ]
  },
  {
    "id": "1331",
    "title": "Bayesian estimation of Dirichlet mixture model with variational inference",
    "abstract": "An analytically tractable solution for Bayesian estimation of the Dirichlet mixture model. Relative convexity of the multivariate log-inverse-gamma function is proved and utilized. The free energy function is approximated by a single lower-bound to guarantee convergence. The method outperforms the ML based method and the VI based method, moreover, it is comparable to the sampling based method. The performances are demonstrated with important multimedia signal processing applications.",
    "keywords": [
      "bayesian estimation",
      "variational inference",
      "extended factorized approximation",
      "relative convexity",
      "dirichlet distribution",
      "gamma prior",
      "mixture modeling",
      "lsf quantization",
      "multiview depth image enhancement"
    ]
  },
  {
    "id": "1332",
    "title": "assessing regret-based preference elicitation with the utpref recommendation system",
    "abstract": "Product recommendation and decision support systems must generally develop a model of user preferences by querying or otherwise interacting with a user. Recent approaches to elicitation using minimax regret have proven to be very powerful in simulation. In this work, we test both the effectiveness of regret-based elicitation, and user comprehension and acceptance of minimax regret in user studies. We report on a study involving 40 users interacting with the UTPref Recommendation System, which helps students navigate and find rental accommodation. UTPref maintains an explicit (but incomplete) generalized additive utility (GAI) model of user preferences, and uses minimax regret for recommendation. We assess the following general questions: How effective is regret-based elicitation in finding optimal or near-optimal products? Do users understand and accept the minimax regret criterion in practice? Do decision-theoretically valid queries for GAI models result in more accurate assessment than simpler, ad hoc queries? On the first two issues, we find that the minimax regret decision criterion is effective, understandable, and intuitively appealing. On the third issue, we find that simple, semantically ambiguous query types perform as well as more demanding, semantically valid queries for GAI models. We also assess the relative difficulty of specific query types.",
    "keywords": [
      "recommender systems",
      "utility elicitation",
      "user studies"
    ]
  },
  {
    "id": "1333",
    "title": "Fast algorithms for uniform semi-unification",
    "abstract": "Uniform semi-unification is a simple combination of matching and unification defined as follows: given two terms s and t, do there exist substitutions ? and ? such that s=tWe present two algorithms for this problem based on Huets unification closure method, one producing (possibly) non-principal solutions, and one producing principal solutions. For both we provide a precise analysis of correctness and asymptotic complexity. Under the uniform cost RAM model (counting assignment, comparison, and arithmetic operations as primitive) our first algorithm is asymptotically as fast as Huets method, O(n?(n)), where ? is the functional inverse of Ackermanns function. Under a model which counts assignments and comparisons of pointers, and arithmetic operations on bits, the cost is O(n2?(n)2). Producing principal solutions is more complex, however, and our second algorithm runs in O(n2?(n)2) and under these two models.",
    "keywords": [
      "semi-unification",
      "algorithms"
    ]
  },
  {
    "id": "1334",
    "title": "Star-free languages are Church-Rosser congruential",
    "abstract": "The class of Church-Rosser congruential languages has been introduced by McNaughton, Narendran, and Otto in 1988. A language L is Church-Rosser congruential (belongs to CRCL), if there is a finite, confluent, and length-reducing semi-Thue system S such that L is a finite union of congruence classes modulo S. To date, it is still open whether every regular language is in CRCL. In this paper, we show that every star-free language is in CRCL. In fact, we prove a stronger statement: for every star-free language L there exists a finite, confluent, and subword-reducing semi-Thue system S such that the total number of congruence classes modulo S is finite and such that L is a union of congruence classes modulo S. The construction turns out to be effective.  ",
    "keywords": [
      "string rewriting",
      "church-rosser system",
      "star-free language",
      "aperiodic monoid",
      "local divisor"
    ]
  },
  {
    "id": "1335",
    "title": "Bisection algorithms for approximating quadratic Bezier curves by G(1) are splines",
    "abstract": "To describe the tool path of a CNC machine, it is often necessary to approximate curves by G(1) arc splines with the number of are segments as small as possible. Ahn et al. have proposed an iterative algorithm for approximating quadratic Bezier curves by G(1) are splines with fewer are segments than the biarc method. This paper gives the formula of the upper bound for are segments used by their algorithm. Based on the formula, two kinds of bisection algorithms for approximating quadratic Bezier curves by G(1) arc splines are presented. Results of some examples illustrate their efficiency.  ",
    "keywords": [
      "cnc",
      "quadratic bezier curves",
      "arc splines",
      "bisection algorithms"
    ]
  },
  {
    "id": "1336",
    "title": "Cloud covering denoising through image fusion",
    "abstract": "This paper presents a solution to the cloud removal problem, based in a recently developed image fusion methodology consisting in applying a 1-D pseudo-Wigner distribution (PWD) transformation to the source images and on the use of a pixel-wise cloud model. Both features could also be interpreted as a denoising method centered in a pixel-level measure. Such procedure is able to process sequences of multi-temporal registered images affected with spatial-variant noise. The goal consists in providing a 2-D clean image, after removing the spatial-variant noise disturbing the set of multi-temporal registered source images. This is achieved by taking as reference a statistically parameterized model of a cloud prototype. Using this model, a pixel-wise measure of the noise degree of the source images can be calculated through their PWDs. This denoising procedure enables to choose the noise-free pixels from the set of given source images. The applicability of the method to the cloud removal paradigm is illustrated with different sets of artificial and natural cloudy or foggy images, partially occluded by clouds in different regions. Another advantage of the present approach is its reduced computational cost, once the 1-D case has been preferred instead of a full 2-D implementation of the PWD.",
    "keywords": [
      "wigner distribution",
      "image fusion",
      "image enhancement"
    ]
  },
  {
    "id": "1337",
    "title": "Neural network for the prediction and supplement of tidal record in Taichung Harbor, Taiwan",
    "abstract": "Accurate tidal prediction and supplement is an important task in determining constructions and human activities in coastal and oceanic areas. The harmonic tidal level is conventionally used to predict tide levels. However, determination of the tidal components using the spectral analysis requires a long-term tidal level record (more than one year [Handbook of coastal and ocean engineering 1 (1990) 534]). In addition, calculating the coefficients abbreviated of tide component using the least-squares method also requires a large database of tide measurements. This paper presents an application of the artificial neural network for predicting and supplementing the long-term tidal-level using the short term observed data. On site, tidal-level data at Taichung Harbor in Taiwan will be used to test the performance of the artificial neural network model. The results show that the tidal levels over a long duration can be efficiently predicted or supplemented using only a short-term tidal record.",
    "keywords": [
      "tidal prediction and supplement",
      "spectral analysis",
      "harmonic analysis",
      "artificial neural network"
    ]
  },
  {
    "id": "1338",
    "title": "Generalized River Crossing Problems",
    "abstract": "Three men, each with a sister, must cross a river using a boat that can carry only two people in such a way that a sister is never left in the company of another man if her brother is not present. This very famous problem appeared in the Latin book Problems to Sharpen the Young, one of the earliest collections of recreational mathematics. This paper considers a generalization of such river crossing problems and provides a new formulation that can treat wide variations. The main result is that, if there is no upper bound on the number of transportations (river crossings), a large class of subproblems can be solved in polynomial time even when the passenger capacity of the boat is arbitrarily large. The authors speculated this fact at FUN 2012. On the other hand, this paper also demonstrates that, if an upper bound on the number of transportations is given, the problem is NP-hard even when the boat capacity is three, although a large class of subproblems can be solved in polynomial time if the boat capacity is two.",
    "keywords": [
      "river crossing problems",
      "polynomial-time algorithms",
      "np-hard",
      "graphs",
      "edgeexpansion",
      "reachability",
      "hypercube"
    ]
  },
  {
    "id": "1339",
    "title": "Bag of words for semantic automatic medical image annotation",
    "abstract": "A medical report contains many elements such as medical images accompanied by text descriptions. We present in this paper a new approach for semantic automatic annotation of medical images. The proposed approach uses the bag of words model to represent the visual content of the medical image combined with text descriptors based on term frequencyinverse document frequency technique and reduced by latent semantic to extract the co-occurrence between text and visual terms. In a first phase, we are interested in indexing texts and extracting all relevant terms using a thesaurus containing medical subject headings and concepts. In a second phase, medical images are indexed while recovering areas of interest which are invariant to change in scale such as light and tilt. To annotate a new medical image, we use the bag of words model to recover the feature vector. Indeed, we use the vector space model to retrieve similar medical images from the training database. The computation of the relevance value of an image according to a query image is based on the cosine function. To evaluate the performance of our proposed approach, we present an experiment carried out on five types of radiological imaging. The results showed that our approach works efficiently, especially with more images taken from the radiology of the skull.",
    "keywords": [
      "automatic medical image annotation",
      "radiology",
      "information retrieval",
      "latent semantic",
      "bag of words",
      "feature detection"
    ]
  },
  {
    "id": "1340",
    "title": "PXR ligand classification model with SFED-weighted WHIM and CoMMA descriptors",
    "abstract": "Understanding which type of endogenous and exogenous compounds serve as agonists for the nuclear pregnane X receptor (PXR) would be valuable for drug discovery and development, because PXR regulates a large number of genes related to xenobiotic metabolism. Although several models have been proposed to classify human PXR activators and non-activators, models with better predictability are necessary for practical purposes in drug discovery. Grid-weighted holistic invariant molecular (G-WHIM) and comparative molecular moment analysis (G-CoMMA) type 3D descriptors that contain information about the solvation free energy of target molecules were developed. With these descriptors, prediction models built using decision tree (DT)-, support vector machine (SVM)-, and Kohonen neural network (KNN)-based models exhibited better predictability than previously proposed models. Solvation free energy density-weighted G-WHIM and G-CoMMA descriptors reveal new insights into PXR ligand classification, and incorporation with machine learning methods (DT, SVM, KNN) exhibits promising results, especially SVM and KNN. SVM- and KNN-based models exhibit accuracy around 0.90, and DT-based models exhibit accuracy around 0.8 for both the training and test sets.",
    "keywords": [
      "pregnane x receptor",
      "solvation free energy density model",
      "whim",
      "comma",
      "machine learning"
    ]
  },
  {
    "id": "1341",
    "title": "Graphical interactive generation of gravity and magnetic fields",
    "abstract": "This paper presents a MATLAB (R) - based geopotential field generator called GamField that constructs and visualizes subsurface sources in 3-D space and computes their gravity and magnetic effects. GamField also computes anomaly gradients and remanent magnetization effects. The user inputs Cartesian prisms along with their physical properties to fabricate subsurface sources. Examples illustrating the utility of GamField for synthetic anomaly generation of gravity and magnetic fields are shown. ftp://ftp.ingv.it/pub/alessandro.pignatelli/Pignatelli  ",
    "keywords": [
      "gravity anomaly computation",
      "magnetic anomaly computation",
      "matlab "
    ]
  },
  {
    "id": "1342",
    "title": "3D-freehand-pose initialization based on operator's cognitive behavioral models",
    "abstract": "Tracking, recognition and interaction based on 3D freehand are a part of our virtual assembly system, in which monocular camera is used to input online freehand videos and the hand pose tracker requires a reliable initial pose in the first frame. A novel approach to initializing 3D pose and position of freehand is put forward in this paper visualization of 3D hand model and modeling the operators' cognitive behaviors. Our approach is composed of three phases: hand posture recognition, coarse-tuning and fine-tuning. The operator moves his/her hand onto the to meet the needs of our virtual assembly system. The main contribution of this paper is that the three core techniques are for the first time integrated together, including human-computer interaction (HCI) in the process of initializing, projection of the 3D hand model in the period of coarse-tuning time. Then, the computer repeatedly fine-tunes the 3D hand model until the projection of the 3D hand model is completely superimposed onto the operator's hand image. We focus on exploring and modeling cognitive behavior of operator's hand upon which we design our initialization algorithm. Our research shows that cognitive behavioral models are not only beneficial to reducing cognitive loads for operators, because it makes the computers cater for the changes of the operators' hand poses, but also helpful to address high dimensionality of articulated 3D hand model. Our experimental results also show that the approach presented in this paper is easier, more pleasurable and satisfactory experience for the operators. Our initialization system has successfully been applied to our 3D freehand tracking system and a simulation virtual assembly system.",
    "keywords": [
      "3d freehand pose model",
      "features extraction",
      "initialization",
      "visualization",
      "cognitive behavioral models"
    ]
  },
  {
    "id": "1343",
    "title": "Package Fingerprints: A visual summary of package interface usage",
    "abstract": "Object-oriented languages such as Java, Smalltalk, and C++ structure their programs using packages. Maintainers of large systems need to understand how packages relate to each other, but this task is complex because packages often have multiple clients and play different roles (class container, code ownership, etc.). Several approaches have been proposed, among which the use of cohesion and coupling metrics. Such metrics help identify candidate packages for restructuring; however, they do not help maintainers actually understand the structure and interrelationships between packages. In this paper, we use pre-attentive processing as the basis for package visualization and see to what extent it could be used in package understanding. We present the Package Fingerprint, a 2D visualization of the references made to and from a package. The proposed visualization offers a semantically rich, but compact and zoomable views centered on packages. We focus on two views (incoming and outgoing references) that help users understand how the package under analysis is used by the system and how it uses the system. We applied these views on four large systems: Squeak, JBoss, Azureus, and ArgoUML. We obtained several interesting results, among which, the identification of a set of recurring visual patterns that help maintainers: (a) more easily identify the role of and the way a package is used within the system (e.g., the package under analysis provides a set of layered services), and (b) detect either problematic situations (e.g., a single package that groups together a large number of basic services) or opportunities for better package restructuring (e.g., removing cyclic dependencies among packages). The visualization generally scaled well and the detection of different patterns was always possible. The proposed visualizations and patterns proved to be useful in understanding and maintaining the different systems we addressed. To generalize to other contexts and systems, a real user study is required.",
    "keywords": [
      "software package",
      "visualization",
      "cohesion",
      "coupling"
    ]
  },
  {
    "id": "1344",
    "title": "Framework for integrated mechanical design automation",
    "abstract": "Engineering design is an iterative process with a fundamental need for the consistent management and propagation of product dependencies. Constraint-based design provides a unified framework to meet this critical need, but there are known issues due to the complexity of the problem within three-dimensional space. This paper presents results from a decade of research into graph theory and numerical solution techniques to address issues such as user comprehension and multiplicity of solutions. The proposed solution, Extended Variational Design Technology (VGX), utilizes an innovative Drag and Drop user interaction paradigm to improve comprehension performance, and usability. It is also demonstrated that VGX technology can provide common mathematical foundation to support flexible and integrated product design, assembly and analysis.",
    "keywords": [
      "three-dimensional constraint-based product design",
      "direct manipulation",
      "extended variational technology"
    ]
  },
  {
    "id": "1345",
    "title": "efficient multiple-click models in web search",
    "abstract": "Many tasks that leverage web search users' implicit feedback rely on a proper and unbiased interpretation of user clicks. Previous eye-tracking experiments and studies on explaining position-bias of user clicks provide a spectrum of hypotheses and models on how an average user examines and possibly clicks web documents returned by a search engine with respect to the submitted query. In this paper, we attempt to close the gap between previous work, which studied how to model a single click, and the reality that multiple clicks on web documents in a single result page are not uncommon. Specifically, we present two multiple-click models: the independent click model (ICM) which is reformulated from previous work, and the dependent click model (DCM) which takes into consideration dependencies between multiple clicks. Both models can be efficiently learned with linear time and space complexities. More importantly, they can be incrementally updated as new click logs flow in. These are well-demanded properties in reality. We systematically evaluate the two models on click logs obtained in July 2008 from a major commercial search engine. The data set, after preprocessing, contains over 110 thousand distinct queries and 8.8 million query sessions. Extensive experimental studies demonstrate the gain of modeling multiple clicks and their dependencies. Finally, we note that since our experimental setup does not rely on tweaking search result rankings, it can be easily adopted by future studies.",
    "keywords": [
      "statistical models",
      "paging",
      "efficiency",
      "implicit feedback",
      "web",
      "experience",
      "timing",
      "bias",
      "model",
      "paper",
      "evaluation",
      "user",
      "search",
      "posit",
      "eye tracking",
      "dependencies",
      "experimentation",
      "users",
      "search engine",
      "click log analysis",
      "space",
      "interpretation",
      "data",
      "flow",
      "demonstrate",
      "queries",
      "future",
      "web search",
      "documentation",
      "query"
    ]
  },
  {
    "id": "1346",
    "title": "Extracting role hierarchies from authentication data flows",
    "abstract": "Current Web-based applications use XML-based formats to represent user descriptions exchanged between parties of a distributed computation. This paper introduces some techniques for extracting frequent patterns of user attributes from this XML flow. Then, patterns are used for bottom-up construction of a role ontology, which is then passed to an ontology engineer for a final pruning. The result is a user ontology built by combining topdown and bottom-up approaches, supporting extraction and validation of knowledge about 'typical' users particularly suitable for fine grained access control applications.",
    "keywords": [
      "role hierarchies",
      "xml flow",
      "data flow"
    ]
  },
  {
    "id": "1347",
    "title": "Leveraging Zipf's Law for Traffic Offloading",
    "abstract": "Internet traffic has Zipf-like properties at multiple aggregation levels. These properties suggest the possibility of offloading most of the traffic from a complex controller (e.g., a software router) to a sitnple forwarder (e.g., a commodity switch), by letting the forwarder handle a very limited set of flows; the heavy hitters. As the volume of traffic from a set of flows is highly dynamic, maintaining a reliable set of heavy hitters over time is challenging. This is especially true when we face a volume limit in the non-offloaded traffic in combination with a constraint in the size of the heavy hitter set or its rate of change. We propose a set selection strategy that takes advantage of the properties of heavy hitters at different time scales. Based on real Internet traffic traces, we show that our strategy is able to offload most of the traffic while limiting the rate of change of the heavy hitter set, suggesting the feasibility of alternative router designs.",
    "keywords": [
      "heavy hitters",
      "software router",
      "software defined network",
      "design",
      "measurement",
      "performance"
    ]
  },
  {
    "id": "1348",
    "title": "Prediction of workpiece elastic deflections under cutting forces in turning",
    "abstract": "One of the problems faced in turning processes is the elastic deformation of the workpiece due to the cutting forces resulting in the actual depth of cut being different than the desirable one. In this paper, a cutting mechanism is described suggesting that the above problem results in an over-dimensioned part. Consequently, the problem of determining the workpiece elastic deflection is addressed from two different points of view. The first approach is based on solving the analytical equations of the elastic line, in discretized segments of the workpiece, by considering a stored modal energy formulation due to the cutting forces. Given the mechanical properties of the workpiece material, the geometry of the final part and the cutting force values, this numerical method can predict the elastic deflection. The whole approach is implemented through a Microsoft Excel workbook. The second approach involves the use of artificial neural networks (ANNs) in order to develop a model that can predict the dimensional deviation of the final part by correlating the cutting parameters and certain workpiece geometrical characteristics with the deviations of the depth of cut. These deviations are calculated with reference to final diameter values measured with precision micrometers or on a CMM. The verification of the numerical method and the development of the ANN model were based on data gathered from turning experiments conducted on a CNC lathe. The results support the proposed cutting mechanism. The numerical method qualitatively agrees with the experimental data while the ANN model is accurate and consistent in its predictions.",
    "keywords": [
      "elastic deflection",
      "cutting forces",
      "cnc turning",
      "elastic line",
      "artificial neural networks"
    ]
  },
  {
    "id": "1349",
    "title": "Adaptive response time control for metadata matching in information dissemination systems",
    "abstract": "Information dissemination is of increasing importance to our society. Existing work mainly focuses on delivering information from sources to sinks in a timely manner based on established subscriptions, with the assumption that those subscriptions are persistent. However, the bottleneck of many information dissemination systems is actually the matching process to continuously reevaluate such subscriptions between numerous sources and numerous sinks, in response to dynamically varying information attributes at runtime. In this paper, we propose an adaptive control architecture to meet the response time constraints on metadata matching in an example information dissemination system. Our adaptive controller features a rigorous design based on well-established control theory for guaranteed control accuracy and system stability. Furthermore, our controller can adapt to changes in the system model without reconfiguration and profiling. Empirical results on a physical testbed demonstrate that our controller has more accurate control and improved system quality of service than both an open-loop solution and a typical heuristic solution.",
    "keywords": [
      "information dissemination",
      "response time",
      "feedback control",
      "metadata matching",
      "adaptive control",
      "real-time systems"
    ]
  },
  {
    "id": "1350",
    "title": "Modelling analysis of human optic nerve fibre excitation based on experimental data",
    "abstract": "The aim of the study is to determine which of the existing myelinated mammalian nerve fibre models better fits experimental data resulting from electrical stimulation of the human optic nerve and from propagation velocity measured on primates. The macroscopic electric potential is computed in a 3D, inhomogeneous and anisotropic nerve model. The Chiu-Sweeney (CS) and the Schwarz-Wesselink (SW) membrane descriptions are then considered. Variations in parameters that are not well established (encapsulation-tissue thickness, nerve-fascicle conductivity, geometric and electrochemical fibre cable parameters) are taken into account Results demonstrate that the SW model predictions are in better agreement with the experimental data than those of the CS model, although thresholds are still too high. When channel densities are varied, the SW model turns out to be more robust than the CS model. For a suitable leakage channel density value (about 8% of the original one), the SW model predicts a conduction velocity of 11.4 ms(-1) and an excitation threshold of 0.055 mA (for 0.1 ms pulse duration), which is in very good agreement with experimental values (11 ms(-1) and 0.055 mA). Potassium current in the SW model is necessary for stability. Introduction of a potassium-like current can restore stability in the CS system.",
    "keywords": [
      "optic nerve",
      "human",
      "self-sizing spiral cuff electrode",
      "finite elements method",
      "fibre model"
    ]
  },
  {
    "id": "1351",
    "title": "Interactive examination of surface quality on car bodies",
    "abstract": "The use of reflection lines and specular high lights for the quality control of car body surfaces is an important issue in the development process of a car. The interactive examination is based on standard graphics tools such as the SceneViewer of OpenInventor which simulates reflection lines by using striped environment maps. The interpolation of texturing and shading values is critical and requires high quality meshes. Thus, element shape and size are essential. In this paper we present a new technique for the tessellation of trimmed surfaces. The result of the algorithm is a 2-manifold mesh with a low triangle count and a triangulation pattern which is best suited for the visualization.",
    "keywords": [
      "nurbs surfaces",
      "mesh generation",
      "reflection lines",
      "hardware supported quality control"
    ]
  },
  {
    "id": "1352",
    "title": "Design and simulation of a GaN/AlGaN quantum cascade laser for terahertz emission",
    "abstract": "Our study indicates that there are significant advantages in developing THz quantum cascade lasers (QCLs) with GaN-based quantum well (QW) structures. While the ultrafast longitudinal optical (LO) phonon scattering in AlGaN/GaN QWs can be used for the rapid depopulation of the lower laser state, the large LO-phonon energy (?90meV) can effectively reduce the thermal population of the lasing states. Our investigation shows that GaN-based THz QCLs can potentially be operating cw at room temperature.",
    "keywords": [
      "quantum cascade laser",
      "terahertz emission",
      "gallium nitride",
      "quantum wells"
    ]
  },
  {
    "id": "1353",
    "title": "A decision aid for intensity-modulated radiation-therapy plan selection in prostate cancer based on a prognostic Bayesian network and a Markov model",
    "abstract": "The prognosis of cancer patients treated with intensity-modulated radiation-therapy (IMRT) is inherently uncertain, depends on many decision variables, and requires that a physician balance competing objectives: maximum tumor control with minimal treatment complications. In order to better deal with the complex and multiple objective nature of the problem we have combined a prognostic probabilistic model with multi-attribute decision theory which incorporates patient preferences for outcomes. The response to IMRT for prostate cancer was modeled. A Bayesian network was used for prognosis for each treatment plan. Prognoses included predicting local tumor control, regional spread, distant metastases, and normal tissue complications resulting from treatment. A Markov model was constructed and used to calculate a quality-adjusted life-expectancy which aids in the multi-attribute decision process. Our method makes explicit the tradeoffs patients face between quality and quantity of life. This approach has advantages over current approaches because with our approach risks of health outcomes and patient preferences determine treatment decisions.",
    "keywords": [
      "bayesian network",
      "markov model",
      "multi-attribute decision theory",
      "radiotherapy",
      "prostate cancer"
    ]
  },
  {
    "id": "1354",
    "title": "to obtain orthogonal feature extraction using training data selection",
    "abstract": "Feature extraction is an effective tool in data mining and machine learning. Many feature extraction methods have been investigated recently. However, few methods can achieve orthogonal components. Non-orthogonal components distort the metric structure of original data space and contain reductant information. In this paper, we propose a feature extraction method, named as incremental orthogonal basis analysis (IOBA), to cope with the challenging endeavors. First, IOBA learns orthogonal components for original data, not only theoretically but also numerically. Second, an innovative way of training data selection is proposed. This selection scheme helps IOBA pick up numerically orthogonal components from training patterns. Third, by designing a self-adaptive threshold technique, no prior knowledge about the number of components is necessary to use IOBA. Moreover, without solving eigenvalue and eigenvector problems, IOBA not only saves large computing loads, but also avoids ill-conditioned problems. Results of experiments show the efficiency of the proposed IOBA.",
    "keywords": [
      "incremental learning",
      "orthogonal components"
    ]
  },
  {
    "id": "1355",
    "title": "top issues in providing successful undergraduate research experiences",
    "abstract": "Undergraduate research is becoming increasingly common in colleges and universities, and, to support this, there is a need to have best practices and forums for promoting exchange of ideas. In particular, a working group at a recent National Science Foundation (NSF) Computer and Information Science and Engineering (CISE) Research Experiences for Undergraduates (REU) sites PI's meeting identified four important issues in undergraduate research: 1) how to design a good research project, 2) how to prepare students for research, 3) how to measure outcomes of undergraduate research and 4) incentives for undergraduates to publish as result of their participation in research. The panelists have all served as PIs or Co-PIs on NSF REU projects in computing and have mentored many undergraduates in a large variety of research projects both in REU settings as well as during the regular academic year. They will each address one of the issues identified above, and share their expertise in addressing the issue, providing solid guidance to anyone interested in promoting undergraduate research. A significant amount of time will be set aside for audience participation and discussion.",
    "keywords": [
      "computing problems",
      "undergraduate research"
    ]
  },
  {
    "id": "1356",
    "title": "attaining soft real-time constraint and energy-efficiency in web servers",
    "abstract": "M/M/1 queues have been traditionally used to model several systems like phone calls at a call center, banking services and so on. However, recent studies showed that it does not model properly a web server system. In this paper we investigate the impact of this assumption in providing timeliness constraint in an energy-efficient web server. Although energy efficiency is a key issue, it should not be attained at the expense of a poor quality of service. The work proposed here describes a technique that uses queueing theory results to balance energy consumption and adequate application response times in heterogeneous CPU-intensive server clusters. Moreover, we investigate the I/O impact on a purely CPU-oriented energy saving strategy. This proposal shows that the assumption of a Poisson process is a good approximation to model a web server.",
    "keywords": [
      "quality of service",
      "web servers",
      "queueing theory",
      "energy saving"
    ]
  },
  {
    "id": "1357",
    "title": "cooperative sensing with transmit diversity based on randomized stbc in cr networks",
    "abstract": "In this paper, a cognitive radio (CR) network composed of K secondary users who cooperatively sense a channel using the k-out-of- K fusion rule to determine the presence of the primary user is studied. The sensing-throughput tradeoff problem is investigated in a realistic environment where both the sensing channels and reporting channels are characterized by fading channels. It is observed that taking into consideration the probability of reporting error in the CR network increases the sensing time and reduces the maximum average throughput of the secondary users. To mitigate the effect of the probability of reporting error, a transmit diversity based cooperative spectrum sensing method using randomized space-time block coding (RSTBC) is proposed. Simulations results show that the spatial diversity gain induced by RSTBC significantly decreases the sensing time and improves the throughput of the secondary users.",
    "keywords": [
      "cognitive radio networks",
      "optimization",
      "cooperative sensing",
      "sensing throughput tradeoff"
    ]
  },
  {
    "id": "1358",
    "title": "A methodology to design a fuzzy logic based supervision of Hybrid Renewable Energy Systems",
    "abstract": "Hybrid Renewable Energy Systems (HRES) are increasingly used to improve the grid integration of wind power generators. The goal of this work is to propose a methodology to design a fuzzy logic based supervision of this new kind of production unit. A graphical modeling tool is proposed to facilitate the analysis and the determination of fuzzy control algorithms adapted to complex hybrid systems. To explain this methodology, the association of wind generators, decentralized generators and storage systems are considered for the production of electrical power. The methodology is divided in six steps covering the design of a supervisor from the system work specifications to an optimized implementation of the control. The performance of this supervisor is shown with the help of simulations. Finally, the application of this methodology to the supervision of different topologies of HRES is also proposed to bring forward the systematic dimension of the approach.  ",
    "keywords": [
      "energy management system",
      "hybrid power integration",
      "power management",
      "renewable energy systems",
      "fuzzy logic supervision"
    ]
  },
  {
    "id": "1359",
    "title": "REPRESENTATIONAL TRAJECTORIES IN CONNECTIONIST LEARNING",
    "abstract": "The paper considers the problems involved in getting neural networks to learn about highly structured task domains. A central problem concerns the tendency of networks to learn only a set of shallow (non-generalizable) representations for the task, i.e., to 'miss' the deep organizing features of the domain. Various solutions are examined, including task specific network configuration and incremental learning. The latter strategy is the more attractive, since it holds out the promise of a task-independent solution to the problem. Once we see exactly how the solution works, however, it becomes clear that ft is limited to a special class of cases in which (1) statistically driven undersampling is (luckily) equivalent to task decomposition, and (2) the dangers of unlearning are somehow being minimized. The technique is suggestive nonetheless, for a variety of developmental factors may yield the functional equivalent of both statistical AND 'informed' undersampling in early learning.",
    "keywords": [
      "connectionism",
      "learning",
      "development",
      "recurrent networks",
      "unlearning",
      "catastrophic forgetting"
    ]
  },
  {
    "id": "1360",
    "title": "The challenge of non-ergodicity in network neuroscience",
    "abstract": "Ergodicity can be assumed when the structure of data is consistent across individuals and time. Neural network approaches do not frequently test for ergodicity in data which holds important consequences for data integration and intepretation. To demonstrate this problem, we present several network models in healthy and clinical samples where there exists considerable heterogeneity across individuals. We offer suggestions for the analysis, interpretation, and reporting of neural network data. The goal is to arrive at an understanding of the sources of non-ergodicity and approaches for valid network modeling in neuroscience.",
    "keywords": [
      "functional neuroimaging",
      "traumatic brain injury",
      "network modeling",
      "neuroscience"
    ]
  },
  {
    "id": "1361",
    "title": "Oppositional krill herd algorithm for optimal location of distributed generator in radial distribution system",
    "abstract": "This paper presents KH algorithm to solve renewable DG allocation problem. OBL strategy in integrated with KH algorithm to improve solution quality. Three renewable sources, namely biomass, wind and solar energy are used. OKH algorithm is implemented on small, medium and large radial system. Proposed OKH method outperforms other approaches.",
    "keywords": [
      "radial distribution system",
      "renewable distributed generators",
      "energy loss reduction",
      "krill herd algorithm",
      "oppositional krill herd algorithm"
    ]
  },
  {
    "id": "1362",
    "title": "SIFT Match Verification by Geometric Coding for Large-Scale Partial-Duplicate Web Image Search",
    "abstract": "Most large-scale image retrieval systems are based on the bag-of-visual-words model. However, the traditional bag-of-visual-words model does not capture the geometric context among local features in images well, which plays an important role in image retrieval. In order to fully explore geometric context of all visual words in images, efficient global geometric verification methods have been attracting lots of attention. Unfortunately, current existing methods on global geometric verification are either computationally expensive to ensure real-time response, or cannot handle rotation well. To solve the preceding problems, in this article, we propose a novel geometric coding algorithm, to encode the spatial context among local features for large-scale partial-duplicate Web image retrieval. Our geometric coding consists of geometric square coding and geometric fan coding, which describe the spatial relationships of SIFT features into three geo-maps for global verification to remove geometrically inconsistent SIFT matches. Our approach is not only computationally efficient, but also effective in detecting partial-duplicate images with rotation, scale changes, partial-occlusion, and background clutter. Experiments in partial-duplicate Web image search, using two datasets with one million Web images as distractors, reveal that our approach outperforms the baseline bag-of-visual-words approach even following a RANSAC verification in mean average precision. Besides, our approach achieves comparable performance to other state-of-the-art global geometric verification methods, for example, spatial coding scheme, but is more computationally efficient.",
    "keywords": [
      "algorithms",
      "experimentation",
      "verification",
      "image retrieval",
      "partial duplicate",
      "large scale",
      "rotation-invariant",
      "geometric square coding",
      "geometric fan coding"
    ]
  },
  {
    "id": "1363",
    "title": "MAD trees and distance-hereditary graphs",
    "abstract": "For a graph G with weight function w on the vertices, the total distance of G is the sum over all unordered pairs of vertices x and y of w(x)w(y) times the distance between x and y. A MAD tree of G is a spanning tree with minimum total distance. We develop a linear-time algorithm to find a MAD tree of a distance-hereditary graph; that is, those graphs where distances are preserved in every connected induced subgraph.",
    "keywords": [
      "distance",
      "spanning tree",
      "minimum average distance",
      "distance-hereditary graphs"
    ]
  },
  {
    "id": "1364",
    "title": "Global convergence of a reactiondiffusion predatorprey model with stage structure and nonlocal delays",
    "abstract": "In this paper, a LotkaVolterra type reactiondiffusion predatorprey model with stage structure for the prey and nonlocal delays due to gestation of the predator is investigated. In the case of a general domain, sufficient conditions are obtained for the global convergence of positive solutions of the proposed problem by using the energy function method. Numerical simulations are carried out to illustrate the main results.",
    "keywords": [
      "stage structure",
      "reactiondiffusion",
      "nonlocal delay",
      "steady-state solution",
      "global convergence"
    ]
  },
  {
    "id": "1365",
    "title": "Cartographic generalization of roads in a local and adaptive approach: A knowledge acquistion problem",
    "abstract": "This paper presents a local and adaptive approach to road generalization, where different algorithms may be successively applied to each part of a road. The specific problem addressed is how to acquire and formalize cartographic knowledge in order to guide the application of the algorithms during the process. Our approach requires toolboxes of algorithms to transform and analyse the data, as well as an engine to chain them together. First, we present the toolboxes used in our experiments for road generalization. Then, we present two different engines, as well as the knowledge-acquisition processes used to determine them. The first engine, named GALBE, is an empirically determined process, where the application of algorithms is mainly based on a single criterion: the coalescence. The second engine, which is more complex, uses multiple measures to describe the road. The choice of which algorithm to use given a particular set of measures is determined from examples using supervised learning techniques. Results obtained with both engines are presented.",
    "keywords": [
      "cartographic generalization",
      "knowledge acquistion",
      "line generalization",
      "machine learning"
    ]
  },
  {
    "id": "1366",
    "title": "Intelligent task planning and action selection of a mobile robot in a multi-agent system through a fuzzy neural network approach",
    "abstract": "This paper proposes an intelligent task planning and action selection mechanism for a mobile robot in a robot soccer system through a fuzzy neural network approach. The proposed fuzzy neural network system is developed through the two dimensional fuzzification of the soccer field. A five layer fuzzy neural network system is trained through error back propagation learning algorithm to impart a strategy based action selection. The action selection depends on the field configuration, and the emergence of a particular field configuration results from the game dynamics. Strategy of the robot changes when the configuration of the objects in the field changes. The proposed fuzzy neural network structure is flexible to accommodate all possible filed configurations. Simulation results indicate that the proposed approach is simple and has the capability in coordinating the multi-agent system through selection of sensible actions.",
    "keywords": [
      "robot soccer",
      "multi-agent system",
      "fuzzy sets",
      "fuzzy neural network",
      "task planning",
      "intelligent action selection"
    ]
  },
  {
    "id": "1367",
    "title": "exploiting hardware performance counters with flow and context sensitive profiling",
    "abstract": "A program profile attributes run-time costs to portions of a program's execution. Most profiling systems suffer from two major deficiencies: first, they only apportion simple metrics, such as execution frequency or elapsed time to static, syntactic units, such as procedures or statements; second, they aggressively reduce the volume of information collected and reported, although aggregation can hide striking differences in program behavior.This paper addresses both concerns by exploiting the hardware counters available in most modern processors and by incorporating two concepts from data flow analysis--flow and context sensitivity--to report more context for measurements. This paper extends our previous work on efficient path profiling to flow sensitive profiling, which associates hardware performance metrics with a path through a procedure. In addition, it describes a data structure, the calling context tree, that efficiently captures calling contexts for procedure-level measurements.Our measurements show that the SPEC95 benchmarks execute a small number (3--28) of hot paths that account for 9--98% of their L1 data cache misses. Moreover, these hot paths are concentrated in a few routines, which have complex dynamic behavior.",
    "keywords": [
      "measurement",
      "sensitive",
      "tree",
      "efficiency",
      "benchmark",
      "context-sensitivity",
      "association",
      "performance",
      "metrication",
      "data structures",
      "concept",
      "context",
      "account",
      "timing",
      "attributes",
      "paper",
      "informal",
      "performance metric",
      "processor",
      "data flow analysis",
      "systems",
      "data cache",
      "procedure",
      "dynamic",
      "aggregate",
      "behavior",
      "flow",
      "complexity",
      "hardware",
      "profiles"
    ]
  },
  {
    "id": "1368",
    "title": "Exact performance analytical model for spectrum allocation in flexible grid optical networks",
    "abstract": "We propose an exact model depicting the performance of spectrum allocation in FGON. We consider multi-class calls with both first-fit and random-fit policy. The interplay between classes of calls does affect the blocking performance. The fragmentation rate reaches its maximum value when the load is moderate. Performance is affected by the ratios of both loads and average residency times.",
    "keywords": [
      "flexible grid optical network",
      "communication system performance",
      "continuous-time markov model",
      "gaussseidel iteration"
    ]
  },
  {
    "id": "1369",
    "title": "MEP: A 3D PIC code for the simulation of the dynamics of a non-neutral plasma",
    "abstract": "The three-dimensional evolution of a pure electron plasma is studied by means of a newly developed particle-in-cell code which solves the drift-Poisson system where kinetic effects in the motion parallel to the magnetic field are taken into account. Different results relevant to the non-linear dynamics of trapped plasmas and low-energy electron beams are presented.",
    "keywords": [
      "Particle-in-cell",
      "Non-neutral plasmas",
      "Beams"
    ]
  },
  {
    "id": "1370",
    "title": "A branch-and-bound algorithm for the solution of chemical production scheduling MIP models using parallel computing",
    "abstract": "Exploiting multiple cores in a computer or grid of computers can reduce the time required to solve a mixed-integer programming (MIP) model. Here, we develop a parallel branch-and-bound algorithm for a chemical production scheduling problem using a discrete-time model. The algorithm consists of initialization, submission, branching, collection, bounding, and pruning steps. We branch by adding a constraint to bound the total number of times each task runs. Each subproblem is solved as an MIP on a single core of a computer, so that many sub problems can be solved simultaneously. Also, we propose an algorithm, executed at each node of our branch-and-bound tree, to improve the bounds on the number of times a task is run based on the current objective value. We present computational results for several instances to show the parallel algorithm with the proposed branching strategy can solve more challenging problems than simply using the default parallel option.  ",
    "keywords": [
      "chemical production scheduling",
      "mixed-integer programming",
      "parallel computing"
    ]
  },
  {
    "id": "1371",
    "title": "Joint multi-user interference and clipping noise cancellation in uplink MC-CDMA system",
    "abstract": "In this paper, an iterative method is proposed to jointly cancel multi-user interference and clipping noise in uplink Multi-Carrier Code Division Multiple Access (MC-CDMA) systems. Clipping is the simplest method to overcome high peak-to-average power ratio of multi-carrier signals but it makes the signals distorted. Reconstruction methods use non-distorted samples to reconstruct distorted samples in the receiver but multi-user interference causes the methods do not work properly because all the received samples are distorted due to clipping and interference and so there is no undistorted samples to be used in recovering clipped samples. On the other hand, multi-user interference cancellation methods do not work properly because clipping in the transmitter is a non-linear process and therefore this process increases the amount of multiple access interference (MAI). In this paper, we propose a joint MAI and clipping noise cancellation method. In the proposed iterative method in each iteration a MC-CDMA detection method accompany with a clipping and filtering process and then by using the received signal as reference signal, an approximation of error due to clipping and residual interference is computed. Simulation results show that the proposed method improves the performance of clipped MC-CDMA systems in uplink for synchronous/asynchronous links.",
    "keywords": [
      "signal reconstruction",
      "signal detection",
      "interference suppression",
      "iterative methods",
      "multi-user detection"
    ]
  },
  {
    "id": "1372",
    "title": "How bad is selfish routing",
    "abstract": "We consider the problem of routing traffic to optimize the performance of a congested network. We are given a network, a rate of traffic between each pair of nodes, and a latency function for each edge specifying the time needed to traverse the edge given its congestion; the objective is to route traffic such that the sum of all travel times-the total latency-is minimized. In many settings, it may be expensive or impossible to regulate network traffic so as to implement an optimal assignment of routes. In the absence of regulation by some central authority, we assume that each network user routes its traffic on the minimum-latency path available to it, given the network congestion caused by the other users. In general such a \"selfishly motivated\" assignment of traffic to paths will not minimize the total latency; hence, this lack of regulation carries the cost of decreased network performance. In this article, we quantify the degradation in network performance due to unregulated traffic. We prove that if the latency of each edge is a linear function of its congestion, then the total latency of the routes chosen by selfish network users is at most 4/3 times the minimum possible total latency (subject to the condition that all traffic must be routed). We also consider the more general setting in which edge latency functions are assumed only to be continuous and nondecreasing in the edge congestion. Here, the total latency of the routes chosen by unregulated selfish network users may be arbitrarily larger than the minimum possible total latency; however, we prove that it is no more than the total latency incurred by optimally routing twice as much traffic.",
    "keywords": [
      "algorithms",
      "economics",
      "theory",
      "braess's paradox",
      "nash equilibria",
      "network flow",
      "selfish routing"
    ]
  },
  {
    "id": "1373",
    "title": "Open Loop DPC Beamforming Effective for Multiuser MIMO Transmissions in FDD Systems",
    "abstract": "This paper proposes an open loop beamforming scheme for downlink multiuser multiple-input multiple-output (MIMO) transmissions in frequency division duplex (FDD). The proposed scheme uses the uplink direction of arrival (DOA) estimation, and then generates the beamforming weight such that the interference caused by the overlapping beams is removed by applying the dirty-paper coding (DPC) principle. The simulation results show that the proposed scheme provides the gain of 32.3% at minimum in terms of the spectral efficiency at the CDF of 50% compared to the conventional DOA based beamforming scheme. In addition, it is shown that the proposed scheme has superior performance to closed loop scheme with the limited feedback information.",
    "keywords": [
      "multiuser mimo",
      "fdd",
      "dpc",
      "beamforming",
      "doa"
    ]
  },
  {
    "id": "1374",
    "title": "analysis of cray-1s architecture",
    "abstract": "An analysis of the Cray-1S architecture based on dataflow graphs is presented. The approach consists of representing the components of a Cray-1S system as the nodes of a dataflow graph and the interconnections between the components as the arcs of the dataflow graph. The elapsed time and the resources used in a component are represented by the attributes of the node corresponding to the component. The resulting dataflow graph model is simulated to obtain timing statistics using as input a control stream that represents the instruction and data stream of the real computer system. The Cray-1S architecture is analyzed by conducting several experiments with the model. It is observed that the architecture is a well balanced one and performance improvements are hard to achieve without major changes. Significant improvement in performance is shown when parallel instruction issue is allowed with multiple CIP/LIPs in the architecture.",
    "keywords": [
      "statistics",
      "streams",
      "data streaming",
      "systems",
      "simulation",
      "input",
      "graph models",
      "parallel",
      "analysis",
      "architecture",
      "performance",
      "computation",
      "instruction",
      "graph",
      "interconnect",
      "experience",
      "timing",
      "attributes",
      "model",
      "dataflow",
      "control",
      "resource",
      "change",
      "component"
    ]
  },
  {
    "id": "1375",
    "title": "Robust color histogram descriptors for video segment retrieval and identification",
    "abstract": "Effective and efficient representation of color features of multiple video frames or pictures is an important yet challenging task for visual information management systems. Key frame-based methods to represent the color features of a group of frames (GoF) are highly dependent on the selection criterion of the representative frame(s), and may lead to unreliable results. In this paper, we present various histogram-based color descriptors to reliably capture and represent the color properties of multiple images or a GoF. One family of such descriptors, called alpha-trimmed average histograms, combine individual frame or image histograms using a specific filtering operation to generate robust color histograms that can eliminate the adverse effects of brightness/color variations, occlusion, and edit effects on the color representation. We show the efficacy of the alpha-trimmed average histograms for video segment retrieval applications, and illustrate how they consistently outperform key frame-based methods. Another color histogram descriptor that we introduce, called the intersection histogram, reflects the number of pixels of a given color that is common to all the frames in the GoF. We employ the intersection histogram to develop a fast and efficient algorithm for identification of the video segment to which a query frame belongs. The proposed color histogram descriptors have been included in the recently completed ISO standard MPEG-7 after extensive evaluation experiments.",
    "keywords": [
      "color descriptors",
      "image/video databases",
      "mpeg-7",
      "video segment retrieval"
    ]
  },
  {
    "id": "1376",
    "title": "Human walking motion synthesis with desired pace and stride length based on HSMM",
    "abstract": "This paper presents a new technique for automatically synthesizing human walking motion. In the technique, a set of fundamental motion units called motion primitives is defined and each primitive is modeled statistically from motion capture data using a hidden semi-Markov model (HSMM), which is a hidden Markov model (HMM) with explicit state duration probability distributions. The mean parameter for the probability distribution function of HSMM is assumed to be given by a function of factors that control the walking pace and stride length, and a training algorithm, called factor adaptive training, is derived based on the EM algorithm. A parameter generation algorithm from motion primitive HSMMs with given control factors is also described. Experimental results for generating walking motion are presented when the walking pace and stride length are changed. The results show that the proposing technique can generate smooth and realistic motion, which are not included in the motion capture data, without the need for smoothing or interpolation.",
    "keywords": [
      "motion synthesis",
      "walking motion",
      "hidden semi-markov model ",
      "hmm",
      "factor adaptive training",
      "motion primitives",
      "motion capture"
    ]
  },
  {
    "id": "1377",
    "title": "Effective top-k computation with term-proximity support",
    "abstract": "Modern web search engines are expected to return the top-k results efficiently. Although many dynamic index pruning strategies have been proposed for efficient top-k computation, most of them are prone to ignoring some especially important factors in ranking functions, such as term-proximity (the distance relationship between query terms in a document). In our recent work [Zhu, M., Shi, S., Li, M., & Wen, J. (2007). Effective top-k computation in retrieving structured documents with term-proximity support. In Proceedings of 16th CIKM conference (pp. 771780)], we demonstrated that, when term-proximity is incorporated into ranking functions, most existing index structures and top-k strategies become quite inefficient. To solve this problem, we built the inverted index based on web page structure and proposed the query processing strategies accordingly. The experimental results indicate that the proposed index structures and query processing strategies significantly improve the top-k efficiency. In this paper, we study the possibility of adopting additional techniques to further improve top-k computation efficiency. We propose a Proximity-Probe Heuristic to make our top-k algorithms more efficient. We also test the efficiency of our approaches on various settings (linear or non-linear ranking functions, exact or approximate top-k processing, etc.).",
    "keywords": [
      "top-k",
      "dynamic index pruning",
      "term-proximity",
      "document structure",
      "proximity-probe",
      "non-linear ranking function",
      "approximate top-k"
    ]
  },
  {
    "id": "1378",
    "title": "Multiphase segmentation using an implicit dual shape prior: Application to detection of left ventricle in cardiac MRI",
    "abstract": "Novel segmentation algorithm is presented incorporating shape prior. Segmentation algorithm is developed in a variational framework using level sets. The prior uses the anatomical shape similarity between endo- and epicardium. Their dissimilarity is modeled following a Gaussian distribution.",
    "keywords": [
      "segmentation",
      "shape analysis",
      "implicit shape prior",
      "level set function",
      "magnetic resonance imaging",
      "left ventricle",
      "cardiac disease"
    ]
  },
  {
    "id": "1379",
    "title": "Reflecting the choice and usage of communication tools in global software development projects with media synchronicity theory",
    "abstract": "Global software development (GSD) projects use a variety of communication tools, such as teleconferences, email, and instant messaging to overcome the challenges caused by distribution. The use of different tools implies different communication needs and practices within the project. Media synchronicity theory (MST) breaks communication down into two processes conveyance of information and convergence of understanding and communication media capabilities into five: immediacy of feedback, parallelism, symbol variety, rehearsability, and reprocessability. According to MST, media capabilities differ in support for conveyance and convergence, and for good performance, there should be match between media capabilities and communication process needed in a given task. In this paper, we present our qualitative study on communication in GSD. We interviewed 79 individuals from 12 GSD projects. We discuss which communication tools were used and how. We analyze the tool use and articulated rationale for choosing the tools for various tasks in distributed software development based on the two communicative processes and five media properties suggested by MST. We found evidence supporting the applicability of MST as an aid in selecting communication tools for GSD projects. ",
    "keywords": [
      "global software engineering",
      "communication tools",
      "media choice"
    ]
  },
  {
    "id": "1380",
    "title": "SCLDGM coded modulation for MIMO systems with spatial multiplexing and space-time block codes",
    "abstract": "We analyze Multiple-Input Multiple-Output (MIMO) coded modulation systems where either Bit-Interleaved Coded Modulation (BICM) with spatial multiplexing or concatenation of channel coding and Space-Time Block Codes (STBCs) is used at transmission, assuming iterative Turbo-like decoding at reception. We optimize Serially-Concatenated Low-Density Generator Matrix (SCLDGM) codes (a subclass of LDPC codes) for each system configuration, with the goal of assessing its ability to approach the capacity limits in either ergodic or quasi-static channels. Our focus is on three relevant STBCs: the Orthogonal Space-Time Block Codes (OSTBCs) for two transmit antennas (i.e., the Alamouti code), which enables optimum detection with low complexity; the Golden code, which provides a capacity increase with respect to the input constellation; and Linear Dispersion (LD) codes, which enable practical detection in asymmetrical antenna configurations (i.e., more transmit than receive antennas) for cases in which optimum detection is infeasible. We conclude that BICM without concatenation with STBCs is in general the best option, except for Alamouti-coded 2 x 1 and Golden-coded 2 x 2 MIMO systems. ",
    "keywords": [
      "ldgm codes",
      "scldgm codes",
      "stbc",
      "linear dispersion codes",
      "alamouti",
      "mimo"
    ]
  },
  {
    "id": "1381",
    "title": "Monitoring event flows and modelling scenarios for crisis prediction: Application to ethnic conflicts forecasting",
    "abstract": "We have developed an early warning prototype, based on a knowledge management approach, so as to carry out the online detection of crises. Experts, with the help of automatic tools, design an ontology describing domain-specific crisis eruption processes. Then a recognition engine performs model-based inference, in order to identify among the events feeding the system, typical sequences that might trigger a crisis. Crises are described in the ontology through the template technique which provides also mechanisms to assess the similarity between stored scenarios and event flows related to the monitored process. This technique takes into account imperfect knowledge and uncertainties: for instance, imprecise temporal constraints between events are represented by fuzzy sets.",
    "keywords": [
      "ontology",
      "fuzzy logic",
      "crisis prediction",
      "xml",
      "spatio-temporal modelling"
    ]
  },
  {
    "id": "1382",
    "title": "Feature-preserved morphing method for panel design",
    "abstract": "In order to design and manufacture the new products with less time and cost, the factories want to remodel based on the older products, it makes the feature-preserved morphing parameterization technology become a new subject and attract increasing attention. To meet this requirement, a feature-preserved morphing method for panel design under complex rigid constraints, abbreviated as FPM, is proposed in this paper. It can also be applied in the parameterization design and modeling for auto-body using digital models. The contrastive implementation results of the experiments conducted using CAD data by the methods of FPM and FFD (Free-Form Deformation) are presented separately, which demonstrate the applicability and validity of FPM in the field of feature-preserved parameterization morphing design.  ",
    "keywords": [
      "feature-preserved morphing",
      "parameterization design",
      "digital models",
      "cad data"
    ]
  },
  {
    "id": "1383",
    "title": "enhancing spark's contract checking facilities using symbolic execution",
    "abstract": "Spark, a subset of Ada for engineering safety and security-critical systems, is one of the best commercially available frameworks for formal-methods-supported development of critical software. Spark is designed for verification and includes a software contract language for specifying functional properties of procedures. Even though Spark and its static analysis components are beneficial and easy to use, its contract language is rarely used for stating properties beyond simple constraints on scalar values due to the burdens the associated tool support imposes on developers. Symbolic execution (SymExe) techniques have made significant strides in automating reasoning about deep semantic properties of source code. However, most work on SymExe has focused on bug-finding and test case generation as opposed to tasks that are more verification-oriented such as contract checking. In previous work we have presented: (a) SymExe techniques for checking software contracts in embedded critical systems, and (b) Bakar Kiasan, a tool that implements these techniques in an integrated development environment for Spark. In this paper, we give a detailed walk-through of Bakar Kiasan as it is applied to an industrial code base for an embedded security device. We illustrate how Bakar Kiasan provides significant increases in automation, usability, and functionality over existing Spark contract checking tools, and we present results from performance evaluations of its application to industrial examples.",
    "keywords": [
      "program analysis",
      "symbolic execution",
      "spark"
    ]
  },
  {
    "id": "1384",
    "title": "Factors affecting the stimulus artifact tail in surface-recorded somatosensory-evoked potentials",
    "abstract": "Surface-recorded somatosensory-evoked potentials (SEPs) are neural signals elicited by an external stimulus. In the case of electrically induced SEPs, the artifact generated by the stimulation process can severely distort the signal. In some cases, the artifact tail often lasts well into the initiation of the SEP making the determination of absolute latency very difficult. In this work, a new approach was taken to identify factors that affect the tail of the artifact. The methodology adopted was the development of a lumped electrical circuit model of the artifact generation process. While the modeling of the instrumentation hardware is relatively simple, this is not the case with tissue and electrode/skin interface effects. Consequently, this paper describes a novel tissue modeling approach that uses an autoregressive moving average (ARMA) parametric technique and an artificial neural network (ANN) to estimate tissue parameters from experimental data. This coupled with an estimation of the stimulation electrodeskin impedance completes the lumped circuit model. Simulink (The Mathworks Inc.) was used to evaluate the model under several different conditions. These results show that both the stimulation electrodeskin interface impedance and nature of the body tissue directly under the recording electrodes have a profound effect on the appearance of the stimulus artifact tail. This was verified by experimentally recorded data obtained from the median nerve using surface electrodes. Conclusions drawn from this work include that stimulation electrodes with low series capacitance should be used whenever possible to minimize the duration of the artifact tail.",
    "keywords": [
      "evoked potentials",
      "stimulus artifact",
      "tissue modeling",
      "instrumentation",
      "time series analysis"
    ]
  },
  {
    "id": "1385",
    "title": "OPTIMIZED SCALE-FREE NETWORKS AGAINST CASCADING FAILURES",
    "abstract": "According to the dynamical characteristics of the local redistribution of the load on a removal node, by the reconnection of the neighboring edge of the most vulnerable node, we propose an effective method to improve the network robustness against cascading failures. Under two constraints, i.e. keeping the degree of each node unchanged and fixing the total protective cost of a network, we investigate the efficiency of the swap method on scale-free networks and analyze the correlation between the optimized network and the Pearson correlation coefficient. We numerically show that effective swapping of the small part of connections can dramatically improve the network robust level against cascading failures and find that the optimized networks obtained by the swap method exhibit an extremely disassortative degree degree correlation, that is, the disassortativity decreases the robustness of the optimized network against cascading failures. While the extent of the disassortative mixing is decided by the parameters in the cascading model. In addition, we also compare the average path length and the diameter of the optimized and the original networks.",
    "keywords": [
      "cascading failure",
      "scale-free network",
      "attack strategy",
      "robustness"
    ]
  },
  {
    "id": "1386",
    "title": "regular approximation and bounded domains for size-change termination",
    "abstract": "Abstract The size-change principle devised by Lee, Jones and Ben-Amram, provides an effective method of determining program termination for recursive functions. It relies on a regular approximation to the call structure of the program, operates only over variables whose \"size\" is well-founded, and ignores the conditionals and return values in the program. The main contribution of our paper is twofold: firstly we improve size-change termination analysis by using better regular approximations to program flow, and secondly we extend the analysis beyond the original well-founded variables to include integer variables. In addition, we pay attention to program conditionals that are expressed by linear constraints and support the analysis of functions in which the return values are relevant to termination. Our analysis is entirely mechanical, exploits the decidability and expressive power of affine constraints and extends the set of programs that are size-change terminating.",
    "keywords": [
      "termination analysis",
      "affine size-change termination"
    ]
  },
  {
    "id": "1387",
    "title": "A compact split-step finite difference method for solving the nonlinear Schrdinger equations with constant and variable coefficients",
    "abstract": "We propose a compact split-step finite difference method to solve the nonlinear Schrdinger equations with constant and variable coefficients. This method improves the accuracy of split-step finite difference method by introducing a compact scheme for discretization of space variable while this improvement does not reduce the stability range and does not increase the computational cost. This method also preserves some conservation laws. Numerical tests are presented to confirm the theoretical results for the new numerical method by using the cubic nonlinear Schrdinger equation with constant and variable coefficients and GrossPitaevskii equation.",
    "keywords": [
      "nonlinear schrdinger equation ",
      "grosspitaevskii equation ",
      "operator splitting",
      "compact split-step finite difference method "
    ]
  },
  {
    "id": "1388",
    "title": "call for bags",
    "abstract": "Inspired by the theme of Everyday Creativity, we asked people across the world to post their old conference bag(s) to us so that they could be recycled into one-of-a-kind reusable conference bags. Each bag sent was then hand crafted and sculpted by up-and-coming British designer Sarah Atkinson. Unlike many consumer products that we buy today, each bag has its previous history attached.",
    "keywords": [
      "recycled",
      "history",
      "bag",
      "everyday creativity"
    ]
  },
  {
    "id": "1389",
    "title": "Exercising at home: Real-time interaction and experience sharing using avatars",
    "abstract": "This paper reports on the design of a vision-based exercise monitoring system. The system aims to promote well-being by making exercise sessions enjoyable experiences, either through real-time interaction and instructions proposed to the user, or via experience sharing or group gaming with peers in a virtual community. The use of avatars is explored as means of representation of the users exercise movements or appearance, and the system employs user-centric approaches in visual processing, behavior modeling via history data accumulation, and user feedback to learn the users appreciation. A preliminary user survey study has been conducted to explore the avatar appreciations across different types of social contexts.",
    "keywords": [
      "exercise monitor",
      "avatars",
      "pose analysis",
      "experience sharing",
      "social networks",
      "user appreciation"
    ]
  },
  {
    "id": "1390",
    "title": "Integrating learning styles and adaptive e-learning system: Current developments, problems and opportunities",
    "abstract": "Review on how learning styles were integrated into adaptive e-learning systems. FelderSilvermans is found to be the most popular theory that was applied. Explore online learning styles predictors and automatic classification methods. Examine different applications of learning styles in adaptive learning system. Recommendation and future research opportunities are proposed.",
    "keywords": [
      "learning styles",
      "adaptive learning system",
      "literature review",
      "e-learning",
      "it in education"
    ]
  },
  {
    "id": "1391",
    "title": "IPv6 Autoconfiguration for Hierarchical MANETs with Efficient Leader Election Algorithm",
    "abstract": "To connect a mobile ad hoe network (MANET) with an IP network and to carryout communication, ad hoc network node needs to be configured with unique IP adress. Dynamic host configuration protocol (DHCP) server autoconfigure nodes in wired networks. However, this cannot be applied to ad hoc network without introducing some changes in auto configuration mechanism, due to intrinsic properties (i.e., multi-hop, dynamic, and distributed nature) of the network. In this paper, we propose a scalable autoconfiguration scheme for MANETs with hierarchical topology consisting of leader and member nodes, by considering the global Internet connectivity with minimum overhead. In our proposed scheme, a joining node selects one of the pre-configured nodes for its duplicate address detection (DAD) operation. We reduce overhead and make our scheme scalable by eliminating the broadcast of DAD messages in the network. We also propose the group leader election algorithm, which takes into account the resources, density, and position information of a node to select a new leader. Our simulation results show that our proposed scheme is effective to reduce the overhead and is scalable. Also, it is shown that the proposed scheme provides an efficient method to heal the network after partitioning and merging by enhancing the role of bordering nodes in the group.",
    "keywords": [
      "address autoconfiguration",
      "duplicate address detection ",
      "leader election",
      "mobile ad hoc network "
    ]
  },
  {
    "id": "1392",
    "title": "The largest nonidentifiable outlier: a comparison of multivariate simultaneous outlier identification rules",
    "abstract": "The aim of detecting outliers in a multivariate sample can be pursued in different ways. We investigate here the performance of several simultaneous multivariate outlier identification rules based on robust estimators of location and scale. It has been shown that the use of estimators with high finite-sample breakdown point in such procedures yields a good behaviour with respect to the prevention of breakdown by the masking effect (Becker, Gather, 1999. J. Amer. Statist. Assoc. 94, 947955). In this article, we investigate by simulation, at which distance from the centre of an underlying model distribution outliers can be placed until certain simultaneous identification rules will detect them as outliers. We consider identification procedures based on the minimum volume ellipsoid, the minimum covariance determinant, and S-estimators.",
    "keywords": [
      "outliers",
      "high breakdown point procedures",
      "mve",
      "mcd",
      "robustness",
      "s-estimators"
    ]
  },
  {
    "id": "1393",
    "title": "A multiwavelet Galerkin boundary element method for the stationary Stokes problem in 3D",
    "abstract": "In this paper, a multiwavelet Galerkin boundary element method is presented for the fast solution of the stationary Stokes problem in three dimensions. Piecewise linear discontinuous multiwavelet bases are constructed on each patch of piecewise smooth surface individually, which allow easy and efficient evaluation of the matrix entries. Because of the use of the multiwavelets, the system matrix can be compressed to O(N) (N denotes the number of unknowns) nonzero entries without compromising the order of convergence as for the conventional Galerkin boundary element method. Numerical results of two test samples are given to demonstrate the availability of the present method.",
    "keywords": [
      "stokes problem",
      "multiwavelet",
      "galerkin boundary element method",
      "boundary integral equation",
      "matrix compression",
      "sparse matrix"
    ]
  },
  {
    "id": "1394",
    "title": "Site of metabolism prediction on cytochrome P450 2C9: a knowledge-based docking approach",
    "abstract": "A novel structure-based approach for site of metabolism prediction has been developed. This knowledge-based method consists of three steps: (1) generation of possible metabolites, (2) docking the predicted metabolites to the CYP binding site and (3) selection of the most probable metabolites based on their complementarity to the binding site. As a proof of concept we evaluated our method by using MetabolExpert for metabolite generation and Glide for docking into the binding site of the CYP2C9 crystal structure. Our method could identify the correct metabolite among the three best-ranked compounds in 69% of the cases. The predictive power of our knowledge-based method was compared to that achieved by substrate docking and two alternative literature approaches.",
    "keywords": [
      "site of metabolism",
      "prediction",
      "cyp 2c9",
      "docking",
      "expert system"
    ]
  },
  {
    "id": "1395",
    "title": "Intelligent dimensioning for mechanical parts based on feature extraction",
    "abstract": "This paper presents an intelligent dimensioning approach to generate dimensions for 2D drawing of mechanical parts automatically from their 3D part models based on feature extraction. The key issues include the strategies and methods for identifying dimension redundancy, recognizing dimensioning features, determining dimensioning scheme, assigning necessary dimensions to suitable views, and locating the dimensions in reasonable positions for each view using artificial intelligent technology. Based on the approach, a corresponding software prototype was developed. Finally, it is demonstrated, from an example of dimensioning a box-type part, that its dimensions were generated successfully using this intelligent dimensioning software prototype.",
    "keywords": [
      "intelligent cad",
      "dimensioning",
      "feature extraction",
      "expert system"
    ]
  },
  {
    "id": "1396",
    "title": "Mixture model analysis of DNA microarray images",
    "abstract": "In this paper, we propose a new methodology for analysis of microarray images. First, a new gridding algorithm is proposed for determining the individual spots and their borders. Then, a Gaussian mixture model (GMM) approach is presented for the analysis of the individual spot images. The main advantages of the proposed methodology are modeling flexibility and adaptability to the data, which are well-known strengths of GMM. The maximum likelihood and maximum a posteriori approaches are used to estimate the GMM parameters via the expectation maximization algorithm. The proposed approach has the ability to detect and compensate for artifacts that might occur in microarray images. This is accomplished by a model-based criterion that selects the number of the mixture components. We present numerical experiments with artificial and real data where we compare the proposed approach with previous ones and existing software tools for microarray image analysis and demonstrate its advantages.",
    "keywords": [
      "cross-validated likelihood",
      "dna microarray image analysis",
      "expectation-maximization algorithm",
      "gaussian mixture models",
      "markov random fields",
      "maximum a posteriori",
      "maximum likelihood",
      "microarray gridding"
    ]
  },
  {
    "id": "1397",
    "title": "Remarks on the future of AI: machines and communication",
    "abstract": "Artificial Intelligence will continue to flourish in many ways. In this article, we present a view that centers on communication. Genuine human-computer communication has been an ambitious goal of artificial intelligence for decades and bears great potential for its future, yet nature still remains the sole successful crafter of systems capable of this art. Observations of natural communication processes suggest a holistic view that unites different means and levels of communication. Patterns from various senses as well as vast amounts of contextual information are integrated to yield an approximate understanding of reality. Rather than focusing on individual aspects of high-level communication techniques, we advocate tracing this holistic approach to communication, and systematically increasing algorithmic, integrative communication capabilities.",
    "keywords": [
      "ai future",
      "communication",
      "holistic view",
      "living organisms"
    ]
  },
  {
    "id": "1398",
    "title": "Bone Remodeling",
    "abstract": "The skeleton is a metabolically active organ that undergoes continuous remodeling throughout life. Bone remodeling involves the removal of mineralized bone by osteoclasts followed by the formation of bone matrix through the osteoblasts that subsequently become mineralized. The remodeling cycle consists of three consecutive phase",
    "keywords": [
      "bone remodeling",
      "osteoblast",
      "osteoclast",
      "cytokines"
    ]
  },
  {
    "id": "1399",
    "title": "An Excel spreadsheet to recast analyses of garnet into end-member components, and a synopsis of the crystal chemistry of natural silicate garnets",
    "abstract": "A Microsoft Excel spreadsheet has been programmed that allows users to calculate with ease the molar proportions of garnet end-members from chemical analyses. Recent advances in the understanding of the crystal chemistry of natural garnets, especially of the Ti-bearing garnets, are used to evaluate 29 end-members (15 species and 14 hypothetical end-members) for each analysis. The amounts of Fe(2+) and Fe(3+) (and Mn(3+), if necessary) are Calculated by stoichiometric constraints if these quantities have not been measured. The input data can include: SiO(2), TiO(2), ZrO(2), SnO(2), Y(2)O(3), Al(2)O(3), SC(2)O(3), Cr(23), V(2)O(3), FeO, Fe(2)O(3), MnO, MgO, CaO, Na(2)O, H(2)O(+) and F. The spreadsheet can be used with large data sets (Lip to 100 analyses at a time), and is accompanied by results calculated for 470 garnet analyses taken from the literature. The spreadsheet employs a simple scoring algorithm to measure the quality of a garnet analysis. The propagation of error from the input chemical data to the Calculation of end-member proportions is also discussed briefly.  ",
    "keywords": [
      "crystal chemistry",
      "electron microprobe",
      "end-members",
      "excel",
      "garnet",
      "spreadsheet"
    ]
  },
  {
    "id": "1400",
    "title": "Automatically finding relevant citations for clinical guideline development",
    "abstract": "Automated citation finding can augment manual literature search. We built a gold standard with citations from 653 guideline recommendations. The query expansion method vs. PubMed improved recall with non-significant loss on precision. The unsupervised citation ranking approach performed better than standard PubMed ranking and a machine learning classifier.",
    "keywords": [
      "information retrieval",
      "pubmed",
      "practice guideline",
      "medical subject headings",
      "natural language processing"
    ]
  },
  {
    "id": "1401",
    "title": "State-Dependent Modulation of Feeding Behavior by Proopiomelanocortin-Derived ?-Endorphin",
    "abstract": "Feeding behavior can be divided into appetitive and consummatory phases, differing in neural substrates and effects of deprivation. Opioids play an important role in the appetitive aspects of feeding, but they also have acute stimulatory effects on food consumption. Because the opioid peptide ?-endorphin is co-synthesized and released with melanocortins from proopiomelanocortin (POMC) neuronal terminals, we examined the physiological role of ?-endorphin in feeding and energy homeostasis using a strain of mutant mice with a selective deficiency of ?-endorphin. Male ?-endorphin-deficient mice unexpectedly became obese with ad libitum access to rodent chow. Total body weight increased by 15% with a 50100% increase in the mass of white fat. The mice were hyperphagic with a normal metabolic rate. Despite the absence of endogenous ?-endorphin, the mutant mice did not differ from wild-type mice in their acute feeding responses to ?-endorphin or neuropeptide Y administered intracerebroventricularly or naloxone administered intraperitoneally. Additional mice were studied using an operant behavioral paradigm to examine their acquisition of food reinforcers under increasing work demands. Food-deprived, ?-endorphin-deficient male mice emitted the same number of lever presses under a progressive ratio schedule compared to wild-type mice. However, the mutant mice worked significantly less than did the wild-type mice for food reinforcers under nondeprived conditions. Controls for nonspecific effects on acquisition of conditioned learning, activity, satiety, and resistance to extinction revealed no genotype differences, supporting our interpretation that ?-endorphin selectively affects a motivational component of reward behavior under nondeprived conditions. Therefore, we propose that ?-endorphin may function in at least two primary modes to modulate feeding. In the appetitive phase, ?-endorphin release increases the incentive value of food as a primary reinforcer. In contrast, it appears that endogenous ?-endorphin may inhibit food consumption in parallel with melanocortins and that the orexigenic properties previously ascribed to it may actually be due to other classes of endogenous opioid peptides.",
    "keywords": [
      "?-endorphin",
      "opioid peptides",
      "operant conditioning",
      "deprivation state",
      "motivation",
      "reinforcer",
      "hyperphagia",
      "knockout mice",
      "metabolic rate",
      "sexual dimorphism"
    ]
  },
  {
    "id": "1402",
    "title": "On listing, sampling, and counting the chordal graphs with edge constraints",
    "abstract": "We discuss the problems to list, sample, and count the chordal graphs with edge constraints. The objects we look at are chordal graphs sandwiched by a given pair of graphs where we assume that at least one of the input graphs is chordal. The setting is a natural generalization of chordal completions and deletions. For the listing problem, we give an efficient algorithm running in polynomial time per output with polynomial space. As for the sampling problem, we give two clues that indicate that a random sampling is not easy. The first clue is that we show #P-completeness results for counting problems. The second clue is that we give an instance for which a natural Markov chain suffers from an exponential mixing time. These results provide a unified viewpoint from algorithms' theory to problems arising from various areas such as statistics, data mining, and numerical computation.  ",
    "keywords": [
      "graph sandwich",
      "chordal completion/deletion",
      "enumeration",
      "#p-completeness",
      "markov chain monte carlo"
    ]
  },
  {
    "id": "1403",
    "title": "A separation-based UI architecture with a DSL for role specialization",
    "abstract": "We propose a new DSL and separation-based architecture and development methodology. The approach separates front-end UI concerns from back-end implementation concerns. A climate science application illustrates the approach and verifies its validity. Evaluation of the approach includes comparative analysis and usability studies. The approach increases developer productivity and enhances UI design flexibility.",
    "keywords": [
      "domain specific language",
      "model driven engineering",
      "user experience"
    ]
  },
  {
    "id": "1404",
    "title": "Joint time-frequency domain cyclostationarity-based approach to blind estimation of OFDM transmission parameters",
    "abstract": "Blind estimation of the transmission parameters of orthogonal frequency division multiplexing (OFDM) signal is an important issue for various civilian and military applications. This paper presents a new cyclostationarity-based approach to blind estimation of OFDM signal parameters. Analytical expression for extracting transmission parameters from second-order cyclic cumulant (CC) of OFDM signal is derived first, with the consideration of the effects of time dispersion and consistent estimator errors on CC. The approach exploits the cyclostationarity of OFDM both in time delay domain and cyclic frequency domain; a statistic hypothesis testing framework was formulated to determine the threshold for detecting the presence of cycles of OFDM signal. The simulation results reveal that the proposed approach is robust for both previous synchronization and channel condition concurrently.",
    "keywords": [
      "ofdm",
      "blind parameter estimation",
      "cyclostationarity",
      "second-order cyclic cumulant"
    ]
  },
  {
    "id": "1405",
    "title": "Learning multiple languages in groups",
    "abstract": "We consider a variant of Gold's learning paradigm where a learner receives as input n different languages (in the form of one text where all input languages are interleaved). Our goal is to explore the situation when a more \"coarse\" classification of input languages is possible, whereas more refined classification is not. More specifically, we answer the following question: under which conditions, a learner, being fed n different languages, can produce m grammars covering all input languages, but cannot produce k grammars covering input languages for any k > m. We also consider a variant of this task, where each of the output grammars may not cover more than r input languages. Our main results indicate that the major factor affecting classification capabilities is the difference n - m between the number n of input languages and the number m of output grammars. We also explore the relationship between classification capabilities for smaller and larger groups of input languages. For the variant of our model with the upper bound on the number of languages allowed to be represented by one output grammar, for classes consisting of disjoint languages, we found complete picture of relationship between classification capabilities for different parameters it (the number of input languages), m (number of output grammars), and r (bound on the number of languages represented by each output grammar). This picture includes a combinatorial characterization of classification capabilities for the parameters n, m, r of certain types.  ",
    "keywords": [
      "computational learning theory",
      "learning in the limit",
      "classification",
      "learning multiple languages"
    ]
  },
  {
    "id": "1406",
    "title": "removing the memory limitations of sensor networks with flash-based virtual memory",
    "abstract": "Virtual memory has been successfully used in different domains to extend the amount of memory available to applications. We have adapted this mechanism to sensor networks, where, traditionally, RAM is a severely constrained resource. In this paper we show that the overhead of virtual memory can be significantly reduced with compile-time optimizations to make it usable in practice, even with the resource limitations present in sensor networks. Our approach, ViMem , creates an efficient memory layout based on variable access traces obtained from simulation tools. This layout is optimized to the memory access patterns of the application and to the specific properties of the sensor network hardware. Our implementation is based on TinyOS. It includes a pre-compiler for nesC code that translates virtual memory accesses into calls of ViMem's runtime component. ViMem uses flash memory as secondary storage. In order to evaluate our system we have modified nontrivial existing applications to make use of virtual memory. We show that its runtime overhead is small even for large data sizes.",
    "keywords": [
      "wireless sensor networks",
      "memory layout",
      "flash memory",
      "virtual memory"
    ]
  },
  {
    "id": "1407",
    "title": "The cable trench problem: combining the shortest path and minimum spanning tree problems",
    "abstract": "Let G=(V,E) be a connected graph with specified vertex v0?V, length l(e)?0 for each e?E, and positive parameters ? and ?. The cable-trench problem (CTP) is to find a spanning tree T such that ?l?(T)+?l?(T) is minimized where l?(T) is the total length of the spanning tree T and l?(T) is the total path length in T from v0 to all other vertices of V. Since all vertices must be connected to v0 and only edges from E are allowed, the solution will not be a Steiner tree. Consider the ratio R=?/?. For R large enough the solution will be a minimum spanning tree and for R small enough the solution will be a shortest path. In this paper, the CTP will be shown to be NP-complete. A mathematical formulation for the CTP will be provided for specific values of ? and ?. Also, a heuristic will be discussed that will solve the CTP for all values of R. Both the shortest path and the minimum spanning tree problems are universally discussed in operations research and management science textbooks. Since the late 1950s, efficient algorithms have been known for both of these problems. In this paper, the cable-trench problem is defined which combines the shortest path problem and the minimum spanning tree problem to create a problem that is shown to be NP-complete. In other words, two easy problems are combined to get a more realistic problem that is difficult to solve. Examples are used to illustrate an efficient and effective heuristic solution procedure for the cable-trench problem.",
    "keywords": [
      "minimum spanning tree",
      "shortest path",
      "networks",
      "graph theory",
      "np-completeness",
      "heuristics"
    ]
  },
  {
    "id": "1408",
    "title": "Computational efficiency and universality of timed P systems with membrane creation",
    "abstract": "P systems are a class of distributed parallel computing models inspired by the structure and the functioning of a living cell, where the execution of each rule is completed in exactly one time unit (a global clock is assumed). However, in living cells, the execution time of different biological processes is difficult to know precisely and very sensitive to environmental factors that might be hard to control. Inspired from this biological motivation, in this work, timed polarization P systems with membrane creation are introduced and their computational efficiency and universality are investigated. Specifically, we give a time-free semi-uniform solution to the SAT problem by a family of P systems with membrane creation in the sense that the correctness of the solution is irrelevant to the times associated with the involved rules. We also prove that time-free P systems with membrane creation are computationally universal.",
    "keywords": [
      "p system",
      "membrane creation",
      "time-free solution",
      "sat problem",
      "universality"
    ]
  },
  {
    "id": "1409",
    "title": "Improved Fourier analysis using parametric frequency-domain transfer-function estimators",
    "abstract": "Frequency components (or sinusoids) can be detected from noise-corrupted signals by means of FFT-based Fourier analysis techniques. The main drawbacks of FFT-based techniques are their poor frequency resolution and the presence of leakage errors. This paper shows that by means of frequency-domain transfer-function estimators it is possible to eliminate the leakage errors and to improve the resolution of the frequency estimates. The advantages of this approach are illustrated by means of both simulations and measurements performed on two applications on rotating machinery.",
    "keywords": [
      "parametric frequency domain technique",
      "transfer-function estimators",
      "rotating machinery"
    ]
  },
  {
    "id": "1410",
    "title": "Man in a garrulous silence",
    "abstract": "Mitten in 1982 for a closed meeting of scholars, provides an insight into the thinking of the time and in particular to the contributions of the author to systems and cybernetics. Illustrates the challenge of technology to humankind and sees the coming of a second Tower of Babel which is as counter-productive as the first. Believed it was necessary to consider practical strategies for change. Looks at the concept of human identity, considering \"Oneself: a systems viewpoint\", \"A Tripartite selfhood\" and \"The community consequence\" Provides a schematic framework for the discussion of the social cybernetics of the human condition. Considers a basic stance, and the application of rules detected by cybernetic scholarship. Outlines strategies for inducing change and examines new dimensions for planning. Emphasizes the importance of regarding planning as a matter of obtaining recognition of the frameworks that have been introduced and then facilitating choice.",
    "keywords": [
      "cybernetics",
      "culture ",
      "systems themy"
    ]
  },
  {
    "id": "1411",
    "title": "Motion compensated film restoration",
    "abstract": "Motion picture films are susceptible to local degradations such as dust spots. Other deteriorations are global such as intensity and spatial jitter. It is obvious that motion needs to be compensated for before the detection/correction of such local and dynamic defects. Therefore, we propose a hierarchical motion estimation method ideally suited for high resolution film sequences. This recursive block-based motion estimator relies on an adaptive search strategy and Radon projections to improve processing speed. The localization of dust particles then becomes straightforward. Thus, it is achieved by simple inter-frame differences between the current image and motion compensated successive and preceding frames. However, the detection of spatial and intensity jitter requires a specific process taking advantage of the high temporal correlation in the image sequence. In this paper, we present our motion compensation-based algorithms for removing dust spots, spatial and intensity jitter in degraded motion pictures. Experimental results are presented showing the usefulness of our motion estimator for film restoration at reasonable computational costs.",
    "keywords": [
      "motion estimation",
      "block matching",
      "motion picture restoration",
      "dust spot removal",
      "spatial and intensity jitter removal"
    ]
  },
  {
    "id": "1412",
    "title": "learning techniques in social and location-based service recommendation",
    "abstract": "As the mobile technologies advance, location-based services (LBS) become promising for improving people's daily lives. These services not only can identify users' activities at a location, but also can attain users' social activity networks. The context-aware information including location and social information of mobile users can help to analyze their common interests and current commercial intentions. This is especially helpful in determining the marketing strategy in a special region on behalf of the users. Upon the rapid development and deployment of mobile location-based services, various kinds of location-based and social data will be enormous. How to learn uses' interests and commercial intentions from the ubiquitously available data come to be essential for the innovation of location-based services. In this talk, we will explore related learning techniques to tackle the data mining problems associated with social and location-based service recommendation. Our talk includes the following topics: What kinds of learning techniques can be adopted and developed to solve the location-based and social data learning problems? What are the pros and cons for content-based learning techniques and collaborative filtering techniques? How to include location-based or social information into content-based learning techniques for effective data mining? How to incorporate location-based or social information into collaborative filtering techniques for effective data mining? How to perform recommendation based on the learning results? What are the future research issues and promising applications in this emerging area?",
    "keywords": [
      "location based service",
      "recommendation"
    ]
  },
  {
    "id": "1413",
    "title": "Stratified LMN-convergence tower spaces",
    "abstract": "We develop a general theory of convergence for lattice-valued spaces based on the concept of s-stratified LM-filters. For different choices of the frames L and M, different kinds of filters arise, and suitable choices of the lattice N allow to view many existing fuzzy and probabilistic convergence spaces as special examples of our stratified LMN-convergence tower spaces. The stratification requires a certain relation between the frames L and M and we use a so-called stratification mapping to this end. The stratification condition is used to show that the resulting category is fiber-small and hence Cartesian closedness is equivalent to the existence of natural function spaces. We give several examples for our spaces for different choices of the lattices L, M and N with a special focus on enriched LM-fuzzy topological spaces. Finally, we study diagonal axioms and regularity.",
    "keywords": [
      "l-topology",
      "l-convergence",
      "stratified lm-filter",
      "diagonal axioms",
      "regularity"
    ]
  },
  {
    "id": "1414",
    "title": "Comparison of relative efficiencies of sampling plans excluding certain neighboring units: a simulation study",
    "abstract": "Ecological and environmental studies frequently involve work in settings where some physical conditions influence the effectiveness of standard sampling plans. These studies may require expensive and time- consuming sampling processes. These conditions motivate researchers to find sampling plans that provide the highest precision at the lowest cost. Several sampling plans that exclude some types of 'neighboring' units are discussed in this article. Sampling of this type is called sampling excluding neighboring units and is a two- dimensional adaptation of balanced sampling excluding contiguous units [ Hedayat, A. S., Rao, C. R. and Stufken, J., 1988, Sampling designs excluding contiguous units. Journal of Statistical Planning and Inference, 19, 159 - 170]. Key features of this article are: ( i) the construction of sampling plans designed to yield representative samples by avoiding the simultaneous selection of units that are, in some sense, neighbors; ( ii) a simulation study which compares the relative efficiencies of these sampling plans on the basis of different correlations and sample sizes. In this study, we assume that correlations ( or correlated errors) between units decrease as a function of the distance between units. The construction of a sampling plan is illustrated using actual data from a 1998 field study, which examined the insect species assemblage in a beech- maple forest.",
    "keywords": [
      "spatial correlation",
      "contiguous units"
    ]
  },
  {
    "id": "1415",
    "title": "adaptive nearest neighbor search for relevance feedback in large image databases",
    "abstract": "Relevance feedback is often used in refining similarity retrievals in image and video databases. Typically this involves modification to the similarity metrics based on the user feedback and recomputing a set of nearest neighbors using the modified similarity values. Such nearest neighbor computations are expensive given that typical image features, such as color and texture, are represented in high dimensional spaces. Search complexity is a ciritcal issue while dealing with large databases and this issue has not received much attention in relevance feedback research. Most of the current methods report results on very small data sets, of the order of few thousand items, where a sequential (and hence exhaustive search) is practical. The main contribution of this paper is a novel algorithm for adaptive nearest neigbor computations for high dimensional feature vectors and when the number of items in the databse is large. The proposed method exploits the correlations between two consecutive nearest neighbor searches when the underlying similarity metric is changing, and filters out a significant number of candidates ina two stage search and retrieval process, thus reducing the number of I/O accesses to the database. Detailed experimental results are provided using a set of about 700,000 images. Comparision to the existing method shows an order of magnitude overall imporovement.",
    "keywords": [
      "order",
      "video",
      "nearest neighbor",
      "color",
      "nearest neighbor search",
      "metrication",
      "correlation",
      "similarity retrieval",
      "imaging",
      "paper",
      "filtering",
      "practical",
      "search",
      "image database",
      "attention",
      "user feedback",
      "research",
      "vectorization",
      "adapt",
      "method",
      "experimentation",
      "values",
      "exploit",
      "space",
      "retrieval",
      "process",
      "data",
      "image",
      "similarity",
      "complexity",
      "algorithm",
      "refine",
      "feature",
      "relevance feedback",
      "database"
    ]
  },
  {
    "id": "1416",
    "title": "Persuasive Cued Click-Points: Design, Implementation, and Evaluation of a Knowledge-Based Authentication Mechanism",
    "abstract": "This paper presents an integrated evaluation of the Persuasive Cued Click-Points graphical password scheme, including usability and security evaluations, and implementation considerations. An important usability goal for knowledge-based authentication systems is to support users in selecting passwords of higher security, in the sense of being from an expanded effective security space. We use persuasion to influence user choice in click-based graphical passwords, encouraging users to select more random, and hence more difficult to guess, click-points.",
    "keywords": [
      "authentication",
      "graphical passwords",
      "usable security",
      "empirical studies"
    ]
  },
  {
    "id": "1417",
    "title": "An improved shuffled complex evolution algorithm with sequence mapping mechanism for job shop scheduling problems",
    "abstract": "An Improved Shuffled Complex Evolution (ISCE) algorithm is proposed. The sequence mapping mechanism was presented. The sequence with job permutation, is adopted for encoding and decoding. A new strategy is used to improve the individuals evolution to overcome stagnation. The results show that the improved algorithm is effective to the job shop scheduling.",
    "keywords": [
      "job shop scheduling",
      "shuffled complex evolution",
      "job permutation",
      "sequence mapping mechanism"
    ]
  },
  {
    "id": "1418",
    "title": "using greenfoot and a moon scenario to teach java programming in cs1",
    "abstract": "In this paper we describe a novel concept for teaching introductory Java programming to post-secondary students in their first year of higher education. The concept includes labs and a capstone project all linked together and all utilizing the Java-based Greenfoot programming environment. The concept is designed with two goals in mind: to improve the students experience in their first computer programming course by making it more entertaining; and to increase retention in the diploma or degree programs by peaking the student's interest early in their studies. This is accomplished through a Going to the Moon scenario we have designed and implemented into the Greenfoot programming environment.",
    "keywords": [
      "simple data types",
      "moon scenario",
      "programming for fun",
      "programming environments",
      "greenfoot"
    ]
  },
  {
    "id": "1419",
    "title": "Adaptive parameter identification of servo control systems with noise and high-frequency uncertainties",
    "abstract": "When identifying the parameters of practical servo systems, the high-frequency system modes are frequently neglected in order to simplify the model. Additionally, avoiding measurement noise in physical experiments is virtually impossible Therefore, selecting an appropriate excitation signal is essential. Specifying white noise, or other such signals, as the input signal may cause the excitation of high-frequency uncertainties, which affects the reliability of the parameter identification process. The current study proposes a novel approach in which a particular class of chaotic signal is employed as the excitation signal in an adaptive parameter identification process. Since chaotic signals typically have stationary, continuous, and band-limited power spectra, they are suitable for on-line parameter identification. The present numerical and experimental results demonstrate that the use of a chaotic excitation signal in the identification process causes the estimated system parameters to converge within identifiable ranges, even when the system includes measurement noise and high-frequency uncertainties. The current results also show that there is good agreement between the dynamics of the real system and those of the estimated model within the operation bandwidth.",
    "keywords": [
      "chaos",
      "servo system",
      "adaptive parameter identification",
      "persistent excitation",
      "band-limitation"
    ]
  },
  {
    "id": "1420",
    "title": "editing by example",
    "abstract": "An editing by example system is an automatic program synthesis facility embedded in a text editor that can be used to solve repetitive text editing problems. The user provides the editor with a few examples of a text transformation. The system analyzes the examples and generalizes them into a program that can perform the transformation to the rest of the user's text. This paper presents the design, analysis, and implementation of a practical editing by example system. In particular, we study the problem of synthesizing a text processing program that generalizes the transformation implicitly described by a small number of input/output examples. We define a class of text processing programs called gap programs , characterize their computational power, study the problems associated with synthesizing them from examples, and derive an efficient heuristic that provably synthesizes a gap program from examples of its input/output behavior. We evaluate how well the gap program synthesis heuristic performs on the text encountered in practice. This evaluation inspires the development of several modifications to the gap program synthesis heuristic that act both to improve the quality of the hypotheses proposed by the system and to reduce the number of examples required to converge to a target program. The result is a gap program synthesis heuristic that can usually synthesize a target gap program from two or three input examples and a single output example. The editing by example system derived from this analysis has been embedded in a production text editor. The system is presented as a group of editor commands that use the standard interfaces of the editor to collect examples, show synthesized programs, and run them. By developing an editing by example system that solves a useful class of text processing problems, we demonstrate that program synthesis is feasible in the domain of text editing.",
    "keywords": [
      "program",
      "product",
      "efficiency",
      "examples",
      "systems",
      "input",
      "association",
      "edit",
      "analysis",
      "behavior",
      "design",
      "text-processing",
      "implementation",
      "text",
      "heuristics",
      "synthesis",
      "paper",
      "practical",
      "user",
      "embedding",
      "class"
    ]
  },
  {
    "id": "1421",
    "title": "Describing the transformation of organic carbon and nitrogen in soil using the MOTOR system",
    "abstract": "This article presents a framework (MOTOR 3.01, MOdular description of the Turnover of Organic matteR), which describes the transformation of organic carbon and nutrients in soil. The state of each component of the organic matter in soil is described by a vector and the transformations by a matrix of terms. Actual turnover is calculated by multiplication of these matrices, state vectors and a rate vector. The resulting system is powerful because it is modular in construction and any one part of it may be altered simply and quickly without reference to the rest of the calculation system. Once the flow of an element is described, related nutrients (nitrogen in relation to carbon, for example) and their isotopic tracers are handled directly without additional programming or without repeated simulations. A great benefit of this approach is that mixtures of substrate or substrates of very different qualities (C:N, fibre content, labelled and unlabelled) can be dealt with easily by inserting another residue pool. If necessary the kinetics of decomposition of this new pool can differ from the others; the fate of its decomposition products is also easy to define. Where nutrients such as N are in short supply, the transformations are no longer a linear function of inputs. A novel two-call algorithm is described which first identifies if shortages of N occur during decomposition as a whole and then modifies decomposition iteratively until supply and demand for N are in balance. The structure of the modules is explained and an example given of the system's application to test a model of the protection of organic carbon and nitrogen in soil during turnover. Parameters derived from a published dataset for carbon turnover are adapted to follow the turnover of 15N-labelled crop residues in soil and the resultant model tested against two independent datasets.",
    "keywords": [
      "isotope labelling",
      "mineralization",
      "model",
      "organic matter",
      "nutrients",
      "nitrogen",
      "organic matter protection"
    ]
  },
  {
    "id": "1422",
    "title": "On graphs with the third largest number of maximal independent sets",
    "abstract": "Let G be a simple and undirected graph. By mi(G) we denote the number of maximal independent sets in G. Erdos and Moser posed the problem to determine the maximum cardinality of mi(G) among all graphs of order n and to characterize the corresponding extremal graphs attaining this maximum cardinality. The above problem has been solved by Moon and Moser in [J.W. Moon, L. Moser, On cliques in graphs, Israel J. Math. 3 (1965) 23-28]. More recently, Jin and Li [Z. Jin, X. Li, Graphs with the second largest number of maximal independent sets, Discrete Mathematics 308 (2008) 5864-5870] investigated the second largest cardinality of mi(G) among all graphs of order n and characterized the extremal graph attaining this value of mi(G). In this paper, we shall determine the third largest cardinality of mi(G) among all graphs G of order n. Additionally, graphs achieving this value are also determined.  ",
    "keywords": [
      "maximal independent set",
      "extremal graph",
      "combinatorial problems"
    ]
  },
  {
    "id": "1423",
    "title": "Performance of affordable neural network for back propagation learning",
    "abstract": "Cell assembly is one of explanations of information processing in the brain, in which an information is represented by a firing space pattern of a group of plural neurons. On the other hand, effectiveness of neural network has been confirmed in pattern recognition, system control, signal processing, and so on, since the back propagation learning was proposed. In this study, we propose a new network structure with affordable neurons in the hidden layer of the feedforward neural network. Computer simulated results show that the proposed network exhibits a good performance for the back propagation learning. Furthermore, we confirm the proposed network has a good generalization ability.",
    "keywords": [
      "cell assembly",
      "affordable neurons",
      "back propagation"
    ]
  },
  {
    "id": "1424",
    "title": "A delay-partitioning projection approach to stability analysis of stochastic Markovian jump neural networks with randomly occurred nonlinearities",
    "abstract": "This paper considers the problem of mean square asymptotic stability of stochastic Markovian jump neural networks with randomly occurred nonlinearities. In terms of linear matrix inequality (LMI) approach and delay-partitioning projection technique, delay-dependent stability criteria are derived for the considered neural networks for cases with or without the information of the delay rates via new LyapunovKrasovskii functionals. We also establish that the conservatism of the conditions is a non-increasing function of the number of delay partitions. An example with simulation results is given to illustrate the effectiveness of the proposed approach.",
    "keywords": [
      "mean square asymptotic stability",
      "time-varying delay",
      "randomly occurred nonlinearities ",
      "delay-partitioning projection",
      "stochastic neural networks"
    ]
  },
  {
    "id": "1425",
    "title": "How to simulate affinities for host-guest systems lacking binding mode information: application to the liquid chromatographic separation of hexabromocyclododecane stereoisomers",
    "abstract": "A novel approach for the simulation of host-guest systems by systematically scanning the host molecule's orientations within the guest cavity is presented along with a thermodynamic strategy for determining preferential binding modes and corresponding optimal interaction energies between host and guest molecules. By way of example, the elution order of hexabromocyclododecane stereoisomers from high performance liquid chromatography separation on a permethylated beta-cyclcodextrin stationary phase has been computed using classical molecular dynamics simulations with the explicit solvents water and acetonitrile. Comparison of estimated with experimental separation data reveals remarkable squared coefficients of correlation with R (2) = 0.87 and a very high correlation using the leave-one-out cross-validation method and water as solvent. In particular, the approach presented shapes up as very robust in terms of the evaluated time range under consideration, reflecting well thermodynamic equilibria. These and further observations correlating with experimental results suggest the suitability of the underlying force fields and our multi-mode approach for the estimation of relative binding affinities for host-guest systems with unknown binding modes.",
    "keywords": [
      "molecular simulation",
      "molecular dynamics",
      "binding affinity",
      "elution order",
      "beta-cyclcodextrin"
    ]
  },
  {
    "id": "1426",
    "title": "alphabetically constrained keypad designs for text entry on mobile devices",
    "abstract": "The creation of text will remain a necessary part of human-computer interaction with mobile devices, even as they continue to shrink in size. On mobile phones, text is often entered using keypads and predictive text entry techniques, which attempt to minimize the effort (e.g., number of key presses) needed to enter words. This research presents results from the design and testing of alphabetically-constrained keypads, optimized on various word lists, for predictive text entry on mobile devices. Complete enumeration and Genetic Algorithm-based heuristics were used to find keypad designs based on different numbers of keys. Results show that alphabetically-constrained designs can be found that are close to unconstrained designs in terms of performance. User testing supports the hypothesis that novice ease of learning, usability, and performance is greater for constrained designs when compared to unconstrained designs. The effect of different word lists on keypad design and performance is also discussed.",
    "keywords": [
      "mobile device user interface design",
      "novice learning and usability",
      "predictive keypad text entry"
    ]
  },
  {
    "id": "1427",
    "title": "A distributed traffic control scheme based on edge-centric resource management",
    "abstract": "The correct admission of flows in the Differentiated Services (DiffServ) environment is critical to provide stable and predictable quality of service (QoS) to the end user. Without a scalable and precise admission control scheme, the service provider is faced with either over-provisioning the network or accepting periods of best-effort like behavior. In this paper, we propose a novel approach for admission control that exploits the unique architectural aspects of DiffServ. Through the use of periodic heartbeats emanating from edge routers to probe the network state on the available egress paths, edge routers are able to quickly conduct admission control with a tunable degree of precision. In this paper, we detail our approach, Edge-centric Resource Management (ERM), and conduct detailed simulation studies regarding the effectiveness of the approach.",
    "keywords": [
      "algorithms",
      "measurement",
      "differentiated services",
      "qos",
      "admission control",
      "traffic engineering"
    ]
  },
  {
    "id": "1428",
    "title": "Approximation of dynamic systems using recurrent neuro-fuzzy techniques",
    "abstract": "Fuzzy systems, neural networks and its combination in neuro-fuzzy systems are already well established in data analysis and system control. Especially, neuro-fuzzy systems are well suited for the development of interactive data analysis tools, which enable the creation of rule-based knowledge from data and the introduction of a-priori knowledge into the process of data analysis. In this article an architecture is presented that was designed to learn and optimize a hierarchical fuzzy rule base with feedback connections using a genetic algorithm for rule base structure learning and a gradient descent method to optimize the fuzzy sets of the learned rule base. Since this architecture is able to store information of prior system states, the model is especially suited for the analysis of dynamic systems.",
    "keywords": [
      "fuzzy system",
      "neural network",
      "neuro-fuzzy",
      "recurrent architecture",
      "hierarchical architecture",
      "function approximation",
      "dynamic system"
    ]
  },
  {
    "id": "1429",
    "title": "Computational methods for case-cohort studies",
    "abstract": "Computational methods, which. can be implemented using standard Cox regression software, are given for fitting \"exact\" pseudo-likehood estimates and robust and asymptotic variance estimators from case-cohort data. These methods are based on the computational approach of Therneau and Li [1999. Computing the Cox model for case cohort designs. Lifetime Data Anal. 5, 99-112] but will be less subject to small sample bias. Further, it is shown how to accommodate time-dependent covariates and estimate absolute risk. Extensions to stratified case-cohort sampled data are also provided. The methods are illustrated in analyses of case-cohort samples from a study of radiation exposure from fluoroscopy and breast cancer using SAS software.  ",
    "keywords": [
      "bias",
      "cox model",
      "cumulative hazard",
      "risk estimation",
      "risk sets",
      "stratified cox model",
      "time-dependent covariates"
    ]
  },
  {
    "id": "1430",
    "title": "Compositional refinement in agent-based security protocols",
    "abstract": "A truly secure protocol is one which never violates its security requirements, no matter how bizarre the circumstances, provided those circumstances are within its terms of reference. Such cast-iron guarantees, as far as they are possible, require formal, rigorous techniques: proof or model-checking. Informally, they are difficult or impossible to achieve. Our rigorous technique is refinement, until recently not much applied to security. We argue its benefits by using refinement-based program algebra to develop several security case studies. That is one of our contributions here. The soundness of the technique follows from its compositional semantics, one which we defined (elsewhere) to support a specialisation of standard refinement by enriching standard semantics with information that tracks correlations between hidden state and visible behaviour. A further contribution is to extend the basic theory of secure refinement (Morgan in Mathematics of program construction, Springer, Berlin, vol. 4014, pp. 359-378, 2006) with special features required by our case studies, namely agent-based systems with complementary security requirements, and looping programs.",
    "keywords": [
      "refinement of security",
      "formalised secrecy",
      "hierarchical security reasoning",
      "compositional semantics"
    ]
  },
  {
    "id": "1431",
    "title": "Resource-aware distributed scheduling strategies for large-scale computational Cluster/Grid systems",
    "abstract": "In this paper, we propose distributed algorithms referred to as Resource-Aware Dynamic Incremental Scheduling ( RADIS) strategies. Our strategies are specifically designed to handle large volumes of computationally intensive arbitrarily divisible loads submitted for processing at cluster/grid systems involving multiple sources and sinks ( processing nodes). We consider a real-life scenario, wherein the buffer space ( memory) available at the sinks ( required for holding and processing the loads) varies over time, and the loads have deadlines and propose efficient \"pull-based\" scheduling strategies with an admission control policy that ensures that the admitted loads are processed, satisfying their deadline requirements. The design of our proposed strategies adopts the divisible load paradigm, referred to as the divisible load theory ( DLT), which is shown to be efficient in handling large volume loads. We demonstrate detailed workings of the proposed algorithms via a simulation study by using real-life parameters obtained from a major physics experiment.",
    "keywords": [
      "divisible loads",
      "grid computing",
      "cluster computing",
      "buffer constraints",
      "processing time",
      "deadlines"
    ]
  },
  {
    "id": "1432",
    "title": "Potential force dynamics of heart rate variability reflect cardiac autonomic modulation with respect to posture, age, and breathing pattern",
    "abstract": "Various physiological and pathological conditions are correlated with cardiac autonomic function. Heart rate variability is a marker of cardiac autonomic modulation and can be measured by several methods. However, the available methods are sensitive to breathing patterns. To quantify cardiac autonomic modulation by observing the potential force dynamics of the R?R interval time series in healthy individuals. We propose two potentials of unbalanced complex kinetic (PUCK) parameters to quantify the characteristics of the potential force dynamics of R?R interval time series: potential strength (slope) and fluctuation size (slope standard deviations [SSD1, SSD2]). We applied this method to the series of R?R intervals obtained from 30 healthy subjects in an experimental condition that elicited cardiac autonomic (i.e., sympathetic and vagal) activation (in supine, sitting, and standing positions). Subjects were categorized into three groups by decade (i.e., 20s, 30s, and 40s) to verify the cardiac autonomic differences by age. Two respiration patterns were introduced to check the influence of the pattern into the analytical results. Sympathetic modulation activation significantly increased the slope and reduced SSD1 and SSD2; these trends were confirmed in all groups. The slope is concordant with the result of the low frequency/high frequency (LF/HF) ratio in frequency components as an indicator of sympathetic modulation. No trend was observed in slope among age groups. However, SSD1 and SSD2 in the 40s group were significantly decreased in the supine and sitting positions. The results with respect to respiration frequency showed lower sympathetic modulation as shown in the LF/HF ratio and slope, whereas higher vagal modulation as shown in the HF appeared with a longer breathing rate. PUCK can quantify the cardiac autonomic modulation in the experimental conditions of different postures. SSD1 and SSD2 are more sensitive to age than frequency components and are unaffected by breathing patterns. This method may be an alternative method for observing cardiac autonomic modulation in clinical practice.",
    "keywords": [
      "potential force dynamics",
      "non-linear analysis",
      "heart rate variability",
      "autonomic nervous system",
      "breathing pattern"
    ]
  },
  {
    "id": "1433",
    "title": "Minimum energy cooperative path routing in all-wireless networks: NP-completeness and heuristic algorithms",
    "abstract": "We study the routing problem in all-wireless networks based on cooperative transmissions. We model the minimum-energy cooperative path (MECP) problem and prove that this problem is NP-complete. We hence design an approximation algorithm called cooperative shortest path (CSP) algorithm that uses Dijkstra's algorithm as the basic building block and utilizes cooperative transmissions in the relaxation procedure. Compared with traditional non-cooperative shortest path algorithms, the CSP algorithm can achieve a higher energy saving and better balanced energy consumption among network nodes, especially when the network is in large scale. The nice features lead to a unique, scalable routing scheme that changes the high network density from the curse of congestion to the blessing of cooperative transmissions.",
    "keywords": [
      "cooperative transmissions",
      "distributed algorithms",
      "energy efficiency",
      "wireless networks"
    ]
  },
  {
    "id": "1434",
    "title": "distance teaching workloads",
    "abstract": "In this paper, we describe a formula for calculating the teaching workload for students who are studying off campus both on and off-line. Initially the faculty of information technology developed a proposal for calculating academic workloads. This proposal reflected the rigid teacher centred learning structures of traditional on-campus delivery and made no allowance for the services required by off-campus students. In response, teachers of off-campus students developed a complementary proposal, based on actual time logs, which reflected their student centred approach to learning. Contrary to popular wisdom, off-campus teaching was found to be more time-consuming than on-campus.",
    "keywords": [
      "student centered learning",
      "flexible delivery",
      "electronic educational environments",
      "teaching workload",
      "distance education"
    ]
  },
  {
    "id": "1435",
    "title": "Improving the efficiency of discrete time scheduling formulation",
    "abstract": "The problem of production scheduling in multipurpose plants can be formulated as a mixed integer linear programming (MILP) problem based on a discrete representation of time. The mathematical difficulties associated with solving integer programs are well established and combining this factor with the increasing complexity and size of scheduling problems, there exists a strong requirement for efficient solution algorithms. This paper attempts to address this requirement by looking at two ways to reduce the gap between the optimal solution and the solution of its relaxed LP counterpart. The first;method involves generating cut constraints in problems where the presence of changeover activities tends to widen the relaxation gap. The second method uses an established reformulation technique based on variable disaggregation which exploits the lot sizing problem found embedded in many scheduling instances. The reformulation is applied to a general scheduling framework and its performance evaluated. Test examples are described for both methods, along with their numerical results.  ",
    "keywords": [
      "cut generation",
      "reformulation",
      "relaxation gap",
      "scheduling"
    ]
  },
  {
    "id": "1436",
    "title": "The post-SCF quantum chemistry characteristics of inter- and intra-strand stacking interactions in d(CpG) and d(GpC) steps found in B-DNA, A-DNA and Z-DNA crystals",
    "abstract": "The energies of intra- and inter-strand stacking interactions in model d(GpC) and d(CpG) two-base-pair steps were estimated by MP2/aug-cc-pVDZ single point calculations corrected for basis superposition errors. The stacked two-nucleobase pairs were constructed using experimental values of base pair and base step parameters taken from Nucleic Acid Database (http://ndbserver.rutgers.edu/). Three distinct polymorphic forms were analysed, namely A-, B-and Z-DNA. The applied methodology enables statistical analysis of structural and energetic diversities. The structural relationships between polymorphic forms are quite complex and depend on the sequence of pairs. The variability of parameters such as shift and tilt is almost the same irrespective of the polymorphic form and sequence of steps analysed. In contrast, shift and twist distributions easily discriminate all three polymorphic forms of DNA. Interestingly, despite significant structural diversities, the energies of the most frequent energy ranges are comparable irrespective of the polymorphic form and base sequence. There was observed compensation of inter-and intra-strand interactions, especially for d(GpC) and d(CpG) steps found in A-and B-DNA. Thus, among many other roles, these pairs act as a kind of energetic buffer, balancing the double helix.",
    "keywords": [
      "cytosine",
      "dna",
      "guanine",
      "intermolecular interactions",
      "polymorphism",
      "stacking"
    ]
  },
  {
    "id": "1437",
    "title": "A modified Newton method in parallel circular iteration of single-step and double-step",
    "abstract": "In this paper, two theorems for the convergence of a modified Newton method in parallel circular iteration are given. The convergent condition of single-step method's circular iteration is relaxed compared with the classical theorem in the same field, while the one of the first proposed double-step method of the five-order is obtained accurately as well.  ",
    "keywords": [
      "zeros of a polynomial",
      "modified newton method",
      "parallel circular iteration",
      "convergent condition",
      "single-step",
      "double-step"
    ]
  },
  {
    "id": "1438",
    "title": "Instability of FIFO at arbitrarily low rates in the adversarial queueing model",
    "abstract": "We study the stability of the commonly used packet forwarding protocol, FIFO (first in first out), in the adversarial queueing model. We prove that FIFO can become unstable, i.e., lead to unbounded buffer-occupancies and queueing delays, at arbitrarily low injection rates. In order to demonstrate instability at rate r, we use a network of size (O) over tildeO(1/r).",
    "keywords": [
      "fifo",
      "stability",
      "adversarial queueing model"
    ]
  },
  {
    "id": "1439",
    "title": "constraints on communication and electronic mail",
    "abstract": "How information flows through an organization is important to many organizational processes. The information people receive influences the perceptions they have of the organization they work for and the tasks they are assigned. Electronic mail constitutes a new medium in organizational communication. It may alter some of the information flow in the organizations in which it is used. My analysis suggests that some new communication occurs in large organizations that have electronic mail. I suggest that this new communication occurs because the way electronic mail is organized allows people to find other people with common interests at a low cost to either party. This new communication creates links between people who would otherwise not share information. Granovetter's work on the significance of weak ties suggests that such connections may have substantial influence on the way in which behavior is shaped and constrained by one's network and in the manipulation of networks to achieve specific goals (1973, 1974). These processes are important to organizational socialization and problem solving, respectively. Other functions of large formal organizations may also be affected.",
    "keywords": [
      "network",
      "sharing",
      "communication",
      "information flow",
      "organization",
      "electronic mail",
      "influence",
      "formalism",
      "goals",
      "analysis",
      "behavior",
      "perception",
      "process",
      "links",
      "social",
      "manipulation",
      "constraint",
      "connection",
      "informal",
      "cost"
    ]
  },
  {
    "id": "1440",
    "title": "Examining network externalities and network structure for new product introduction",
    "abstract": "Much of the existing work on network externalities has assumed that network effects are dependent on network size. Very little consideration has been given to the view that marginal benefits from joining the network may not increase with network size, if consumer benefits come from direct interactions with neighbors in a local network. We use agent-based simulation to examine the effectiveness of the traditional network market strategy in the presence of a local network. Our results show that it is appropriate to establishing a sustainable network by leveraging the properties of local networks.",
    "keywords": [
      "agent-based simulation",
      "firm strategy",
      "local networks",
      "network effects",
      "network externalities",
      "network structure",
      "product introduction"
    ]
  },
  {
    "id": "1441",
    "title": "Crossing textual and visual content in different application scenarios",
    "abstract": "This paper deals with multimedia information access. We propose two new approaches for hybrid text-image information processing that can be straightforwardly generalized to the more general multimodal scenario. Both approaches fall in the trans-media pseudo-relevance feedback category. Our first method proposes using a mixture model of the aggregate components, considering them as a single relevance concept. In our second approach, we define trans-media similarities as an aggregation of monomodal similarities between the elements of the aggregate and the new multimodal object. We also introduce the monomodal similarity measures for text and images that serve as basic components for both proposed trans-media similarities. We show how one can frame a large variety of problem in order to address them with the proposed techniques: image annotation or captioning, text illustration and multimedia retrieval and clustering. Finally, we present how these methods can be integrated in two applications: a travel blog assistant system and a tool for browsing the Wikipedia taking into account the multimedia nature of its content.",
    "keywords": [
      "text-image information processing",
      "trans-media similarities",
      "cross-content information retrieval and browsing",
      "image auto-annotation",
      "multimedia document generation"
    ]
  },
  {
    "id": "1442",
    "title": "A new family of proximity graphs: Class cover catch digraphs",
    "abstract": "Motivated by issues in machine learning and statistical pattern classification, we investigate a class cover problem (CCP) with an associated family of directed graphsclass cover catch digraphs (CCCDs). CCCDs are a special case of catch digraphs. Solving the underlying CCP is equivalent to finding a smallest cardinality dominating set for the associated CCCD, which in turn provides regularization for statistical pattern classification. Some relevant properties of CCCDs are studied and a characterization of a family of CCCDs is given.",
    "keywords": [
      "class cover problem",
      "pattern classification",
      "machine learning",
      "digraph"
    ]
  },
  {
    "id": "1443",
    "title": "automatically defined functions for learning classifier systems",
    "abstract": "This work introduces automatically defined functions (ADFs) for learning classifier systems (LCS). ADFs had been successfully implemented in genetic programming (GP)for various domain problems such as multiplexer and even-odd parity, but they have never been attempted in LCS research field before. ADFs in GP contract program trees and shorten training times whilst providing resilience to destructive genetic operators. We have implemented ADFs in Wilson's accuracy based LCS, known as XCS [14]. This initial investigation of ADFs in LCS shows that the multiple genotypes to a phenotype issue in feature rich encodings disables the subsumption deletion function. The additional methods and increased search space also leads to much longer training times. This is compensated by the ADFs containing useful knowledge, such as the importance of the address bits in the multiplexer problem. The ADFs also create masks that autonomously subdivide the search space into areas of interest and uniquely, areas of not interest. The next stage of this work is to implement simplification methods and then determine methods by which ADFs can facilitate scaling for more complex problems within the same problem domain.",
    "keywords": [
      "pattern recognition",
      "cuda",
      "automatically defined functions",
      "learning classifier systems",
      "genetic programming"
    ]
  },
  {
    "id": "1444",
    "title": "An Integrated Model for Activity-dependent Synaptic Modifications",
    "abstract": "An integrated model is proposed for activity-dependent synaptic modifications. These include two forms of homosynaptic modification, two forms of associative modification and one form of heterosynaptic modification. The model is constructed by referring to the physiological and biochemical data that likely underlie the induction and expression of long-term potentiation and long-term depression and by making certain assumptions. Computer simulations are performed, and the simulation results prove to have good agreement with relevant experimental results. Thus, the model may produce realistically different forms of synaptic modification.  1997 Elsevier Science Ltd. ",
    "keywords": [
      "integrated model ltp-like synaptic modification  ltd-like synaptic modification  receptor  phosphorylation  dephosphorylation"
    ]
  },
  {
    "id": "1445",
    "title": "a fast hybrid classification algorithm based on the minimum distance and the k-nn classifiers",
    "abstract": "Some of the most commonly used classifiers are based on the retrieval and examination of the k Nearest Neighbors of unclassified instances. However, since the size of datasets can be large, these classifiers are inapplicable when the time-costly sequential search over all instances is used to find the neighbors. The Minimum Distance Classifier is a very fast classification approach but it usually achieves much lower classification accuracy than the k-NN classifier. In this paper, a fast, hybrid and model-free classification algorithm is introduced that combines the Minimum Distance and the k-NN classifiers. The proposed algorithm aims at maximizing the reduction of computational cost, by keeping classification accuracy at a high level. The experimental results illustrate that the proposed approach can be applicable in dynamic, time-constrained environments.",
    "keywords": [
      "nearest neighbors",
      "classification",
      "data reduction",
      "scalability"
    ]
  },
  {
    "id": "1446",
    "title": "Adaptive Fuzzy Filtering in a Deterministic Setting",
    "abstract": "Manyreal-world applications involve the filtering and estimation of process variables. This study considers the use of interpretable Sugeno-type fuzzy models for adaptive filtering. Our aim in this study is to provide different adaptive fuzzy filtering algorithms in a deterministic setting. The algorithms are derived and studied in a unified way without making any assumptions on the nature of signals (i.e., process variables). The study extends, in a common framework, the adaptive filtering algorithms (usually studied in signal processing literature) and p- norm algorithms (usually studied in machine learning literature) to semilinear fuzzy models. A mathematical framework is provided that allows the development and an analysis of the adaptive fuzzy filtering algorithms. We study a class of nonlinear LMS-like algorithms for the online estimation of fuzzy model parameters. A generalization of the algorithms to the p- norm is provided using Bregman divergences (a standard tool for online machine learning algorithms).",
    "keywords": [
      "adaptive filtering algorithms",
      "bregman divergences",
      "p-norm",
      "robustness",
      "sugeno fuzzy models"
    ]
  },
  {
    "id": "1447",
    "title": "Gain modulation of recurrent networks",
    "abstract": "Gain modulation is an important mechanism by which attentional and other inputs modify the amplitude of neuronal responses without changing their selectivity. Gain modulation has been studied previously in feedforward circuits but not in recurrent neural networks. We show how gain modulation modifies the response of a recurrent network to feedforward inputs. Even modest gain modulation of the recurrent network can cause downstream neurons to switch from a state in which they are unresponsive to a stimulus to a state where they respond selectively. Funneling the recurrent connections of a network through gain modulated neurons allows the selectivity within the network to be modified by modulatory inputs.",
    "keywords": [
      "recurrent model",
      "switching",
      "gain modulation",
      "attention"
    ]
  },
  {
    "id": "1448",
    "title": "Design for Delay Measurement Aimed at Detecting Small Delay Defects on Global Routing Resources in FPGA",
    "abstract": "Small delay defects can cause serious issues such as very short lifetime in the recent VLSI devices. Delay measurement is useful to detect small delay defects in manufacturing testing. This paper presents a design for delay measurement to detect small delay defects on global routing resources, such as double, hex and long lines, In a Xilinx Virtex 4 based FPGA. This paper also shows a measurement method using the proposed design. The proposed measurement method is based on an existing one for SoC using delay value measurement circuit (DVMC). The proposed measurement modifies the construction of configurable logic blocks (CLBs) and utilizes an on-chip DVMC newly added. The number of configurations required by the proposed measurement is 60, which is comparable to that required by stuck-at fault testing for global routing resources in FPGAs. The area overhead is low for general FPGAs, in which the area of routing resources is much larger than that of the other elements such as CLBs. The area of every modified CLB is 7% larger than an original CLB, and the area of the on-chip DVMC is 22% as large as that of an original CLB. For recent FPGAs, we can estimate that the area overhead is approximately 2% or less of the FPGAs.",
    "keywords": [
      "small delay defects",
      "delay measurement",
      "dvmc ",
      "fpga ",
      "global routing resource"
    ]
  },
  {
    "id": "1449",
    "title": "Actuator fault-tolerant control for discrete systems with strong uncertainties",
    "abstract": "In this paper, a reliable control method is first proposed for discrete systems with strong uncertainties. Actuator fault compensation is also investigated based on state feedback controller. The reliable control approach uses analytical redundancy and does not need duplicated actuators. Fault compensation controller is designed using accommodation mechanism which compensates the fault effects. The closed-loop stability is established based on Lyapunov's sense. The novelty of this paper is in controller design part. This reliable control approach is applied to three-tank system. The fault compensation method is applied to winding machine in continuous annealing process systems. Applications of the proposed design indicate that the method is effective for three-tank system and continuous annealing process systems.  ",
    "keywords": [
      "actuator fault",
      "reliable control",
      "fault compensation",
      "strong uncertainties",
      "three-tank system",
      "winding machine"
    ]
  },
  {
    "id": "1450",
    "title": "A nonlinear rigid-plastic analysis for metal forming problem using the rigid-plastic point collocation method",
    "abstract": "A rigid-plastic point collocation method is applied to the analysis of plane strain forging, which is a nonlinear rigid-plastic problem of bulk metal forming. In general, the bulk metal forming problems are nonlinear and large deformation problems, used to be analyzed with the conventional rigid-plastic finite element methods. While the conventional rigid-plastic finite element methods have some shortcomings such as necessities of mesh generation, remeshing and numerical integration for these methods. The rigid-plastic point collocation method not only is a kind of truly meshless method, but also is a kind of no integration and no fundamental solution method. In this paper, the first, a linear elastic cantilever beam problem is analyzed by using the point collocation method, into which the concepts of the considered nodes and the unconsidered nodes are introduced. The numerical solution of the problem is compared with the exact solution of it, and quite high accuracy of the numerical solution has been achieved. The second, a plane strain metal forming problem has been analyzed by using the rigid-plastic point collocation method. Because the considered nodes and the unconsidered nodes are used, no renoding is needed. The solution of the rigid-plastic problem is compared with a conventional rigid-plastic finite element solution, and reasonable results have been obtained.",
    "keywords": [
      "point collocation method",
      "meshless method",
      "rigid-plasticity",
      "metal forming"
    ]
  },
  {
    "id": "1451",
    "title": "Coherence resonance in propagating spikes in FitzHugh-Nagumo model",
    "abstract": "Coherence resonance in propagating spikes generated by noise in spatially distributed excitable media is studied with computer simulation and circuit experiment on the FitzHugh-Nagumo model. White noise is added to the one end of the media to generate spikes, which propagate to the other end. The mean and standard deviation of the interspike intervals of the spikes after propagation take minimum values at the intermediate strength of the added noise. This shows stronger coherence than obtained in the previous studies.",
    "keywords": [
      "stochastic resonance",
      "coherence resonance",
      "excitable media",
      "fitzhugh-nagumo model",
      "spike propagation"
    ]
  },
  {
    "id": "1452",
    "title": "Empirical performance evaluation of schedulers for cluster of workstations",
    "abstract": "Cluster computing is receiving exponential popularity as a choice for high performance computing. This is mainly due to its effective cost performance ratio. Resource management systems (RMS) are the key component to manage the resources of clusters efficiently and have a very vital role in the performance of distributed parallel systems especially a job scheduling module. In this paper, we have empirically evaluated four resource management systems (SGE, TORQUE, and MAUI Scheduler and SLURM) with special focus on job scheduler component. These schedulers have been evaluated on a more comprehensive set of metrics such as throughput, CPU, memory and network utilization. Experiments were carried out on three different size testbeds with a range of scheduler configurations such as FCFS, Backfilling, Fair share and SJF scheduling techniques. A head-to-head comparison of different scheduling techniques has also been presented which highlights the effect of RMS on the performance of scheduling techniques. It has been observed from results that relative difference among the performance of scheduling techniques reached up to 63%. We conclude from the experiments that there is no single choice of RMS which can be identified as the best but SLURM performs better than others in most of the cases.",
    "keywords": [
      "cluster computing",
      "scheduling",
      "resource management systems",
      "performance evaluation and distributed system"
    ]
  },
  {
    "id": "1453",
    "title": "Classification of respiratory sounds by using an artificial neural network",
    "abstract": "In this paper, a classification method for respiratory sounds (RSs) in patients with asthma and in healthy subjects is presented. Wavelet transform is applied to a window containing 256 samples. Elements of the feature vectors are obtained from the wavelet coefficients. The best feature elements are selected by using dynamic programming. Grow and Learn (GAL) neural network, Kohonen network and multi-layer perceptron (MLP) are used for the classification. It is observed that RSs of patients (with asthma) and healthy subjects are successfully classified by the GAL network.",
    "keywords": [
      "respiratory sounds",
      "classification of biomedical signals",
      "artificial neural network",
      "wavelet transform",
      "pattern recognition"
    ]
  },
  {
    "id": "1454",
    "title": "Hierarchical clustering with planar segments as prototypes",
    "abstract": "Clustering methods divide a set of observations into groups in such a way that members of the same group are more similar to one another than to the members of the other groups. One of the scientifically well known methods of clustering is the hierarchical agglomerative one. For data of different properties different clustering methods appear favorable. If the data possess locally linear form, application of planar (or hyperplanar) prototypes should be advantageous. However, although a clustering method using planar prototypes, based on a criterion minimization, is known, it has a crucial drawback. It is an infinite extent of such prototypes that can result in addition of very distant data points to a cluster. Such distant points can considerably differ from the majority within a cluster. The goal of this work is to overcome this problem by developing a hierarchical agglomerative clustering method that uses the prototypes confined to the segments of hyperplanes. In the experimental part, we show that for data that possess locally linear form this method is highly competitive to the method of the switching regression models (the accuracy improvement of 24%) as well as to other well-known clustering methods (the accuracy improvement of 16%).",
    "keywords": [
      "hierarchical clustering",
      "prototype based clustering",
      "switching regression models",
      "maximum density of planar segment linkage"
    ]
  },
  {
    "id": "1455",
    "title": "Fuzzy Logic Control Design for a Stair-Climbing Robot",
    "abstract": "In the paper, a robot is designed to move up-and-down stairs for providing service. This robot consists of a main body, actuating system, roller chains, a front arm and a rear arm for assisting in moving up and down stairs. The actuating system includes two brushless DC motors for locomotion, two worm gears for torque magnification, and two DC motors to control arms. A DSP-based board acts as control center. Some rubber blocks attached to the roller chains are used to generate friction with ground and stairs for moving. A fuzzy logic controller integrates the outputs of DC bus current sensor and an inclinometer to steer the robot. Two walking experiments of moving up and down stairs with the rise/depth of 120/400 mm and 175/280 mm are shown in the taped pictures from videos to verify the proposed design.",
    "keywords": [
      "stair-climbing robot",
      "fuzzy logic control",
      "brushless dc motor"
    ]
  },
  {
    "id": "1456",
    "title": "Preclinical evaluation and molecular docking of 4-phenyl-1-Napthyl phenyl acetamide (4P1NPA) from Streptomyces sp DPTB16 as a potent antifungal compound",
    "abstract": "The incidence of fungal disease has increased dramatically over the past decades, mainly due to the emergence and transmission of antifungal resistance within the fungal pathogens. The present study investigates the use of novel antifungal compound 4-Phenyl-1-Napthyl Phenyl Acetamide (4P1NPA), isolated from marine Streptomyces sp. DPTB16 as a potent antifungal drug. The preclinical studies and molecular docking for 4P1NPA against Cytochrome P450 51 (CYP 51) were performed using in silico pharmacology and docking tools. The finding reveals the drug likeliness of 4P1NPA and satisfactory interaction of 4P1NPA with CYP 51. These results collectively evidence the use of 4P1NPA as a drug to treat fungal infections. On the whole, we highlight the findings of this research will be helpful to design 4P1NPA as novel antifungal drug to defend the emerging antifungal resistance.  ",
    "keywords": [
      "antifungal resistance",
      "antifungal",
      "4p1npa",
      "in silico pharmacology and docking tools",
      "docking",
      "cyp51"
    ]
  },
  {
    "id": "1457",
    "title": "What Do Hormones Have to Do with Aging? What Does Aging Have to Do with Hormones",
    "abstract": "It is clear that aging results in alterations of endocrine physiology, which in turn appear to contribute to development of the senescent phenotype. How the underlying basic aging process or processes cause the endocrine cell dysfunctions leading to hormone imbalance is far from clear, but oxidative alteration of cell membranes is an attractive candidate mechanism that might be susceptible to some degree of global remediation.",
    "keywords": [
      "hormones",
      "aging",
      "diet",
      "cell",
      "damage",
      "oxidative stress"
    ]
  },
  {
    "id": "1458",
    "title": "A new recombination lower bound and the minimum perfect phylogenetic forest problem",
    "abstract": "Understanding recombination is a central problem in population genetics. In this paper, we address an established computational problem in this area: compute lower bounds on the minimum number of historical recombinations for generating a set of sequences (Hudson and Kaplan in Genetics 111, 147-164, 1985; Myers and Griffiths in Genetics 163, 375-394, 2003; Gusfield et al. in Discrete Appl. Math. 155, 806-830, 2007; Bafna and Bansal in IEEE/ACM Trans. Comput. Biol. Bioinf. 1, 78-90, 2004 and in J. Comput. Biol. 13, 501-521, 2006; Song et al. in Bioinformatics 421, i413-i244, 2005). In particular, we propose a new recombination lower bound: the forest bound. We show that the forest bound can be formulated as the minimum perfect phylogenetic forest problem, a natural extension to the classic binary perfect phylogeny problem, which may be of interests on its own. We then show that the forest bound is provably higher than the optimal haplotype bound (Myers and Griffiths in Genetics 163, 375-394, 2003), a very good lower bound in practice (Song et al. in Bioinformatics 421, i413-i422, 2005). We prove that, like several other lower bounds (Bafna and Bansal in J. Comput. Biol. 13, 501-521, 2006), computing the forest bound is NP-hard. Finally, we describe an integer linear programming (ILP) formulation that computes the forest bound precisely for certain range of data. Simulation results show that the forest bound may be useful in computing lower bounds for low quality data.",
    "keywords": [
      "recombination",
      "lower bound on the minimum number of recombination",
      "ancestral recombination graph",
      "population genetics",
      "computational complexity"
    ]
  },
  {
    "id": "1459",
    "title": "An introduction to periodical discrete sets from a tomographical perspective",
    "abstract": "In this paper we introduce a new class of binary matrices whose entries show periodical configurations, and we furnish a first approach to their analysis from a tomographical point of view. In particular we propose a polynomial-time algorithm for reconstructing matrices with a special periodical behavior from their horizontal and vertical projections. We succeeded in our aim by using a reduction involving polyominoes which can be characterized by means of 2 - SAT formulas.  ",
    "keywords": [
      "discrete tomography",
      "computational complexity",
      "polyomino",
      "2-sat reduction"
    ]
  },
  {
    "id": "1460",
    "title": "Choices for interaction with things on Internet and underlying issues",
    "abstract": "Currently, a large number of smart objects and different types of devices are interconnected and communicate via Internet Protocol that creates a worldwide ubiquitous and pervasive network called the Internet of Things (IoT). With an increase in the deployment of smart objects, IoT is expected to have a significant impact on human life in the near future. A majorbreakthrough in bridging the gap between virtual and physical worlds came from the vision of the Web of Things (WoT), which employs open Web standards in achieving information sharing and objects interoperability. Social Web of Things (SWoT) further extends WoT to integrate smart objects with social networks and is observed to not only bridge between physical and virtual worlds but also facilitate continued interaction between physical devices and human. This makes SWoT the most promising approach and has now become an active research area. This survey introduces necessary background and fundamentals to understand current efforts in IoT, WoT and SWoT by reviewing key enabling technologies. These efforts are investigated in detail from several different perspectives such as architecture design, middleware, platform, systems implementation, and application in hand. Moreover, a large number of platforms and applications are analyzed and evaluated from various alternatives have become popular during the past decade. Finally, we address associated challenges and highlight potential research to be perused in future.",
    "keywords": [
      "internet of things",
      "social internet of things",
      "social web of things",
      "web of things",
      "wireless sensor networks"
    ]
  },
  {
    "id": "1461",
    "title": "Structure analysis and property improvements of the computer-simulated fullerene-based ultralow-k dielectrics",
    "abstract": "We have recently proposed new ultralow-k dielectric materials using a theoretical approach called molecular design. This approach requires the application of complementary theoretical methods to describe the complex problems. The methods include classical, continuum theoretical, and quantum-chemical approximations. The advantage of the present approach is that various possible candidates for ultralow-k dielectrics can be tested theoretically without performing expensive and time-consuming experiments. In this study, we analyze the way to connect linker molecules to the node molecules, in order to improve mechanical and dielectric properties of generated ultralow-k structures. Two different types of bonding linker molecules to the cage C60 molecule with the >C C CH2 CH2 C< linker molecules are possible. It is shown that at the present improvement step it is possible to get property combinations with dielectric constant of k=2.2 and bulk modulus of B=33GPa for the simple cubic topology.",
    "keywords": [
      "low- and ultralow-k dielectrics",
      "interlayer dielectric",
      "molecular design",
      "nanostructured materials",
      "fullerenes and fullerene-based dielectrics"
    ]
  },
  {
    "id": "1462",
    "title": "The Seelinder: Cylindrical 3D display viewable from 360 degrees",
    "abstract": "We propose a 3D video display technique that allows multiple viewers to see 3D images from a 360-degree horizontal arc without wearing 3D glasses. This technique uses a cylindrical parallax barrier and a one-dimensional light source array. We have developed an experimental display system using this technique. Since this technique is based on the parallax panoramagram, the parallax number and resolution are limited by the diffraction at the parallax barrier. In order to solve this problem, we improved the technique by revolving the parallax barrier. The improved technique was incorporated into two experimental display systems. The newer one is capable of displaying 3D color video images within a 200-mm diameter and a 256-mm height. Images have a resolution of 1254 circumferential pixels and 256 vertical pixels, and are refreshed at 30Hz. Each pixel has a viewing angle of 60 degrees that is divided into over 70 views so that the angular parallax interval of each pixel is less than 1 degree. These pixels are arranged on a cylindrical surface to allow for the produced 3D images to be observed from all directions. In this case, observers may barely perceive the discrete parallax.",
    "keywords": [
      "autostereoscopic display",
      "multi-view",
      "omnidirectional",
      "ray-space",
      "light field",
      "parallax barrier"
    ]
  },
  {
    "id": "1463",
    "title": "joint rate allocation, routing and spectrum sharing for multi-hop cognitive radio networks with imperfect spectrum sensing",
    "abstract": "In this paper we study the joint rate allocation, routing and spectrum sharing policy for multi-hop Cognitive Radio Networks (CRNs). We formulate this cross layer optimization problem as a sequential decision process which aims to minimize the average total power consumption of CRNs in each scheduling cycle under the constraint that each Cognitive Radio (CR) user's traffic demand is guaranteed. We consider imperfect spectrum sensing in our problem formulation and address Primary Users (PUs) protection with the interference regulation. We use Dynamic Programming (DP) to solve the formulated problem and derive the optimal rate allocation, routing and spectrum sharing policy for CRNs.",
    "keywords": [
      "cognitive radio networks",
      "dynamic resource allocations"
    ]
  },
  {
    "id": "1464",
    "title": "SPAN: Finding collaborative frauds in online auctions",
    "abstract": "Fraud is an ongoing concern for online auction websites. Current methods to detect or prevent fraud have been limited in several ways, making them difficult to apply in real world settings. Firstly, existing methods cannot adapt to changes in the behaviour of fraudulent users over time: new models must be continuously constructed as they gradually lose accuracy. In addition, each method can only be used to detect a specific type of fraud. Secondly, existing methods are generally poor at identifying collaborative frauds. And thirdly, method training and evaluation has been limited by the quality of available datasets. We propose an algorithm named SPAN (Score Propagation over an Auction Network), for detecting users committing collaborative fraud that addresses these problems. SPAN is a two phase method that first applies anomaly detection on multiple 2-dimensional feature subspaces to generate an initial anomaly score for each user, then applies belief propagation to revise those scores to identify suspicious groups of users. We report extensive experimental results using synthetic data which shows that SPAN performs well across three different types of fraud, and outperforms a previous approach for collaborative fraud detection called 2-Level Fraud Spotting. Experiments on a real dataset shows that SPAN behaves reasonably, and can identify groups of users that appear anomalous.",
    "keywords": [
      "online auction fraud",
      "fraud detection",
      "anomaly detection",
      "markov random fields",
      "belief propagation"
    ]
  },
  {
    "id": "1465",
    "title": "Using a discrete event simulator as real time graphic applications kernel",
    "abstract": "Simulation is often used to solve problems in many areas in the form of problem analysis. Real time graphic applications such as videogames typically use a continuous simulation scheme. This operating scheme has disadvantages that can be avoided by using a discrete event simulator as the application kernel. This paper proposes the integration of a discrete event simulator into a real time graphic application to control the kernel simulation. Using a discrete methodology avoids disorderly event execution or the execution of cancelled events. The use of this methodology involves using events as the method of modeling the system dynamics and the interaction and behavior of the objects.",
    "keywords": [
      "discrete events",
      "simulation",
      "computer graphics",
      "videogames"
    ]
  },
  {
    "id": "1466",
    "title": "Statistical mixture model for documents skew angle estimation",
    "abstract": "We present a statistical approach to skew detection, where the distribution of textual features of document images is modeled as a mixture of straight lines in Gaussian noise. The Expectation Maximization (EM) algorithm is used to estimate the parameters of the statistical model and the estimated skew angle is extracted from the estimated parameters. Experiments demonstrate that our method is favorably comparable to other existing methods in terms of accuracy and efficiency.",
    "keywords": [
      "skew detection",
      "statistical mixture models",
      "expectation maximization ",
      "linear least squares"
    ]
  },
  {
    "id": "1467",
    "title": "A novel hybridization of artificial neural networks and ARIMA models for time series forecasting",
    "abstract": "Improving forecasting especially time series forecasting accuracy is an important yet often difficult task facing decision makers in many areas. Both theoretical and empirical findings have indicated that integration of different models can be an effective way of improving upon their predictive performance, especially when the models in combination are quite different. Artificial neural networks (ANNs) are flexible computing frameworks and universal approximators that can be applied to a wide range of forecasting problems with a high degree of accuracy. However, using ANNs to model linear problems have yielded mixed results, and hence; it is not wise to apply ANNs blindly to any type of data. Autoregressive integrated moving average (ARIMA) models are one of the most popular linear models in time series forecasting, which have been widely applied in order to construct more accurate hybrid models during the past decade. Although, hybrid techniques, which decompose a time series into its linear and nonlinear components, have recently been shown to be successful for single models, these models have some disadvantages. In this paper, a novel hybridization of artificial neural networks and ARIMA model is proposed in order to overcome mentioned limitation of ANNs and yield more general and more accurate forecasting model than traditional hybrid ARIMA-ANNs models. In our proposed model, the unique advantages of ARIMA models in linear modeling are used in order to identify and magnify the existing linear structure in data, and then a neural network is used in order to determine a model to capture the underlying data generating process and predict, using preprocessed data. Empirical results with three well-known real data sets indicate that the proposed model can be an effective way to improve forecasting accuracy achieved by traditional hybrid models and also either of the components models used separately.  ",
    "keywords": [
      "artificial neural networks ",
      "auto-regressive integrated moving average ",
      "time series forecasting",
      "hybrid models"
    ]
  },
  {
    "id": "1468",
    "title": "Effects of additives in polymer thick film-organic light emitting diodes (PTF-OLED)",
    "abstract": "The optimum composition and film thickness determined for a single-layer organic light emitting diodes (120150cd/m2 at 30mA/cm2) has been determined in one of our previous publication [Display 21 (2001) 199]. In this following study, a series of additives was included in the original composition for the purposes of color tuning, enhancement of the device efficiency as well as the stability of the polymer thick film-organic light emitting diodes (PTF-OLED). The additives attempted including three dyes (rubrene, perylene and DCM), three sensitizers (naphthalene, anthracene and p-terphenyl), several stabilizers (antioxidant and photo-stabilizer) and a number of organic soluble salts known as phase transfer catalyst. Rubrene was found being able to improve the OLED efficiency by 3-fold and its durability by at least 60 times at a concentration of 510pph (in part per hundred of Alq3 replaced). The color of the OLED can also be tuned according to the additional dye used. The sensitizer naphtahlene can also improved the efficiency of the OLED by 2-fold while the effect of the other additives was less significant. In addition, the lowering of the turn-on voltage and thus a higher current efficiency resulted by the addition of a charge injection layer (lithium fluoride or calcium fluoride) in between the cathode and the organic layer. The performance of the doped PTF-OLED is within an order of magnitude compared to a heterojunction small molecule-based OLED.",
    "keywords": [
      "polymer thick film inks",
      "organic light emitting diodes",
      "additives",
      "sensitizers"
    ]
  },
  {
    "id": "1469",
    "title": "Biomolecular committor probability calculation enabled by processing in network storage",
    "abstract": "Computationally complex and data intensive atomic scale biomolecular simulation is enabled via processing in network storage (PINS): a novel distributed system framework to overcome bandwidth, compute, storage, organizational, and security challenges inherent to the wide-area computation and storage grid. PINS is presented as an effective and scalable scientific simulation framework to meet the unbounded requirements of a user of infinite need. The novel hybrid databasefilesystem architecture enables the high throughput computation and data generation required by our scientific target. Biomolecular simulation methods are correlated with the primary PINS components, including: client tools, hybrid database/file management service (GEMS), computation engine (Condor), virtual file system adapter (Parrot), and local file servers (Chirp). Performance for the PINS prototype is reported for the committor probability calculation of a solvated protein domain requiring 500 independent simulations and the generation of over 1,000,000 output files.",
    "keywords": [
      "high throughput computing",
      "distributed systems",
      "computational biophysics"
    ]
  },
  {
    "id": "1470",
    "title": "Null FoleySammon transform",
    "abstract": "In this paper, an important conclusion for linear discriminant analysis is proved: in the case of the small sample size, (number-of-classes-1) null projection directions (NPDs) do exist, on which, the within-class distance equals zero and the between-class distance is positive. The Null FoleySammon Transform (NFST), which is constituted by NPDs, is also proposed. Its effectiveness is evidenced by experiments on face recognition.",
    "keywords": [
      "linear discriminant analysis",
      "null foleysammon transform",
      "feature extraction",
      "fisher criterion",
      "foleysammon transform"
    ]
  },
  {
    "id": "1471",
    "title": "advanced techniques for the creation and propagation of modules in cartesian genetic programming",
    "abstract": "The choice of an appropriate hardware representation model is key to successful evolution of digital circuits. One of the most popular models is cartesian genetic programming, which encodes an array of logic gates into a chromosome. While several smaller circuits have been successfully evolved on this model, it lacks scalability. A recent approach towards scalable hardware evolution is based on the automated creation of modules from primitive gates. In this paper, we present two novel approaches for module creation, an age-based and a cone-based technique. Further, we detail a cone-based crossover operator for use with cartesian genetic programming. We evaluate the different techniques and compare them with related work. The results show that age-based module creation is highly effective, while cone-based approaches are only beneficial for regularly structured, multiple output functions such as multipliers.",
    "keywords": [
      "crossover operator",
      "module acquisition",
      "automatically defined functions ",
      "cartesian genetic programming "
    ]
  },
  {
    "id": "1472",
    "title": "Searching for Coexpressed Genes in Three-Color cDNA Microarray Data Using a Probabilistic Model-Based Hough Transform",
    "abstract": "The effects of a drug on the genomic scale can be assessed in a three-color cDNA microarray with the three color intensities represented through the so-called hexaMplot. In our recent study, we have shown that the Hough Transform (HT) applied to the hexaMplot can be used to detect groups of coexpressed genes in the normal-disease-drug samples. However, the standard HT is not well suited for the purpose because 1) the assayed genes need first to be hard-partitioned into equally and differentially expressed genes, with HT ignoring possible information in the former group; 2) the hexaMplot coordinates are negatively correlated and there is no direct way of expressing this in the standard HT and 3) it is not clear how to quantify the association of coexpressed genes with the line along which they cluster. We address these deficiencies by formulating a dedicated probabilistic model-based HT. The approach is demonstrated by assessing effects of the drug Rg1 on homocysteine-treated human umbilical vein endothetial cells. Compared with our previous study, we robustly detect stronger natural groupings of coexpressed genes. Moreover, the gene groups show coherent biological functions with high significance, as detected by the Gene Ontology analysis.",
    "keywords": [
      "three-color cdna microarray",
      "probabilistic hough transform",
      "gene coexpression",
      "hexamplot",
      "gene ontology"
    ]
  },
  {
    "id": "1473",
    "title": "Position-Invariant Robust Features for Long-Term Recognition of Dynamic Outdoor Scenes",
    "abstract": "A novel Position-Invariant Robust Feature, designated as PIRF, is presented to address the problem of highly dynamic scene recognition. The PIRF is obtained by identifying existing local features (i.e. SIFT) that have a wide baseline visibility within a place (one place contains more than one sequential images). These wide-baseline visible features are then represented as a single PIRF, which is computed as an average of all descriptors associated with the PIRF. Particularly, PIRFs are robust against highly dynamical changes in scene: a single PIRF can be matched correctly against many features from many dynamical images. This paper also describes an approach to using these features for scene recognition. Recognition proceeds by matching an individual PIRF to a set of features from test images, with subsequent majority voting to identify a place with the highest matched PIRF. The PIRF system is trained and tested on 2000+ outdoor omnidirectional images and on COLD datasets. Despite its simplicity, PIRF offers a markedly better rate of recognition for dynamic outdoor scenes (ca. 90%) than the use of other features. Additionally, a robot navigation system based on PIRF (PIRF-Nav) can outperform other incremental topological mapping methods in terms of time (70% less) and memory. The number of PIRFs can be reduced further to reduce the time while retaining high accuracy, which makes it suitable for long-term recognition and localization.",
    "keywords": [
      "scene localization",
      "scale invariant feature transformation ",
      "scene recognition",
      "topological mapping"
    ]
  },
  {
    "id": "1474",
    "title": "Quality function deployment implementation based on analytic network process with linguistic data: An application in automotive industry",
    "abstract": "Quality Function Deployment (QFD) is a cross-functional planning tool, which is used to ensure that the voice of the customer is deployed throughout the product planning and design stages. The QFD process involves various inputs in the form of linguistic data, e.g., human perception, judgment, and evaluation on importance or relationship strength. Such data are usually ambiguous and uncertain. This paper aims to implement of QFD under a fuzzy environment. In addition, the analytic network process (ANP), the general form of the analytic hierarchy process (AHP) is used, to prioritize design requirements by taking into account the degree of the interdependence between the customer needs and design requirements and the inner dependence among them. The developed approach is used to study the basic product planning stage of a car design.",
    "keywords": [
      "qfd",
      "house of quality",
      "anp",
      "ahp",
      "fuzzy numbers",
      "linguistic data",
      "car design"
    ]
  },
  {
    "id": "1475",
    "title": "An efficient simplex type algorithm for sparse and dense linear programs",
    "abstract": "We present a big-M method for solving general linear programming problems with a recently developed exterior point simplex algorithm (EPSA). We also provide intuitive motivations to EPSA. We describe major steps in implementing EPSA for large-scale linear programming. Our implementation was carried out under the MATLAB environment. This algorithm seems to be more efficient than the classical primal simplex algorithm (PSA), employing Dantzigs rule. Preliminary computational studies on randomly generated sparse and dense small and medium linear programs support this belief. In particular, EPSA is up to 10 times faster than PSA on medium size linear programs. This translates into corresponding savings in CPU time. Although the computational effort required in each step of EPSA requires more time compared to an iteration step of PSA, the improvement of EPSA comes from the fact that it requires adequately less iterations than PSA. Moreover, as the problem size increases and the problem density decreases, EPSA gets relatively faster.",
    "keywords": [
      "linear programming",
      "simplex type algorithms",
      "big-m method",
      "algorithm evaluation",
      "numerical experiments"
    ]
  },
  {
    "id": "1476",
    "title": "Independent Domination in Cubic Graphs",
    "abstract": "A set S of vertices in a graph G is an independent dominating set of G if S is an independent set and every vertex not in S is adjacent to a vertex inS. The independent domination number of G, denoted by, is the minimum cardinality of an independent dominating set. In this article, we show that if is a connected cubic graph of ordern that does not have a subgraph isomorphic to K2, 3, then . As a consequence of our main result, we deduce Reed's important result [Combin Probab Comput 5 (1996), 277295] that if G is a cubic graph of ordern, then , where denotes the domination number of G.",
    "keywords": [
      "independent domination number",
      "domination number",
      "cubic graphs "
    ]
  },
  {
    "id": "1477",
    "title": "Path based algorithms for metro network design",
    "abstract": "We propose a flexible and modular heuristic for the planning of metro networks. Two criteria are optimized jointly: population coverage and construction cost. Non-dominated lines are generated within corridors specified by the planners. These lines are then assembled into a network through mathematical programming. The algorithm is intuitive, flexible and computationally efficient.",
    "keywords": [
      "metro network design",
      "path based algorithm",
      "bicriteria optimization"
    ]
  },
  {
    "id": "1478",
    "title": "Effectiveness of the global modulus of continuity on metric spaces",
    "abstract": "In the \"delta-epsilon\" - definition of continuity of a function f between metric spaces, the value of delta depends on x, epsilon and function f as well. This kind of dependence can be described by a function, so called, \"global modulus of continuity\", which maps the triple (f,x, epsilon) to corresponding delta. By a recent result of Repovs and Semenov (Proc. Int. Conf. Topol. (Trieste, 1993), G. Gentili (Ed.), Rent. Ist. Mat. Univ. Trieste, vol. 25, 1993, pp. 441-446), there is a continuous global modulus of continuity for the function space C(X, Y) of all continuous functions from a locally compact metric space X to an arbitrary metric space Y. Based on Weihrauch's framework on computable metric spaces (K. Weihrauch, Theoret. Comput. Sci. 113 (1993) 191-210), we show that there is a computable global modulus of continuity for C(X, Y), if X is an \"effectively locally compact\" metric space and Y is a computable metric space. The proof is a direct construction not depending on the proof of Repovs and Semenov.  ",
    "keywords": [
      "continuous function",
      "computable metric space",
      "global modulus of continuity"
    ]
  },
  {
    "id": "1479",
    "title": "Rehearsing Naval Tactical Situations Using Simulated Teammates and an Automated Tutor",
    "abstract": "This paper describes a deployed simulation-based Intelligent Tutoring System (ITS) for training of Tactical Action Officers (TAOs). The TAO on board a Navy ship is responsible for the operation of the entire watch team manning the ship's command center. The ITS goal is to train the TAO in \"command by negation,\" in which watchstanders perform their duties autonomously, while the TAO supervises, intervening in order to correct mistakes and rectify omissions. The ITS uses artificial intelligence (AI) techniques to provide Automated Role Players (ARPs) representing the watchstanders in the ship, and to provide a Natural Language interface to communicate with these automated teammates. An adaptive coaching strategy is used to provide coaching and feedback during an exercise. The paper presents a discussion of the ITS instructional design, its architecture, and the AI techniques it employs.",
    "keywords": [
      "artificial intelligence applications",
      "computer-assisted instruction",
      "intelligent tutoring systems"
    ]
  },
  {
    "id": "1480",
    "title": "Six strategies for generalizing software engineering theories",
    "abstract": "General theories of software engineering must balance between providing full understanding of a single case and providing partial understanding of many cases. In this paper we argue that for theories to be useful in practice, they should give sufficient understanding of a sufficiently large class of cases, without having to be universal or complete. We provide six strategies for developing such theories of the middle range. In lab-to-lab strategies, theories of laboratory phenomena are developed and generalized to other laboratory phenomena. This is a characteristic strategy for basic science. In lab-to-field strategies, theories are developed of artifacts that first operate under idealized laboratory conditions, which are then scaled up until they can operate under uncontrolled field conditions. This is the characteristic strategy for the engineering sciences. In case-based strategies, we generalize about components of real-world cases, that are supposed to exhibit less variation than the cases as a whole. In sample-based strategies, we generalize about the aggregate behavior of samples of cases, which can exhibit patterns not visible at the case level. We discuss three examples of sample-based strategies. Throughout the paper, we use examples of theories and generalization strategies from software engineering to illustrate our analysis. The paper concludes with a discussion of related work and implications for empirical software engineering research.",
    "keywords": [
      "generalization",
      "external validity",
      "scaling up",
      "architectural mechanisms",
      "statistical inference"
    ]
  },
  {
    "id": "1481",
    "title": "THE EFFECTS OF THE BEYOND MEAN FIELD CORRECTIONS OF FERMI SUPERFLUID GAS IN A DOUBLE-WELL POTENTIAL",
    "abstract": "By considering the contribution of the higher-order term representing the lowest approximation of beyond mean field corrections, we investigate a superfluid Fermi gas confined in a double-well potential in Bose-Einstein Condensation (BEC) side of the Bardeen-Cooper-Schrieffer (BCS) to BEC crossover. Two limited cases of deep BEC regime and BEC regime of BCS-BEC crossover, corresponding to the two-body scattering length a(sc) is small enough and large enough, respectively. We derive a simple two-mode model that could depict the dynamics effectively. With making thorough analysis on the two-mode model and its corresponding classical Hamiltonian, we find that the Josephson oscillation or self-trapping phenomenon could emerge at certain parameters. We find three kinds of the phase states: Josephson oscillation (JO), oscillating-phase-type self-trapping (OPTST) and running-phase-type self-trapping (RPTST). The dependence of these three phase states on the dimensionless interaction parameter y = 1/(k(F)a(sc)) and the initial system energy are given in this paper.",
    "keywords": [
      "self-trapping phenomenon",
      "fermi superfluid gas",
      "double-well potential"
    ]
  },
  {
    "id": "1482",
    "title": "HIGH-PERFORMANCE THREE-DIMENSIONAL IMAGE RECONSTRUCTION FOR MOLECULAR STRUCTURE DETERMINATION",
    "abstract": "We describe an efficient parallel implementation of a reliable iterative reconstruction algorithm for estimating the three-dimensional (3D) density map of a macromolecular complex from a large number of two-dimensional (2D) cryo-electron microscopy (Cryo-EM) images. Our algorithm is based on a hybrid regularization approach first developed by Bjorck and O'Leary-Simmons. Our implementation uses a special data structure to represent the 3D density map to improve data locality in the reconstruction computation. Our parallelization strategy allows both 2D images and 3D data to be distributed on a 2D processor grid. We have used our implementation successfully on several datasets of different sizes, and we are able to achieve scalable parallel performance on a distributed memory cluster using over 15,000 CPUs for the largest dataset.",
    "keywords": [
      "cryo-em",
      "3d image reconstruction",
      "parallel computing",
      "regularization"
    ]
  },
  {
    "id": "1483",
    "title": "A mixed-integer programming formulation for the general capacitated lot-sizing problem",
    "abstract": "A new mixed-integer programming (MIP) formulation is presented for the production planning of single-stage multi-product processes. The problem is formulated as a multi-item capacitated lot-sizing problem in which (a) multiple items can be produced in each planning period, (b) sequence-independent set-ups can carry over from previous periods,  ",
    "keywords": [
      "production planning",
      "multi-item capacitated lot sizing",
      "mixed-integer programming",
      "set-up carry over",
      "set-up cross over"
    ]
  },
  {
    "id": "1484",
    "title": "Improving evaluation criteria for monitoring networks of weak radioactive plumes after nuclear emergencies",
    "abstract": "Networks of monitoring stations have been set up in many European countries to detect the passage of a radioactive cloud in the event of a large-scale nuclear emergency. The layout and spatial density of these networks differs according to the needs and criteria defined by national authorities. Germany and the Netherlands decided to set up relatively dense networks for the detection of weak radioactive plumes and, additionally, environmental radioactivity from deposited aerosols. Plausible evaluation criteria are presented here to assess important properties which determine the reliability and efficiency of sections of these networks. As a test case the existing sampling design of the Dutch and German networks with 193 sensors in an area of 200km around the nuclear power plant near the city of Lingen (Emsland) in the German federal state of Lower Saxony has been selected. For a hypothetical accident scenario 292 radioactive plumes have been simulated which are shaped by recorded weather conditions of the year 2007. To quantify the network performance frequency distributions of the proposed evaluation parameters have been analyzed. Simulation results show that 95% of the plumes are detected within 4h after the release. Maximal values of the ?-dose rate 1m above the ground mostly occur near the source within a radius of 5km. However, under certain weather conditions maximal ground values may also be found more than 50km away from the source. Within a circle of radius 90km 98% of the recorded maximal ?-dose rates of the plumes were found by 62% of the 193 sensors. But only in a joint network of German and Dutch sensors all simulated plumes triggered an alarm. This result encourages efforts of close international collaboration, e.g. between EU member states, in network design and operation. Test series which involved the removal of sensors either randomly or in a controlled way showed that the network configuration is fit for the intended purpose of detecting a large majority of plumes. But already a small reduction in the number of sensors would degrade the performance. Whereas the joint network triggers alarms reliably, it fails to detect the true plume maxima. The assessment criteria can be used for a revision of existing networks or for planning purposes in countries such as those applying for EU membership.",
    "keywords": [
      "anthropogenic environmental radioactivity",
      "emergency preparedness",
      "monitoring networks",
      "spatial data analysis"
    ]
  },
  {
    "id": "1485",
    "title": "A 200-mA, 93% peak power efficiency, single-inductor, dual-output DC-DC buck converter",
    "abstract": "A single-inductor dual-output (SIDO) DC-DC buck converter is presented. The circuit uses only one (external) inductor to provide two independent output voltages ranging from 1.2 V to the power supply (2.6-5 V) with a maximum total output current of 200 mA. The proposed converter has been fabricated in a 0.35-mu m p-substrate CMOS technology. Measurement results demonstrate that a peak power efficiency as high as 93.3% can be achieved. An automatic substrate bias switch technique, that cancels the body effect of the p-channel output power transistors, improves the converter power efficiency performance.",
    "keywords": [
      "dc-dc converters",
      "buck converter",
      "single-inductor dual-output"
    ]
  },
  {
    "id": "1486",
    "title": "Multi-agent framework for third party logistics in E-commerce",
    "abstract": "In an e-commerce environment, the third party logistics (3PL) takes charge of the logistics design, delivery, storage and transportation in a supply chain with its professional and complete value-added services. Beginning with an analysis of the relationships between the 3PL and supply chain members, the authors suggest that only when the 3PL reengineers its logistics business process to accommodate the customer could it maximize the value of the customer. Finally, five intelligent agents, order management agent, logistics process reengineering agent, resource scheduling agent, dynamic union management agent and simulating and evaluating agent are designed to form an e-commerce based 3PL system which, with the collaboration of the five agents, could construct a virtual private logistics teamwork suitable for a certain customer's need and furthermore, realize the winwin between the customer and the logistics service vendor.",
    "keywords": [
      "third party logistics ",
      "e-commerce",
      "business process reengineering",
      "multi-agent"
    ]
  },
  {
    "id": "1487",
    "title": "the implication and finite implication problems for typed template dependencies",
    "abstract": "The class of typed template dependencies is a class of data dependencies that includes embedded multivalued and join dependencies. We show that the implication and the finite implication problems for this class are unsolvable. An immediate corollary is that this class has no formal system for finite implication. We also show how to construct a finite set of typed template dependencies whose implication and finite implication problems are unsolvable.The class of simple template dependencies is a proper subclass of the above class, and it generalizes slightly embedded join dependencies. It is shown that the implication and the finite implication problems for this class are also unsolvable. An immediate corollary is that this class has no formal system for either implication or finite implication.",
    "keywords": [
      "dependencies",
      "systems",
      "formalism",
      "template",
      "data dependence",
      "embedding",
      "class"
    ]
  },
  {
    "id": "1488",
    "title": "using the burks 2 cd-rom in a principles of programming languages course",
    "abstract": "In this paper, we describe the use of the BURKS 2 CD-ROM in a principles of programming languages course. The CD-ROM contains compilers and documentation for many programming languages. The course described in this paper emphasises principles and paradigms of languages rather than the details of one or two specific languages.",
    "keywords": [
      "undergraduate curriculum",
      "programming languages"
    ]
  },
  {
    "id": "1489",
    "title": "A proof of the invariance of the contact angle in electrowetting",
    "abstract": "We prove the invariance of the contact angle in liquid-solid electrowetting phenomena: an electrified droplet is spreading on a solid surface. The drop minimizes its energy. We express the differential of this energy with respect to the shape of the drop and deduce necessary conditions for optimality. By variational methods, using well-chosen test functions, we obtain the main result about the contact angle between the drop and the solid.  ",
    "keywords": [
      "shape optimization",
      "variational calculus",
      "electrowetting"
    ]
  },
  {
    "id": "1490",
    "title": "A branch and bound algorithm for the cyclic job-shop problem with transportation",
    "abstract": "The cyclic job-shop problem with transportation can be used to describe optimization problems in fully automated manufacturing systems or assembly lines. We study the problem where the machines have no buffers, which rapidly decreases the number of feasible solutions and, therefore, makes it a lot harder to find those feasible solutions. After formulating the problem, we will characterize feasible solutions based on the route of the robot and their properties. With the aim of minimizing the cycle time, we have developed a tree search method to construct feasible solutions and combined it with a bounding procedure. Computational results are reported and compared to those gained by solving the problem with an LP solver.",
    "keywords": [
      "cyclic job-shop",
      "transport",
      "blocking",
      "branch and bound",
      "minimal cycle time"
    ]
  },
  {
    "id": "1491",
    "title": "a practical approach of memory access parallelization to exploit multiple off-chip ddr memories",
    "abstract": "3D stacked memory enables more off-chip DDR memories. Redesigning existing IPs to exploit the increased memory parallelism will be prohibitively costly. In our work, we propose a practical approach to exploit the increased bandwidth and reduced latency of multiple off-chip DDR memories while reusing existing IPs without modification. The proposed approach is based on two new concepts: transaction id renaming and distributed soft arbitration. We present two on-chip network components, request parallelizer and read data serializer, to realize the concepts. Experiments with synthetic test cases and an industrial strength DTV SoC design show that the proposed approach gives significant improvements in total execution cycle (21.6%) and average memory access latency (31.6%) in the DTV case with a small area overhead (30.1% in the on-chip network, and less than 1.4% in the entire chip).",
    "keywords": [
      "network",
      "latency",
      "stack",
      "test",
      "industrial",
      "exploit",
      "memory",
      "design",
      "arbitration",
      "concept",
      "data",
      "parallelization",
      "bandwidth",
      "case",
      "experience",
      "access",
      "distributed",
      "read",
      "practical",
      "transaction",
      "component"
    ]
  },
  {
    "id": "1492",
    "title": "Control approach for high sensitivity cardiopulmonary exercise testing during stimulated cycle ergometry in spinal cord injured subjects",
    "abstract": "People with complete lower-limb paralysis resulting from spinal cord injury (SCI) can perform cycle ergometry by means of functional electrical stimulation. Here, we propose and evaluate new exercise testing methods for estimation of cardiopulmonary performance parameters during this form of exercise. We utilised a customised ergometer incorporating feedback control of stimulated exercise workrate and cycling cadence. This allowed the imposition of arbitrary workrate profiles with high precision with the potential for improved sensitivity in exercise testing. New incremental exercise test (IET) and step exercise test (SET) protocols for determination of peak and steady-state performance parameters were assessed. The IET protocol allowed reliable determination of the ventilatory threshold, peak workrate and oxygen uptake-workrate relationship, but gave unrepresentative peak oxygen uptake values and highly variable measures of oxygen uptake kinetics. The SET protocol gave reliable estimation of steady-state oxygen uptake and metabolic efficiency of constant load exercise, but high variability in the estimation of oxygen uptake kinetics. The feedback-controlled testbed and the new IET and SET protocols have the potential for estimation of cardiopulmonary performance parameters with improved sensitivity during stimulated cycle ergometry in subjects with SCI.",
    "keywords": [
      "feedback control",
      "exercise physiology",
      "spinal cord injury"
    ]
  },
  {
    "id": "1493",
    "title": "Gauging human mobility characteristics and its impact on mobile routing performance",
    "abstract": "A new generation of 'behaviour-aware' services are emerging, defining the future mobile social networks. It is important for current mobility models to capture mobile users' behavioural characteristics and also reproduce their effects on the performance of networking protocols. Recent work in mobility modelling focused on replicating metrics of encounter statistics and spatio-temporal preferences. In this study, we attempt to show the sufficiency (or inadequacy) of these mobility metrics in reproducing realistic performance of networking protocols. We provide three main findings: (a) careful parameterisation of the models can replicate mobility metrics; (b) a rich set of communities in real mobile societies exist with distinct behavioural clusters of users; (c) even carefully crafted models surprisingly result in structural dynamics and protocol performance that is dramatically different from the trace-driven performance. These findings strongly suggest a need to re-visit mobility modelling to incorporate accurate behavioural characteristics.",
    "keywords": [
      "delay tolerant network",
      "encounter statistics",
      "mobility models",
      "protocol testing",
      "similarity",
      "mobile societies"
    ]
  },
  {
    "id": "1494",
    "title": "An automated method for the detection of pulmonary embolism in V/Q-scans",
    "abstract": "In this paper, a fully automatic method for the diagnosis of pulmonary embolism (PE) from V/Q-scans is presented. Image analysis is applied to the ventilation and the perfusion images obtained in the V/Q-scan. The difference of the ventilation and the perfusion is calculated after transformation and hot-spot reduction of the images. From this difference image the integral of the underperfused areas are used as features. With the aid of these features a simple test for PE is devised. The method is evaluated on two sets of patients. One set comprises 102 patients who have undergone both V/Q-scanning and angiography. The performance given as the area under the Receiver Operating Characteristic (ROC) curve is 0.85. Another set is made up by the 507 consecutive patients examined with V/Q-scanning at Lund University Hospital in Sweden. In this case, the reference was the consensus opinion of two radiologists, the ROC-area is 0.67. A fully automatic and reasonably robust expert system is developed to aid the radiologist in the interpretation of V/Q-scans for PE.",
    "keywords": [
      "automated method",
      "pulmonary embolism",
      "v/q-scans"
    ]
  },
  {
    "id": "1495",
    "title": "Validating visual clusters in large datasets: fixed point clusters of spectral features",
    "abstract": "Finding clusters in large datasets is a difficult task. Almost all computationally feasible methods are related to k-means and need a clear partition structure of the data, while most such datasets contain masking outliers and other deviations from the usual models of partitioning cluster analysis. It is possible to look for clusters informally using graphic tools like the grand tour, but the meaning and the validity of such patterns is unclear. In this paper, a three-step-approach is suggested: In the first step, data visualization methods like the grand tour are used to find cluster candidate subsets of the data. In the second step, reproducible clusters are generated from them by means of fixed point clustering, a method to find a single cluster at a time based on the Mahalanobis distance. In the third step, the validity of the clusters is assessed by the use of classification plots. The approach is applied to an astronomical dataset of spectra from the Hamburg/ESO survey.",
    "keywords": [
      "discriminant coordinates",
      "outliers",
      "contamination model",
      "sky surveys",
      "stellar populations"
    ]
  },
  {
    "id": "1496",
    "title": "Retrieval constraints and word frequency distributions a log-logistic model for IR",
    "abstract": "We first present in this paper an analytical view of heuristic retrieval constraints which yields simple tests to determine whether a retrieval function satisfies the constraints or not. We then review empirical findings on word frequency distributions and the central role played by burstiness in this context. This leads us to propose a formal definition of burstiness which can be used to characterize probability distributions with respect to this phenomenon. We then introduce the family of information-based IR models which naturally captures heuristic retrieval constraints when the underlying probability distribution is bursty and propose a new IR model within this family, based on the log-logistic distribution. The experiments we conduct on several collections illustrate the good behavior of the log-logistic IR model: It significantly outperforms the Jelinek-Mercer and Dirichlet prior language models on most collections we have used, with both short and long queries and for both the MAP and the precision at 10 documents. It also compares favorably to BM25 and has similar performance to classical DFR models such as InL2 and PL2.",
    "keywords": [
      "retrieval constraints",
      "burstiness",
      "information retrieval theory",
      "log-logistic distribution"
    ]
  },
  {
    "id": "1497",
    "title": "Analysis of Electrical Industrial Systems Using Probabilistic Networks",
    "abstract": "This paper presents a methodology in order to evaluate the security of electrical industrial systems considering security, quality, reliability and availability factors and the system response to sudden disturbances such as short circuit and harmonics. The models proposed using the probabilistic networks theory: Bayesian Networks and Generalized Stochastic Petri Nets. The obtained results allow the identification of hidden failures and erroneus coordination of protection. The models are tested on the IEEE 493 system proposed by the IEEE Gold Book.",
    "keywords": [
      "bayes networks",
      "petri nets",
      "power system reliability"
    ]
  },
  {
    "id": "1498",
    "title": "Fuzzy random renewal process and renewal reward process",
    "abstract": "So far, there have been several concepts about fuzzy random variables and their expected values in literature. One of the concepts defined by Liu and Liu (2003a) is that the fuzzy random variable is a measurable function from a probability space to a collection of fuzzy variables and its expected value is described as a scalar number. Based on the concepts, this paper addresses two processes-fuzzy random renewal process and fuzzy random renewal reward process. In the fuzzy random renewal process, the interarrival times are characterized as fuzzy random variables and a fuzzy random elementary renewal theorem on the limit value of the expected renewal rate of the process is presented. In the fuzzy random renewal reward process, both the interarrival times and rewards are depicted as fuzzy random variables and a fuzzy random renewal reward theorem on the limit value of the long-run expected reward per unit time is provided. The results obtained in this paper coincide with those in stochastic case or in fuzzy case when the fuzzy random variables degenerate to random variables or to fuzzy variables.",
    "keywords": [
      "renewal process",
      "renewal reward process",
      "fuzzy random variable",
      "fuzzy variable",
      "stochastic renewal process"
    ]
  },
  {
    "id": "1499",
    "title": "Detection of highly articulated moving objects by using co-segmentation with application to athletic video sequences",
    "abstract": "In this paper, we adapt the co-segmentation for the fundamental problem of segmenting automatically and accurately highly articulated athletes in a large variety of poses without any initialization or prior knowledge. Our intention is to reduce the complexity of athlete segmentation by formulating it as a constrained 2D pair of frames co-segmentation, in order to extract the common foreground objects under unconstrained environments without any user input. In fact, the co-segmentation allows to integrate implicitly the temporal information for automatic moving object segmentation without any assumption or prior knowledge on camera motion. The proposed method was applied on various real-world video sequences of athletic sports meetings, and promising results are obtained. Experiments show that suggested method witnessed a significant improvement over background subtraction methods, which are commonly used for athlete segmentation.",
    "keywords": [
      "video-based analysis of athlete action",
      "moving object segmentation",
      "co-segmentation"
    ]
  },
  {
    "id": "1500",
    "title": "Identification of the state-space dynamics of oil flames through computer vision and modal techniques",
    "abstract": "This paper presents a new approach to estimate unstable flame dynamics. Computer vision processed images from CCD cameras provide a discriminant characteristic vector. A four-degree-of-freedom second order model is identified through Operational Modal Analysis. Unstable and stable flame models are cross-validated through spectral analysis. The method is capable of predicting flame extinction.",
    "keywords": [
      "flame instabilization characterizing",
      "operational modal analysis",
      "discriminant characteristic vectors",
      "artificial intelligence image processing"
    ]
  },
  {
    "id": "1501",
    "title": "Sound and Extensible Renaming for Java",
    "abstract": "Descriptive names are crucial to understand code. However, good names are notoriously hard to choose and manually changing a globally visible name can be a maintenance nightmare. Hence, tool support for automated renaming is an essential aid for developers and widely supported by popular development environments. This work improves on two limitations in current refactoring tools: too weak preconditions that lead to unsoundness where names do not bind to the correct declarations after renaming, and too strong preconditions that prevent renaming of certain programs. We identify two main reasons for unsoundness: complex name lookup rules make it hard to define sufficient preconditions, and new language features require additional preconditions. We alleviate both problems by presenting a novel extensible technique for creating symbolic names that are guaranteed to bind to a desired entity in a particular context by inverting lookup functions. The inverted lookup functions can then be tailored to create qualified names where otherwise a conflict would occur, allowing the refactoring to proceed and improve on the problem with too strong preconditions. We have implemented renaming for Java as an extension to the JastAdd Extensible Java Compiler and integrated it in Eclipse. We show examples for which other refactoring engines have too weak preconditions, as well as examples where our approach succeeds in renaming entities by inserting qualifications. To validate the extensibility of the approach we have implemented renaming support for Java 5 and AspectJ like inter-type declarations as modular extensions to the initial Java 1.4 refactoring engine. The renaming engine is only a few thousand lines of code including extensions and performance is on par with industrial strength refactoring tools.",
    "keywords": [
      "languages",
      "refactoring",
      "name analysis",
      "renaming",
      "extensible compilers"
    ]
  },
  {
    "id": "1502",
    "title": "large-scale behavioral targeting with a social twist",
    "abstract": "Behavioral targeting (BT) is a widely used technique for online advertising. It leverages information collected on an individual's web-browsing behavior, such as page views, search queries and ad clicks, to select the ads most relevant to user to display. With the proliferation of social networks, it is possible to relate the behavior of individuals and their social connections. Although the similarity among connected individuals are well established (i.e., homophily), it is still not clear whether and how we can leverage the activities of one's friends for behavioral targeting; whether forecasts derived from such social information are more accurate than standard behavioral targeting models. In this paper, we strive to answer these questions by evaluating the predictive power of social data across 60 consumer domains on a large online network of over 180 million users in a period of two and a half months. To our best knowledge, this is the most comprehensive study of social data in the context of behavioral targeting on such an unprecedented scale. Our analysis offers interesting insights into the value of social data for developing the next generation of targeting services.",
    "keywords": [
      "social targeting",
      "large-scale data mining",
      "advertising",
      "social-network analysis",
      "behavioral targeting"
    ]
  },
  {
    "id": "1503",
    "title": "Investigation of a combined embossing and blanking process using laser shock wave",
    "abstract": "Laser shock forming is a three-dimensional forming technique. It is promising for achieving precise, well-controlled, low-cost, high efficiency 3D metallic microstructures. In this research, a combined process of embossing and blanking is studied by experimental and simulation methods, in which the embossing and blanking of copper sheets are performed at the same time in only one operation. In order to obtain multi-workpieces in one process, overlapping laser spots were applied in the experiment. Moreover, the influence of laser energies on the work pieces were considered, and the results show that the micro features on the boss surface of the mold were successfully replicated on the blanking copper foil surface with the suitable energy. By evaluating the surface roughness data, it can imply that the work piece surface can fit the mold surface. Meanwhile, the process of the combined embossing and blanking was also studied by numerical simulation, the simulation results show that the finite element analysis can predict the final shape of work piece properly, verifying the feasibility of this process.",
    "keywords": [
      "laser shock wave",
      "micro embossing",
      "micro blanking"
    ]
  },
  {
    "id": "1504",
    "title": "The sensory ego-sphere: a mediating interface between sensors and cognition",
    "abstract": "The Sensory Ego-Sphere (SES) is an interface for a robot that serves to mediate information between sensors and cognition. The SES can be visualized as a sphere centered on the coordinate frame of the robot, spatially indexed by polar and azimuthal angles. Internally, the SES is a graph with a fixed number of edges that partitions surrounding space and contains localized sensor information from the robot. This paper describes the SES and gives the results of implementing the SES on multiple robots, both humanoid and mobile, to support essential functions such as a localized short-term memory, spatio-temporal sensory-motor event detection, attentional processing, data sharing, and ego-centric navigation.",
    "keywords": [
      "robotics",
      "artificial intelligence",
      "sensory fusion",
      "sensory-motor coordination",
      "short-term memory"
    ]
  },
  {
    "id": "1505",
    "title": "DNA Lesions and Repair in Immunoglobulin Class Switch Recombination and Somatic Hypermutation",
    "abstract": "Abstract: Immunoglobulin (Ig) gene somatic hypermutation (SHM) and class switch DNA recombination (CSR) are critical for the maturation of the antibody response. These processes endow antibodies with increased antigen-binding affinity and acquisition of new biological effector functions, thereby underlying the generation of memory B cells and plasma cells. They are dependent on the generation of specific DNA lesions and the intervention of activation-induced cytidine deaminase as well as newly identified translesion DNA polymerases, which are expressed in germinal center B cells. DNA lesions include mismatches, abasic sites, nicks, single-strand breaks, and double-strand breaks (DSBs). DSBs in the switch (S) region DNA are critical for CSR, but they also occur in V(D)J regions and possibly contribute to the events that lead to SHM. The nature of the DSBs in the Ig locus, their generation, and the repair processes that they trigger and that are responsible for their regulation remain poorly understood. Aberrant regulation of these events can result in chromosomal breaks and translocations, which are significant steps in B-cell neoplastic transformation.",
    "keywords": [
      "activation-induced cytosine deaminase ",
      "class switch recombination ",
      "dna lesion",
      "double-strand break ",
      "error-prone dna repair",
      "lesion bypass or translesion dna polymerase",
      "pol ?",
      "pol ?",
      "pol ?",
      "somatic hypermutation ",
      "vj recombination"
    ]
  },
  {
    "id": "1506",
    "title": "Euler circuits and DNA sequencing by hybridization",
    "abstract": "Sequencing by hybridization is a method of reconstructing a long DNA string  that is, figuring out its nucleotide sequence  from knowledge of its short substrings. Unique reconstruction is not always possible, and the goal of this paper is to study the number of reconstructions of a random string. For a given string, the number of reconstructions is determined by the pattern of repeated substrings; in an appropriate limit substrings will occur at most twice, so the pattern of repeats is given by a pairing: a string of length 2n in which each symbol occurs twice. A pairing induces a 2-in, 2-out graph, whose directed edges are defined by successive symbols of the pairing  for example the pairing ABBCAC induces the graph with edges AB, BB, BC, and so forth  and the number of reconstructions is simply the number of Euler circuits in this 2-in, 2-out graph. The original problem is thus transformed into one about pairings: to find the number fk(n) of n-symbol pairings having k Euler circuits. We show how to compute this function, in closed form, for any fixed k, and we present the functions explicitly for k=1,,9. The key is a decomposition theorem: the Euler circuit number of a pairing is the product of the circuit numbers of component sub-pairings. These components come from connected components of the interlace graph, which has the pairing's symbols as vertices, and edges when symbols are interlaced. (A and B are interlaced if the pairing has the form ?A?B?A?B? or ?B?A?B?A?.) We carry these results back to the original question about DNA strings, and provide a total variation distance upper bound for the approximation error. We perform an asymptotic enumeration of 2-in, 2-out digraphs to show that, for a typical random n-pairing, the number of Euler circuits is of order no smaller than 2n/n, and the expected number is asymptotically at least e?1/22n?1/n. Since any n-pairing has at most 2n?1 Euler circuits, this pinpoints the exponential growth rate.",
    "keywords": [
      "combinatorial enumeration",
      "pairing",
      "circle graph",
      "interlace graph",
      "euler path",
      "euler circuit",
      "matrix-tree theorem",
      "best theorem",
      "dna sequencing",
      "hybridization",
      "catalan number"
    ]
  },
  {
    "id": "1507",
    "title": "A robust control law with estimated perturbation compensation for robot manipulators",
    "abstract": "By considering the dynamic response of a robot manipulator as characterized by the sliding function, a technique is proposed to estimate the perturbation in the robot control system. Perturbation compensation is then incorporated in the design of a robust control law to cancel the effects of system parametric uncertainties and external disturbances. A normalized power rate component is introduced to replace the discontinuous control that is usually associated with variable structure system. The suggested robust control law ensures that the robot control system reaches an user specified neighbourhood of the sliding manifold in finite time and with prescribed transient behaviour. Explicit estimates of the bounds on modelling errors and external disturbances are not required while signal measurement uncertainties can be accommodated. Furthermore, using the equivalent control concept, a very simple expression is derived to estimate the system perturbation signal. Detailed computer simulation results are presented to demonstrate the effectiveness of the proposed control law.",
    "keywords": [
      "robot manipulators",
      "variable structure control",
      "sliding mode",
      "perturbation compensation",
      "equivalent control"
    ]
  },
  {
    "id": "1508",
    "title": "optimized color gamuts for tiled displays",
    "abstract": "We consider the problem of finding a large color space that can be generated by all units in multi-projector tiled display systems. Viewing the problem geometrically as one of finding a large parallelepiped within the intersection of multiple parallelepipeds, and using colorimetric principles to define a volume-based objective function for comparing feasible solutions, we develop an algorithm for finding the optimal gamut in time O(n 3 ) , where n denotes the number of projectors in the system. We also discuss more efficient quasiconvex programming algorithms for alternative objective functions based on maximizing the quality of the color space extrema.",
    "keywords": [
      "tiled displays",
      "geometric optimization",
      "additive color",
      "high-resolution display systems",
      "quasiconvex programming",
      "color gamuts",
      "gamut mapping"
    ]
  },
  {
    "id": "1509",
    "title": "Accurate contact angle boundary conditions for the Cahn-Hilliard equations",
    "abstract": "The contact angle dynamics between a two-phase interface and a solid surface is important in physical interpretations, mathematical modeling, and numerical treatments. We present a novel formulation based on a characteristic interpolation for the contact angle boundary conditions for the Cahn-Hilliard equation. The new scheme inherits characteristic properties, such as the mass conservation, the total energy decrease, and the unconditionally gradient stability. We demonstrate the accuracy and robustness of the proposed contact angle boundary formulation with various numerical experiments. The numerical results indicate a potential usefulness of the proposed method for accurately calculating contact angle problems.  ",
    "keywords": [
      "cahn-hilliard equation",
      "contact angle",
      "unconditionally gradient stable scheme",
      "nonlinear multigrid method"
    ]
  },
  {
    "id": "1510",
    "title": "A model of non-elemental olfactory learning in Drosophila",
    "abstract": "The pathways for olfactory learning in the fruitfly Drosophila have been extensively investigated, with mounting evidence that that the mushroom body is the site of the olfactory associative memory trace (Heisenberg, Nature 4:266275, 2003; Gerber et al., Curr Opin Neurobiol 14:737744, 2004). Heisenbergs description of the mushroom body as an associative learning device is a testable hypothesis that relates the mushroom bodys function to its neural structure and input and output pathways. Here, we formalise a relatively complete computational model of the network interactions in the neural circuitry of the insect antennal lobe and mushroom body, to investigate their role in olfactory learning, and specifically, how this might support learning of complex (non-elemental; Giurfa, Curr Opin Neuroethol 13:726735, 2003) discriminations involving compound stimuli. We find that the circuit is able to learn all tested non-elemental paradigms. This does not crucially depend on the number of Kenyon cells but rather on the connection strength of projection neurons to Kenyon cells, such that the Kenyon cells require a certain number of coincident inputs to fire. As a consequence, the encoding in the mushroom body resembles a unique cue or configural representation of compound stimuli (Pearce, Psychol Rev 101:587607, 1994). Learning of some conditions, particularly negative patterning, is strongly affected by the assumption of normalisation effects occurring at the level of the antennal lobe. Surprisingly, the learning capacity of this circuit, which is a simplification of the actual circuitry in the fly, seems to be greater than the capacity expressed by the fly in shock-odour association experiments (Young etal. 2010).",
    "keywords": [
      "computational model",
      "olfactory pathways",
      "mushroom body",
      "coincidence detection",
      "non-elemental"
    ]
  },
  {
    "id": "1511",
    "title": "A GPU-based discrete event simulation kernel",
    "abstract": "The graphic processing unit (GPU) can perform some large-scale simulations in an economical way. However, harnessing the power of a GPU to discrete event simulation (DES) is difficult because of the mismatch between GPU's synchronous execution mode and DES's asynchronous time advance mechanism. In this paper, we present a GPU-based simulation kernel (gDES) to support DES and propose three algorithms to support high efficiency. Since both limited parallelism and redundant synchronization affect the performance of DES based on a GPU, we propose a breadth-expansion conservative time window algorithm to increase the degree of parallelism while retaining the number of synchronizations. By using the expansion method, it can import as many as possible safe' events. The irregular and dynamic requirement for storing the events leads to uneven and sparse memory usage, thereby causing waste of memory and unnecessary overhead. A memory management algorithm is proposed to store events in a balanced and compact way by using a lightweight stochastic method. When events processed by threads in a warp have different types, the performance of gDES decreases rapidly because of branch divergence. An event redistribution algorithm is proposed by reassigning events of the same type to neighboring threads to reduce the probability of branch divergence. We analyze the superiority of the proposed algorithms and gDES with a series of experiments. Compared to a CPU-based simulator on a multicore platform, the gDES can achieve up to 11x, 5x, and 8x speedup in PHOLD, QUEUING NETWORK, and epidemic simulation, respectively.",
    "keywords": [
      "parallel discrete event simulation",
      "gpu",
      "time management",
      "memory management",
      "event redistribution"
    ]
  },
  {
    "id": "1512",
    "title": "Selective Flexibility: Creating Domain-Specific Reconfigurable Arrays",
    "abstract": "Historically, hardware acceleration technologies have either been application-specific, therefore lacking in flexibility, or fully programmable, thereby suffering from notable inefficiencies on an application-by-application basis. To address the growing need for domain-specific acceleration technologies, this paper describes a design methodology (i) to automatically generate a domain-specific coarse-grained array from a set of representative applications and (ii) to introduce limited forms of architectural generality to increase the likelihood that additional applications can be successfully mapped onto it. In particular, coarse-grained arrays generated using our approach are intended to be integrated into customizable processors that use application-specific instruction set extensions to accelerate performance and reduce energy; rather than implementing these extensions using application-specific integrated circuit (ASIC) logic, which lacks flexibility, they can be synthesized onto our reconfigurable array instead, allowing the processor to be used for a variety of applications in related domains. Results show that our array is around 2x slower and 15x larger than an ultimately efficient ASIC implementation, and thus far more efficient than field-programmable gate arrays (FPGAs), which are known to be 3-4x slower and 20-40x larger. Additionally, we estimate that our array is usually around 2x larger and 2x slower than an accelerator synthesized using traditional datapath merging, which has, if any, very limited flexibility beyond the design set of DFGs.",
    "keywords": [
      "datapaths",
      "domain-specific customization",
      "flexibility",
      "fpga routing",
      "reconfigurable arrays"
    ]
  },
  {
    "id": "1513",
    "title": "Feature selection using localized generalization error for supervised classification problems using RBFNN",
    "abstract": "A pattern classification problem usually involves using high-dimensional features that make the classifier very complex and difficult to train. With no feature reduction, both training accuracy and generalization capability will suffer. This paper proposes a novel hybrid filterwrapper-type feature subset selection methodology using a localized generalization error model. The localized generalization error model for a radial basis function neural network bounds from above the generalization error for unseen samples located within a neighborhood of the training samples. Iteratively, the feature making the smallest contribution to the generalization error bound is removed. Moreover, the novel feature selection method is independent of the sample size and is computationally fast. The experimental results show that the proposed method consistently removes large percentages of features with statistically insignificant loss of testing accuracy for unseen samples. In the experiments for two of the datasets, the classifiers built using feature subsets with 90% of features removed by our proposed approach yield average testing accuracies higher than those trained using the full set of features. Finally, we corroborate the efficacy of the model by using it to predict corporate bankruptcies in the US.",
    "keywords": [
      "feature selection",
      "neural network",
      "generalization error",
      "rbfnn"
    ]
  },
  {
    "id": "1514",
    "title": "A Preprocessing Procedure for Haplotype Inference by Pure Parsimony",
    "abstract": "Haplotype data are especially important in the study of complex diseases since it contains more information than genotype data. However, obtaining haplotype data is technically difficult and costly. Computational methods have proved to be an effective way of inferring haplotype data from genotype data. One of these methods, the haplotype inference by pure parsimony approach ( HIPP), casts the problem as an optimization problem and as such has been proved to be NP-hard. We have designed and developed a new preprocessing procedure for this problem. Our proposed algorithm works with groups of haplotypes rather than individual haplotypes. It iterates searching and deleting haplotypes that are not helpful in order to find the optimal solution. This preprocess can be coupled with any of the current solvers for the HIPP that need to preprocess the genotype data. In order to test it, we have used two state-of-the-art solvers, RTIP and GAHAP, and simulated and real HapMap data. Due to the computational time and memory reduction caused by our preprocess, problem instances that were previously unaffordable can be now efficiently solved.",
    "keywords": [
      "biology and genetics",
      "haplotype inference",
      "optimization"
    ]
  },
  {
    "id": "1515",
    "title": "Robustness of globally exponential stability of delayed neural networks in the presence of random disturbances",
    "abstract": "This paper analyzes the robustness of globally exponential stability of time-varying delayed neural networks (NNs) subjected to random disturbances. Given a globally exponentially stable neural network, and in the presence of noise, we quantify how much noise intensity that the delayed neural network can remain to be globally exponentially stable. We characterize the upper bounds of the noise intensity for the delayed NNs to sustain globally exponential stability. The upper bounds of parameter uncertainty intensity are characterized by using transcendental equation. A numerical example is provided to illustrate the theoretical result.",
    "keywords": [
      "delayed neural networks",
      "globally exponential stability",
      "random disturbances",
      "robustness"
    ]
  },
  {
    "id": "1516",
    "title": "Throughput Scaling of Wireless Networks With Random Connections",
    "abstract": "This work studies the throughput scaling laws of ad hoc wireless networks in the limit of a large number of nodes. A random connections model is assumed in which the channel connections between the nodes are drawn independently from a common distribution. Transmitting nodes are subject to an on-off strategy, and receiving nodes employ conventional single-user decoding. The following results are proven: 1) for a class of connection models with finite mean and variance, the throughput scaling is upper-bounded by (n(1/3)) for single-hop schemes, and O(n(1/2)) for two-hop (and multihop) schemes; 2) the Theta(n(1/2)) throughput scaling is achievable for a specific connection model by a two-hop opportunistic relaying scheme, which employs full, but only local channel state information (CSI) at the receivers, and partial CSI at the transmitters; 3) by relaxing the constraints of finite mean and variance of the connection model, linear throughput scaling Theta(n) is achievable with Pareto-type fading models.",
    "keywords": [
      "ad hoc networks",
      "channel state information ",
      "multiuser diversity",
      "opportunistic communication",
      "random connections",
      "scaling law",
      "throughput"
    ]
  },
  {
    "id": "1517",
    "title": "Flow structures in cerebral aneurysms",
    "abstract": "Mechanical properties of blood flow are commonly correlated to a wide range of cardiovascular diseases. In this work means to describe and characterise the flow field in the free-slip and no-slip domains are discussed in the context of cerebral aneurysms, reconstructed from in vivo medical imaging. The approaches rely on a Taylor series expansion of the velocity field to first order terms that leads to a system of ODEs, the solution to which locally describes the motion of the flow. On performing the expansion on the vessel wall using the wall shear stress, the critical points can be identified and the near-wall flow field parallel to the wall can be concisely described and visualised. Furthermore the near-wall expansion can be expressed in terms of relative motion, and the near-wall convective transport normal and parallel to the wall can be accurately derived on the no-slip domain. Together, these approaches give a viable and robust means to identify and describe fluid mechanic phenomena both qualitatively and quantitatively, leading to feasible practical use in biomedical applications. From analysis of steady and unsteady flow simulations in two anatomically accurate cerebral saccular aneurysm cases, a set of measures can be readily obtained at all time intervals, including the impingement region, separation lines, convective transport near the wall and vortex core lines or structures, which have all been related to diseased states. Other fluid mechanic measures are also discussed in order to give further detail and insight during post-processing, and may play an important role in the growth and rupture of the aneurysm.  ",
    "keywords": [
      "description of flow field",
      "flow structures",
      "cerebral aneurysm",
      "computational haemodynamics",
      "critical points",
      "vortex core extraction"
    ]
  },
  {
    "id": "1518",
    "title": "An integrated production planning model with load-dependent lead-times and safety stocks",
    "abstract": "The divergence over the years or research paradigms addressing the production planning problem has led to the development of an extensive set of techniques, each of which can address a particular aspect of the practical problem and none of which provides a complete Solution. In particular most approaches fail to. address the circular, non-linear dependency between resource utilization, lead-times and safety stocks. We present a non-linear programming formulation of the integrated problem using clearing functions that determines a work release schedule guaranteeing a specified service level in the face of stochastic demand We introduce an iterative heuristic solution procedure that solves a relaxed LP approximation of the original NLP at each iteration to determine the lead-time profile to set safety-stock levels Computational experiments suggest that our proposed iterative procedure performs well relative to conventional LP models that assume fixed. workload-independent lead-times.  ",
    "keywords": [
      "capacity planning",
      "clearing function",
      "safety stock",
      "load-dependent lead-times",
      "linear programming"
    ]
  },
  {
    "id": "1519",
    "title": "Price markdown scheme in a multi-echelon supply chain in a high-tech industry",
    "abstract": "This paper studies the price markdown scheme in a supply chain that consists of a supplier, a contract manufacturer (CM), and a buyer (retailer). The buyer subcontracts the production of the final product to the CM. The CM buys the components from the supplier and charges the buyer a service fee for the final product produced. The price markdown is made possible by the supplier with the development of new manufacturing technologies that reduce the production cost for the sourced component. Consequently, the buyer adjusts the retail price in order to possibly stimulate stronger demand that may benefit both the supplier and the buyer. Under this scenario, we identify the optimal discount pricing strategies, capacity reservation, and the stocking policies for the supplier and the buyer. We also investigate the optimal inventory decision for the CM to cope with the price discount by considering both demand and delivery uncertainties. Our results suggest that higher production cost accelerates the effects of higher price sensitivity on lowering the optimal capacity and stocking policies in the supply chain. The effect of mean demand error on the optimal prices is relatively marginal compared with that from price sensitivity. We also found that increasing the standard deviation of the random demand does not necessarily increase the stocking level as one would predict. The results show that delivery uncertainty plays an important role in the inventory carried beyond the price break. We discuss potential extensions for future research.",
    "keywords": [
      "three-echelon supply chain",
      "price markdown",
      "game theory",
      "lead-time demand"
    ]
  },
  {
    "id": "1520",
    "title": "Human evolutionary model: A new approach to optimization",
    "abstract": "The aim of this paper is to propose the Human Evolutionary Model (HEM) as a novel computational method for solving search and optimization problems with single or multiple objectives. HEM is an intelligent evolutionary optimization method that uses consensus knowledge from experts with the aim of inferring the most suitable parameters to achieve the evolution in an intelligent way. HEM is able to handle experts knowledge disagreements by the use of a novel concept called Mediative Fuzzy Logic (MFL). The effectiveness of this computational method is demonstrated through several experiments that were performed using classical test functions as well as composite test functions. We are comparing our results against the results obtained with the Genetic Algorithm of the Matlabs Toolbox, Evolution Strategy with Covariance Matrix Adaptation (CMA-ES), Particle Swarm Optimizer (PSO), Cooperative PSO (CPSO), G3 model with PCX crossover (G3-PCX), Differential Evolution (DE), and Comprehensive Learning PSO (CLPSO). The results obtained using HEM outperforms the results obtained using the abovementioned optimization methods.",
    "keywords": [
      "hem",
      "intelligent evolutionary algorithm",
      "optimization",
      "fuzzy adaptation"
    ]
  },
  {
    "id": "1521",
    "title": "a new algorithm for scalar register promotion based on ssa form",
    "abstract": "We present a new register promotion algorithm based on Static Single Assignment (SSA) form. Register promotion is aimed at promoting program names from memory locations to registers. Our algorithm is profile-driven and is based on the scope of intervals. In cases where a complete promotion is not possible because of the presence of function calls or pointer references, the proposed algorithm is capable of eliminating loads and stores on frequently executed paths by placing loads and stores on less frequently executed paths. We also describe an efficient method to incrementally update SSA form when new definitions are cloned from an existing name during register promotion. On SPECInt95 benchmarks, our algorithm removes about ~12% of memory operations which access scalar variables.",
    "keywords": [
      "place",
      "variability",
      "method",
      "interval",
      "efficiency",
      "locatability",
      "benchmark",
      "definition",
      "functional",
      "assignment",
      "presence",
      "operability",
      "memorialized",
      "access",
      "algorithm",
      "profiles",
      "pointers",
      "update",
      "completeness"
    ]
  },
  {
    "id": "1522",
    "title": "Comparing environments for developing software agents",
    "abstract": "In the last years, dozens of environments for modeling, testing and finally implementing multi-agent systems have been developed. Unfortunately, no standard criteria for understanding what kind of application profile a particular development environment is good for have been individuated yet, and the question 'How should I choose an existing environment which best suits the features and requirements of my application?' is still difficult to answer. This paper addresses this question, and aims at helping the multi-agent system developer to solve this problem. It provides a set of criteria for evaluating development environments, and then applies these criteria to five selected tools and multi-agent systems prototypes. Furthermore, some application-driven guidelines are described to help identifying the features of a suitable environment for developing an implementation of the given application. The features we identify can be used to find the right development framework among the frameworks we evaluate for doing the right application.",
    "keywords": [
      "agent frameworks",
      "software agent development",
      "tools",
      "multi-agent systems",
      "classification"
    ]
  },
  {
    "id": "1523",
    "title": "On the stability of the Immersed Finite Element Method with high order structural elements",
    "abstract": "The Immersed Finite Element Method (IFEM) is a mathematical formulation for fluidstructure interaction problem like the Immersed Boundary Method; in IFEM the immersed structure has the same space dimension of the fluid domain. We present a stability of IFEM for a scheme where the Dirac delta distribution is treated variationally, as in [1]; moreover the finite element space related to the structure displacement consists of piecewise continuous Lagrangian elements, at least quadratic. The analysis is performed on two different time-stepping scheme. We demonstrate also that when the structure density is smaller than the fluid one, the stability is assured only if the time step size is bounded from below.",
    "keywords": [
      "immersed finite element method",
      "fluid structure interaction",
      "numerical stability",
      "cfl condition"
    ]
  },
  {
    "id": "1524",
    "title": "Late adolescent identity definition and intimate disclosure on Facebook",
    "abstract": "We investigate the disclosures of intimate information through Facebook by late adolescents. We relate behaviors with stages of psychosocial development. We collect data using focus groups and a content analysis of Facebook profiles. Users share routine and intimate information and show little concern for managing privacy. There is no significant difference in disclosures across different status categories.",
    "keywords": [
      "facebook",
      "adolescent",
      "identity",
      "intimacy",
      "disclosure",
      "social networking site"
    ]
  },
  {
    "id": "1525",
    "title": "Electronic and optical properties of bundled single-walled carbon nanotubes investigated by the first-principles method",
    "abstract": "We performed first-principles calculations to investigate the energetic, electronic and optical properties of bundled armchair and zigzag carbon nanotubes (CNTs). The nanotubes are assumed to be aligned in a hexagonal closed-packed array in the bundle. The total energy and electronic band structure show stronger dependence on the orientation of the tube for the (n,n) ( n , n ) and (n,0) ( n , 0 ) bundles if n=3q(q=integer) n = 3 q ( q = integer ) than if n?3q n ? 3 q . The optical properties are also sensitive to the orientation of the tubes. For the (n,n) ( n , n ) tubes, the calculated imaginary part of the dielectric functions of the tube bundles are similar to that of the isolated tube, except for the appearance of broadened peaks and an extra peak at low energies due to the avoided crossing of theandbands. This extra peak is absent in the (n,n) ( n , n ) tubes with n=3q n = 3 q in special orientations where the symmetry of the tube is compatible with that of the hexagonal lattice. For the (n,0) ( n , 0 ) tubes, the imaginary part of the dielectric functions of tubes with larger radius are very similar to that of the isolated tube, while for the (5,0) and (6,0) tubes with smaller radii, coupling causes gaps near the Fermi level, which contribute to an extra peak at low energies.",
    "keywords": [
      "first-principles calculation",
      "carbon nanotube",
      "optical properties",
      "dielectric function"
    ]
  },
  {
    "id": "1526",
    "title": "Brush up your painting skills - Realistic brush design for interactive painting applications",
    "abstract": "Most present-day interactive paint applications lack the means of adequately capturing a user's gestures and translating them into realistic and predictable strokes, despite the importance of such a mechanism. We present a novel brush design that adopts constrained energy optimization to deform the brush tuft according to the user's input movement. It incorporates bidirectional paint transfer and an anisotropic friction model. The main advantage of our method is its ability to handle a wide range of brush tuft shapes that are animated using a freeform deformation lattice, which is associated with the tuft's geometry. This way, almost no conditions or limitations are placed upon the appearance of the brush. Examples range from round brushes modeled as polygon meshes, to flat brushes with individual bristles. Less common deformable tools that are used to apply or remove paint on the canvas, like sponges, can be created as well. The model is integrated in our interactive painting system for creating images with watery paint.",
    "keywords": [
      "paint systems",
      "physically-based modeling",
      "constrained optimization"
    ]
  },
  {
    "id": "1527",
    "title": "Short-term and long-term effectiveness of a post-hospital care transitions program in an older, medically complex population",
    "abstract": "Care transition programs can potentially reduce 30 day readmission; however, the effect on long-term hospital readmissions is still unclear. We compared short-term (30 day) and long-term (180 day) utilization of participants enrolled in care transitions versus those matched referents eligible but not enrolled. This cohort study was conducted from January 1, 2011 until June 30, 2013 within a primary care academic practice. Patients at high risk for hospital readmission based on age and comorbid health conditions had participated in care transitions group (cases) or usual care (referent). The primary outcomes were 30, 90, and 180 day hospital readmissions.. Secondary outcomes included: mortality; emergency room visits and days; combined rehospitalizations and emergency room visits; and total intensive care unit days. Cox proportional hazard models using propensity score matching were used to assess rehospitalization, emergency room visits and mortality. Poisson regression models were used to compare the numbers of hospital days. Compared to referent (n=365), Mayo Clinic Care Transitions patients exhibited a lower 30 day rehospitalization rate compared to referent; 12.4% (95% CI 8.915.7) versus 20.1% (95% CI 15.824.1%), respectively (P=0.002). At 180-days, there was no difference in rehospitalization between transitions and referent; 39.9% (95% CI 34.644.9%) versus 44.8% (95% CI 39.449.8%), (P=0.07). We observed a reduction in 30 day rehospitalization rates among those enrolled in care transitions compared to referent. However, this effect was not sustained at 180 days. More work is needed to identify how the intervention can be sustained beyond 30 days.",
    "keywords": [
      "care transitions",
      "geriatric",
      "hospitalization"
    ]
  },
  {
    "id": "1528",
    "title": "Website Interactivity and Repeated Exposure, what Influences User Experience",
    "abstract": "This paper reports a study of the influence of website design and repeated exposure to websites on user judgment. Thirty respondents participated in this study; each respondent viewed three websites on three occasions, with a two-week gap between each visit. The three websites differed at their interactivity level; a basic site with limited interactivity, an interactive website with customization features, and a highly interactive website with a virtual agent. Several criteria were assessed through questionnaires. Interviews were conducted to support questionnaire results. Finally, the relative importance of the quality criteria and websites overall preferences were investigated. Results showed that respondents were more positive about the websites with higher interactivity, and the preference for the more interactive site increased over time.",
    "keywords": [
      "user experience",
      "website interactivity",
      "repeated exposure"
    ]
  },
  {
    "id": "1529",
    "title": "Structural modal reanalysis for large, simultaneous and multiple type modifications",
    "abstract": "We study modal reanalysis for large and multiple type structural modifications. The structure is modified with type, topology and boundary parameters. ICMO strategy with RayleighRitz analysis is used. High quality reanalysis result was obtained for multiple type modifications.",
    "keywords": [
      "modal reanalysis",
      "boundary",
      "topology and type modifications",
      "icmo",
      "rayleighritz analysis"
    ]
  },
  {
    "id": "1530",
    "title": "A new key assignment scheme for enforcing complicated access control policies in hierarchy",
    "abstract": "In a traditional key assignment scheme, an access control policy is used to solve the access control problem in a hierarchy. A higher security class can access lower security classes, but the opposite is not allowed. However, in some cases, this can be troublesome because of the lack of flexibility. In this paper, we shall propose a secure key assignment scheme which can be performed not only in a hierarchy but also in more complicated policies with anti-symmetrical and transitive exceptions.",
    "keywords": [
      "access control",
      "cryptography",
      "data security",
      "key assignment",
      "multilevel security"
    ]
  },
  {
    "id": "1531",
    "title": "Mean squared error of prediction (MSEP) estimates for principal component regression (PCR) and partial least squares regression (PLSR)",
    "abstract": "This paper presents results from simulations based on real data, comparing several competing mean squared error of prediction (MSEP) estimators on principal component regression (PCR) and partial least squares regression (PLSR): leave-one-out cross-validation, K-fold and adjusted K-fold cross-validation, the ordinary bootstrap estimate, the bootstrap smoothed cross-validation (BCV) estimate and the 0.632 bootstrap estimate. The overall performance of the estimators is compared in terms of their bias, variance and squared,error. The results indicate that the 0.632 estimate and leave-one-out cross-validation are preferable when one can afford the computation. Otherwise adjusted 5- or 10-fold cross-validation are good candidates because of their computational efficiency. ",
    "keywords": [
      "mean squared error of prediction ",
      "cross-validation",
      "adjusted cross-validation",
      "bootstrap",
      "0.632 estimate",
      "principal component regression ",
      "partial least squares regression "
    ]
  },
  {
    "id": "1532",
    "title": "Intrinsic Dimensionality Predicts the Saliency of Natural Dynamic Scenes",
    "abstract": "Since visual attention-based computer vision applications have gained popularity, ever more complex, biologically inspired models seem to be needed to predict salient locations (or interest points) in naturalistic scenes. In this paper, we explore how far one can go in predicting eye movements by using only basic signal processing, such as image representations derived from efficient coding principles, and machine learning. To this end, we gradually increase the complexity of a model from simple single-scale saliency maps computed on grayscale videos to spatiotemporal multiscale and multispectral representations. Using a large collection of eye movements on high-resolution videos, supervised learning techniques fine-tune the free parameters whose addition is inevitable with increasing complexity. The proposed model, although very simple, demonstrates significant improvement in predicting salient locations in naturalistic videos over four selected baseline models and two distinct data labeling scenarios.",
    "keywords": [
      "computational models of vision",
      "video analysis",
      "computer vision",
      "spatiotemporal saliency",
      "eye movement prediction",
      "intrinsic dimension",
      "visual attention",
      "interest point detection"
    ]
  },
  {
    "id": "1533",
    "title": "A distributed framework for trimmed Kernel k-Means clustering",
    "abstract": "Kernel k-Means extension. Improved performance over baseline Kernel k-Means. Reduced kernel matrix size to make clustering big datasets possible. Distributed versions of all assorted algorithms is provided. Competitive with the state of the art.",
    "keywords": [
      "data clustering",
      "face clustering",
      "kernel k-means",
      "distributed computing"
    ]
  },
  {
    "id": "1534",
    "title": "Using DempsterShafer theory and real options theory to assess competing strategies for implementing IT infrastructures: A case study",
    "abstract": "This paper discusses the selection of a preferred strategy for implementing an IT infrastructure from a range of competing alternatives. The model presented here combines the use of an evidential reasoning approach based on the DempsterShafer theory of belief functions with real options analysis. We discuss the combined use of both theories and show that combining the DempsterShafer theory with real options analysis provides flexible support that takes account of the multi-dimensional nature of implementation decisions. We also go into the fundamental requirements that need to be met when selecting a strategy for implementing an IT infrastructure. We conclude by outlining a number of the model's limitations.",
    "keywords": [
      "multi-attribute decision analysis",
      "risk",
      "belief functions",
      "real options theory",
      "it infrastructure implementation strategy"
    ]
  },
  {
    "id": "1535",
    "title": "An empirical study of cycles among classes in Java",
    "abstract": "Advocates of the design principle avoid cyclic dependencies among modules have argued that cycles are detrimental to software quality attributes such as understandability, testability, reusability, buildability and maintainability, yet folklore suggests such cycles are common in real object-oriented systems. In this paper we present the first significant empirical study of cycles among the classes of 78 open- and closed-source Java applications. We find that, of the applications comprising enough classes to support such a cycle, about 45% have a cycle involving at least 100 classes and around 10% have a cycle involving at least 1,000 classes. We present further empirical evidence to support the contention these cycles are not due to intrinsic interdependencies between particular classes in a domain. Finally, we attempt to gauge the strength of connection among the classes in a cycle using the concept of a minimum edge feedback set.",
    "keywords": [
      "dependency cycles",
      "java",
      "software corpus",
      "program analysis",
      "object-oriented design"
    ]
  },
  {
    "id": "1536",
    "title": "A New Lower Bound on the Maximum Number of Satisfied Clauses in Max-SAT and Its Algorithmic Applications",
    "abstract": "A pair of unit clauses is called conflicting if it is of the form (x), . A CNF formula is unit-conflict free (UCF) if it contains no pair of conflicting unit clauses. Lieberherr and Specker (J. ACM 28:411-421, 1981) showed that for each UCF CNF formula with m clauses we can simultaneously satisfy at least clauses, where . We improve the Lieberherr-Specker bound by showing that for each UCF CNF formula F with m clauses we can find, in polynomial time, a subformula F' with m' clauses such that we can simultaneously satisfy at least clauses (in F), where naEuro(3) is the number of variables in F which are not in F'. We consider two parameterized versions of MAX-SAT, where the parameter is the number of satisfied clauses above the bounds m/2 and . The former bound is tight for general formulas, and the later is tight for UCF formulas. Mahajan and Raman (J. Algorithms 31:335-354, 1999) showed that every instance of the first parameterized problem can be transformed, in polynomial time, into an equivalent one with at most 6k+3 variables and 10k clauses. We improve this to 4k variables and clauses. Mahajan and Raman conjectured that the second parameterized problem is fixed-parameter tractable (FPT). We show that the problem is indeed FPT by describing a polynomial-time algorithm that transforms any problem instance into an equivalent one with at most variables. Our results are obtained using our improvement of the Lieberherr-Specker bound above.",
    "keywords": [
      "maxsat",
      "lower bound",
      "2-satisfiable",
      "fixed parameter tractable",
      "kernel"
    ]
  },
  {
    "id": "1537",
    "title": "Learning a Channelized Observer for Image Quality Assessment",
    "abstract": "It is now widely accepted that image quality should be evaluated using task-based criteria, such as human-observer performance in a lesion-detection task. The channelized Hotelling observer (CHO) has been widely used as a surrogate for human observers in evaluating lesion detectability. In this paper, we propose that the problem of developing a numerical observer can be viewed as a system-identification or supervised-learning problem, in which the goal is to identify the unknown system of the human observer. Following this approach, we explore the possibility of replacing the Hotelling detector within the CHO with an algorithm that learns the relationship between measured channel features and human observer scores. Specifically, we develop a channelized support vector machine (CSVM) which we compare to the CHO in terms of its ability to predict human-observer performance. In the examples studied, we find that the CSVM is better able to generalize to unseen images than the CHO, and therefore may represent a useful improvement on the CHO methodology, while retaining its essential features.",
    "keywords": [
      "channelized hotelling observer ",
      "image quality",
      "machine learning",
      "numerical observer",
      "support vector machine ",
      "task based image evaluation"
    ]
  },
  {
    "id": "1538",
    "title": "Achieving interoperability in synchronous digital hierarchy network management through RM-ODP",
    "abstract": "Ever since the introduction of computerized digital switching systems and related management systems, network operators have been struggling with the question of how to circumvent the issues of customized and proprietary network management solutions. This topic became of paramount importance to network equipment manufacturers and telecommunications service providers with the advent of the SDH and TMN standards in the late 1980s and early 1990s. It is in the context of this problem that the authors and their colleagues, as telecom standards experts, discovered the Reference Model for Open Distributed Processing (RM-ODP). This paper describes the approach taken by the authors and their colleagues to use RM-ODP to these issues in the ITU-T study groups, how those efforts were applied and the relevance of RM-ODP to current computing systems based on the concepts of Cloud Computing. This paper thus describes real-world business needs that have been addressed by and illustrate the benefits of RM-ODP.",
    "keywords": [
      "interoperability",
      "telecom standards",
      "tmn",
      "corba",
      "rm-odp",
      "distributed computing"
    ]
  },
  {
    "id": "1539",
    "title": "acquisition of rule-based knowledge for analyzing dna-binding sites in proteins",
    "abstract": "This study aims to analyze DNA-binding proteins via acquisition of interpretable knowledge which can accurately predict binding sites in proteins to understand DNA-protein recognition mechanism. For mining accurate and interpretable knowledge, a large-scale dataset consisting of 982 DNA-binding proteins is constructed. This study investigates a novel feature set consisting of 11 features, including solvent accessibility, secondary structure, charge information near the residue, amino acid group and neighbor property. The derived binding and non-binding rules reveal that besides the well-known solvent accessibility, the electric charge distribution near the residue and the amino acid groups also play important roles in prediction of binding sites. The interpretable and accurate knowledge is helpful for biologist to analyze DNA-binding proteins.",
    "keywords": [
      "decision tree",
      "binding site",
      "protein",
      "knowledge acquisition"
    ]
  },
  {
    "id": "1540",
    "title": "A cross-layer approach for per-station fairness in TCP over WLANs",
    "abstract": "In this paper, we investigate the issue of per-station fairness in TCP over IEEE 802.11-compliant wireless local area networks (WLANs), especially in Wi-Fi hot spots. It is asserted that the hot spot suffers from the unfairness among wireless stations in exploiting the wireless medium. The source of this unfairness is analyzed from two aspects, TCP-induced asymmetry and MAC-induced asymmetry; the former causes TCP congestion control with a cumulative acknowledgment mechanism to prefer the wireless sending stations to the wireless receiving stations, whereas the later exacerbates the unfairness problem in the hot spots. By investigating the interaction between TCP congestion control and MAC contention control, we reveal that, even when a wireless station has a sufficiently large amount of traffic to send, it cannot always participate in the competition to access the wireless medium since the attempts to access the medium in the IEEE 802.11 MAC layer are controlled by the TCP congestion control. Therefore, we propose a cross-layer feedback approach to assure per-station fairness and to ensure high channel utilization. In this approach, we introduce the notion of channel access cost to quantify the system-wide traffic load and per-station channel usage. The access cost is estimated at the MAC in an access point and conveyed to the TCP sender. Then, the TCP sender adjusts its sending rate based on the access cost, so as to assure per-station fairness. The novel aspect of the approach is that it does not entail any modification of either the TCP or the IEEE 802.11 MAC. Furthermore, it neither resorts to any complex fair scheduling, nor compromises the modularity of the layered architecture. The simulation results indicate that the proposed approach can provide both per-station fairness and high channel utilization, irrespective of network configurations.",
    "keywords": [
      "fairness",
      "ieee 802.11 wlan",
      "tcp congestion control",
      "mac contention control"
    ]
  },
  {
    "id": "1541",
    "title": "The influence of liners with Ti, Ta or Ru finish on thin Cu films",
    "abstract": "The influence of the different liner material stacks TiN/Ti, TaN/Ta and TaN/Ru on the microstructure of 1?m thick sputtered Cu films is examined prior to and after annealing. It is shown that the liner has an impact on the crystallographic orientation distribution of the Cu grains and seriously affects the defect density in the Cu films. The measured Cu resistivity shows a strong dependence on the microstrain.",
    "keywords": [
      "copper",
      "titanium",
      "tantalum",
      "ruthenium",
      "microstructure",
      "resistivity"
    ]
  },
  {
    "id": "1542",
    "title": "Generalized (C)-conditions and related fixed point theorems",
    "abstract": "In this manuscript, the notion of C-condition [K. Suzuki, Fixed point theorems and convergence theorems for some generalized nonexpansive mappings, J. Math. Anal. Appl. 340 (2008) 1088-1095] is generalized. Some new fixed point theorems are obtained.  ",
    "keywords": [
      "contraction mapping",
      "fixed point theory",
      "opial property",
      "suzuki c-conditions"
    ]
  },
  {
    "id": "1543",
    "title": "Probabilistic control of chaos: Chaotic maps under control",
    "abstract": "Recently, we have proposed a new probabilistic method for the control of chaotic systems [1]. In this paper, we apply our method to characteristic cases of chaotic maps (one-and two-dimensional examples). As these chaotic maps are structurally stable, they cannot be controlled using conventional control methods without significant change of the dynamics. Our method consists in the probabilistic coupling of the original system with a controlling system. This coupling can be understood as a feedback control. of probabilistic nature. The chosen periodic orbit of the original system is a global attractor for the probability densities. The generalized spectral decomposition of the associated Frobenius-Perron operator provides a spectral condition of controllability for chaotic dynamical systems.",
    "keywords": [
      "chaos",
      "control",
      "probabilistic approach",
      "maps"
    ]
  },
  {
    "id": "1544",
    "title": "Semantics of recursive relationships in entity-relationship model",
    "abstract": "The recursive relationship of the ER model is used to represent a hierarchical situation in a natural way. However, the semantics of recursive relationships are quite difficult to grasp because entities carry out different roles in relationships. This article proposes four classes of entity types allowed in recursive relationships and provides a thorough analysis of all the types of recursive relationships through the four types of the recursive dependency and four classes of the entity type. By the proposed concepts of the entity type properties and the existing relationship constraints of the recursive relationships, we can more clearly capture the semantics of hierarchical structures and integrity constraints in practical databases.",
    "keywords": [
      "entity-relationship model",
      "recursive relationship",
      "recursive dependency"
    ]
  },
  {
    "id": "1545",
    "title": "The L(2,1)-labelling problem for cubic Cayley graphs on dihedral groups",
    "abstract": "A k-L(2,1)-labelling of a graph G is a mapping f:V(G)?{0,1,2,,k} such that |f(u)?f(v)|?2 if uv?E(G) and f(u)?f(v) if u,v are distance two apart. The smallest positive integer k such that G admits a k-L(2,1)-labelling is called the ?-number ofG. In this paper we study this quantity for cubic Cayley graphs (other than the prism graphs) on dihedral groups, which are called brick product graphs or honeycomb toroidal graphs. We prove that the ?-number of such a graph is between5 and7, and moreover we give a characterisation of such graphs with ?-number5.",
    "keywords": [
      "l-labelling",
      "?-number",
      "brick product",
      "honeycomb toroidal graph",
      "honeycomb torus",
      "cayley graph",
      "dihedral group"
    ]
  },
  {
    "id": "1546",
    "title": "Particle Filter With a Mode Tracker for Visual Tracking Across Illumination Changes",
    "abstract": "In this correspondence, our goal is to develop a visual tracking algorithm that is able to track moving objects in the presence of illumination variations in the scene and that is robust to occlusions. We treat the illumination and motion (x-y translation and scale) parameters as the unknown \"state\" sequence. The observation is the entire image, and the observation model allows for occasional occlusions (modeled as outliers). The nonlinearity and multimodality of the observation model necessitate the use of a particle filter (PF). Due to the inclusion of illumination parameters, the state dimension increases, thus making regular PFs impractically expensive. We show that the recently proposed approach using a PF with a mode tracker can be used here since, even in most occlusion cases, the posterior of illumination conditioned on motion and the previous state is unimodal and quite narrow. The key idea is to importance sample on the motion states while approximating importance sampling by posterior mode tracking for estimating illumination. Experiments demonstrate the advantage of the proposed algorithm over existing PF-based approaches for various face and vehicle tracking. We are also able to detect illumination model changes, e. g., those due to transition from shadow to sunlight or vice versa by using the generalized expected log-likelihood statistics and successfully compensate for it without ever loosing track.",
    "keywords": [
      "monte carlo methods",
      "particle filter ",
      "tracking",
      "visual tracking"
    ]
  },
  {
    "id": "1547",
    "title": "A comparison of different paging mechanisms for mobile IP",
    "abstract": "This paper proposes two Individual paging schemes, then presents a comparative analysis on the signaling cost functions of Mobile IP (MIP) with different paging protocols and paging schemes and investigates constructing optimal paging areas using discrete system model as a mobility model. In wireless mobile Internet, mobile hosts often visit foreign networks that might be far away from their home networks and the occurrences of their inter-domain movement are relatively rare. In this scenario, our analytical results show that paging, particularly individual paging, can significantly improve the total signaling cost of MIP. We show that Domain paging can bring about considerable cost saving compared to FA (Foreign Agent) paging. Our results also demonstrate the significant advantages of Individual Paging over Static Aggregate Paging. The results show that specifying the optimal paging area size is critical in saving signaling cost of MIP with paging support.",
    "keywords": [
      "mobile ip",
      "paging protocol",
      "paging scheme",
      "paging area",
      "signaling cost"
    ]
  },
  {
    "id": "1548",
    "title": "Assigning types to processes",
    "abstract": "In wide area distributed systems it is now common for higher-order code to be transferred from one domain to another; the receiving host may initialise parameters and then execute the code in its, local environment. In this paper we propose a fine-grained typing system for a higher-order pi-calculus which can be used to control the effect of such migrating code on local environments. Processes may be assigned different types depending on their intended use. This is in contrast to most of the previous work on typing processes where all processes are typed by a unique constant type, indicating essentially that they are well typed relative to a particular environment. Our fine-grained typing facilitates the management of access rights and provides host protection from potentially malicious behaviour. Our process type takes the form of an interface limiting the resources to which it has access and the types at which they may be used. Allowing resource names to appear both in process types and process terms, as interaction ports, complicates the typing system considerably. For the development of a coherent typing system, we use a kinding technique, similar to that used by the subtyping of the system F, and order-theoretic properties of our subtyping relation. Various examples of this paper illustrate the usage of our fine-grained process types in distributed systems. ",
    "keywords": [
      "concurrency theory",
      "the higher-order pi-calculus",
      "type theory",
      "access control"
    ]
  },
  {
    "id": "1549",
    "title": "convergence analysis of quantum-inspired genetic algorithms with the population of a single individual",
    "abstract": "In this paper, the Quantum-inspired Genetic Algorithms with the population of a single individual are formalized by a Markov chain model using a single and the stored best individual. Here, we analyze the convergence property of the Quantum-inspired Genetic Algorithms based on our proposed mathematical model, and with assumption in which its special genetic operation in the generation changes is restricted to a quantum operator; and show by means of the Markov chain analysis that the algorithm with preservation of the best individual in the population and comparison of it with the existing individual, will converge on the global optimal solution.",
    "keywords": [
      "global convergence",
      "quantum-inspired genetic algorithm",
      "markov chain model"
    ]
  },
  {
    "id": "1550",
    "title": "Deep Inspection of Unreachable BitTorrent Swarms",
    "abstract": "BitTorrent is one of the most popular P2P file sharing applications worldwide. Each BitTorrent network is called a swarm, and millions of peers may join multiple swarms. However, there are many unreachable peers (NATed (network address translated), firewalled, or inactive at the time of measurement) in each swarm; hence, existing techniques can only measure a part of all the peers in a swarm. In this paper, we propose an improved measurement method for BitTorrent swarms that include many unreachable peers. In essence, NATed peers and those behind firewalls are found by allowing them to connect to our crawlers by actively advertising our crawlers' addresses. Evaluation results show that the proposed method increases the number of unique contacted peers by 112% compared to the conventional method. Moreover, the proposed method increases the total volume of downloaded pieces by 66%. We investigate the sampling bias among the proposed and conventional methods, and we find that different measurement methods yield significantly different results.",
    "keywords": [
      "peer-to-peer networks",
      "bit torrent",
      "network measurement",
      "unreachable peers"
    ]
  },
  {
    "id": "1551",
    "title": "promoting social inclusion through public library e-government partnerships",
    "abstract": "This paper explores the issues faced by public libraries in the provision of e-government access and education to their patrons and communities, particularly through partnerships with other community organizations aimed at promoting social inclusion of disadvantaged populations. Due to a complex set of factors - including policy decisions, widespread trust of libraries, and a lack of social institutions that play similar roles - public libraries now stand as a significant social guarantor of public access to and education about e-government in the United States. Drawing from data collected through a 2009 national survey of public libraries, a 2009 series of site visits of public libraries, and previous research by the authors, this paper examines the challenges of implementing public library networks and connectivity to support e-government access and education, as well as the numerous management issues raised by providing these services. The primary focus of this paper is examining the dual role of public libraries as providers of public Internet access and education and as partners with government agencies and other community organizations to collaborate effectively in the provision of e-government to meet community needs.",
    "keywords": [
      "public access",
      "social inclusion",
      "public libraries",
      "equity of access",
      "e-government",
      "public access technology"
    ]
  },
  {
    "id": "1552",
    "title": "using assorted color spaces and pixel window sizes for colorization of grayscale images",
    "abstract": "There is no exact solution for colorization of grayscale images. In the initial work done [23], color traits transfer techniques to color grayscale images are proposed. The main focus of the techniques [23] is to minimise the human efforts needed in manually coloring the grayscale images. The human interaction is needed only to find a reference color image, then the job of transferring color traits from reference color image to grayscale image is done by proposed techniques. Here the techniques of color traits transfer to grayscale images are revisited with various color spaces as well as different sizes of pixel window. For the experimentation seven different color spaces are considered like RGB, Kekre's LUV, XYZ, YIQ, YUV, YCbCr and newly proposed YCbCg. The pixel window sizes used are total twelve ranging from window with size 1x2 to 4x2. So in all 84 different versions of color traits transfer to grayscale algorithms are tested over five different images for deciding the color space and pixel window size giving best quality of coloring. The experimental results gives that the YCbCr color space gives better coloring, and the pixel window of size 3x3 suits the best in all color spaces.",
    "keywords": [
      "color palette",
      "color spaces",
      "pixel windows",
      "colorization",
      "color transfer"
    ]
  },
  {
    "id": "1553",
    "title": "A generic method to develop simulation models for ambulance systems",
    "abstract": "In this paper, we address the question of generic simulation models and their role in improving emergency care around the world. After reviewing the development of ambulance models and the contexts in which they have been applied, we report the construction of a reusable model for ambulance systems. Further, we describe the associated parameters, data sources, and performance measures, and report on the collection of information, as well as the use of optimisation to configure the service to best effect. Having developed the model, we have validated it using real data from the emergency medical system in a Brazilian city, Belo Horizonte. To illustrate the benefits of standardisation and reusability we apply the model to a UK context by exploring how different rules of engagement would change the performance of the system. Finally, we consider the impact that one might observe if such rules were adopted by the Brazilian system.",
    "keywords": [
      "simulation",
      "modelling",
      "healthcare",
      "ambulance services"
    ]
  },
  {
    "id": "1554",
    "title": "goal-oriented preservation of essential genetic information by offspring selection",
    "abstract": "This contribution proposes an enhanced and generic selection model for Genetic Algorithms (GAs) and Genetic Programming (GP) which is able to preserve the alleles which are part of a high quality solution. Some selected aspects of these enhanced techniques are discussed exemplarily on the basis of standardized benchmark problems.",
    "keywords": [
      "selection",
      "genetic algorithms",
      "adaptation/self adaptation"
    ]
  },
  {
    "id": "1555",
    "title": "Intraoperative segmentation of iodine and palladium radioactive sources in C-arm images",
    "abstract": "Dynamic dosimetry is becoming the standard to evaluate the quality of radioactive implants during brachytherapy. For this, it is essential to obtain a 3D visualization of the implanted seeds and their relative position to the prostate. A method was developed to obtain a robust and precise segmentation of seeds in C-arm images, and this approach was tested using clinical datasets.",
    "keywords": [
      "segmentation",
      "declustering",
      "radioactive seeds",
      "c-arm fluoroscopy",
      "brachytherapy",
      "iodine implants",
      "palladium implants"
    ]
  },
  {
    "id": "1556",
    "title": "A Comprehensive Strategy for Contention Management in Software Transactional Memory",
    "abstract": "In Software Transactional Memory (STM), contention management refers to the mechanisms used to ensure forward progress-to avoid livelock and starvation, and to promote throughput and fairness. Unfortunately, most past approaches to contention management were designed for obstruction-free STM frameworks, and impose significant constant-time overheads. Priority-based approaches in particular typically require that reads be visible to all transactions, an expensive property that is not easy to support in most STM systems. In this paper we present a comprehensive strategy for contention management via fair resolution of conflicts in an STM with invisible reads. Our strategy depends on (1) lazy acquisition of ownership, (2) extendable timestamps, and (3) an efficient way to capture both priority and conflicts. We introduce two mechanisms-one using Bloom filters, the other using visible read bits-that implement point (3). These mechanisms unify the notions of conflict resolution, inevitability, and transaction retry. They are orthogonal to the rest of the contention management strategy, and could be used in a wide variety of hardware and software TM systems. Experimental evaluation demonstrates that the overhead of the mechanisms is low, particularly when conflicts are rare, and that our strategy as a whole provides good throughput and fairness, including livelock and starvation freedom, even for challenging workloads.",
    "keywords": [
      "algorithms",
      "design",
      "performance",
      "software transactional memory",
      "contention management",
      "priority",
      "inevitability",
      "condition synchronization"
    ]
  },
  {
    "id": "1557",
    "title": "Tracking deforming aortas in two-photon autofluorescence images and its application on quantitative evaluation of aorta-related drugs",
    "abstract": "This paper describes a novel approach to an objective measurement of aorta samples of rats and a quantitative evaluation of aorta-related drugs. Two-photon fluorescence microscopy is used for recording image sequences of deforming aorta. Time sequence snake models are used to track the structural deformations of aorta walls caused by drug stimulation of the elastic lamina in the aorta. Several objective and quantitative biomarkers extracted from these models are used as diagnostic indicators. In a preliminary study, the technique was successfully used for evaluating the effect of a newly developed drughuman erythrocyte-derived depressing factor quantitatively and objectively.",
    "keywords": [
      "two-photon autofluorescence imaging",
      "aorta",
      "quantitative evaluation",
      "time sequence snake model",
      "human erythrocyte-derived depressing factor"
    ]
  },
  {
    "id": "1558",
    "title": "Multi-Class Active Learning by Uncertainty Sampling with Diversity Maximization",
    "abstract": "As a way to relieve the tedious work of manual annotation, active learning plays important roles in many applications of visual concept recognition. In typical active learning scenarios, the number of labelled data in the seed set is usually small. However, most existing active learning algorithms only exploit the labelled data, which often suffers from over-fitting due to the small number of labelled examples. Besides, while much progress has been made in binary class active learning, little research attention has been focused on multi-class active learning. In this paper, we propose a semi-supervised batch mode multi-class active learning algorithm for visual concept recognition. Our algorithm exploits the whole active pool to evaluate the uncertainty of the data. Considering that uncertain data are always similar to each other, we propose to make the selected data as diverse as possible, for which we explicitly impose a diversity constraint on the objective function. As a multi-class active learning algorithm, our algorithm is able to exploit uncertainty across multiple classes. An efficient algorithm is used to optimize the objective function. Extensive experiments on action recognition, object classification, scene recognition, and event detection demonstrate its advantages.",
    "keywords": [
      "active learning",
      "uncertainty sampling",
      "diversity maximization"
    ]
  },
  {
    "id": "1559",
    "title": "Structure-based design of inhibitors of NS3 serine protease of hepatitis C virus",
    "abstract": "We have designed small focused combinatorial library of hexapeptide inhibitors of NS3 serine protease of the hepatitis C virus (HCV) by structure-based molecular design complemented by combinatorial optimisation of the individual residues. Rational residue substitutions were guided by the structure and properties of the binding pockets of the enzymes active site. The inhibitors were derived from peptides known to inhibit the NS3 serine protease by using unusual amino acids and ?-ketocysteine or difluoroaminobutyric acid, which are known to bind to the S1 pocket of the catalytic site. Inhibition constants (Ki) of the designed library of inhibitors were predicted from a QSAR model that correlated experimental Ki of known peptidic inhibitors of NS3 with the enthalpies of enzymeinhibitor interaction computed via molecular mechanics and the solvent effect contribution to the binding affinity derived from the continuum model of solvation. The library of the optimised inhibitors contains promising drug candidateswater-soluble anionic hexapeptides with predicted in the picomolar range.",
    "keywords": [
      "ac acetyl",
      "asa ?-carboxyaspartic acid",
      "tbu tert-butylglycine",
      "cha ?-cyclohexylalanine",
      "cyo ?-ketocysteine",
      "dif ?",
      "?-diphenylalanine",
      "fab ?",
      "?-difluoro-?-amino-?-ketopentanoic acid",
      "gla ?-carboxyglutamic acid",
      "glr glutaric acid",
      "cpa ?-carboxypropionylalanine",
      "nal ?-napthylalanine",
      "nap napthylglycin",
      "nva norvaline",
      "phg ?-phenylglycine",
      "suc succinic acid",
      "tro 7-hydroxytryptofan",
      "trc 4-carboxytryptofan",
      "hcv hepatitis c virus",
      "ns3 viral non-structural protein number 3 of hcv",
      "ns4a viral non-structural protein number 4 of hcv",
      "cofactor of the ns3",
      "ns3/4a complex of ns3 with cofactor ns4a",
      "mm molecular mechanics",
      "qsar quantitative structureactivity relationships",
      "ki inhibition constant",
      "logpo/w log of partitioning coefficient in octanol/water system."
    ]
  },
  {
    "id": "1560",
    "title": "Communication of statistical concepts: Examples in medical collaboration",
    "abstract": "While effective communication of statistical concepts is important for the enthusiastic adoption of these concepts by collaborators, statisticians are not necessarily trained in the process of communication with collaborators in other substantive fields. It is proposed that increased attention be paid to pedagogical techniques for communicating to our non-statistical colleagues what statisticians have to offer to the design and analysis aspects of a collaborative effort. One approach is to offer examples relevant to our colleagues' fields when we explain statistical ideas. This paper provides several such examples from the field of neurology, focusing on the issue of sample selection bias and prospective study designs.",
    "keywords": [
      "communication",
      "sample selection bias",
      "prospective studies",
      "outcome assessment",
      "diagnosis",
      "prevalence"
    ]
  },
  {
    "id": "1561",
    "title": "White hole, black whole, and the book",
    "abstract": "Physical and intellectual spaces are visualized making use of concepts from intuitive set theory. Intellectual space is defined as the set of all proofs of mathematical logic contained in The Book conceived by Erdos.  ",
    "keywords": [
      "physical space",
      "intellectual space",
      "visualization"
    ]
  },
  {
    "id": "1562",
    "title": "Functional Characterization of Amyloid ? Precursor Protein Regulatory Elements: Rationale for the Identification of Genetic Polymorphism",
    "abstract": "Alzheimer's disease (AD) is characterized by the formation of senile plaques of the amyloid peptide (A?) derived from a large A? precursor protein (APP). Autosomally inherited or familial AD has only been previously demonstrated in connection with coding sequence missense mutations. Abnormal regulation of APP gene expression has been demonstrated to play a role in AD. Genome screen and linkage analysis suggest that the APP locus may predispose to AD. The aim is to characterize genetic variability in the APP gene within its upstream regulatory region and to determine whether that variability is associated with AD and affects the expression of APP. This article describes the rationale and strategy for identifying genetic polymorphisms in the APP regulatory region, including its promoter, to associate any variability with the disease.",
    "keywords": [
      "aging",
      "amyloid",
      "?-protein",
      "brain",
      "dementia",
      "gene regulation",
      "polymorphism",
      "promoter",
      "snp",
      "transcription"
    ]
  },
  {
    "id": "1563",
    "title": "Dynamic avalanche in diodes with local lifetime control by means of palladium",
    "abstract": "The increase of the static breakdown voltage and the reduction of dynamic avalanche in a fast recovery 2.5kV/150A P-i-N diode subjected to the radiation enhanced diffusion of a palladium are discussed. The in-diffusing palladium compensates the doping profile in a lightly doped N-base close to the anode junction. Using a device simulation, the increase of the breakdown voltage and the reduction of the dynamic avalanche are explained by the reduction of peak electric field in the additional low-doped P-type layer created by the compensation effect. This is presented for both a dc and transient device operation and confirmed experimentally as well. An improved technology curve for the static versus recovery losses at a high line voltage has been obtained. A high thermal budget of deep levels and a low leakage current are additional benefits of the method.",
    "keywords": [
      "silicon",
      "power diode",
      "dynamic avalanche",
      "avalanche breakdown"
    ]
  },
  {
    "id": "1564",
    "title": "DTW Based Classification of Diverse Pre-Processed Time Series Obtained from Handwritten PIN Words",
    "abstract": "Personal identity verification by means of signature handwriting dynamics is a widely researched aspect of behavioral biometrics. The Dynamic Time Warping (DTW) technique has been successfully used for accessing the similarity of time series of handwritten objects by minimizing non-linear time distortions. Generally, in DTW based classifiers, the sequences are normalized in time and amplitude domains. In the paper, different length and amplitude normalization techniques are applied on signatures and handwritten PIN word sequences and their influence on accuracy of recognition are examined. A special approach to amplitude normalization based on reference level assigned Dynamic Time Warping (DTW) technique is presented. The standard deviation values calculated from the time series are used as so called bio-reference levels to improve the performance of classification. For this, they are added to the time series of query and sample datasets prior to DTW matching. The acquisition of online data is carried out by a digital pen equipped with pressure and inclination sensors. The time series obtained from the pen during handwriting provide valuable insight into the unique characteristics of the writers. Experimental results show that with the help of proposed length and amplitude normalizations of sequences including the bio-reference levels, the computational time is reduced and false acceptance rates are decreased.",
    "keywords": [
      "biometric signature authentication",
      "biometric person authentication",
      "handwritten pin recognition",
      "dynamic time warping ",
      "time series pre-processing"
    ]
  },
  {
    "id": "1565",
    "title": "quality, cleanroom and formal methods",
    "abstract": "We have proposed a new approach to software quality combining cleanroom methodologies and formal methods. Cleanroom emphasizes defect prevention rather than defect removal. Formal methods use mathematical and logical formalizations to find defects early in the software development lifecycle. These two methods have been used separately to improve software quality since the 1980's. The combination of the two methods may provide further quality improvements through reduced software defects. This result, in turn, may reduce development costs, improve time to market, and increase overall product excellence.Defects in computer software are costly. Their detection is usually postponed to the test phase, and their removal is also a very time consuming and expensive task. Cleanroom software engineering is a methodology which relies on preventing the defects, rather than removing them. It is based on incremental development and it emphasizes the development phase. An enhancement to this methodology is presented in this paper, which combines formal methods and cleanroom. The efficiency of the new model rests on an appropriate logical representation, to write the specification of the intended system. In the new model, design plans are formally verified before any implementation is done. The advantages of finding defects in the early stages are decreased cost and increased quality. Results show that, by using formal methods, a higher quality will be achieved and the software project can also benefit from the existing mechanized tools of these two techniques.",
    "keywords": [
      "software quality",
      "formal methods",
      "cleanroom"
    ]
  },
  {
    "id": "1566",
    "title": "Evolutionary circuit design for fast FPGA-based classification of network application protocols",
    "abstract": "The evolutionary design can produce fast and efficient implementations of digital circuits. It is shown in this paper how evolved circuits, optimized for the latency and area, can increase the throughput of a manually designed classifier of application protocols. The classifier is intended for high speed networks operating at 100Gbps. Because a very low latency is the main design constraint, the classifier is constructed as a combinational circuit in a field programmable gate array (FPGA). The classification is performed using the first packet carrying the application payload. The improvements in latency (and area) obtained by Cartesian genetic programming are validated using a professional FPGA design tool. The quality of classification is evaluated by means of real network data. All results are compared with commonly used classifiers based on regular expressions describing application protocols.",
    "keywords": [
      "application protocol",
      "classifier",
      "cartesian genetic programming",
      "field programmable gate array"
    ]
  },
  {
    "id": "1567",
    "title": "Development of a parallel Poisson's equation solver with adaptive mesh refinement and its application in field emission prediction",
    "abstract": "A parallel electrostatic Poisson's equation solver coupled with parallel adaptive mesh refinement (PAMR) is developed in this paper. The three-dimensional Poisson's equation is discretized using the Galerkin finite element method using a tetrahedral mesh. The resulting matrix equation is then solved through the parallel conjugate gradient method using the non-overlapping subdomain-by-subdomain scheme. A PAMR module is coupled with this parallel Poisson's equation solver to adaptively refine the mesh where the variation of potentials is large. The parallel performance of the parallel Poisson's equation is studied by simulating the potential distribution of a CNT-based triode-type field emitter. Results with ?100?000 nodes show that a parallel efficiency of 84.2% is achieved in 32 processors of a PC-cluster system. The field emission properties of a single CNT triode- and tetrode-type field emitter in a periodic cell are computed to demonstrate their potential application in field emission prediction.",
    "keywords": [
      "parallel poisson's equation",
      "galerkin finite element method",
      "parallel adaptive mesh refinement",
      "field emission"
    ]
  },
  {
    "id": "1568",
    "title": "Extraordinary magnetization of amorphous TbDyFe films",
    "abstract": "Giant magnetostrictive, amorphous TbDyFe thin films prepared by magnetron sputtering. Presents giant magnetostriction of 300400ppm at room temperature. Spin-glass behavior at temperatures lower than 200K. Huge coercivity values were observed at T<40K reaching 2.5T at T=5K.",
    "keywords": [
      "thin films",
      "magnetoelastic",
      "spin glass",
      "amorphous magnetism"
    ]
  },
  {
    "id": "1569",
    "title": "a strategy for efficient verification of relational specifications, based on monotonicity analysis",
    "abstract": "We introduce a strategy for the verification of relational specifications based on the analysis of monotonicity of variables within formulas. By comparing with the Alloy Analyzer, we show that for a relevant class of problems this technique drastically outperforms analysis of the same problems using SAT-solvers, while consuming a fraction of the memory SAT-solvers require.",
    "keywords": [
      "alloy",
      "software validation",
      "software specification",
      "relational methods"
    ]
  },
  {
    "id": "1570",
    "title": "C-7-coloring problem",
    "abstract": "H-coloring problem is a coloring problem with restrictions such that some pairs of colors cannot be used for adjacent vertices, where H is a graph representing the restrictions of colors. We deal with the case that H is the complement graph (C2p+1) over bar of a cycle of odd order 2p + 1. This paper presents the following results: (1) chordal graphs and internally maximal planar graphs are (C2p+1) over bar -colorable if and only if they are p-colorable (p greater than or equal to 2), (2) C-7-coloring problem on planar graphs is NP-complete, and (3) there exists a class that includes infinitely many (C-7) over bar -colorable but non-3-colorable planar graphs.",
    "keywords": [
      "h-coloring",
      "cycles of odd order",
      "complement graphs",
      "np-completeness"
    ]
  },
  {
    "id": "1571",
    "title": "e-governance initiatives in india",
    "abstract": "Information and Communication Technology (ICT) in government agencies as well as educational and research institutions facilitates an efficient, speedy and transparent dissemination of information to the public and other agencies for performance of governments' administrative activities. The importance of ICT in governance has been recognised the world over. Electronic governance (e-governance) highlights several elements of good governance such as transparency, accountability, participation, social integration, public financial management reforms and development. This study is based on personal interview of selected representatives of the Government of National Capital Territory of Delhi and review of e-Governance initiatives taken by the government. The findings of this study highlight the role of Internet, particularly the World Wide Web (WWW), which has made it easier for citizens to locate and download official information and to conduct transactions. We conclude that e-Governance needs to be integrated into the broader public management framework so as to make a substantial change in the government to citizen (G2C) relationship.",
    "keywords": [
      "delhi government",
      "e-governance",
      "networked environment",
      "information communication technologies"
    ]
  },
  {
    "id": "1572",
    "title": "Counting occurrences of some subword patterns",
    "abstract": "We find generating functions for the number of strings (words) containing a specified number of occurrences of certain types of order-isomorphic classes of substrings called subword patterns. In particular, we find generating functions for the number of strings containing a specified number of occurrences of a given 3-letter subword pattern.",
    "keywords": [
      "generalized patterns",
      "subword patterns"
    ]
  },
  {
    "id": "1573",
    "title": "Progressive encoding of binary voxel models using pyramidal decomposition",
    "abstract": "In this paper, we propose a progressive encoding algorithm for the geometric information of a 3D object, which is represented by binary voxels. Using the morphological pyramidal decomposition, the proposed algorithm first generates the multi-resolution models of a 3D object. Then, each resolution model is predicted from its lower resolution model, and the prediction errors are encoded using an arithmetic coding technique. To yield high compression ratio, each model is partitioned into the inside, boundary, and outside regions based on the lower resolution model. This partitioning method greatly reduces the amount of data to be encoded, since the prediction errors are compactly concentrated near the boundary region. Moreover, the neighborhood relation of each boundary voxel is used as the context for the arithmetic coding to further increase the compression efficiency. It is demonstrated by extensive simulation results that the proposed algorithm provides better coding gain than the conventional voxel and mesh compression algorithms.",
    "keywords": [
      "progressive encoding",
      "binary voxels"
    ]
  },
  {
    "id": "1574",
    "title": "Design and implementation of a standards-based interoperable clinical decision support architecture in the context of the Korean EHR",
    "abstract": "In 2000 the Korean government initiated efforts to secure healthcare accessibility and efficiency anytime and anywhere via the nationwide healthcare information system by the end of 2010. According to the master plan, electronic health record (EHR) research and development projects were designed in 2005. One subproject was the design and implementation of standards-based interoperable clinical decision support (CDS) capabilities in the context of the EHR system. The purpose of this study was to describe the challenges, process, and outcomes of defining and implementing a national CDS architecture to stimulate and motivate the widespread adoption of CDS services in Korea. CDS requirements and design principles were established by conducting a selective literature review and a survey of clinicians, managers, and hospital and industrial health information technology engineers regarding issues related to CDS architectures. The previous relevant works of the American Medical Informatics Association, the Healthcare Information and Management Systems Society, and Health Level Seven were used to validate the scope and themes of the service architecture. The Arden Syntax, Standards-Based Sharable Active Guideline Environment, First DataBank, and SEBASTIAN approaches were used to assess the coverage of the application architecture thus defined. A CDS prototype of an outpatient hypertension management system was implemented and assessed in a simulated experimental setting to evaluate the feasibility of the proposed architecture. Four CDS service features were identified: knowledge application, knowledge management, audit and evaluation, and CDS and knowledge governance. Five core components of CDS application architecture were also identified: knowledge-execution component, knowledge-authoring component, data-interface component, knowledge repository, and service-interface component. The coverage and characteristics of the architecture identified herein were found to be comparable with those described previously. Two scenarios of deployment architecture were identified in the context of Korean healthcare. The preliminary feasibility test revealed that the architecture exhibited good performance and made it easy to integrate patient data. We have described the efforts that have been made to realize CDS service features, core components, application, and deployment architectures in the context of the Korean EHR. These outcomes showed the potential to contribute to the adoption of CDS at the national level.",
    "keywords": [
      "clinical decision support systems",
      "electronic health records",
      "system architecture",
      "systems integration",
      "knowledge bases"
    ]
  },
  {
    "id": "1575",
    "title": "A comparison of genetic programming and artificial neural networks in metamodeling of discrete-event simulation models",
    "abstract": "Genetic programming (GP) and artificial neural networks (ANNs) can be used in the development of surrogate models of complex systems. The purpose of this paper is to provide a comparative analysis of GP and ANNs for metamodeling of discrete-event simulation (DES) models. Three stochastic industrial systems are empirically studied: an automated material handling system (AMHS) in semiconductor manufacturing, an (s,S) inventory model and a serial production line. The results of the study show that GP provides greater accuracy in validation tests, demonstrating a better generalization capability than ANN. However, GP when compared to ANN requires more computation in metamodel development. Even given this increased computational requirement, the results presented indicate that GP is very competitive in metamodeling of DES models.",
    "keywords": [
      "simulation metamodel",
      "genetic programming",
      "symbolic regression",
      "neural networks",
      "design of experiments",
      "decision support tool"
    ]
  },
  {
    "id": "1576",
    "title": "Effect of trace platinum additions on the interfacial morphology of Sn3.8Ag0.7Cu alloy aged for long hours",
    "abstract": "Pt was introduced into Sn3.8Ag0.7Cu (SAC387) solder alloy. Reflow was carried out on a hot plate at 240C for 60s. The interfacial samples were then aged up to 1000h at 150C. IMCs and Kirkendall voids decreased with increasing amount of Pt. SAC387Pt/Cu show a more reliable solder interconnection to SAC387/Cu.",
    "keywords": [
      "composite solder",
      "intermetallic layer",
      "pt addition",
      "interfacial morphology",
      "isothermal aging"
    ]
  },
  {
    "id": "1577",
    "title": "Pathogenic Mechanisms of Diseases Caused by Rickettsia",
    "abstract": "The specter of bioterrorism employing genetically engineered Rickettsia resistant to all antibiotics should reawaken the world's desire to elucidate the pathogenesis of typhus and spotted fever rickettsioses in a search for mechanisms vulnerable to interdiction. The pathogenetic sequence includes rickettsial entry into the dermis, hematogenous dissemination to vascular endothelial cells (most critically in brain and lungs), increased vascular permeability, edema, and immunity mediated by NK cells, IFN-?, TNF-?, RANTES, antibodies, and cytotoxic T lymphocytes. Silverman has demonstrated the role of reactive oxygen species (ROS) produced by R. rickettsii-infected endothelial cells in peroxidative damage to cell membranes in vitro, and Heinzen has described actin-based rickettsial intracellular mobility and intercellular spread. At this point the availability of sequences of rickettsial genomes and excellent animal models of rickettsioses have yielded insufficient progress towards the identification of rickettsial virulence factors and knowledge of the importance of injury mediated by ROS, phospholipase A2, protease(s) or other mechanisms in vivo. Attention to the rickettsiosis-associated procoagulant state led to determination that hemostatic mechanisms largely prevent major hemorrhage without disseminated intravascular coagulation or thrombosis-mediated ischemia. Particularly lacking is knowledge of early events in vivo at the portal of entry in skin (or lung), of the effects of the inoculum medium (arthropod saliva or feces), mediators produced by infected endothelium under conditions of flow and of the contributions in vivo of immune effectors to pathology, of the role of apoptosis in rickettsial infection, and of the endothelial cell alterations that account for increased vascular permeability. The host cell receptor for the Rickettsia ligand and the mechanism of rickettsial escape from the phagosome need to be elucidated.",
    "keywords": [
      "rickettsia",
      "pathogenesis",
      "pathophysiology",
      "host defenses"
    ]
  },
  {
    "id": "1578",
    "title": "Inverse multi-objective combinatorial optimization",
    "abstract": "Inverse multi-objective combinatorial optimization consists of finding a minimal adjustment of the objective functions coefficients such that a given set of feasible solutions becomes efficient. An algorithm is proposed for rendering a given feasible solution into an efficient one. This is a simplified version of the inverse problem when the cardinality of the set is equal to one. The adjustment is measured by the Chebyshev distance. It is shown how to build an optimal adjustment in linear time based on this distance, and why it is right to perform a binary search for determining the optimal distance. These results led us to develop an approach based on the resolution of mixed-integer linear programs. A second approach based on a branch-and-bound is proposed to handle any distance function that can be linearized. Finally, the initial inverse problem is solved by a cutting plane algorithm.",
    "keywords": [
      "multi-objective optimization",
      "inverse optimization",
      "combinatorial optimization"
    ]
  },
  {
    "id": "1579",
    "title": "Dynamic system identification via recurrent multilayer perceptrons",
    "abstract": "In this paper continuous-time recurrent multilayer perceptrons (RMLP) are proposed to identify nonlinear systems. Using the function approximation theorem for multilayer perceptrons (MLP), we conclude that RMLP can approximate any dynamic system in any degree of accuracy. By means of a Lyapunov-like analysis, a stable learning algorithm for RMLP is determined. The suggested learning algorithm is similar to the well-known backpropagation rule of the MLP but with an additional term which assure the stability of identification error.",
    "keywords": [
      "system identification",
      "function approximation",
      "recurrent multilayer perceptrons"
    ]
  },
  {
    "id": "1580",
    "title": "A reliable and timely medium access control protocol for vehicular ad hoc networks",
    "abstract": "Many vehicular safety applications can be implemented more easily and practically by using reliable and real-time communications between vehicles and roadside infrastructures. Neither schedule-based nor contention-based conventional MAC protocols can properly satisfy the safety requirements of such applications. In contention-based protocols, packets may experience unbounded delays and schedule-based protocols are sensitive to users' motion. In this paper, a new STDMA MAC protocol which provides both reliable and real-time medium access is introduced. Simulation results show that the proposed protocol has succeeded in providing an acceptable level of reliable and real-time communications, simultaneously.",
    "keywords": [
      "vanets",
      "vehicular ad hoc networks",
      "mac",
      "medium access control",
      "ivc",
      "inter-vehicular communications",
      "reliable and real-time communications"
    ]
  },
  {
    "id": "1581",
    "title": "Unsteady flow of viscous fluid over the vacillate stretching cylinder",
    "abstract": "In this paper, we consider the unsteady boundary layer flow of an incompressible viscous fluid produced by periodic motion of an elastic cylinder. The number of independent variables involved in the governing partial differential equations is reduced by using the similarity transformation. The transformed equations are then solved numerically with the help of a finite difference scheme by altering the semi-infinite domain to a finite domain. The numerical results are compared with a previously published work and a good agreement is achieved. The imperative parameters rising in the governing equations because of the effects of oscillations and the curvature are St (Strouhal number), Re (Reynolds number), and epsilon (amplitude of oscillations). The effects of these parameters on the vacillating velocity and the skin friction are discussed through graphs and tables. The large values of Re correspond to small curvature, so for large Reynolds number the solution approaches to that of the stretching flat plate case. ",
    "keywords": [
      "boundary layer flow",
      "oscillatory stretching cylinder",
      "unsteady",
      "numerical solution"
    ]
  },
  {
    "id": "1582",
    "title": "exploiting novelty, coverage and balance for topic-focused multi-document summarization",
    "abstract": "Novelty, coverage and balance are important requirements in topic-focused summarization, which to a large extent determine the quality of a summary. In this paper, we propose a novel method that incorporates these requirements into a sentence ranking probability model. It differs from the existing methods in that the novelty, coverage and balance requirements are all modeled w.r.t. a given topic, so that summaries are highly relevant to the topic and at the same time comply with topic-aware novelty, coverage and balance. Experimental results on the DUC 2005, 2006 and 2007 benchmark data sets demonstrate the effectiveness of our method.",
    "keywords": [
      "topic-focused summarization",
      "topic-aware novelty",
      "topic-aware coverage and balance",
      "relevance measure"
    ]
  },
  {
    "id": "1583",
    "title": "Reliability-based assessment of vehicle safety in adverse driving conditions",
    "abstract": "The framework of a reliability-based assessment model of vehicle safety under adverse driving conditions is developed. Such a framework is built based on the advanced transient dynamic vehicle simulation models which can consider the coupling effects between vehicles and adverse driving conditions, such as wind gust, snow-covered or icy road surface and/or curving. The single-vehicle safety index is introduced to provide rational assessment of accident risks by considering uncertainties of critical variables. In order to consider the complicated implicit limit state functions, the response surface method (RMS) is adopted to provide an efficient estimation of accident risks. Finally, a parametric study is conducted to demonstrate the methodology and the impacts of different critical variables on accident risks of a typical truck under several representative hazardous scenarios are investigated.",
    "keywords": [
      "traffic",
      "truck safety",
      "simulation",
      "reliability",
      "implicit limit state function",
      "response surface method "
    ]
  },
  {
    "id": "1584",
    "title": "an extensible digital ink segmentation and classification framework for natural notetaking",
    "abstract": "With the emergence of digital pen and paper technologies, we have witnessed an increasing number of enhanced paper-digital notetaking solutions. However, the natural notetaking process includes a variety of individual work practices that complicate the automatic processing of paper notes and require user intervention for the classification of digital ink data. We present an extensible digital ink processing framework that simplifies the classification of digital ink data in natural notetaking applications. Our solution deals with the manual as well as automatic ink data segmentation and classification based on Delaunay triangulation and a strongest link algorithm. We further highlight how our solution can be extended with new digital ink classifiers and describe a paper-digital reminder application that has been realised based on the presented digital ink processing framework.",
    "keywords": [
      "natural notetaking",
      "digital ink",
      "digital pen and paper"
    ]
  },
  {
    "id": "1585",
    "title": "The integration of design of experiments, surrogate modeling and optimization for thermoscience research",
    "abstract": "This paper presents an integrated approach for the solution of complex optimization problems in thermoscience research. The cited approach is based on the design of computational experiments (DOE), surrogate modeling, and optimization. The DOE/surrogate modeling techniques under consideration include: A-optimal/classical linear regression, Latin hypercube/artificial neural networks, and Latin hypercube/Sugeno-type fuzzy models. These techniques are coupled with both local (modified Newtons method) and global (genetic algorithms) optimization methods. The proposed approach proved to be an effective, efficient and robust modeling and optimization tool in the context of a case study, and holds promise for use in larger scale optimization problems in thermoscience research.",
    "keywords": [
      "surrogate modeling",
      "doe",
      "optimization",
      "soft computing"
    ]
  },
  {
    "id": "1586",
    "title": "Consistency issues in distributed checkpoints",
    "abstract": "A global checkpoint is a set of local checkpoints, one per process. The traditional consistency criterion for global checkpoints states that a global checkpoint is consistent if it does not include messages received and not sent. This paper investigates other consistency criteria, transitlessness, and strong consistency. A global checkpoint is transitless if it does not exhibit messages sent and nor received. Transitlessness can be seen as a dual of traditional consistency. Strong consistency is the addition of transitlessness to traditional consistency. The main result of this paper is a statement of the necessary and sufficient condition answering the following question: \"Given an arbitrary set of local checkpoints, can this set be extended to a global checkpoint that satisfies P-m (where LP is traditional consistency, transitlessness, or strong consistency). From a practical point of view, this condition, when applied to transitlessness, is particularly interesting as it helps characterize which messages do not need to be recorded by checkpointing protocols.",
    "keywords": [
      "checkpointing",
      "consistency",
      "strong consistency",
      "transitlessness",
      "distributed systems",
      "fault-tolerance",
      "rollback recovery"
    ]
  },
  {
    "id": "1587",
    "title": "Adaptive simulation of two dimensional hyperbolic problems by Collocated Discrete Least Squares Meshless method",
    "abstract": "An adaptive refinement technique is presented in this paper and used in conjunction with the Collocated Discrete Least Squares Meshless (CDLSM) method for the effective simulation of two-dimensional shocked hyperbolic problems. The CDLSM method is based on minimizing the least squares functional calculated at collocation points chosen on the problem domain and its boundaries. The functional is defined as the weighted sum of the squared residuals of the differential equation and its boundary conditions. A Moving Least Squares (MLS) method is used here to construct the meshless shape functions. An error estimator based on the value of functional at nodal points used to discretize the problem domain and its boundaries is developed and used to predict the areas of poor solutions. A node moving strategy is then used to refine the predicted zones of poor solutions before the problem is resolved on the refined distribution of nodes. The proposed methodology is applied to some two dimensional hyperbolic benchmark problems and the results are presented and compared to the exact solutions. The results clearly show the capabilities of the proposed method for the effective and efficient solution of hyperbolic problems of shocked and high gradient solutions.  ",
    "keywords": [
      "adaptive refinement",
      "meshless method",
      "node moving",
      "hyperbolic problems"
    ]
  },
  {
    "id": "1588",
    "title": "Mitochondrial DNA Alterations in Aging",
    "abstract": "During the aging process, the increase of mitochondrial DNA (mtDNA) alterations has been reported. In this study, we investigated deletions/insertions in the ? 2.4-kb region (from 14680 to 578 bp) of mtDNA covering D-loop region. A total of 96 individuals (ages between 20 and 94 years) were screened in this study. Genomic DNA was purified from whole blood samples. The 2.4-kb region of mtDNA was amplified with PCR and visualized by agarose gel electrophoresis. The sequence of the amplicon was confirmed in one sample by sequencing. We detected mtDNA deletions in only two cases (ages 26 and 30 years) at this resolution. As a result, there is no increase in the major deletions/insertions in the analyzed mtDNA region with aging. Complete sequencing of this region is needed to detect any age-dependent changes",
    "keywords": [
      "mitochondrial dna",
      "mutations",
      "aging"
    ]
  },
  {
    "id": "1589",
    "title": "Statistics for the dynamic analysis of scientometric data: the evolution of the sciences in terms of trajectories and regimes",
    "abstract": "The gap in statistics between multi-variate and time-series analysis can be bridged by using entropy statistics and recent developments in multi-dimensional scaling. For explaining the evolution of the sciences as non-linear dynamics, the configurations among variables can be important in addition to the statistics of individual variables and trend lines. Animations enable us to combine multiple perspectives (based on configurations of variables) and to visualize path-dependencies in terms of trajectories and regimes. Path-dependent transitions and systems formation can be tested using entropy statistics.",
    "keywords": [
      "dynamic",
      "evolution",
      "trajectories",
      "regimes",
      "entropy"
    ]
  },
  {
    "id": "1590",
    "title": "A recurrent self-evolving fuzzy neural network with local feedbacks and its application to dynamic system processing",
    "abstract": "This paper proposes a recurrent self-evolving fuzzy neural network with local feedbacks (RSEFNN-LF) for dynamic system processing A RSEFNN-LF is composed of zero-order or first-order Takagi-Sugeno-Kang (TSK)-type recurrent fuzzy if-then rules The recurrent structure in a RSEFNN-LF comes from locally feeding the tiring strength of a fuzzy rule back to itself. A RSEFNN-LF is constructed on-line via simultaneous structure and parameter learning In structure learning, an efficient rule and fuzzy set generation algorithm is proposed to generate fuzzy rules on-line and reduce the number of fuzzy sets in each dimension In parameter learning, the consequent part parameters are learned (hi nth a varying-dimensional Kalman filter algorithm whose input dimension varies with structure learning The antecedent part and feedback loop parameters are learned using a gradient descent algorithm The RSEFNN-LF is applied to dynamic system identification, chaotic sequence prediction, and speech recognition problems. This paper also compares the performance of the RSEFNN-LF with other recurrent fuzzy neural networks.  ",
    "keywords": [
      "fuzzy system models",
      "neuro-fuzzy systems",
      "recurrent fuzzy systems",
      "dynamic system identification",
      "dynamic sequence prediction",
      "speech recognition"
    ]
  },
  {
    "id": "1591",
    "title": "Inside the adaptive enterprise: an information technology capabilities perspective on business process agility",
    "abstract": "Recent innovations in utility computing, web services, and service-oriented architectures, combined with a growing array of IT skills, have improved firms ability to be more agile in responding to change. Using the resource-based view of the firm, prior research suggests that IT resources, in isolation, are unlikely to yield superior performance and so as firms try to boost their agility, the question becomes how to configure IT resources to prepare for, or react to, change. In this paper, we posit that managerial IT capabilities based on IT-business partnerships, strategic planning, and ex-post IT project analysis lead to the development of technical IT capabilities associated with a flexible IT infrastructure which in turn drives agility or a firms ability to react to change in its products and markets. Using data from matched surveys of IT and business executives in 241 firms, we find that managerial and technical capabilities affect agility. In further testing, we reveal that in a stable setting, technical IT capabilities are more important to agility than managerial IT capabilities, while in a dynamic setting, the opposite is true. Thus, for firms operating in volatile markets, effective models of managerial IT governance are essential for delivering superior agility or adaptiveness.",
    "keywords": [
      "it capabilities",
      "it infrastructure flexibility",
      "process agility",
      "it governance",
      "environmental dynamism"
    ]
  },
  {
    "id": "1592",
    "title": "built-in self test architectures for multistage interconnection networks",
    "abstract": "A novel built-in self test architecture for locally controlled cube-type N x N multistage interconnection networks (MINs) is presented. First, a state-based pseudoexhaustive test procedure for this class of MINs is outlined. Then, a labelling algorithm on a binary n-cube is described which generates the necessary inputs for the tests. From the dependence graph of this algorithm a tree architecture is derived which results in a hardware overhead of O(1/log N).",
    "keywords": [
      "dependence graph",
      "multistage interconnection network",
      "locally controlled min",
      "labelling algorithm",
      "built-in self testing",
      "state-based pseudoexhaustive test",
      "tree architecture",
      "trees ",
      "multistage interconnection networks",
      "binary n-cube",
      "hypercube networks",
      "built-in self test",
      "hardware overhead"
    ]
  },
  {
    "id": "1593",
    "title": "Voting almost maximizes social welfare despite limited communication",
    "abstract": "In cooperative multiagent systems an alternative that maximizes the social welfare the sum of utilities can only be selected if each agent reports its full utility function. This may be infeasible in environments where communication is restricted. Employing a voting rule to choose an alternative greatly reduces the communication burden, but leads to a possible gap between the social welfare of the optimal alternative and the social welfare of the one that is ultimately elected. Procaccia and Rosenschein (2006) [13] have introduced the concept of distortion to quantify this gap. In this paper, we present the notion of embeddings into voting rules: functions that receive an agent's utility function and return the agent's vote. We establish that very low distortion can be obtained using randomized embeddings, especially when the number of agents is large compared to the number of alternatives. We investigate our ideas in the context of three prominent voting rules with low communication costs: Plurality, Approval, and Veto. Our results arguably provide a compelling reason for employing voting in cooperative multiagent systems.  ",
    "keywords": [
      "computational social choice"
    ]
  },
  {
    "id": "1594",
    "title": "using a distance metric to guide pso algorithms for many-objective optimization",
    "abstract": "In this paper we propose to use a distance metric based on user-preferences to efficiently find solutions for many-objective problems. We use a particle swarm optimization (PSO) algorithm as a baseline to demonstrate the usefulness of this distance metric, though the metric can be used in conjunction with any evolutionary multi-objective (EMO) algorithm. Existing user-preference based EMO algorithms rely on the use of dominance comparisons to explore the search-space. Unfortunately, this is ineffective and computationally expensive for many-objective problems. In the proposed distance metric based PSO, particles update their positions and velocities according to their closeness to preferred regions in the objective-space, as specified by the decision maker. The proposed distance metric allows an EMO algorithm's search to be more effective especially for many-objective problems, and to be more focused on the preferred regions, saving substantial computational cost. We demonstrate how to use a distance metric with two user-preference based PSO algorithms, which implement the reference point and light beam search methods. These algorithms are compared to a user-preference based PSO algorithm relying on the conventional dominance comparisons. Experimental results suggest that the distance metric based algorithms are more effective and efficient especially for difficult many-objective problems.",
    "keywords": [
      "multi-objective optimization",
      "particle swarm optimization",
      "light beam search",
      "user-preference methods",
      "reference point method",
      "many-objective optimization"
    ]
  },
  {
    "id": "1595",
    "title": "The effects of inhomogeneities on MCG forward solution",
    "abstract": "The aim of this study was to quantify the effects of inhomogeneities on magnetocardiography (MCG) forward solutions. It can serve to guide the selection of inhomogeneities to include in any geometric model used to compute magnetocardiographies fields. A numerical model of a human torso was used which construction included geometry for major anatomical structures such as subcutaneous fat, skeletal muscle, lungs, major arteries and veins, and the bones. Simulations were done with a single current dipole placed at different sites of heart. The boundary element method (BEM) was utilized for numerical treatment of magnetic field calculations. Comparisons of the effects of different conductivity on MCG forward solution followed one of two basic schemes: 1) consider the difference between the magnetic fields of the homogeneous torso model and the same model with one inhomogeneity of a single organ or tissue added; 2) consider the difference between the magnetic fields of the full inhomogeneous model and the same model with one inhomogeneity of individual organ or tissue removed. When single inhomogeneities were added to an otherwise homogeneous model, the skeletal muscle, the right lung, the both lungs and the left lung had larger average effects (15.9, 15.1, 14.9, 14.4% relative error (RE), respectively) than the other inhomogeneities tested. When single inhomogeneities were removed from an otherwise full inhomogeneneous model, the both lungs, the left, lung, and the skeletal muscle and the right lung had larger effects (17.3, 14.9, 14.3, 14.2% relative error (RE) respectively) than other inhomogeneities tested. The results of this study suggested that accurate representation of tissue inhomogeneity has a significant effect on the accuracy of the MCG forward solution. Our results showed that the inclusion of the boundaries also had effects on the topology of the magnetic fields and on the MCG inverse solution accuracy.",
    "keywords": [
      "mcg",
      "inhomogeneity",
      "current dipole",
      "forward problem",
      "bem"
    ]
  },
  {
    "id": "1596",
    "title": "Criteria for inductive inference with mind changes and anomalies of recursive real-valued functions",
    "abstract": "This paper investigates the interaction of mind changes and anomalies for inductive inference of recursive real-valued functions. We show that the criteria for inductive inference of recursive real-valued functions by bounding the number of mind changes and anomalies preserve the same hierarchy as that of recursive functions, if the length of each anomaly as an interval is bounded. However, we also show that, without bounding it, the hierarchy of some criteria collapses. More precisely, while the class of recursive real-valued functions inferable in the limit allowing no more than one anomaly is properly contained in the class allowing just two anomalies, the latter class coincides with the class allowing arbitrary and bounded number of anomalies.",
    "keywords": [
      "learning theory",
      "inductive inference",
      "real-valued function"
    ]
  },
  {
    "id": "1597",
    "title": "a semantics-based aspect-oriented approach to adaptation in web engineering",
    "abstract": "In the modern Web, users are accessing their favourite Web applications from any place, at any time and with any device. In this setting, they expect the application to user-tailor and personalize content access upon their particular needs. Exhibiting some kind of user- and context-dependency is thus crucial in Web Engineering. In this research, we focus on separating the adaptation engineering process from regular Web engineering by applying aspect-oriented techniques. We show how semantic information and metadata associated with the content can be exploited in our aspect-oriented approach. Furthermore, the approach allows the use of global (structural) properties of the Web application in adaptation specification. We thus obtain several advantages, which are demonstrated in this paper: to control adaptation specification) separate from (regular) Web Engineering oncerns in a richer, more consistent, robust and flexible way.",
    "keywords": [
      "adaptation",
      "web engineering",
      "semantic web",
      "aspect-orientation"
    ]
  },
  {
    "id": "1598",
    "title": "Approximation Properties of Some Multivariate Generalized Singular Integrals in the Unit Polydisk",
    "abstract": "The aim of this paper is to obtain several results in approximation by Jackson-type generalizations of multi-complex Picard, Poisson-Cauchy and Gauss-Weierstrass singular integrals in terms of higher order moduli of smoothness in polydisks.",
    "keywords": [
      "generalized multi-complex singular integrals",
      "jackson-type estimates",
      "global smoothness preservation"
    ]
  },
  {
    "id": "1599",
    "title": "Autocontinuity, convergence in measure, and convergence in distribution",
    "abstract": "It is shown that the autocontinuity is equivalent to the property that the convergence in measure implies the convergence in distribution. This result is applied to Denneberg's dominated convergence theorem for the Choquet integral. ",
    "keywords": [
      "measure theory",
      "fuzzy measure",
      "choquet integral",
      "autocontinuity",
      "convergence in measure",
      "convergence in distribution"
    ]
  },
  {
    "id": "1600",
    "title": "Molecular recognition of CYP26A1 binding pockets and structureactivity relationship studies for design of potent and selective retinoic acid metabolism blocking agents",
    "abstract": "The homology modeling of CYP26A1 was constructed. The superimposition, pharmacophore model and molecular docking were performed. The structureactivity relationship (SAR) was obtained by the model. This study provides guidance for the design of more potent RAMBAs.",
    "keywords": [
      "all-trans-retinoic acid ",
      "cyp26a1",
      "retinoic acid metabolism blocking agents ",
      "homology modeling",
      "pharmacophore model"
    ]
  },
  {
    "id": "1601",
    "title": "Performance modeling of microsecond scale biological molecular dynamics simulations on heterogeneous architectures",
    "abstract": "Performance improvements in biomolecular simulations based on molecular dynamics (MD) codes are widely desired. Unfortunately, the factors, which allowed past performance improvements, particularly the microprocessor clock frequencies, are no longer increasing. Hence, novel software and hardware solutions are being explored for accelerating performance of widely used MD codes. In this paper, we describe our efforts on porting, optimizing and tuning of Large-scale Atomic/Molecular Massively Parallel Simulator, a popular MD framework, on heterogeneous architectures: multi-core processors with graphical processing unit (GPU) accelerators. Our implementation is based on accelerating the most computationally expensive non-bonded interaction terms on the GPUs and overlapping the computation on the CPU and GPUs. This functionality is built on top of message passing interface that allows multi-level parallelism to be extracted even at the workstation level with the multi-core CPUs and allows extension of the implementation on GPU-enabled clusters. We hypothesize that the optimal benefit of heterogeneous architectures for applications will come by utilizing all possible resources (for example, CPU-cores and GPU devices on GPU-enabled clusters). Benchmarks for a range of biomolecular system sizes are provided, and an analysis is performed on four generations of NVIDIA's GPU devices. On GPU-enabled Linux clusters, by overlapping and pipelining computation and communication, we observe up to 10-folds application acceleration in multi-core and multi-GPU environments illustrating significant performance improvements. Detailed analysis of the implementation is presented that allows identification of bottlenecks in algorithm, indicating that code optimization and improvements on GPUs could allow microsecond scale simulation throughput on workstations and inexpensive GPU clusters, putting widely desired biologically relevant simulation time-scales within reach of a large user community. In order to systematically optimize simulation throughput and to enable performance prediction, we have developed a parameterized performance model that will allow developers and users to explore the performance potential of future heterogeneous systems for biological simulations. ",
    "keywords": [
      "performance modeling",
      "gpus",
      "molecular dynamics"
    ]
  },
  {
    "id": "1602",
    "title": "An interactive framework for power system harmonics measurement using graphical programming and the Internet",
    "abstract": "Traditional methods to measure power system harmonics employ the power harmonic analyser or the software package such as Matlab. They, however, have limitations in capability of graphical programming environment and extension to the Internet connection for remote monitoring and control. This paper presents an alternative approach using a virtual instrument (VI) that can carry out power system harmonics measurement based on Fast Fourier Transform (FFT). The experimental results testify to its good performance and remote monitoring capability. ",
    "keywords": [
      "fft",
      "power system harmonics",
      "virtual instrument"
    ]
  },
  {
    "id": "1603",
    "title": "A multiobjective approach for solving cooperative n-person games",
    "abstract": "A linear programming model is introduced to solve cooperative games. The solution is always Pareto optimal. It is based on the idea of the core but instead of requiring rationality for all groups, a multiobjective approach is proposed including the importance weights of the players. A case study illustrates the application of this method.",
    "keywords": [
      "game theory",
      "multiobjective programming",
      "transmission expansion cost allocation"
    ]
  },
  {
    "id": "1604",
    "title": "using landing pages for sponsored search ad selection",
    "abstract": "We explore the use of the landing page content in sponsored search ad selection. Specifically, we compare the use of the ad's intrinsic content to augmenting the ad with the whole, or parts, of the landing page. We explore two types of extractive summarization techniques to select useful regions from the landing pages: out-of-context and in-context methods. Out-of-context methods select salient regions from the landing page by analyzing the content alone, without taking into account the ad associated with the landing page. In-context methods use the ad context (including its title, creative, and bid phrases) to help identify regions of the landing page that should be used by the ad selection engine. In addition, we introduce a simple yet effective unsupervised algorithm to enrich the ad context to further improve the ad selection. Experimental evaluation confirms that the use of landing pages can significantly improve the quality of ad selection. We also find that our extractive summarization techniques reduce the size of landing pages substantially, while retaining or even improving the performance of ad retrieval over the method that utilize the entire landing page.",
    "keywords": [
      "extractive summarization",
      "sponsored search",
      "landing pages",
      "compositional semantics"
    ]
  },
  {
    "id": "1605",
    "title": "The exploration of consumers' behavior in choosing hospital by the application of neural network",
    "abstract": "The economic in Taiwan has been dramatically improved in the last two decades. During this period, the national health insurance was first conducted in 1995 and plans of health insurance payment have been modified several times. Demands of high quality and service on medical care are brought up in consumers' mind. Nowadays, hospital operating environment is getting more and more competitive. Therefore, how to take the advantage of competitiveness is the urgent topic of gaining advantage of competitiveness. The research applied neural network to classify consumers' behavior in choosing hospitals. A quantitative research of questionnaire was first conducted to explore consumers' behavior in choosing hospitals in southern Taiwan. Factors of consumers' behavior were categorized into four types. Then, a back propagation neural network classification model was developed. The model demonstrates the usefulness of 85.1% classification rate in classifying consumers' styles. Finally, their marketing implications were discussed. Based on the results of the research, the evidence is enough to suggest that the neural network model is useful in identifying existing patterns of hospitals' consumers.  ",
    "keywords": [
      "consumers' behavior",
      "neural networks",
      "recognition",
      "marketing strategy"
    ]
  },
  {
    "id": "1606",
    "title": "Obesity-induced metabolic stresses in breast and colon cancer",
    "abstract": "Epidemiological studies have suggested that excess body weight gain may be a major risk factor for colon and breast cancer. A positive energy balance creates metabolic stresses, including the excess production of reactive oxygen species (ROS), hyperinsulinemia, the elevated adipokine secretion, and increased gut permeability. Obesity is a risk factor for breast cancer in postmenopausal women, and overweight women are more likely to have poor outcomes. The higher circulating concentration of insulin-like growth factor 1 (IGF-1) in overweight and obese women is thought to be an important mediator to promote cell proliferation and survival via the activation of phosphatidylinositol 3-kinase (PI3K)/Akt and mitogen-activated protein kinase (MAPK)/p38 signaling pathways. In an animal model of colon carcinogenesis, overweight mice fed a high-fat diet exhibited a greater number of colon tumors than lean animals. The increased abdominal fat was associated with higher concentrations of leptin, insulin, and IGF-1, which possibly mediate tumor growth. These data suggest that the metabolic burden created by excess adiposity accelerates uncontrolled cell growth and survival, thereby increasing the risk of developing breast and colon cancer.",
    "keywords": [
      "obesity",
      "breast cancer",
      "colon cancer"
    ]
  },
  {
    "id": "1607",
    "title": "Quantitative fault analysis of roller bearings based on a novel matching pursuit method with a new step-impulse dictionary",
    "abstract": "We proposed a novel matching pursuit method forquantitative analysis of roller bearings. A new step-impulse dictionary is constructed for atom selecting. The method presents good results in the application of both simulated signals and experimental signals. An atomic selection mechanism is proposed to improve the measure accuracy of the bearing spall-like fault size.",
    "keywords": [
      "spall-like fault size assessment",
      "step-impulse dictionary",
      "matching pursuit",
      "bearing fault"
    ]
  },
  {
    "id": "1608",
    "title": "Using Articulated Models for Tracking Multiple C. elegans in Physical Contact",
    "abstract": "We present a method for tracking and distinguishing multiple C. elegans in a video sequence, including when they are in physical contact with one another. The worms are modeled with an articulated model composed of rectangular blocks, arranged in a deformable configuration represented by a spring-like connection between adjacent parts. Dynamic programming is applied to reduce the computational complexity of the matching process. Our method makes it possible to identify two worms correctly before and after they touch each other, and to find the body poses for further feature extraction. All joint points in our model can be also considered to be the pseudo skeleton points of the worm body. It solves the problem that a previously presented morphological skeleton-based reversal detection algorithm fails when two worms touch each other. The algorithm has many applications in the study of physical interactions between C. elegans.",
    "keywords": [
      "articulated model",
      "c. elegans",
      "computer vision",
      "dynamic programming",
      "image processing",
      "model matching"
    ]
  },
  {
    "id": "1609",
    "title": "Making context-sensitive points-to analysis with heap cloning practical for the real world",
    "abstract": "Context-sensitive pointer analysis algorithms with full \"heap cloning\" are powerful but are widely considered to be too expensive to include in production compilers. This paper shows, for the first time, that a context-sensitive, field-sensitive algorithm with full heap cloning (by acyclic call paths) can indeed be both scalable and extremely fast in practice. Overall, the algorithm is able to analyze programs in the range of 100K-200K lines of C code in 1-3 seconds, takes less than 5% of the time it takes for GCC to compile the code (which includes no whole-program analysis), and scales well across five orders of magnitude of code size. It is also able to analyze the Linux kernel (about 355K lines of code) in 3.1 seconds. The paper describes the major algorithmic and engineering design choices that are required to achieve these results, including (a) using flow-insensitive and unification-based analysis, which are essential to avoid exponential behavior in practice; (b) sacrificing context-sensitivity within strongly connected components of the call graph; and (c) carefully eliminating several kinds of O(N-2) behaviors (largely without affecting precision). The techniques used for (b) and (c) eliminated several major bottlenecks to scalability, and both are generalizable to other context-sensitive algorithms. We show that the engineering choices collectively reduce analysis time by factors of up to 3x-21x in our ten largest programs, and that the savings grow strongly with program size. Finally, we briefly summarize results demonstrating the precision of the analysis.",
    "keywords": [
      "pointer analysis",
      "context-sensitive",
      "field-sensitive",
      "interprocedural",
      "static analysis",
      "recursive data structure"
    ]
  },
  {
    "id": "1610",
    "title": "using signal processing to analyze wireless data traffic",
    "abstract": "Experts have long recognized that theoretically it was possible to perform traffic analysis on encrypted packet streams by analyzing the timing of packet arrivals (or transmissions). We report on experiments to realize this possiblity using basic signal processing techniques taken from acoustics to perform traffic analysis on encrypted transmissions over wireless networks. While the work discussed here is preliminary, we are able to demonstrate two very interesting results. First, we can extract timing information, such as round-trip times of TCP connections, from traces of aggregated data traffic. Second, we can determine how data is routed through a network using coherence analysis. These results show that signal processing techniques may prove to be valuable network analysis tools in the future.",
    "keywords": [
      "network",
      "signal processing",
      "wireless",
      "traces",
      "traffic analysis",
      "extraction",
      "encryption",
      "analysis",
      "data",
      "demonstrate",
      "wireless networks",
      "experience",
      "network analysis",
      "timing",
      "future",
      "traffic",
      "connection",
      "informal",
      "tools",
      "coherence"
    ]
  },
  {
    "id": "1611",
    "title": "Analog Layout Retargeting Using Geometric Programming",
    "abstract": "To satisfy the requirements of complex and special analog layout constraints, a new analog layout retargeting method is presented in this article. Our approach uses geometric programming (GP) to achieve new technology design rules, implement device symmetry and matching constraints, and manage parasitics optimization. The GP, a class of nonlinear optimization problem, can be transferred or fitted into a convex optimization problem. Therefore, a global optimum solution can be achieved. Moreover, the GP can address problems with large-scale variables and constraints without setting an initialization variable range. To meet the prerequisites of the GP methodology for analog layout automation, we propose three kinds of mathematical transformations, including negative coefficient transformation, fraction transformation, and maximum of posynomial transformation. The efficiency and effectiveness of the proposed algorithm, as compared with the other existing methods, are demonstrated by a basic case-study example: a two-stage Miller-compensated operational amplifier and a single-ended folded cascode operational amplifier.",
    "keywords": [
      "global optimization",
      "geometric programming",
      "layout",
      "retargeting",
      "transformation"
    ]
  },
  {
    "id": "1612",
    "title": "Modeling and forecasting duration-dependent mortality rates",
    "abstract": "Mortality data of disabled individuals are studied and parametric modeling approaches for the force of mortality are discussed. Empirical observations show that the duration since disablement has a strong effect on mortality rates. In order to incorporate duration effects, different generalizations of the LeeCarter model are proposed. For each proposed model, uniqueness properties and fitting techniques are developed, and parameters are calibrated to mortality observations of the German Pension Insurance. Difficulties with coarse tabulation of the empirical data are solved by an ageperiod-duration Lexis diagram. Forecasting is demonstrated for an exemplary model, leading to the conclusion that duration dependence should not be neglected. While the data shows a clear longevity trend with respect to age, significant fluctuations but no systematic trend is observed for the duration effects.",
    "keywords": [
      "stochastic mortality",
      "duration dependence",
      "disability insurance",
      "leecarter model",
      "lexis diagram"
    ]
  },
  {
    "id": "1613",
    "title": "DRBEM for Cauchy convection-diffusion problems with variable coefficients",
    "abstract": "In this study we present a numerical technique based on the Dual Reciprocity Boundary Element Method (DRBEM) combined with the Tikhonov regularisation method, or with the Truncated Singular Value Decomposition (TSVD) method, in order to solve the Cauchy problem associated with the steady-state convection-diffusion equation with variable coefficients. This is an interesting problem from the practical point of view as it can be used to model certain water pollution problems. The numerical results obtained in the test examples investigated show that the Tikhonov regularisation method is very suitable for the smooth geometries considered, i.e. circular and annular domains, while for the non-smooth rectangular domain the TSVD method is preferable. The choice of the regularisation parameter and of the truncation parameter is based on the L-curve method.",
    "keywords": [
      "cauchy problem",
      "convection-diffusion equation",
      "drbem",
      "tikhonov regularisation",
      "tsvd",
      "l-curve"
    ]
  },
  {
    "id": "1614",
    "title": "On the Dimension of a Face Exposed by Proper Separation of Convex Polyhedra",
    "abstract": "Whenever two nonempty convex polyhedra can be properly separated, a separating hyperplane may be chosen to contain a face of either polyhedron. It is demonstrated that, in fact, one or the other of the polyhedra admits such an exposed face having dimension no smaller than approximately half the larger dimension of the two polyhedra. An example shows that the bound on face dimension is optimal, and a linear programming representation of the problem is given.",
    "keywords": [
      "linear programming",
      "calculation of supporting hyperplanes",
      "detecting set intersection",
      "polyhedral relative interiors"
    ]
  },
  {
    "id": "1615",
    "title": "Estrogen Sulfotransferases in Breast and Endometrial Cancers",
    "abstract": "Estrogen sulfotransferase is significantly more active in the normal breast cell (e.g., Human 7) than in the cancer cell (e.g., MCF-7). The data suggest that in breast cancer sulfoconjugated activity is carried out by another enzyme, the SULT1A, which acts at high concentration of the substrates. In breast cancer cells sulfotransferase (SULT) activity can be stimulated by various progestins: medrogestone, promegestone, and nomegestrol acetate, as well as by tibolone and its metabolites. SULT activities can also be controlled by other substances including phytoestrogens, celecoxib, flavonoids (e.g., quercetin, resveratrol), and isoflavones. SULT expression was localized in breast cancer cells, which can be stimulated by promegestone and correlated with the increase of the enzyme activity. The estrogen sulfotransferase (SULT1E1), which acts at nanomolar concentration of estradiol, can inactivate most of this hormone present in the normal breast; however, in the breast cancer cells, the sulfotransferase denoted as SULT1A1 is mainly present, and this acts at micromolar concentrations of E2. A correlation was postulated among breast cancer cell proliferation, the effect of various progestins, and sulfotransferase stimulation. In conclusion, it is suggested that factors involved in the stimulation of the estrogen sulfotransferases could provide new possibilities for the treatment of patients with hormone-dependent breast and endometrial cancers.",
    "keywords": [
      "estrogens",
      "sulfotransferase ",
      "breast",
      "endometrium",
      "cancer",
      "estrogen sulfotransferase "
    ]
  },
  {
    "id": "1616",
    "title": "Coordination and communication issues in multi-agent expert system: concurrent configuration design advisor",
    "abstract": "The model of concurrent configuration design and the architecture of Concurrent Configuration Design Advisor (CCDA) are considered. The CCDA is developed as an open dynamic expert system of interacting agents. In the context of this article a configuration problem is defined as a decision-making procedure performed by concurrent processes (agents). The analysis of concurrent and sequential activities in this system is based on structural transformations (in the form of graph grammars) that permits the definition of correct criteria for data integrity and the consistency of configured project data model, and any dynamic changes in the project to be modeled conveniently. This approach also makes it possible to handle complex hierarchial data structures of real configured objects and to model communication and synchronization of decision making in a distributed expert system in a common formalism. The system to be configured is decomposed into structured objects, called fragments. The proposed model consists of three types of agents. D-agents are those capable of the object configuration design within the constraints on their attributes. To cope with agents' coordination, objects' consistency and data integrity problems, a special type of agent, facilitator or F-agent, is introduced. Finally, project assistants or A-agents are responsible for the user interface at the stage of object model definition. The discussion is illustrated with examples from the application domain of flexible manufacturing systems. Experimental results, current and future work on the expert system implementation are considered.",
    "keywords": [
      "expert systems",
      "agent concurred configuration design"
    ]
  },
  {
    "id": "1617",
    "title": "A methodology for strategic sourcing",
    "abstract": "Strategic sourcing is critical for firms practicing the principles of supply chain management. It specifically deals with managing the supply base in an effective manner by identifying and selecting suppliers for strategic long-term partnerships, involving in supplier development initiatives by effectively allocating resources to enhance supplier performance, providing benchmarks and continuous feedback to suppliers, and in some cases involving in supplier pruning activities. Currently, the methodologies in practice for strategic sourcing have mostly been subjective in nature with few objective decision models focused at supplier evaluation, which are also not devoid of limitations. This paper proposes an objective framework for effective supplier sourcing, which considers multiple strategic and operational factors in the evaluation process. Suppliers are categorized into groups based on performance, which assists managers in identifying candidates for strategic long-term partnerships, supplier development programs, and pruning. In addition, this research investigates the differences among supplier groups in proposing possible improvement strategies for ineffectively performing suppliers. Also, we demonstrate the methodological richness of our framework when compared to some of the traditional methods proposed and utilized for supplier evaluation purposes. The supplier data utilized in the study is obtained from a large multinational corporation in the telecommunications industry.",
    "keywords": [
      "nonparametric efficiency analysis",
      "purchasing",
      "strategic sourcing"
    ]
  },
  {
    "id": "1618",
    "title": "Offline recognition of unconstrained handwritten texts using HMMs and statistical language models",
    "abstract": "This paper presents a system for the offline recognition of large vocabulary unconstrained handwritten texts. The only assumption made about the data is that it is written in English. This allows the application of Statistical Language Models in order to improve the performance of our system. Several experiments have been performed using both single and multiple writer data. Lexica of variable size (from 10,000 to 50,000 words) have been used. The use of language models is shown to improve the accuracy of the system (when the lexicon contains 50,000 words, the error rate is reduced by similar to 50 percent for single writer data and by similar to 25 percent for multiple writer data). Our approach is described in detail and compared with other methods presented in the literature to deal with the same problem. An experimental setup to correctly deal with unconstrained text recognition is proposed.",
    "keywords": [
      "offline cursive handwriting recognition",
      "statistical language models",
      "n-grams",
      "continuous density hidden markov models"
    ]
  },
  {
    "id": "1619",
    "title": "QR code based blind digital image watermarking with attack detection code",
    "abstract": "A QR code based blind digital image watermarking technique with an attack detection feature is described here. The technique describes a key based framework to incorporate image, server port address or website address as watermark data; which increases the extended usability of the embedded data and the adaptability of the verification application. The watermarking problem is formulated as a signal communication problem with watermark data representation, embedding of watermark and attack detection as a source encoding, channel encoding and attenuation detection problems respectively. The mathematical aspects of the respective signal processing problems are extended to digital image watermarking with sufficient background support. The use of QR code ensures extended usability, while the application specific watermark data achieves adaptability of the verification application. The QR code is embedded into the attack resistant HH component of 1st level DWT domain of the cover image and to detect malicious interference by an attacker, a unique image registry code generated from the high frequency structural components of the stego-image is used. The key based approach and the attack resistant embedding domain makes this method robust against visually invariant attacks. The testing results show the compliance of the method with all the proposed aspects.",
    "keywords": [
      "blind watermarking",
      "qr code",
      "data hiding",
      "distortion detection",
      "image registry code"
    ]
  },
  {
    "id": "1620",
    "title": "AN IMPROVED RULE-BASED DUMMY METAL FILL METHOD FOR 65 NM ASIC DESIGN",
    "abstract": "Chemical-mechanical polishing (CMP) is an essential process in deep-submicrometer LSI manufacturing to achieve Chip's planarization. It includes two processes: back-end-of-line (BEOL) and front-end-of-line (FEOL). This paper focuses on the problem of BEOL in 65 nm copper process. Although model-based dummy metal fill has become a tendency recently, the proposed improved rule-based dummy fill is appropriate still. A middle scale design is used for simulation. The metal density, oxide thickness, copper thickness, capacitance variation and variation of layout data size were investigated. The results show that improved rule-based dummy fill and model-based dummy fill have the same planarization, and proposed method has small capacitance variation. The GDS file size of the proposed rule-based fill is less than the model-based fill's.",
    "keywords": [
      "chemical-mechanical polishing ",
      "rule-based fill",
      "dfm",
      "asic"
    ]
  },
  {
    "id": "1621",
    "title": "Numerical simulation code for self-gravitating BoseEinstein condensates",
    "abstract": "We completed the development of simulation code that is designed to study the behavior of a conjectured dark matter galactic halo that is in the form of a BoseEinstein Condensate (BEC). The BEC is described by the GrossPitaevskii equation, which can be solved numerically using the CrankNicholson method. The gravitational potential, in turn, is described by Poissons equation, that can be solved using the relaxation method. Our code combines these two methods to study the time evolution of a self-gravitating BEC. The inefficiency of the relaxation method is balanced by the fact that in subsequent time iterations, previously computed values of the gravitational field serve as very good initial estimates. The code is robust (as evidenced by its stability on coarse grids) and efficient enough to simulate the evolution of a system over the course of 109 10 9 years using a finer (100100100) spatial grid, in less than a day of processor time on a contemporary desktop computer. Program title: bec3p Catalogue identifier: AEOR_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEOR_v1_0.html Program obtainable from: CPC Program Library, Queens University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 5248 No. of bytes in distributed program, including test data, etc.: 715402 Distribution format: tar.gz Programming language: C++or FORTRAN. Computer: PCs or workstations. Operating system: Linux or Windows. Classification: 1.5. Nature of problem: Simulation of a self-gravitating BoseEinstein condensate by simultaneous solution of the GrossPitaevskii and Poisson equations in three dimensions. Solution method: The GrossPitaevskii equation is solved numerically using the CrankNicholson method; Poissons equation is solved using the relaxation method. The time evolution of the system is governed by the GrossPitaevskii equation; the solution of Poissons equation at each time step is used as an initial estimate for the next time step, which dramatically increases the efficiency of the relaxation method. Running time: Depends on the chosen size of the problem. On a typical personal computer, a 100100100 grid can be solved with a time span of 10Gyr in approx. a day of running time.",
    "keywords": [
      "gravity",
      "poissons equation",
      "grosspitaevskii equation",
      "dark matter",
      "galaxy rotation"
    ]
  },
  {
    "id": "1622",
    "title": "Resource management and control in converged optical data center networks: Survey and enabling technologies",
    "abstract": "In addition to the optical interconnection among servers for the Intra Data Center (IDC), the optical interconnection of geographically distributed data centers also becomes increasingly important since data centers are geographically distributed so that one data center maybe far away from another. So the converged Optical and Data Center Network (ODCN) emerges as the times require. In the ODCN, each optically interconnected IDC locates at the edge of the optical backbone. In this article, we first make an extensive survey on the resource management and control in the ODCN, and we find that: (1) for this new network paradigm, the intelligent coexistence of heterogeneous technologies should be considered because the ODCN will be required to satisfy diverse and highly dynamic network services; (2) the integrated virtualization of backbone bandwidth and computing resources should be performed for the resource management in ODCNs, with the objective to improve the underlying infrastructure utilization; (3) after performing the integrated virtualization, a set of virtual networks are generated, and each of them has virtual lightpaths and virtual machines. But a static virtual network merely satisfies a certain range of Service Level Agreements (SLAs), and it merely adapts to a particular network status. When the SLA significantly varies or the Quality of Transmission (QoT) gets worse, it is necessary to trigger the dynamic planning for the virtual network reconfiguration; (4) to decrease the control overhead and the delay of making decisions for the dynamic planning, it is practical to embed a highly effective network control plane into intelligent ODCN. Consequently, we make a blueprint where we execute the intelligent coexistence, integrated virtualization and dynamic planning for the resource management in ODCNs. Some preliminary works and simulation results will guide the future work.",
    "keywords": [
      "converged optical and data center network",
      "intelligent coexistence",
      "integrated virtualization",
      "dynamic planning",
      "network intelligence"
    ]
  },
  {
    "id": "1623",
    "title": "Comparing Malmquist and HicksMoorsteen productivity indices: Exploring the impact of unbalanced vs. balanced panel data",
    "abstract": "Compare Malmquist and HicksMoorsteen productivity indices on several technologies. First test of impact of balancing unbalanced panel data. First test of ability of the Malmquist index to approximate the HicksMoorsteen index. Empirical differences show up between both indices due to unbalancedness. Malmquist index offers poor empirical approximation of HicksMoorsteen index.",
    "keywords": [
      "malmquist productivity index",
      "hicksmoorsteen productivity index",
      "balanced panel",
      "unbalanced panel"
    ]
  },
  {
    "id": "1624",
    "title": "Square reflection cryptanalysis of 5-round Feistel networks with permutations",
    "abstract": "New distinguishers are derived for 4-round Feistel Networks with permutational round functions under a weak key condition. A new generic attack is mounted on 5-round Feistel Networks by combining the square attack and the reflection attack. Hellman tables are utilized to construct a tradeoff between the time complexity and the memory complexity of the new attack. The generic attack is applied to 5-round DEAL as a concrete example. The complexity of the attack on DEAL is 265 DES encryptions, working on a key set of cardinality 272.",
    "keywords": [
      "cryptography",
      "block cipher",
      "feistel network",
      "square attack",
      "reflection attack",
      "deal",
      "fixed point"
    ]
  },
  {
    "id": "1625",
    "title": "On the time-space complexity of geometric elimination procedures",
    "abstract": "In [25] and [22] a new algorithmic concept was introduced for the symbolic solution of a zero dimensional complete intersection polynomial equation system satisfying a certain generic smoothness condition. The main innovative point of this algorithmic concept consists in thp introduction nf a innovative point of this algorithmic concept consists in the introduction of a new geometric invariant, called the degree of the input system, and the proof that the most common elimination problems have time complexity which is polynomial in this degree and the length of the input. In this paper we apply this algorithmic concept in order to exhibit an elimination procedure whose space complexity is only quadratic and its time complexity is only cubic in the degree of the input system.",
    "keywords": [
      "algorithmic elimination theory",
      "polynomial equation solving",
      "symbolic computation",
      "algebraic complexity theory",
      "time-space complexity",
      "computation tree",
      "straight-line program"
    ]
  },
  {
    "id": "1626",
    "title": "the future of multiprocessor systems-on-chips",
    "abstract": "This paper surveys the state-of-the-art and pending challenges in MPSoC design. Standards in communications, multimedia, networking, and other areas encourage the development of high-performance platforms that can support a range of implementations of the standard. A multiprocessor system-on-chip includes embedded processors, digital logic, and mixed-signal circuits combined into a heterogeneous multiprocessor. This mix of technologies creates a major challenge for MPSoC design teams. We will look at some existing MPSoC designs and then describe some hardware and software challenges for MPSoC designers.",
    "keywords": [
      "communication",
      "network",
      "challenge",
      "platform",
      "signaling",
      "survey",
      "developer",
      "design",
      "high-performance",
      "standardization",
      "art",
      "paper",
      "real-time",
      "heterogeneity",
      "digitize",
      "processor",
      "software",
      "technologies",
      "systems",
      "mpsoc",
      "multimedia",
      "implementation",
      "embedded software",
      "system-on-chip",
      "support",
      "logic",
      "hardware",
      "future",
      "multiprocessor",
      "low power",
      "embedding",
      "teams",
      "circuits"
    ]
  },
  {
    "id": "1627",
    "title": "A linguistic evaluation approach for universal design",
    "abstract": "Universal design (UD) is an approach for developing products and environments that are usable by all people to the greatest extent possible. It benefits users of all ages and abilities without the need for adaptation or specialized design. Although a set of acknowledged principles has been developed and commonly used by industry and academia, it is difficult to quantitatively evaluate whether a product is indeed a good example of UD. This study improves the shortcomings of traditional analytical hierarchy process (AHP) methods in quantitative judgments, priority analysis, and aggregation performance, and proposes a linguistic evaluation approach for UD. The aims of this study are (1) to construct a hierarchy for evaluation using criteria against the UD principles, (2) to develop a convenient and effective eigenvalue algorithm for deriving the weights of criteria, and (3) to aggregate preference information and rank the order of decision alternatives by using linguistic variables associated with fuzzy weighted average techniques. An empirical study is conducted to illustrate the practicability of the proposed approach. In addition to UD, the proposed linguistic evaluation approach can be used to systematically assess alternatives from criteria that are relevant to a set of qualitative attributes.",
    "keywords": [
      "universal design",
      "linguistic evaluation",
      "analytical hierarchy process ",
      "eigenvalue algorithm",
      "fuzzy weighted average"
    ]
  },
  {
    "id": "1628",
    "title": "Multi-Level Planning for Semi-autonomous Vehicles in Traffic Scenarios Based on Separation Maximization",
    "abstract": "The planning of semi-autonomous vehicles in traffic scenarios is a relatively new problem that contributes towards the goal of making road travel by vehicles free of human drivers. An algorithm needs to ensure optimal real time planning of multiple vehicles (moving in either direction along a road), in the presence of a complex obstacle network. Unlike other approaches, here we assume that speed lanes are not present and that different lanes do not need to be maintained for inbound and outbound traffic. Our basic hypothesis is to carry forward the planning task to ensure that a sufficient distance is maintained by each vehicle from all other vehicles, obstacles and road boundaries. We present here a 4-layer planning algorithm that consists of road selection (for selecting the individual roads of traversal to reach the goal), pathway selection (a strategy to avoid and/or overtake obstacles, road diversions and other blockages), pathway distribution (to select the position of a vehicle at every instance of time in a pathway), and trajectory generation (for generating a curve, smooth enough, to allow for the maximum possible speed). Cooperation between vehicles is handled separately at the different levels, the aim being to maximize the separation between vehicles. Simulated results exhibit behaviours of smooth, efficient and safe driving of vehicles in multiple scenarios; along with typical vehicle behaviours including following and overtaking.",
    "keywords": [
      "unmanned ground vehicles",
      "dijkstra's algorithm",
      "optimization",
      "robotic motion planning",
      "nonholonomic constraints",
      "autonomous vehicles"
    ]
  },
  {
    "id": "1629",
    "title": "Integrating intrusion alert information to aid forensic explanation: An analytical intrusion detection framework for distributive IDS",
    "abstract": "The objective of this research is to show an analytical intrusion detection framework (AIDF) comprised of (i) a probability model discovery approach, and (ii) a probabilistic inference mechanism for generating the most probable forensic explanation based on not only just the observed intrusion detection alerts, but also the unreported signature rules that are revealed in the probability model. The significance of the proposed probabilistic inference is its ability to integrate alert information available from IDS sensors distributed across subnets. We choose the open source Snort to illustrate its feasibility, and demonstrate the inference process applied to the intrusion detection alerts produced by Snort. Through a preliminary experimental study, we illustrate the applicability of AIDF for information integration and the realization of (i) a distributive IDS environment comprised of multiple sensors, and (ii) a mechanism for selecting and integrating the probabilistic inference results from multiple models for composing the most probable forensic explanation.  ",
    "keywords": [
      "probabilistic inference",
      "model discovery",
      "intrusion detection",
      "forensic analysis"
    ]
  },
  {
    "id": "1630",
    "title": "The effects of pre-processing of image data on self-modeling image analysis",
    "abstract": "The use of chemical imaging of secondary ion mass spectrometry (SIMS) data for self-modeling image analysis (SIA) has special challenges because of the following reasons: (a) At higher counting rates, the data are non-linear. (b) The heteroscedastic nature of the noise causes structure in the data which gives rise to extra components. ",
    "keywords": [
      "chemometrics",
      "sims",
      "self-modeling mixture analysis",
      "heteroscedastic noise"
    ]
  },
  {
    "id": "1631",
    "title": "on the critical success factors for b2b e-marketplace",
    "abstract": "B2B e-marketplaces have a profound influence on the traditional market and on the way business is conducted. All kinds of e-marketplaces are emerging and developing now, and we have witnessed both successes and failures among these e-marketplaces. However, the failures are far outnumber the successes. Under this background, the paper discusses the critical success factors for operating e-marketplaces from different perspectives. It is shown that the core of e-marketplaces is to build liquidity and capture value. Based on these analyses, comprehensive critical factors including functional factors, strategic factors and technical Factors are discussed for the success of electronic marketplaces; then a tentative framework for the analysis on the critical success factors is proposed.",
    "keywords": [
      "critical success factors",
      "e-marketplace",
      "e-commerce"
    ]
  },
  {
    "id": "1632",
    "title": "G2G information sharing among government agencies",
    "abstract": "Despite its importance in government operations, G2G information sharing remains a challenge for IT professionals worldwide. While recent literature has examined this challenge in Western countries, little has been published on Eastern countries. We developed a four-layer model of G2G information sharing across horizontal functional agencies and used it to conduct an analysis of sites within Chinese contexts. Included in the model were the external environment, interagency partnership, organizational readiness, and user expectation. Through empirical testing, we found that the authority of the upper-level leadership, GuanXi, compatibility, top-management support, cost, process security, and expected risks and benefits had a significant influence on the degree of G2G information sharing. However, laws and policies, interagency trust and IT capability had no significant influence on the degree of G2G information sharing. Also, the expected benefits and the degree had positive effect on the performance of G2G information sharing while expected risks had no significant effect on the performance of G2G information sharing.",
    "keywords": [
      "e-government",
      "government to government ",
      "information sharing",
      "realization degree",
      "performance of g2g information sharing"
    ]
  },
  {
    "id": "1633",
    "title": "Value-cell bar charts for visualizing large transaction data sets",
    "abstract": "One of the common problems businesses need to solve is how to use large volumes of sales histories, Web transactions, and other data to understand the behavior of their customers and increase their revenues. Bar charts are widely used for daily analysis, but only show highly aggregated data. Users often need to visualize detailed multidimensional information reflecting the health of their businesses. In this paper, we propose an innovative visualization solution based on the use of value cells within bar charts to represent business metrics. The value of a transaction can be discretized into one or multiple cells: high-value transactions are mapped to multiple value cells, whereas many small-value transactions are combined into one cell. With value-cell bar charts, users can 1) visualize transaction value distributions and correlations, 2) identify high-value transactions and outliers at a glance, and 3) instantly display values at the transaction record level. Value-Cell Bar Charts have been applied with success to different sales and IT service usage applications, demonstrating the benefits of the technique over traditional charting techniques. A comparison with two variants of the well-known Treemap technique and our earlier work on Pixel Bar Charts is also included.",
    "keywords": [
      "information visualization",
      "multivariate visualization",
      "visualization techniques",
      "methodologies"
    ]
  },
  {
    "id": "1634",
    "title": "Conceptualizing co-ordination and competition in supply chains as complex adaptive system",
    "abstract": "A model has been developed to explore the dynamics of outsourcing strategies and specialization effects. Using transaction cost economics the literature showed that IT investments can create an incentive for a firm to pursue a specialization strategy, spinning off and outsourcing peripheral activities to focus on its core competence. Web services technology, for example, which has been explicitly designed to facilitate inter-operability of machine-to-machine communication, is expected to support such strategy. We extend this analysis proposing a research model that explicitly recognizes industry-level feedback. While specialized firms can enjoy lower production cost they are also more reliant on market conditions and interaction. This dependence exposes a specialized firm to the risk of market failure or vertical foreclose, all of which increase transaction cost. The complication from a modeling perspective has been to properly recognize industry structure and transaction cost as endogenous variables. As a solution we propose formalizing this problem as a complex adaptive system. We present a model with the firm as unit of analysis and its behavior based on micro-economic cost theory and the theory of noncooperative games. The paper follows Zmuds concept of a pure theory manuscript adapted from the Academy of Management Review (1998).",
    "keywords": [
      "outsourcing",
      "complex adaptive system ",
      "multi-agent system modeling",
      "simulation",
      "computational explanation"
    ]
  },
  {
    "id": "1635",
    "title": "Collaboration - a new IT-service in the next generation of regional health care networks",
    "abstract": "During the past 10-15 years, Regional Health Care Networks (RHCN) have been established in many regions throughout the world. RHCN build on well-known techniques, methodologies and appropriate standards. Most of the European Countries today have set up IT strategic plans that focus on the establishment of RHCN. The benefits of having access to all relevant information are tremendous and contribute to cost-effective and coherent health services. By the rapid spread and use of Internet, technology has made it possible to interconnect all kinds of applications. In 2000, the most experienced regions in Europe joined PICNIC, a European project to develop the Next Generation Regional Health Care Networks and to support their new ways of providing health and social care. The previous generation of Regional Health Care Networks supported the interconnection of applications by transfer of messages. Messaging is an effective means of integration for isolated high-specialised systems that only need to exchange data. This service will continue to be one of the most important services in the future health care networks. However, tighter coupling may be desirable in some instances to avoid replicating the same functionality in several applications. In other words, certain services can be common and used by a number of applications instead of building that service inside each application. These common services are called middleware services. In PICNIC (http://www.medcom.dk/picnic), a new middleware Collaboration IT service has been identified and developed. This service allows the end users to perform real-time clinical collaboration, with exchange of text, structured data, voice and images across the limits of a single region. A clinical collaboration is associated with the shared clinical context to provide a record of relevant clinical information and facilitates synchronous as well as asynchronous collaboration. This new ITservice builds on the increasing popularity of instance messaging and presence systems that facilitate smooth transition between synchronous and asynchronous interaction. The new Collaboration IT service is expected to have a strong impact on the practice of health care in the next generation of Regional Health Care Networks. ",
    "keywords": [
      "regional health care networks",
      "picnic",
      "collaboration it service"
    ]
  },
  {
    "id": "1636",
    "title": "Toward the three-dimensional structure and lysophosphatidic acid binding characteristics of the LPA4/p2y9/GPR23 receptor: A homology modeling study",
    "abstract": "Lysophosphatidic acid (LPA) is a naturally occurring phospholipid that initiates a broad array of biological processes, including those involved in cell proliferation, survival and migration via activation of specific G protein-coupled receptors located on the cell surface. To date, at least five receptor subtypes (LPA15) have been identified. The LPA13 receptors are members of the endothelial cell differentiation gene (Edg) family. LPA4, a member of the purinergic receptor family, and the recently identified LPA5 are structurally distant from the canonical Edg LPA13 receptors. LPA4 and LPA5 are linked to Gq, G12/13 and Gs but not Gi, while LPA13 all couple to Gi in addition to Gq and G12/13. There is also evidence that LPA4 and LPA5 are functionally different from the Edg LPA receptors. Computational modeling has provided useful information on the structureactivity relationship (SAR) of the Edg LPA receptors. In this work, we focus on the initial analysis of the structural and ligand-binding properties of LPA4, a prototype non-Edg LPA receptor. Three homology models of the LPA4 receptor were developed based on the X-ray crystal structures of the ground state and photoactivated bovine rhodopsin and the recently determined human ?2-adrenergic receptor. Docking studies of LPA in the homology models were then conducted, and plausible LPA binding loci were explored. Based on these analyses, LPA is predicted to bind to LPA4 in an orientation similar to that reported for LPA13, but through a different network of hydrogen bonds. In LPA13, the ligand polar head group is reported to interact with residues at positions 3.28, 3.29 and 7.36, whereas three non-conserved amino acid residues, S114(3.28), T187(EL2) and Y265(6.51), are predicted to interact with the polar head group in the LPA4 receptor models.",
    "keywords": [
      "lysophosphatidic acid ",
      "g protein-coupled receptors ",
      "lpa4/p2y9/gpr23",
      "homology modeling",
      "automated docking"
    ]
  },
  {
    "id": "1637",
    "title": "ant colony system based on receding horizon control for aircraft arrival sequencing and scheduling",
    "abstract": "The aircraft arrival sequencing and scheduling (ASS) problem is one of the most significant problems in the air traffic control (ATC). This paper makes the first attempt to design an ant colony system (ACS) based approach to solve this NP-hard problem. In order to reduce the computational effort of the optimization process, the receding horizon control (RHC) strategy is integrated into the ACS to divide the optimization process into several sub-processes and solve them one by one. This strategy can reduce the problem scale in each sub-optimization process, resulting in lighter computational effort and higher quality solution for the whole problem. Experiments are conducted to demonstrate the effectiveness and efficiency of the proposed RHC based ACS algorithm for the ASS problem (RHC-ACS-ASS). Simulation results show that the RHC-ACS-ASS not only outperforms the GA based approaches, but also the ACS based approach without using the RHC strategy.",
    "keywords": [
      "receding horizon control",
      "ant colony system",
      "arrival sequencing and scheduling",
      "air traffic control"
    ]
  },
  {
    "id": "1638",
    "title": "Fade countermeasure using signal degradation estimation for demand-assignment satellite systems",
    "abstract": "This paper describes a complete fade countermeasure system, designed for thin route user-oriented and fully meshed satellite networks. The signal degradation due to the residual up-link attenuation after up-power control intervention, plus the down-link attenuation, is compensated for by varying the FEC coding and bit rates of the data. Down and up-link signal degradations are evaluated separately: the former by collecting statistics of quantized levels of the demodulated PSK signal, and the latter by using a narrow band signal level estimator. Measurement times are optimized using a model to evaluate the scintillation variance. The performance evaluation of the whole system in the presence of additive white Gaussian noise (AWGN), shows that very small link power margins can be adopted.",
    "keywords": [
      "fade countermeasure",
      "scintillation",
      "fec",
      "signal quality estimation",
      "power margin"
    ]
  },
  {
    "id": "1639",
    "title": "Solvability and approximate solvability of fuzzy relation equations",
    "abstract": "We give here a discussion of approximate solvability of a system of fuzzy relation equations. We demonstrate how problems of interpolation and approximation of fuzzy functions are connected with solvability of systems of fuzzy relation equations. First we explain the general framework, and later on we prove some particular results related to the problem of the best approximation.",
    "keywords": [
      "system of fuzzy relation equations",
      "solvability and approximate solvability of a fuzzy relation equation system",
      "fuzzy function",
      "interpolation and approximation of fuzzy functions"
    ]
  },
  {
    "id": "1640",
    "title": "Existence, uniqueness, calculus and properties of triangular approximations of fuzzy numbers under a general condition",
    "abstract": "We consider a general family of parameters associated to a fuzzy number. We study the existence and uniqueness of triangular fuzzy numbers which preserve a fixed parameter in the family. We compute the nearest triangular fuzzy number of a fuzzy number which preserves the fixed parameter. Properties of the obtained approximation operator are studied.",
    "keywords": [
      "fuzzy number",
      "triangular fuzzy number",
      "triangular approximation",
      "extended approximation"
    ]
  },
  {
    "id": "1641",
    "title": "On Certain Values of Kloosterman Sums",
    "abstract": "Let K(q)n (a) be a Kloosterman sum over the finite field F(q)n of characteristic p. In this note so called subfield conjecture is proved: if a not equal 0 belongs to the proper subfield F(q) of F(q)n, then K(q)n (a) not equal -1. This completes recent works on the subfield conjecture by Shparlinski, and Moisio and Lisonek. The problem is motivated by some applications to bent functions. Moreover, in the course of the proof a large class of translates of Dickson polynomials are shown to be irreducible.",
    "keywords": [
      "dickson polynomial",
      "kloosterman sum",
      "lucas number",
      "primitive divisor"
    ]
  },
  {
    "id": "1642",
    "title": "MethylPCA: a toolkit to control for confounders in methylome-wide association studies",
    "abstract": "In methylome-wide association studies (MWAS) there are many possible differences between cases and controls (e.g. related to life style, diet, and medication use) that may affect the methylome and produce false positive findings. An effective approach to control for these confounders is to first capture the major sources of variation in the methylation data and then regress out these components in the association analyses. This approach is, however, computationally very challenging due to the extremely large number of methylation sites in the human genome.",
    "keywords": [
      "principal component analysis",
      "methylome-wide association studies",
      "eigen-decomposition",
      "association test",
      "mbd-seq"
    ]
  },
  {
    "id": "1643",
    "title": "Parallel computing as a vehicle for engineering design of complex functional surfaces",
    "abstract": "Thin liquid film flow over surfaces containing complex multiply connected topography is modelled using lubrication theory. The resulting time dependent nonlinear coupled set of governing equations for film thickness and pressure is solved on different parallel computing platforms using a purpose written portable and scalable parallel multigrid algorithm in order to achieve the fine-scale resolution required to guarantee mesh independent solutions. The robustness of the approach is demonstrated via the solution of three problems: one to establish the convergence characteristics viz. the partitioning and message passing strategies adopted, taking flow over a well-defined trench topography as a benchmark against existing experimental and corresponding numerical predictions; two, flow through a sparsely distributed set of occlusions with computations performed on different parallel architectures; three, free-surface planarisation with respect to flow over complex topography  the first an engineered functional substrate, the second a naturally occurring surface.",
    "keywords": [
      "multigrid",
      "adaptive time-stepping",
      "parallelisation",
      "thin film flow",
      "lubrication approximation",
      "topography"
    ]
  },
  {
    "id": "1644",
    "title": "Exploiting local linear geometric structure for identifying correct matches",
    "abstract": "We focus on identifying correct matches from a set of tentative correspondences. A subset of correct matches is first obtained for identifying all correct matches. We test the method on both matching image pairs and re-ranking images. The performance of the method is satisfactory compared with the state-of-the-art.",
    "keywords": [
      "local geometric structure",
      "image matching",
      "image re-ranking"
    ]
  },
  {
    "id": "1645",
    "title": "A game-theoretic approach to fault diagnosis and identification of hybrid systems",
    "abstract": "Physical systems can fail. For this reason the problem of identifying and reacting to faults has received a lot of attention in the control and computer science communities. In this paper we study the fault diagnosis problem for hybrid systems from a game-theoretical point of view. A hybrid system is a system mixing continuous and discrete behaviours that cannot be faithfully modelled neither by using a formalism with continuous dynamics only nor by a formalism including only discrete dynamics. We model hybrid systems as Hybrid Automata and add distinguished actions to describe faults. We define a Fault Identification Game on them, using two players: the environment and the identifier. The environment controls the evolution of the system and chooses whether and when a fault occurs. The identifier observes the external behaviour of the system and announces whether a fault has occurred or not. Existence of a winning strategy for the identifier implies that faults can be detected correctly, while computing such a winning strategy corresponds to implementing an identifier for the system. We will show how to determine the existence of a winning strategy, and how to compute it, for all decidable classes of hybrid automata that admit a finite bisimulation quotient.  ",
    "keywords": [
      "hybrid systems",
      "fault identification",
      "game theory",
      "state estimation",
      "bisimulation"
    ]
  },
  {
    "id": "1646",
    "title": "The Influence of Computer Graphics on the Recall of Information",
    "abstract": "This article describes an experiment that examined the influence of computer plots of three dimensional graphics on the recall of information. Three dimensional graphics and a tabular mode of presentation were used to convey information to two experimental groups of subjects. The graphics were produced by using perspective projections. The outcome was that the three dimensional graphics did not result in greater recall of information than did a tabular presentation for the task that was assigned.",
    "keywords": [
      "computer graphics",
      "mis management",
      "information recall",
      "three dimensional graphics",
      "information systems"
    ]
  },
  {
    "id": "1647",
    "title": "A BIST scheme for RTL circuits based on symbolic testability analysis",
    "abstract": "This paper introduces a novel scheme for testing register-transfer level (RTL) controller/data paths using built-in self-test (BIST), The scheme uses the controller netlist and the data path of a circuit to extract a test control/data flow (TCDF) graph. This TCDF is used to derive a set of symbolic justification and propagation paths (known as test environment) to test some of the operations and variables present in it. If it becomes difficult to generate such test environments with the derived TCDF's, a few test multiplexers are added at suitable points in the circuit to increase its controllability and observability, The test environment of an operation (variable) guarantees the existence of a path from the primary inputs of the circuit to the inputs of the module (register) to which the operation (variable) is mapped, and a path from the output of the module (register) to a primary output of the circuit. Since the search for a test environment is done symbolically, it is very fast and needs to be done only once for each module or register in the circuit, This test environment can then be used to exercise a module or register in the circuit with pseudorandom pattern generators which are placed only at the primary inputs of the circuit. The test responses can be analyzed with signature analyzers which are only placed at the primary outputs of the circuit, Unlike many RTL BIST schemes, an increase in the data path bit-width does not adversely impact the complexity of our testability analysis scheme since the analysis is symbolic. Every module in the module library is made random-pattern testable, whenever possible, using gate-level testability insertion techniques, This is a one-time cost. Finally, a BIST controller is synthesized to provide the necessary control signals to form the different test environments during testing, and a BIST architecture is superimposed on the circuit, Experimental results on a number of industrial and university benchmarks show that high fault coverage (> 99 %) can be obtained with our scheme. The average area overhead of the scheme is 6.9 % which is much lower than many existing logic-level BIST schemes. The average delay overhead is only 2.5 %, The test application time to achieve the high fault coverage for the whole circuit is also quite low.",
    "keywords": [
      "built-in self-test",
      "controller/data path",
      "rtl testability",
      "symbolic testability analysis",
      "test control/data flow"
    ]
  },
  {
    "id": "1648",
    "title": "Dynamic programming algorithms for RNA secondary structure prediction with pseudoknots",
    "abstract": "This paper shows simple dynamic programming algorithms for RNA secondary structure prediction with pseudoknots. For a basic version of the problem (i.e., maximizing the number of base pairs), this paper presents an O(n4) time exact algorithm and an O(n4) time approximation algorithm. The latter one outputs, for most RNA sequences, a secondary structure in which the number of base pairs is at least 1of the optimal, where ?,? are any constants satisfying 0<?,?<1. Several related results are shown too.",
    "keywords": [
      "rna secondary structure",
      "pseudoknot",
      "approximation algorithms",
      "computational biology",
      "dynamic programming"
    ]
  },
  {
    "id": "1649",
    "title": "Learning object models from semistructured Web documents",
    "abstract": "This paper presents an automated approach to learning object models by means of useful object data extracted from data-intensive semistructured web documents such as product descriptions. Modeling intensive data on the Web involves the following three phrases: First, we identify the object region covering the descriptions of object data when irrelevant contents from the web documents are excluded. Second, we partition the contents of different object data appearing in the object region and construct object data using hierarchical XML outputs. Third, we induce the abstract object model from the analogous object data. This model will match the corresponding object data from a Web site more precisely and comprehensively than the existing handcrafted ontologies. The main contribution of this study is in developing a fully automated approach to extract object data and object model from semistructured web documents using kernel-based matching and View Syntax interpretation. Our system, OnModer, can automatically construct object data and induce object models from complicated web documents, such as the technical descriptions of personal computers and digital cameras downloaded from manufacturers' and vendors' sites. A comparison with the available hand-crafted ontologies and tests on an open corpus demonstrate that our framework is effective in extracting meaningful and comprehensive models.",
    "keywords": [
      "web mining",
      "machine learning",
      "intelligent web services and semantic web",
      "web text analysis",
      "knowledge acquisition",
      "ontology design",
      "computational geometry and object modeling",
      "dom"
    ]
  },
  {
    "id": "1650",
    "title": "Robotic impact-acoustics system for tile-wall bonding integrity inspection",
    "abstract": "Impact acoustic is an effective non-destructive evaluation (NDE) method for many applications especially for inspecting the bonding quality of mosaic tile-walls. However, the audio noise can affect the power spectrum density (PSD) distribution of an acquired signal seriously. So, the traditional method of using PSD as the main identification tool is not sufficient. This paper proposes an evaluation method based on wavelet packet decomposition (WPD). Using WPD, the PSD of the signal is allocated into certain component fields. Investigation on the component PSD indicates it can reveal the bonding quality even in a noisy environment. An artificial neural network (ANN) is chosen as a classifier to simplify the evaluation system and makes it more effective and efficient. The performance of the proposed approach is evaluated experimentally. It is verified that this WPD approach can be applied to impact acoustic method to enhance its evaluation capability in a noisy environment. For practical implementation an automatic, gondola based climbing robot, called WICBOT, is being developed. To reduce the difficulty in handling large amount of cables, ZigBee and Bluetooth wireless network are used to transmit the inspection results and sample data from the climbing robot to the ground station.  ",
    "keywords": [
      "impact acoustic",
      "non-destructive evaluation",
      "zigbee",
      "wavelets",
      "climbing service robot"
    ]
  },
  {
    "id": "1651",
    "title": "Chebyshev's approximation algorithms and applications",
    "abstract": "We introduce a new family of multipoint methods to approximate a solution of a nonlinear operator equation in Banach spaces. An existence-uniqueness theorem and error estimates are provided for these iterations using a technique based on a new system of recurrence relations. To finish, we apply the results obtained to some nonlinear integral equations of the Fredholm type.  ",
    "keywords": [
      "multipoint iteration",
      "recurrence relations",
      "a priori error bounds"
    ]
  },
  {
    "id": "1652",
    "title": "An asynchronous architecture for modeling intersegmental neural communication",
    "abstract": "This paper presents an asynchronous VLSI architecture for modeling the oscillatory patterns seen in segmented biological systems. The architecture emulates the intersegmental synaptic connectivity observed in these biological systems. The communications network uses address-event representation (AER), a common neuromorphic protocol for data transmission. The asynchronous circuits are synthesized using communicating hardware processes (CHP) procedures. The architecture is scalable, supports multichip communication, and operates independent of the type of silicon neuron (spiking or burst envelopes). A 16-segment prototype system was developed, tested, and implemented; data from this system are presented.",
    "keywords": [
      "address event representation ",
      "asynchronous circuits",
      "central pattern generator ",
      "neurobiological modeling",
      "neuromorphic engineering",
      "silicon neuron",
      "vlsi architecture"
    ]
  },
  {
    "id": "1653",
    "title": "Systematic prediction of linear dependencies in the concentration profiles and implications on the kinetic hard-modelling of spectroscopic data",
    "abstract": "A novel method is presented for the systematic identification of the minimum requirements regarding mathematical pre-treatment, a priori information. or experimental design, in order to allow optimising rate constants and pure component spectra associated with a kinetic model via multivariate kinetic hard-modelling of spectroscopic data. Rank deficiencies in the kinetic concentration matrix represent a major problem for the calibration free method developed by Maeder and Zuberbuhler, as its pseudo-inverse, required for the optimisation process, is not defined. In this contribution, the underlying linear dependencies in the concentration profiles are systematically elucidated and appropriate strategies are discussed in order to break them. Also, conditions are predicted for which full spectral resolution can be expected. The method is based on the kernel of a time invariant augmented matrix covering potential rank deficiency due to stoichiometry and rate laws, also relevant for the concentration matrix. Compared to employing the full concentration matrix. this augmented matrix does not require a numerical integration of the differential equations describing the kinetic model and thus can easily be set up. The kernel can be calculated numerically by Singular Value Decomposition (SVD) or determined in a symbolical way, the latter allowing the detection of particular stoichiometric conditions leading to spectral resolution of species. The capabilities of the method are demonstrated analysing three kinetic mechanisms of increasing complexity covering consecutive and parallel reactions.  ",
    "keywords": [
      "kinetics",
      "spectroscopy",
      "hard-modelling",
      "concentration matrix",
      "rank deficiency and augmentation",
      "a priori information and experimental design"
    ]
  },
  {
    "id": "1654",
    "title": "Coral Point Count with Excel extensions (CPCe): A Visual Basic program for the determination of coral and substrate coverage using random point count methodology",
    "abstract": "Photographic and video methods are frequently used to increase the efficiency of coral reef monitoring efforts. The random point count method is commonly used on still images or frame-grabbed video to estimate the community statistics of benthos. A matrix of randomly distributed points is overlaid on an image, and the species or substrate-type lying beneath each point is visually identified. Coral Point Count with Excel extensions (CPCe) is a standalone Visual Basic program which automates, facilitates, and speeds the random point count analysis process. CPCe includes automatic frame-image sequencing, single-click species/substrate labeling, auto-advancement of data point focus, zoom in/out, zoom hold, and specification of random point number, distribution type, and frame border location. Customization options include user-specified coral/substrate codes and data point shape, size, and color. CPCe can also perform image calibration and planar area and length calculation of benthic features. The ability to automatically generate analysis spreadsheets in Microsoft Excel based upon the supplied species/substrate codes is a significant feature. Data from individual frames can be combined to produce both inter- and intra-site comparisons. Spreadsheet contents include header information, statistical parameters of each species/substrate type (relative abundance, mean, standard deviation, standard error) and the calculation of the Shannon-Weaver diversity index for each species. Additional information can be found at http:// www.nova.edu/ocean/cpce/.  ",
    "keywords": [
      "coral point count",
      "random point count",
      "coral reef assessment",
      "coral reef monitoring",
      "coral area measurement"
    ]
  },
  {
    "id": "1655",
    "title": "Universal zero-delay joint source-channel coding",
    "abstract": "We consider zero-delay joint source-channel coding of individual source sequences for a general known channel. Given an arbitrary finite set of schemes with finite-memory (not necessarily time-invariant) decoders, a scheme is devised that does essentially as well as the best in the set on all individual source sequences. Using this scheme, we construct a universal zero-delay joint source-channel coding scheme that is guaranteed to achieve, asymptotically, the performance of the best zero-delay encoding-decoding scheme with a finite-state encoder and a Markov decoder, on all individual sequences. For the case where the channel is a discrete memoryless channel (DMC), we construct an implementable zero-delay joint source-channel coding scheme that is based on the \"follow the perturbed leader\" scheme of Gyorgy et al. for lossy source coding of individual sequences. Our scheme is guaranteed to attain asymptotically the performance of the best in the set of all encoding-decoding schemes with a \"symbol-by-symbol\" decoder (and arbitrary encoder), on all individual sequences'.",
    "keywords": [
      "discrete memoryless channel ",
      "finite-state encoder/decoder",
      "individual sequences",
      "joint source-channel coding",
      "markov encoder/decoder",
      "zero-delay schemes"
    ]
  },
  {
    "id": "1656",
    "title": "Controlled non-uniform random generation of decomposable structures",
    "abstract": "Consider a class of decomposable combinatorial structures, using different types of atoms Z = {Z(1), . . . , Z(vertical bar Z vertical bar)}. We address the random generation of such structures with respect to a size n and a targeted distribution in k of its distinguished atoms. We consider two variations on this problem. In the first alternative, the targeted distribution is given by k real numbers mu(1), . . . , mu(k) such that 0  = 0 is the number of undistinguished atoms. The structures must be generated uniformly among the set of structures of size n that contain exactly n(i) atoms Z(i) (1 <= i <= k). We give a O(r(2) Pi(k)(i=1) n(i)(2) + mnk log n) algorithm for generating m structures, which simplifies into a O(r Pi(k)(i=1) n(i) + mn) for regular specifications.  ",
    "keywords": [
      "combinatorics",
      "random generation",
      "decomposable structures",
      "combinatorial specification",
      "non-uniform distribution"
    ]
  },
  {
    "id": "1657",
    "title": "Extraction of fuzzy rules from fuzzy decision trees: An axiomatic fuzzy sets (AFS) approach",
    "abstract": "In this study, we introduce a new type of coherence membership function to describe fuzzy concepts, which builds upon the theoretical findings of the Axiomatic Fuzzy Set (AFS) theory. This type of membership function embraces both the factor of fuzziness (by capturing subjective imprecision) and randomness (by referring to the objective uncertainty) and treats both of them in a consistent manner. Furthermore we propose a method to construct a fuzzy rule-based classifier using coherence membership functions. Given the theoretical developments presented there, the resulting classification systems are referred to as AFS classifiers. The proposed algorithm consists of three major steps: (a) generating fuzzy decision trees by assuming some level of specificity (detailed view) quantified in terms of threshold; (b) pruning the obtained rule-base; and (c) determining the optimal threshold resulting in a final tree. Compared with other fuzzy classifiers, the AFS classifier exhibits several essential advantages being of practical relevance. In particular, the relevance of classification results is quantified by associated confidence levels. Furthermore the proposed algorithm can be applied to data sets with mixed data type attributes. We have experimented with various data commonly present in the literature and compared the results with that of SVM, KNN, C4.5, Fuzzy Decision Trees (FDTs), Fuzzy SLIQ Decision Tree (FS-DT), FARC-HD and FURIA. It has been shown that the accuracy is higher than that being obtained by other methods. The results of statistical tests supporting comparative analysis show that the proposed algorithm performs significantly better than FDTs, FS-DT, KNN and C4.5.",
    "keywords": [
      "fuzzy decision trees",
      "fuzzy rules",
      "afs fuzzy logic",
      "knowledge representation",
      "comparative analysis"
    ]
  },
  {
    "id": "1658",
    "title": "A sensitivity analysis of vibrations in cracked turbogenerator units versus crack position and depth",
    "abstract": "The dynamic behaviour of heavy, horizontal axis, turbogenerator units affected by transverse cracks can be analysed in the frequency domain by means of a quasi linear approach, using a simplified breathing crack model applied to a traditional finite element model of the shaft-line. This allows to perform a series of analyses with affordable computational efforts. Modal analysis combined to a simplified approach for simulating the dynamical behaviour allows to predict the severity of the crack-excited vibrations, resolving the old-age question on how deep a crack must be to be detected by means of vibration measurements of the machine during normal operating conditions. The model of a 320MW turbogenerator unit has been used to perform a numerical sensitivity analysis, in which the vibrations of the shaft-line, and more in detail the vibrations of the shaft in correspondence to the bearings, have been calculated for all possible positions of the crack along the shaft-line, and for several different values of the depth of the crack.",
    "keywords": [
      "cracked rotors",
      "rotor dynamics",
      "sensitivity to crack depth"
    ]
  },
  {
    "id": "1659",
    "title": "Positron emission tomography for the evaluation and treatment of cardiomyopathy",
    "abstract": "Congestive heart failure accounts for tremendous morbidity and mortality worldwide. There are numerous causes of cardiomyopathy, the most common of which is coronary artery disease. Positron emission tomography (PET) has an established and expanding role in the evaluation of patients with cardiomyopathy. The specific application of PET to hypertrophic cardiomyopathy, cardiac sarcoidosis, and diabetic cardiomyopathy has been studied extensively and promises to be a useful tool for managing these patients. Furthermore, evaluating the efficacy of standard treatments for congestive heart failure is important as health care costs continue to rise. Recently, there have been significant developments in the field of cardiovascular stem cell research. Familiarity with the mechanisms by which stem cells benefit patients with cardiovascular disease is the key to understanding these advances. Molecular imaging techniques including PET/CT imaging play an important role in monitoring stem cell therapy in both animals and humans. These noninvasive imaging techniques will be highlighted in this paper.",
    "keywords": [
      "cardiomyopathy",
      "stem cells",
      "positron emission tomography",
      "congestive heart failure",
      "molecular imaging"
    ]
  },
  {
    "id": "1660",
    "title": "Sequentially two-leveled egalitarianism for TU games: Characterization and application",
    "abstract": "A new linear value for cooperative transferable utility games is introduced. The recursive definition of the new value for an n-person game involves a sequential process performed at n?1 stages, applying the value to subgames with a certain size k,1?k<n, combining with the rule of two-leveled egalitarianism (additive normalization) in order to guarantee the efficiency property for the new value, sequentially two-leveled egalitarianism, shortly S2EG value, applied to subgames of size k+1. The new value will be characterized in various ways. The S2EG value differs from the Shapley value since, besides efficiency, linearity, and symmetry, it verifies an additional property with respect to so-called scale-dummy player (replacing dummy player property). Consequently, the S2EG value of a game may be determined as the solidarity value of the per-capita game (incorporating the proportional rule due to different levels of efficiency). Various potential representations of the new value are established. In the application to a land corn production economy, it yields allocations, in which the landlords interest coincides with striving for a maximum production level. For economies with the linear production function, not only the unique landlord but also all the workers have incentives to increase the scale of the economy.",
    "keywords": [
      "transferable utility game",
      "two-leveled egalitarianism",
      "scale-dummy",
      "sequential approach",
      "potential representation",
      "land corn production economy"
    ]
  },
  {
    "id": "1661",
    "title": "The minimum size instance of a Pallet Loading Problem equivalence class",
    "abstract": "The Pallet Loading Problem (PLP) maximizes the number of identical rectangular boxes placed within a rectangular pallet. Boxes may be rotated 90 so long as they are packed with edges parallel to the pallets edges, i.e., in an orthogonal packing. This paper defines the Minimum Size Instance (MSI) of an equivalence class of PLP, and shows that every class has one and only one MSI. We develop bounds on the dimensions of box and pallet for the MSI of any class. Applying our new bounds on MSI dimensions, we present an algorithm for MSI generation and use it to enumerate all 3,080,730 equivalence classes with an area ratio (pallet area divided by box area) smaller than 101 boxes. Previous work only provides bounds on the ratio of box dimensions and only considers a subset of all classes presented here.",
    "keywords": [
      "packing",
      "pallet loading problem"
    ]
  },
  {
    "id": "1662",
    "title": "semantic email",
    "abstract": "This paper investigates how the vision of the Semantic Web can be carried overto the realm of email. We introduce a general notion of semantice mail, in which an email message consists of an RDF query or update coupled with corresponding explanatory text. Semantic email opens the door to a wide range of automated, email-mediated applications with formally guaranteed properties. In particular, this paper introduces a broad class of semantic email processes . For example consider the process of sending an email to a program committee asking who will attend the PC dinner automatically collecting the responses and tallying them up. We define bothlogical and decision-theoretic models where an email process ismodeled as a set of updates to a data set on which we specify goals via certain constraints or utilities. We then describe a set ofinference problems that arise while trying to satisfy these goals and analyze their computational tractability. In particular weshow that for the logical model it is possible to automatically infer which email responses are acceptable w.r.t. a set ofconstraints in polynomial time and for the decision-theoreticmodel it is possible to compute the optimal message-handling policy in polynomial time. Finally we discuss our publicly available implementation of semantic email and outline research challenges inthis realm.",
    "keywords": [
      "formal model",
      "semantic web",
      "satisfiability",
      "decision-theoretic"
    ]
  },
  {
    "id": "1663",
    "title": "Investigating the concordance of Gene Ontology terms reveals the intra- and inter-platform reproducibility of enrichment analysis",
    "abstract": "Reliability and Reproducibility of differentially expressed genes (DEGs) are essential for the biological interpretation of microarray data. The microarray quality control (MAQC) project launched by US Food and Drug Administration (FDA) elucidated that the lists of DEGs generated by intra- and inter-platform comparisons can reach a high level of concordance, which mainly depended on the statistical criteria used for ranking and selecting DEGs. Generally, it will produce reproducible lists of DEGs when combining fold change ranking with a non-stringent p-value cutoff. For further interpretation of the gene expression data, statistical methods of gene enrichment analysis provide powerful tools for associating the DEGs with prior biological knowledge, e.g. Gene Ontology (GO) terms and pathways, and are widely used in genome-wide research. Although the DEG lists generated from the same compared conditions proved to be reliable, the reproducible enrichment results are still crucial to the discovery of the underlying molecular mechanism differentiating the two conditions. Therefore, it is important to know whether the enrichment results are still reproducible, when using the lists of DEGs generated by different statistic criteria from inter-laboratory and cross-platform comparisons. In our study, we used the MAQC data sets for systematically accessing the intra- and inter-platform concordance of GO terms enriched by Gene Set Enrichment Analysis (GSEA) and LRpath.",
    "keywords": [
      "dna microarray",
      "intra-/inter-platform comparison",
      "gene ontology enrichment",
      "microarray quality control "
    ]
  },
  {
    "id": "1664",
    "title": "Mining Discriminative Patterns for Classifying Trajectories on Road Networks",
    "abstract": "Classification has been used for modeling many kinds of data sets, including sets of items, text documents, graphs, and networks. However, there is a lack of study on a new kind of data, trajectories on road networks. Modeling such data is useful with the emerging GPS and RFID technologies and is important for effective transportation and traffic planning. In this work, we study methods for classifying trajectories on road networks. By analyzing the behavior of trajectories on road networks, we observe that, in addition to the locations where vehicles have visited, the order of these visited locations is crucial for improving classification accuracy. Based on our analysis, we contend that (frequent) sequential patterns are good feature candidates since they preserve this order information. Furthermore, when mining sequential patterns, we propose to confine the length of sequential patterns to ensure high efficiency. Compared with closed sequential patterns, these partial (i.e., length-confined) sequential patterns allow us to significantly improve efficiency almost without losing accuracy. In this paper, we present a framework for frequent pattern-based classification for trajectories on road networks. Our comparative study over a broad range of classification approaches demonstrates that our method significantly improves accuracy over other methods in some synthetic and real trajectory data.",
    "keywords": [
      "trajectory classification",
      "frequent pattern-based classification",
      "road network analysis",
      "sequential patterns"
    ]
  },
  {
    "id": "1665",
    "title": "Ferroresonance suppression in power transformers using chaos theory",
    "abstract": "The main goal of this paper is the determination of the effect of the metal oxide varistor (MOV) on various ferroresonance modes including fundamental resonance, subharmonic and chaos mode which are generated in electrical power systems. Chaos theory is used for analyzing this effect. Also, the bifurcation, phase plan diagram and time domain simulation are used for this purpose. The proposed power system contains a no-load or lightly loaded power transformer. The magnetization curve of the transformer core is modeled by a single-value two-term polynomial. The core loss is modeled based on the flux of the transformer. The MOV modeled as a nonlinear voltage dependent resistance. The suppression effect of MOV on chaotic ferroresonance in power transformer is studied in this paper. The simulation results confirm that connecting the MOV to the transformer has a considerable suppression effect on ferroresonance phenomena.",
    "keywords": [
      "metal oxide varistor",
      "control of chaos",
      "bifurcation",
      "ferroresonance",
      "power transformers"
    ]
  },
  {
    "id": "1666",
    "title": "The complexity of approximating bounded-degree Boolean #CSP",
    "abstract": "The degree of a CSP instance is the maximum number of times that any variable appears in the scopes of constraints. We consider the approximate counting problem for Boolean CSP with bounded-degree instances, for constraint languages containing the two unary constant relations {0} and {1}. When the maximum allowed degree is large enough (at least 6) we obtain a complete classification of the complexity of this problem. It is exactly solvable in polynomial time if every relation in the constraint language is affine. It is equivalent to the problem of approximately counting independent sets in bipartite graphs if every relation can be expressed as conjunctions of {0}, {1} and binary implication. Otherwise, there is no FPRAS unless NP = RP. For lower degree bounds, additional cases arise, where the complexity is related to the complexity of approximately counting independent sets in hypergraphs.  ",
    "keywords": [
      "counting constraint satisfaction problem",
      "csp",
      "approximation algorithm",
      "complexity"
    ]
  },
  {
    "id": "1667",
    "title": "Quasi-reversibility and truncation methods to solve a Cauchy problem for the modified Helmholtz equation",
    "abstract": "In this paper, the Cauchy problem for the modified Helmholtz equation in a rectangular domain is investigated. We use a quasi-reversibility method and a truncation method to solve it and present convergence estimates under two different a priori boundedness assumptions for the exact solution. The numerical results show that our proposed numerical methods work effectively.  ",
    "keywords": [
      "cauchy problem",
      "modified helmholtz equation",
      "quasi-reversibility method",
      "truncation method",
      "convergence estimates"
    ]
  },
  {
    "id": "1668",
    "title": "secure code distribution in dynamically programmable wireless sensor networks",
    "abstract": "Remote reprogramming of in situ wireless sensor networks (WSNs) via the wireless link is an important capability. Securing the process of reprogramming allows each sensor node to authenticate each received code image. Due to the resource constraints of WSNs, public key schemes must be used sparingly. This paper introduces a mechanism for secure and efficient code distribution that employs public key cryptography only to sign the root of a combined structure consisting of both hash chains and hash trees. The chain based scheme works best when packets are received in the order they are sent with very few losses. Our hash tree based scheme allows nodes to authenticate packets and verify their integrity quickly, even when the packets may arrive out of order, but can result in too many public key operations. Integrating hash chains and hash trees produces a mechanism that is both resilient to losses and lightweight in terms of reducing memory consumption and the number of public key operations that a node has to perform. Simulation shows that the proposed secure reprogramming schemes add only a modest amount of overhead to a conventional non-secure reprogramming scheme, namely Deluge, and are therefore feasible and practical in a wireless sensor network.",
    "keywords": [
      "sensor networks",
      "security",
      "secure reprogramming"
    ]
  },
  {
    "id": "1669",
    "title": "Detecting adaptive evolution and functional divergence in aminocyclopropane-1-carboxylate synthase (ACS) gene family",
    "abstract": "Ethylene is an essential plant gaseous hormone that controls many aspects of plant growth and development, especially the fruit ripening. It is important to know how this hormone is synthesized and how its production is regulated to understand the roles of ethylene in plant development. The aminocyclopropane-1-carboxylate synthase (ACS) gene is a rate-limiting enzyme in the ethylene biosynthesis pathway, which is encoded by a highly divergent multi-gene family in plant species. Although many ACS genes have been cloned from a wide variety of plant species previously, their origin and evolutionary process are still not clear. In this study, we conducted a phylogenetic analysis based on an updated dataset including 107 members of plant ACS genes and eight ACS-like genes from animal as well as six AATase genes. The motifs were identified and the positive selection and functional divergence in the ACS gene family were detected. The results obtained from these analyses are consistent with previous division of the ACS gene family in angiosperm, i.e., three distinct clades, and show that the duplications of three subclades (I, II and III) ACS genes have occurred after the divergence of gymnosperm and angiosperm. We conclude that the ACS genes could have experienced three times significant positive selection as they underwent expansion in land plants and gain the full-scale ethylene biosynthesis and regulatory functions, and all plant ACS genes originated from plant-ACS-like genes which come from AATase genes.",
    "keywords": [
      "aminocyclopropane-1-carboxylate synthase",
      "gene family",
      "adaptive evolution",
      "functional divergence"
    ]
  },
  {
    "id": "1670",
    "title": "Suitable switching policies for FMS scheduling",
    "abstract": "Switching server, otherwise known as hybrid dynamical approaches can be used highly effectively to solve flexible manufacturing systems scheduling problems. Papers published in this field have accepted that the demand rates determining the production tasks have some given values. In the present paper it is shown that the demand rates should be selected from some given domains. The \"control variables\" to realize suitable processes are the switching policies, and the demand rates. These uniquely determine the processes through the switching time sequences. In the present paper, two variants: the single machine processing, and the multiple machine processing cases are analyzed. It is very problematic to use continuously coupled parts flows for stable and effective production. To eliminate this difficulty, in the present paper, the method called controlled buffer technique is proposed, which uses the opportunity that the computer controlled \"virtual buffers\" can be filled up before the regular working time, and used for part flow compensation. ",
    "keywords": [
      "switching server",
      "hybrid dynamical approach",
      "flexible manufacturing system ",
      "scheduling",
      "demand rates",
      "single and multiple machine processing",
      "virtual buffer",
      "controlled buffer technique"
    ]
  },
  {
    "id": "1671",
    "title": "A lower bound for the job insertion problem",
    "abstract": "This note deals with the job insertion problem in job-shop scheduling: Given a feasible schedule of n jobs and a new job which is not scheduled, the problem is to find a feasible insertion of the new job into the schedule which minimises the makespan. Since the problem is NP-hard, a relaxation method is proposed to compute a strong lower bound. Conditions under which the relaxation provides us with the makespan of the optimal insertion are derived. After the analysis of the polytope of feasible insertions, a polynomial time procedure is proposed to solve the relaxed problem. Our results are based on the theory of perfect graphs and elements of polyhedral theory.",
    "keywords": [
      "job-shop scheduling",
      "perfect graphs",
      "polyhedral methods"
    ]
  },
  {
    "id": "1672",
    "title": "In silico study of potential autoimmune threats from rotavirus infection",
    "abstract": "Rotaviral outer capsid protein VP6 shares two eight amino acid regions with ryanodine receptor 2, a target in myasthenia gravis, a neurodegenerative disorder. These two regions remain conserved among most circulating rotavirus strains isolated from all over the world. These regions are potential B cell with considerable relative surface availability and antigenecity. These regions are potential T cell epitope with respect to HLA subtypes associated with myasthenia gravis.",
    "keywords": [
      "rota virus",
      "molecular mimicry",
      "myasthenia gravis",
      "autoimmunity"
    ]
  },
  {
    "id": "1673",
    "title": "LucasKanade based entropy congealing for joint face alignment",
    "abstract": "Entropy Congealing is an unsupervised joint image alignment method, in which the transformation parameters are obtained by minimizing a sum-of-entropy function. Our previous work presented a forward formulation of entropy Congealing to estimate all the transformation parameters at the same time. In this paper, we propose an inverse compositional LucasKanade formulation of entropy Congealing. This yields constant parts in Jacobian and Hessian which can be precomputed to decrease the computational complexity. Moreover, we combine Congealing with POEM descriptor to catch more information about face. Experimental results indicate that the proposed algorithm performs better than other alignment methods, regarding several evaluation criteria on different databases. Concerning the complexity, the proposed algorithm is more efficient than other considered approaches. Also, compared to the forward formulation, the inverse method produces a speed improvement of 20%.",
    "keywords": [
      "entropy congealing",
      "lucaskanade method",
      "face alignment",
      "poem descriptor"
    ]
  },
  {
    "id": "1674",
    "title": "A neural network-based sliding-mode control for rotating stall and surge in axial compressors",
    "abstract": "A decoupled sliding-mode neural network variable-bound control system (DSMNNVB) is proposed to control rotating stall and surge in jet engine compression systems in presence of disturbance and uncertainty. The control objective is to drive the system state to the original equilibrium point and it proves that the control system is asymptotically stable. In this controller, an adaptive neural network (NN) control scheme is employed for unknown dynamic of nonlinear plant without using a model of the plant. Moreover, no prior knowledge of the plant is assumed. The proposed DSMNNVB controller ensures Lyapunov stability of the nonlinear dynamic of the system.  ",
    "keywords": [
      "axial compressors",
      "sliding mode",
      "neural network",
      "variable bound"
    ]
  },
  {
    "id": "1675",
    "title": "PVM based 3-D Kirchhoff depth migration using dynamically computed travel-times: An application in seismic data processing",
    "abstract": "Seismic depth migration is an image reconstruction technique used to generate realistic images of the Earth's interior from surface recordings of sound waves reflected by buried geological structures. Migration algorithms that have been developed for digitally recorded seismic data are computationally intensive and are typically implemented on massively parallel architecture. In this paper we describe a highly accurate method of computing seismic travel-times and study the feasibility of a PVM-based SPMD parallelization of a migration algorithm based on this method. The parallel algorithm developed in this study is coarse grained and utilizes a simple geometrical decomposition of the input problem. We show that near linear speedup can be achieved on a homogeneous cluster and the algorithm can be ported to a variety of platforms, i.e. clusters of Sun workstations, Cray Y-MP, nCUBE-2, Cray T3E and SGI Origin 2000. The performance of the scheme on different computational platforms ultimately depends on a number of different factors, e.g., machine architecture, machine speed, network configuration etc. For a simple test case that was studied in this research, the performances of modest PVM clusters were found to be encouraging. This underscores the advantages of adopting a strategy based on parallelizing computationally intensive tasks and also demonstrates the suitability of the travel-time scheme for developing coarsely parallel applications.",
    "keywords": [
      "geophysics",
      "seismic",
      "migration",
      "pvm"
    ]
  },
  {
    "id": "1676",
    "title": "Jump on the innovators train: cognitive principles for creating appreciation in innovative product designs",
    "abstract": "In everyday life, we find shared preferences for idiosyncratic product features paradigmatically displayed by bestselling gadgets like Apples iPhones touch screen, which after gaining acceptance and appreciation are susceptible to being copied by competitors. Psychological research on the phenomenon of shared preferences for innovative design features and the probable benefit of copying them is still lacking. We tested gains of acceptance for imitators through an adaptation paradigm where typicality and liking of potentially innovative features were analysed dynamically. We found significant changes in typicality and liking for imitators being highly similar to the original. These adaptation processes in combination with transfer effects create the specific opportunity for imitators to jump on the innovators train by providing similar innovative features and thereby participating in the initial innovators success. Importantly, they participate best not by solely copying a specific novel feature, but by additionally generally looking very similar to the innovator.",
    "keywords": [
      "aesthetic appreciation",
      "prototypicality",
      "dynamics",
      "adaptation",
      "after effects",
      "transfer",
      "liking",
      "imitation",
      "copycats",
      "innovation"
    ]
  },
  {
    "id": "1677",
    "title": "Top-k Diversity Queries over Bounded Regions",
    "abstract": "Top-k diversity queries over objects embedded in a low-dimensional vector space aim to retrieve the best k objects that are both relevant to given user's criteria and well distributed over a designated region. An interesting case is provided by spatial Web objects, which are produced in great quantity by location-based services that let users attach content to places and are found also in domains like trip planning, news analysis, and real estate. In this article we present a technique for addressing such queries that, unlike existing methods for diversified top-k queries, does not require accessing and scanning all relevant objects in order to find the best k results. Our Space Partitioning and Probing (SPP) algorithm works by progressively exploring the vector space, while keeping track of the already seen objects and of their relevance and position. The goal is to provide a good quality result set in terms of both relevance and diversity. We assess quality by using as a baseline the result set computed by MMR, one of the most popular diversification algorithms, while minimizing the number of accessed objects. In order to do so, SPP exploits score-based and distance-based access methods, which are available, for instance, in most geo-referenced Web data sources. Experiments with both synthetic and real data show that SPP produces results that are relevant and spatially well distributed, while significantly reducing the number of accessed objects and incurring a very low computational overhead.",
    "keywords": [
      "algorithms",
      "design",
      "experimentation",
      "performance",
      "diversification",
      "scoring",
      "top-k",
      "ranking",
      "aggregation"
    ]
  },
  {
    "id": "1678",
    "title": "Biomedical Informatics Publications: a Global Perspective Part II: Journals",
    "abstract": "Background: Biomedical Informatics (BMI) is a broad discipline, having evolved from both Medical Informatics (MI) and Bioinformatics (BI). An analysis of publications in the field-should provide an indication about the geographic distribution of BMI research contributions and possible lessons for the future, both for research and professional practice. Objectives: In part I of our analysis of biomedical informatics publications we presented results from BMI conferences. In this second part, we analyse BMI journals, which provide a broader perspective and comparison between data from conferences and journals that ought to confirm or suggest alternatives to the original distributional findings from the conferences. Methods:We manually collected data about authors and their geographical origin from various MI journals: the International Journal of Medical Informatics (IJMI), the Journal of Biomedical Informatics (JBI), Methods of Information in Medicine (MIM) and The Journal of the American Medical Informatics Association (JAMIA). Focusing on first authors, we also compared these findings with data from the journal Bioinformatics. Results: Our results confirm those obtained in our analysis of BMI conferences that local and regional authors favor their corresponding MI journals just as they do their conferences. Consideration of other factors, such as the increasingly open source nature of data and software tools, is consistent with these findings Conclusions: Our analysis suggests various indicators that could lead to further, deeper analyses, and could provide additional insights for future BMI research and professional activities.",
    "keywords": [
      "biomedical informatics",
      "medical informatics",
      "bioinformatics",
      "conference rankings",
      "journal impact factor"
    ]
  },
  {
    "id": "1679",
    "title": "Designing an information system for updating land records in Bangladesh: Action design ethnographic research (ADER)",
    "abstract": "Information Systems (IS) has developed through adapting, generating and applying diverse methodologies, methods, and techniques from reference disciplines. Further, Action Design Research (ADR) has recently developed as a broad research method that focuses on designing and redesigning IT and IS in organizational contexts. This paper reflects on applying ADR in a complex organizational context in a developing country. It shows that ADR requires additional lens for designing IS in such a complex organizational context. Through conducting ADR, it is seen that an ethnographic framework has potential complementarities for understanding complex contexts thereby enhancing the ADR processes. This paper argues that conducting ADR with an ethnographic approach enhances design of IS and organizational contexts. Finally, this paper aims presents a broader methodological framework, Action Design Ethnographic Research (ADER), for designing artefacts as well as IS. This is illustrated through the case of a land records updating service in Bangladesh.",
    "keywords": [
      "action design research",
      "action design ethnographic research",
      "service delivery",
      "land records",
      "bangladesh"
    ]
  },
  {
    "id": "1680",
    "title": "On multi-level kappa-ranges for range search",
    "abstract": "We investigate an implementation of the multi-level or l-level k-range data structure. The l-level k-range is compared to naive and R*tree search over N randomly generated k-dimensional points. Results indicate that multi-level k-ranges are not competitive due to their (previously unreported) complexity. We show that storage is S(N, k, l) = O(N(1+2(k-1)/l)) and S(N, k) = Theta(N(1+2(k-1)/log2 N)). Our results also indicate that the l-level k-range requires Q(N, k, l) = O((2l)(k)(log N + A)) time for range search, for A = number of points reported in range.",
    "keywords": [
      "multidimensional orthogonal range search",
      "space and time complexity"
    ]
  },
  {
    "id": "1681",
    "title": "Gamification for smarter learning: tales from the trenches",
    "abstract": "Gamification has been explored recently as a way to promote content delivery in education, yielding promising results. However, little is known regarding how it helps different students experience learning and acquire knowledge. In this paper we study and analyze data from a gamified engineering course, to search for distinct behavior patterns. We examined data collected from two gamified years, between which game changes took place. By clustering students according to their performance, we identified three distinct student types, common to both years: Achievers, Disheartened, and Underachievers. Interestingly, in the second year a new type of student emerged: the Late Awakeners. In this paper we carefully describe each student type, and explain how gamification can provide for smarter learning by catering to students with different profiles. Furthermore, we discuss how our findings, both in gamification and cluster analysis can be used to develop adaptive and smart learning environments.",
    "keywords": [
      "gamification",
      "education",
      "cluster analysis",
      "student participation",
      "adaptive learning"
    ]
  },
  {
    "id": "1682",
    "title": "Communication and control system for a 15-channel hermetic retinal prosthesis",
    "abstract": "A small, hermetic, wirelessly-controlled retinal prosthesis has been developed for pre-clinical studies in Yucatan minipigs. The device was attached conformally to the outside of the eye in the socket and received both power and data wirelessly from external sources. Based on the received image data, the prosthesis drove a subretinal thin-film polyimide array of sputtered iridium oxide stimulating electrodes. The implanted device included a hermetic titanium case containing a 15-channel stimulator and receiver chip and discrete circuit components. Feedthroughs in the hermetic case connected the chip to secondary power- and data-receiving coils, which coupled to corresponding external power and data coils driven by power amplifiers. Power was delivered by a 125kHz carrier, and data were delivered by amplitude shift keying of a 15.5MHz carrier at 100kbps. Stimulation pulse strength, duration and frequency were programmed wirelessly from an external computer system. The final assembly was tested in vitro in physiological saline and in vivo in two minipigs for up to five and a half months by measuring stimulus artifacts generated by the implant's current drivers.",
    "keywords": [
      "rp retinitis pigmentosa",
      "amd age-related macular degeneration",
      "pxi pci extensions for instrumentation",
      "ask amplitude shift keying",
      "sirof sputtered iridium oxide film",
      "pwm pulse width modulation",
      "erg electroretinogram",
      "pdms poly"
    ]
  },
  {
    "id": "1683",
    "title": "Structural tendencies in complex systems development and their implication for software systems",
    "abstract": "Contemporary distributed software systems, exposed to highly unpredictable environments, are reaching extremely high complexity levels. For example, open heterogeneous multi-agent systems that may potentially be spread all around the globe are interacting with different types of dynamically changing web-services and web-technologies. Traditional control-based handling of adaptability may not be suitable anymore in such systems. Therefore there is a tendency for exploring different adaptability models inspired by biological phenomena. Biological systems inherently are faced with complexity and unpredictable environments, and they exhibit high levels of adaptability. In this article, we present a theoretical model of development of complex system, which was built originally by Andrzej Gecow, as a computational model in evolutionary biology. This model represents a generic complex system subjected to long sequences of adaptive changes. The model was used for analysis of development processes and also structural tendencies. By tendencies we mean some phenomena that should be expected in any complex system, subjected to a long development process. Some of these tendencies are not desirable, for example bloat of the system. Some of the phenomena, however, show characteristics of changes that improve the system. These characteristics can be applied to optimisation of self-producing and self-adapting algorithms of self-maintaining complex software systems. The main structural tendencies described in this article are: terminal modifications, terminal majority of additions, and covering ( reconstructing within the system itself disappearing environmental signals).",
    "keywords": [
      "complex systems",
      "adaptable architectures",
      "software life cycle"
    ]
  },
  {
    "id": "1684",
    "title": "Mobile robot localization based on Ultra-Wide-Band ranging: A particle filter approach",
    "abstract": "This article addresses the problem of mobile robot localization using Ultra-Wide-Band (UWB) range measurements. UWB is a radio technology widely used for communications, that is recently receiving increasing attention for positioning applications. In these cases, the position of a mobile transceiver is determined from the distances to a set of fixed, well-localized beacons. Though this is a well-known problem in the scientific literature (the trilateration problem), the peculiarities of UWB range measurements (basically, distance errors and multipath effects) demand a different treatment to other similar solutions, as for example, those based on laser. This work presents a thorough experimental characterization of UWB ranges within a variety of environments and situations. From these experiments, we derive a probabilistic model which is then used by a particle filter to combine different readings from UWB beacons as well as the vehicle odometry. To account for the possible offset error due to multipath effects, the state tracked by the particle filter includes the offset of each beacon in addition to the planar robot pose (x,y,?) ( x , y , ? ) , both estimated sequentially. We show navigation results for a robot moving in indoor scenarios covered by three UWB beacons that validate our proposal.",
    "keywords": [
      "mobile robot localization",
      "sensor characterization",
      "particle filter",
      "ultra-wide-band"
    ]
  },
  {
    "id": "1685",
    "title": "Adaptive stabilization for generator excitation system",
    "abstract": "Purpose - The purpose of this paper is to develop a controller for damping of oscillations of a synchronous generator connected to the electric network. The goal is to determine the configuration of the controller and to set up the procedure for determination of the controller parameters. Design/methodology/approach - On the basis of the analytical and numerical analysis of the so-far proposed stabilizers, the new directions towards improved and efficient stabilizer have been established. The advantage of the proposed approach has been confirmed with simulations and experimental results. Findings - Three main contributions can be highlighted: on the basis of the synchronous generator analysis, it is shown that the conventional power system stabilizer is inappropriate for optimal oscillation damping through the entire operating range; the possibility of application of the model reference adaptive control theory for stabilizer design is confirmed; and the rules have been set up for selection of the stabilizer parameters. Research limitations/implications - The power system control is rather conservative and does not allow new approaches to the control concepts. Originality/value - The paper's originality lies in the fact that the proposed adaptive approach for realizing the control system for damping of oscillations is presented completely. The configuration of the controller is presented, as well as the method for determining the adaptation mechanism parameters.",
    "keywords": [
      "generators",
      "stability ",
      "control technology"
    ]
  },
  {
    "id": "1686",
    "title": "A concurrency control scheme for mobile transactions in broadcast disk environments",
    "abstract": "Broadcast disk technique has been often used to disseminate frequently requested data efficiently to a large volume of mobile clients over wireless channels. In broadcast disk environments, a server often broadcasts different data items with differing frequencies to reflect the skewed data access patterns of mobile clients. Previously proposed concurrency control methods for mobile transactions in wireless broadcast environments are focused on the mobile transactions with uniform data access patterns. These protocols perform poorly in broadcast disk environments where the data access patterns of mobile transactions are skewed. In broadcast disk environments, the time length of a broadcast cycle usually becomes large to reflect the skewed data access patterns. This will often cause read-only transactions to access old data items rather than the latest data items. Furthermore, updating mobile transactions will be frequently aborted and restarted in the final validation stage due to the update conflict of the same data items with high access frequencies. This problem will increase the average response time of the update mobile transactions and waste the uplink communication bandwidth. In this paper, we extend the existing FBOCC concurrency control method to efficiently handle mobile transactions with skewed data access patterns in broadcast disk environments. Our method allows read-only transactions to access the more updated data, and reduces the average response time of updating transactions through early aborts and restarts. Our method also reduces the amount of uplink communication bandwidth for the final validation of the update transactions. We present an in-depth experimental analysis of our method by comparing with existing concurrency control protocols. Our performance analysis shows that it significantly decreases the average response time and the amount of uplink bandwidths over existing methods.",
    "keywords": [
      "mobile databases",
      "mobile transactions",
      "concurrency control",
      "wireless broadcast channels",
      "data dissemination",
      "broadcast data",
      "broadcast disks",
      "mobile computing"
    ]
  },
  {
    "id": "1687",
    "title": "3D-VIEWER: An atlas-based system for individual and statistical investigations of the human brain",
    "abstract": "3D-VIEWER is a new software tool for neurosurgical planning and population studies. It is based on digitized three-dimensional brain atlases derived from standard stereotactic atlases that can be adapted to an individual's brain and shown as a series of displayed images. If the patient's brain has been imaged in different modalities, the standardized anatomical information can be adapted to the individual images, which will bring the images into registration. The 3D-VIEWER can be used as a tool for combining multimodal information from the same patient. In addition, several tools are available that allow oblique views of anatomical structures or the view along the intended trajectory during a neurosurgical intervention. Furthermore, using the atlas transformation matrices, anatomical information can be determined when comparing an individual's brain to the anatomy of the atlas brain. Thus, standardized anatomical information from the atlas Gan be introduced into individual images. This standardization is used to perform individual-group and group-by-group comparisons between patients and normal controls in anatomical studies.",
    "keywords": [
      "brain atlas",
      "multimodal registration",
      "neurosurgical planning",
      "intersubject standardization"
    ]
  },
  {
    "id": "1688",
    "title": "Beyond mirroring: multi-version disk arraywith improved performance and energy efficiency",
    "abstract": "Performance and power consumption are two important design objectives for data centers consisting of thousands or tens of thousands of disks (or disk arrays). To leverage the two objectives, in this study we propose a multi-version disk array (MDA). The main idea of MDA is to exploit the I/O workload characteristics to guide the replication strategy by replicating multiple versions of the popular data blocks and simply offloading the write data to the free space of the reserved version region, thus achieving high performance in the burst period and low power consumption in the idle period. Our prototype implementation of MDA and the performance evaluations show that the performance of MDA outperforms that of traditional RAID10 by up to 34.4% and 42.3% in terms of the average response time for the online transaction processing (OLTP) application I/O and search engine I/O, respectively. Moreover, the energy efficiency of MDA outperforms that of RAID10 by up to 48.7% and 36.4%, respective to the aforementioned measures.",
    "keywords": [
      "storage systems",
      "disk arrays",
      "power consumption",
      "performance evaluation"
    ]
  },
  {
    "id": "1689",
    "title": "Numerical analysis of a dental implant system in three-dimension",
    "abstract": "One of the important problems in dental implant systems is screw loosening. In order to prevent this loosening a preload is applied to the retaining screw. However, this measure has not been able to eliminate its occurrence. In this study the effect of a washer in a Branemark-type implant on the loosening conditions of the retaining screw was investigated using a finite element simulation in three-dimensional models. The simulation indicated that a washer has an important affect against loosening. This also increases the flexibility of implant system against the deformation. The application of a washer is easy and cheap.",
    "keywords": [
      "three-dimensional contact modeling",
      "implant system"
    ]
  },
  {
    "id": "1690",
    "title": "maximum likelihood resolution of multi-block genotypes",
    "abstract": "We present a new algorithm for the problems of genotype phasing and block partitioning. Our algorithm is based on a new stochastic model, and on the novel concept of probabilistic common haplotypes. We formulate the goals of genotype resolving and block partitioning as a maximum likelihood problem, and solve it by an EM algorithm. When applied to real biological SNP data, our algorithm outperforms two state of the art phasing algorithms. Our algorithm is also considerably more sensitive and accurate than a previous method in predicting and identifying disease association.",
    "keywords": [
      "algorithm",
      "genotype",
      "haplotype",
      "haplotype block",
      "snp",
      "haplotype resolution",
      "disease association",
      "haplotyping",
      "genotype phasing",
      "maximum likelihood"
    ]
  },
  {
    "id": "1691",
    "title": "A characterization of c.e. random reals",
    "abstract": "A real alpha is computably enumerable if it is the limit of a computable, increasing, converging sequence of rationals. A real a is random if its binary expansion is a random sequence. Our aim is to offer a self-contained proof, based on the papers (Calude et al.. in: M. Morvan, C. Meinel, D. Krob (Eds.), Proc. 15th Symp. on Theoretical Aspects of Computer Science, Paris, Springer, Berlin, 1998, pp. 596-606; Chaitin, J. Assoc. Comput. Mach. 22 (1975) 329; Slaman, manuscript, 14 December 1998, 2 pp., Solovay, unpublished manuscript, IBM Thomas J. Watson Research Center. Yorktown Heights, New York, May 1975, 215 pp,), of the following theorem: a real is ce. and random if and only, if it is a Chaitin Omega real, i.e., the halting probability of some universal self-delimiting Turing machine.  ",
    "keywords": [
      "chaitin omega real",
      "random real",
      "c.e. real"
    ]
  },
  {
    "id": "1692",
    "title": "Employee job attitudes and organizational characteristics as predictors of cyberloafing",
    "abstract": "Cyberloafing is the personal use of the Internet by employees while at work. The purpose of this study is to examine whether employee job attitudes, organizational characteristics, attitudes towards cyberloafing, and other non-Internet loafing behaviors serve as antecedents to cyberloafing behaviors. We hypothesize that the employee job attitudes of job involvement and intrinsic involvement are related to cyberloafing. In addition, we hypothesize that organizational characteristics including the perceived cyberloafing of ones coworkers and managerial support for internet usage are related to cyberloafing. We also hypothesize that attitudes towards cyberloafing and the extent to which employees participate in non-Internet loafing behaviors (e.g., talking with coworkers, running personal errands) will both be related to cyberloafing. One hundred and forty-three working professional from a variety of industries were surveyed regarding their Internet usage at work. As hypothesized, the employee job attitudes of job involvement and intrinsic involvement were negatively related to cyberloafing. Also as predicted, the organizational characteristics of the perceived cyberloafing of ones coworkers and managerial support for internet usage were positively related to cyberloafing. Finally, results showed that attitudes towards cyberloafing and participation in non-Internet loafing behaviors were positively related to cyberloafing. Implications for both organizations and employees are discussed.",
    "keywords": [
      "cyberloafing",
      "computer mediated communication",
      "internet",
      "organizations",
      "job attitudes",
      "production deviance"
    ]
  },
  {
    "id": "1693",
    "title": "Using homography relationship for auto-calibration in mobile smart-project device system",
    "abstract": "As technology advances, mobile devices have become indispensable 3C products. Smart phones and tablet computers have become the favorite products of technology, and they are commonly seen everywhere. In addition to making phone calls and sending messages, people also use smart phones and tablet computers to browse the Internet, listen to music, take videos and pictures, and play games, all of which have become an integral part of peoples lives. In addition, according to a market survey, the most expected function of future smart phones by consumers is the projection. However, the projection will accelerate the battery power consumption. In this paper, we conducted an auto-calibration based on homography relationship in mobile smart-project device system to take advantage of the capability of the projector to project an image onto any opaque and unflat plane, thus breaking through the restricted display interfaces of mobile devices. Moreover, we also proposed the method that named FDPA (standing for Fast Detecting Projection Area) to reduce the battery power consumption via decreasing the calculation cost of the auto-calibration. The experimental results show that FDPA is able to improve the dynamic projection performance and to reduce battery power consumption in the mobile smart-projector device system. This was expected to help further develop an interactive projection system between mobile devices, adding diversity to mobile device applications and development.",
    "keywords": [
      "mobile device",
      "dynamic projection",
      "camera-projector",
      "homography relationship",
      "auto-calibration"
    ]
  },
  {
    "id": "1694",
    "title": "Region matching with missing parts",
    "abstract": "We present a variational approach to the problem of registering planar shapes despite missing parts. Registration is achieved through the evolution of a partial differential equation that simultaneously estimates the shape of the missing region, the underlying 'complete shape' and the collection of group elements (Euclidean, affine) corresponding to the registration. Our technique can be used both for shapes, for instance represented as characteristic functions (binary images) and for grayscale images where it can be interpreted as region 'inpainting.' The novelty of the approach lies on the fact that, rather than estimating the missing region in each image independently. we pose the problem as a joint registration with respect to an underlying 'complete shape' from which the complete version of the original data is obtained via a group action. We simultaneously estimate the complete shape and the group action in an alternating minimization scheme.  ",
    "keywords": [
      "shape",
      "variational",
      "registration",
      "missing part",
      "inpainting"
    ]
  },
  {
    "id": "1695",
    "title": "A 2 GHz phase-locked loop frequency synthesizer with on-chip VCO",
    "abstract": "This paper discusses the implementation of the building blocks for a 2 GHz phase-locked loop frequency synthesizer in a standard 0.5 mu m BICMOS process. These blocks include a low-power optimized dual modulus prescaler which is able to operate with input frequencies up to 2.7 GHz, a phase detector with extremely constant gain throughout the input phase difference range, a chargepump with a rail-to-rail output, and an on-chip voltage-controlled oscillator.",
    "keywords": [
      "frequency synthesis",
      "phase-locked loop",
      "radio frequency",
      "voltage-controlled oscillator"
    ]
  },
  {
    "id": "1696",
    "title": "SoC with an integrated DSP and a 2.4-GHz RF transmitter",
    "abstract": "We present a system-on-chip (SoC) that integrates a TMS320C54x digital signal processor (DSP), which is commonly used in cellular phones, with a multigigahertz digital RF transmitter that meets the Bluetooth specifications. The RF transmitter is tightly coupled with the DSP and is directly mapped to its address space. The transmitter architecture is based on an all-digital phase-locked loop (ADPLL), which is built from the ground up using digital techniques and digital creation flow that exploit high speed and high density of a deep-submicrometer CMOS process while avoiding its weaker handling of voltage. The frequency synthesizer features a wideband frequency modulation capability. As part of the digital flow, the digitally controlled oscillator (DCO) and a class-E power-amplifier are created as ASIC cells with digital I/Os. All digital blocks, including the 2.4-GHz logic, are synthesized from VHDL and auto routed. The use of VHDL allows for a tight and seamless integration of RF with the DSP. To take advantage of the direct DSP-RF coupling and to demonstrate a software-defined radio (SDR) capability, a DSP program is written to perform modulation of the GSM standard. The chip is fabricated in a baseline 130-nm CMOS process with no analog extensions and features high logic gate density of 150 kgates per mm(2). The RF transmitter area occupies only 0.54 mm(2), and the current consumption (including the companion DSP) is 49 mA at 1.5-V supply and 4 mW of RF output. This proves attractiveness and competitiveness of the \"digital RF\" approach, whose goal is to replace RF functions with high-speed digital logic gates.",
    "keywords": [
      "all-digital",
      "bluetooth",
      "digital signal processor ",
      "frequency synthesizer",
      "integration",
      "phase domain",
      "single-chip",
      "system-on-chip ",
      "transmitter"
    ]
  },
  {
    "id": "1697",
    "title": "Performance of one's complement caches",
    "abstract": "On-chip caches to reduce average memory access latency are commonplace in today's commercial microprocessors. These on-chip caches generally have low associativity and small cache sizes. Cache line conflicts are the main source of cache misses, which are critical for overall system performance. This paper introduces an innovative design for on-chip data caches of microprocessors, called one's complement cache. While binary complement numbers have been successfully used in designing arithmetic units, to the best of our knowledge, no one has ever considered using such complement numbers in cache memory designs. This paper will show that such complement numbers help greatly in reducing cache misses in a data cache, thereby improving data cache performance. By parallel computation of cache addresses and memory addresses, the new design does not increase the critical hit time of cache accesses. Cache misses caused by line interference are reduced by evenly distributing data items referenced by program loops across all sets in a cache. Even distribution of data in the cache is achieved by making the number of sets In the cache a prime or an odd number, so that the chance of related data being mapped to a same set is small. Trace-driven simulations are used to evaluate the performance of the new design. Performance results on benchmarks show that the new design improves cache performance significantly with negligible additional hardware cost. ",
    "keywords": [
      "cache memory",
      "memory conflicts",
      "memory hierarchy",
      "performance evaluation"
    ]
  },
  {
    "id": "1698",
    "title": "Synthesis of robust water reuse networks for single-component retrofit problems using symmetric fuzzy linear programming",
    "abstract": "Water integration techniques can be used to minimize the utility water consumption and effluent generation of process plants through the implementation of reuse or recycle networks. There are a number of graphical and mathematical programming techniques available for the synthesis of such water reuse networks. However, effective use of these methods requires the availability of reliable process data, which in reality might be difficult to acquire. This paper describes a procedure for the synthesis of robust water reuse networks from imprecise data using symmetric fuzzy linear programming (SFLP). Two model variants, one based on mass exchange units and the other on source/sink allocation, are presented. Each variant is illustrated with a numerical example.  ",
    "keywords": [
      "process integration",
      "water reuse network",
      "fuzzy linear programming",
      "water pollution"
    ]
  },
  {
    "id": "1699",
    "title": "Shape matching of partially occluded curves invariant under projective transformation",
    "abstract": "This paper describes a method to identify partially occluded shapes which are randomly oriented in 3D space. The goal is to match the object contour present in an image with an object in a database. The approach followed is the alignment method which has been described in detail in the literature. Using this approach the recognition process is divided into two stages: first, the transformation between the viewed object and the model object is determined, and second, the model that best matches the viewed object is found. In the first stage, invariant points under projective transformation (based on bitangency) are used, which drastically reduced the selection space for alignment. Next, the curves are compared after the transformation matrix is estimated between the image and the model in order to determine the pose of the curve that undergoes the perspective projection. The evaluation process is performed using a novel estimation of the Hausdorff distance (HD), called the continuity HD. It evaluates partially occluded curves in the image in relation to the complete contour in the database. The experimental results showed that the present algorithm can cope with noisy figures, projective transformations, and complex occlusions.",
    "keywords": [
      "computer vision",
      "shape matching",
      "object recognition",
      "hausdorff distance",
      "alignment approach",
      "bitangency",
      "affine transformation"
    ]
  },
  {
    "id": "1700",
    "title": "Orthogonalized distinctive phonetic feature extraction for noise-robust automatic speech recognition",
    "abstract": "In this paper, we propose a noise-robust automatic speech recognition system that uses orthogonalized distinctive phonetic features (DPFs) as input of HMM with diagonal covariance. In an orthogonalized DPF extraction stage, first, a speech signal is converted to acoustic features composed of local features (LFs) and DeltaP, then a multilayer neural network (MLN) with 15 x 3 output units composed of context-dependent DPFs of a preceding context DPF vector, a current DPF vector, and a following context DPF vector maps the LFs to DPFs. Karhunen-Loeve transform (KLT) is then applied to orthogonalize each DPF vector in the context-dependent DPFs, using orthogonal bases calculated from a DPF vector that represents 38 Japanese phonemes. Each orthogonalized DPF vector is finally decor-related one another by using Gram-Schmidt orthogonalization procedure. related one another by using Gram In experiments, after evaluating the parameters of the MLN input and output units in the DPF extractor. the orthogonalized DPFs are compared with original DPFs. The orthogonalized DPFs are then evaluated in comparison with a standard parameter set of MFCCs and dynamic features. Next, noise robustness is tested using four types of additive noise. The experimental results show that the use of the proposed orthogonalized DPFs can significantly reduce the error rate in an isolated spoken-word recognition task both with clean speech and with speech contaminated by additive noise. Furthermore, we achieved significant improvements when combining the orthogonalized DPFs with conventional static MFCCs and DeltaP.",
    "keywords": [
      "automatic speech recognition ",
      "feature extraction",
      "distinctive phonetic feature ",
      "orthogonalization",
      "local feature "
    ]
  },
  {
    "id": "1701",
    "title": "Open-source MATLAB implementation of consistent discretisations on complex grids",
    "abstract": "Accurate geological modelling of features such as faults, fractures or erosion requires grids that are flexible with respect to geometry. Such grids generally contain polyhedral cells and complex grid-cell connectivities. The grid representation for polyhedral grids in turn affects the efficient implementation of numerical methods for subsurface flow simulations. It is well known that conventional two-point flux-approximation methods are only consistent for K-orthogonal grids and will, therefore, not converge in the general case. In recent years, there has been significant research into consistent and convergent methods, including mixed, multipoint and mimetic discretisation methods. Likewise, the so-called multiscale methods based upon hierarchically coarsened grids have received a lot of attention. The paper does not propose novel mathematical methods but instead presents an open-source Matlab(R) toolkit that can be used as an efficient test platform for (new) discretisation and solution methods in reservoir simulation. The aim of the toolkit is to support reproducible research and simplify the development, verification and validation and testing and comparison of new discretisation and solution methods on general unstructured grids, including in particular corner point and 2.5D PEBI grids. The toolkit consists of a set of data structures and routines for creating, manipulating and visualising petrophysical data, fluid models and (unstructured) grids, including support for industry standard input formats, as well as routines for computing single and multiphase (incompressible) flow. We review key features of the toolkit and discuss a generic mimetic formulation that includes many known discretisation methods, including both the standard two-point method as well as consistent and convergent multipoint and mimetic methods. Apart from the core routines and data structures, the toolkit contains add-on modules that implement more advanced solvers and functionality. Herein, we show examples of multiscale methods and adjoint methods for use in optimisation of rates and placement of wells.",
    "keywords": [
      "mimetic schemes",
      "mpfa methods",
      "consistent discretisations",
      "unstructured grids",
      "open-source implementation",
      "multiscale methods",
      "rate optimisation"
    ]
  },
  {
    "id": "1702",
    "title": "A robust and fast combination algorithm for deblurring and denoising",
    "abstract": "In this paper, we propose an efficient combined algorithm of split Bregman method, algebraic multigrid (AMG) method and Krylov acceleration method for deblurring and denoising. The split Bregman method is used to convert nonlinear TV model into three linear systems. But the linear system with blur operator is difficult to solve. We add an auxiliary linear stabilizing term to the linear system, then apply an AMG method and Krylov acceleration method to solve the new linear system. Various numerical experiments and comparisons demonstrate that the combined algorithm is efficient, fast, comparable to several existing algorithms, and robust over a wide range of parameters.",
    "keywords": [
      "image restoration",
      "total variation",
      "split bregman method",
      "algebraic multigrid method",
      "krylov method"
    ]
  },
  {
    "id": "1703",
    "title": "Hands-free vision-based interface for computer accessibility",
    "abstract": "Physically disabled and mentally challenged people are an important part of our society that has not yet received the same opportunities as others in their inclusion in the Information Society. Therefore, it is necessary to develop easily accessible systems for computers to achieve their inclusion within the new technologies. This paper presents a project whose objective is to draw disabled people nearer to new technologies. It presents a vision-based user interface designed to achieve computer accessibility for disabled users with motor impairments. The interface automatically finds the user's face and tracks it through time to recognize gestures within the face region in real time. Subsequently, a new information fusion procedure is proposed to acquire data from computer vision algorithms and its results are used to carry out a robust recognition process. Finally, we show how the system is used to replace a conventional mouse device for computer interaction and as a communication system for non-verbal children.",
    "keywords": [
      "humancomputer interaction",
      "multimodal interfaces",
      "accessibility"
    ]
  },
  {
    "id": "1704",
    "title": "Organizational knowledge management: A contingency perspective",
    "abstract": "Prior research examines several knowledge management processes, considering each as universally appropriate. Instead, we propose that the context influences the suitability of a knowledge management process. We develop a contingency framework, including two attributes of the organizational subunit's tasks: process or content orientation, and focused or broad domain, and links knowledge management processes to them: internalization for focused, process-oriented tasks; externalization for focused, content-oriented tasks; combination for broad, content-oriented tasks; and socialization for broad, process-oriented tasks. The empirical research was done at the Kennedy Space Center (KSC), based on several interviews and survey data from 159 individuals across 8 subunits. The results supported the contingency framework. All the knowledge management processes except externalization had a positive impact in the expected cell. At the overall level, combination and externalization, but not internalization and socialization, affect knowledge satisfaction. Some implications for practice and research are identified.",
    "keywords": [
      "contingency theory",
      "knowledge management",
      "structural equation modeling",
      "task characteristics"
    ]
  },
  {
    "id": "1705",
    "title": "Exact approaches to the single-source network loading problem",
    "abstract": "This article considers the network design problem that searches for a minimum-cost way of installing capacities on the edges of a network to simultaneously route a flow from a given access point to a subset of nodes representing customers with positive demands. We first consider compact and exponential-sized Mixed Integer Programming (MIP) formulations of the problem and provide their theoretical and computational comparison. We also consider a stronger disaggregated flow formulation. To solve the problem in practice, we project out the flow variables and generate Benders cuts within a branch-and-cut framework. To the best of our knowledge, the combination of Benders approach and this specific disaggregation has not been considered so far. In an extensive computational study, we compare the performance of compact MIP models against a textbook implementation and several normalization variants of Benders decomposition. We introduce a set of 32 real-world instances and use these, together with 64 other instances from the literature, to test our approaches. The results show that our branch-and-cut approach outperforms the best performing compact formulation leading to the best exact algorithm today for solving the considered dataset. ",
    "keywords": [
      "local access network design",
      "network loading",
      "capacitated network design",
      "benders decomposition"
    ]
  },
  {
    "id": "1706",
    "title": "A class of nth-order BVPs with nonlocal conditions",
    "abstract": "The aim of this paper is to present some existence results for a nonlinear nth-order boundary value problem with nonlocal conditions. Various fixed point theorems are used in the proofs. Examples are included to illustrate the results.  ",
    "keywords": [
      "bvps",
      "existence",
      "nonlocal conditions",
      "fixed point theorems"
    ]
  },
  {
    "id": "1707",
    "title": "asymmetry in media spaces",
    "abstract": "In any collaborative system, there are both symmetries and asymmetries present in the design of the technology and in the ways that technology is appropriated. Yet media space research tends to focus more on supporting and fostering the symmetries than the asymmetries. Throughout more than 20 years of media space research, the pursuit of increased symmetry, whether achieved through technical or social means, has been a recurrent theme. The research literature on the use of contemporary awareness systems, in contrast, displays little if any of this emphasis on symmetrical use; indeed, this body of research occasionally highlights the perceived value of asymmetry. In this paper, we unpack the different forms of asymmetry present in both media spaces and contemporary awareness systems. We argue that just as asymmetry has been demonstrated to have value in contemporary awareness systems, so might asymmetry have value in media spaces and in other CSCW systems, more generally. To illustrate, we present a media space that emphasizes and embodies multiple forms of asymmetry and does so in response to the needs of a particular work context.",
    "keywords": [
      "awareness",
      "media space",
      "asymmetry",
      "reciprocity"
    ]
  },
  {
    "id": "1708",
    "title": "Transmission costs, selfish nodes, and protocol design",
    "abstract": "We study the influence of transmission costs on the behavior of selfish nodes in wireless local area networks. Intuitively, it seems that transmission costs should have a stabilizing effect as (rational) nodes will defer packet transmissions when congestion develops and the cost for (successfully) transmitting a packet becomes high. In this paper we investigate whether this intuition is true. We use the slotted Aloha to model the communication channel where we capture the interaction among nodes as a non-cooperative game. For this game, we study the existence and properties of a (symmetric) Nash equilibrium. We show that the existence of a transmission cost is not always sufficient to guarantee stability. In particular, a stable equilibrium strategy will not exist if the transmission cost is too small. We then propose and analyze a price-based mechanism to guarantee stability and to optimize system performance in terms of throughput and delay.",
    "keywords": [
      "wireless networks",
      "mac protocols",
      "random access protocols",
      "slotted aloha",
      "game theory",
      "pricing"
    ]
  },
  {
    "id": "1709",
    "title": "Energy-aware error correction for QoS-provisioning real-time communications in wireless networks",
    "abstract": "A key issue in QoS-provisioning real-time wireless communications is to provide the QoS requirement with low energy consumption. In this paper, we propose an energy-efficient error correction scheme for real-time communications with QoS requirements in wireless networks. The QoS requirement of a message stream is modeled with (m, k) constraint, implying that at least m messages should be sent to a receiver during any window of k periods. The proposed scheme adaptively selects an error correcting code in an energy-efficient manner so that it maximizes the number of QoS provisionings per unit energy consumption.",
    "keywords": [
      "energy efficiency",
      "error correction",
      "qos provision",
      "real-time communications",
      "wireless networks"
    ]
  },
  {
    "id": "1710",
    "title": "Two-dimensional domain decomposition based on skeleton computation for parameterization and isogeometric analysis",
    "abstract": "Two-dimensional domains are decomposed into subdomains based on skeleton computation. Domain partition provides better parameterization than a one-patch representation. The new parameterization is superior to other techniques in isogeometric analysis.",
    "keywords": [
      "parameterization",
      "isogeometric analysis",
      "skeleton",
      "domain decomposition"
    ]
  },
  {
    "id": "1711",
    "title": "A game theoretical approach to sharing penalties and rewards in projects",
    "abstract": "This paper analyzes situations in which a project consisting of several activities is not realized according to plan. If the project is expedited, a reward arises. Analogously, a penalty arises if the project is delayed. This paper considers the case of arbitrary nondecreasing reward and penalty functions on the total expedition and delay, respectively. Attention is focused on how to divide the total reward (penalty) among the activities: the core of a corresponding cooperative project game determines a set of stable allocations of the total reward (penalty). In the definition of project games, surplus (cost) sharing mechanisms are used to take into account the specific characteristics of the reward (penalty) function at hand. It turns out that project games are related to bankruptcy and taxation games. This relation allows us to establish nonemptiness of the core of project games.",
    "keywords": [
      "game theory",
      "project management",
      "delay and expedition",
      "cost and surplus sharing mechanism",
      "bankruptcy and taxation problems"
    ]
  },
  {
    "id": "1712",
    "title": "Constrained decision-making for low-count radiation detection by mobile sensors",
    "abstract": "This paper approaches from an optimal control perspective the problem of fixed-time detection of mobile radioactive sources in transit by means of a collection of mobile sensors. Under simplifying assumptions on the motion and geometry of the source, the sensors, and the surrounding environment, the optimal control problem admits an intuitive, analytic closed-form solution. This solution is obtained thanks to analytic expressions for bounds on the probabilities of detection and false alarm for a NeymanPearson detection test. The intuition derived from this analytic solution supports the development of a motion control law that steers (suboptimally) the sensors to a given neighborhood of the suspected source, while navigating among stationary obstacles in their environment. This motion controller closes the loop at the acceleration level of a heterogeneous collection of sensor platforms. Experimental studies with these platforms corroborate the theoretical convergence results.",
    "keywords": [
      "constrained optimization",
      "decision making",
      "radiation detection",
      "sensor networks",
      "navigation function"
    ]
  },
  {
    "id": "1713",
    "title": "Numerical approximation of vector-valued highly oscillatory integrals",
    "abstract": "We present a method for the efficient approximation of integrals with highly oscillatory vector-valued kernels, such as integrals involving Airy functions or Bessel functions. We construct a vector-valued version of the asymptotic expansion, which allows us to determine the asymptotic order of a Levin-type method. Levin-type methods are constructed using collocation, and choosing a basis based on the asymptotic expansion results in an approximation with significantly higher asymptotic order.",
    "keywords": [
      "highly oscillatory integrals",
      "asymptotics",
      "quadrature"
    ]
  },
  {
    "id": "1714",
    "title": "tandem queues production systems with base stocks",
    "abstract": "We consider a production system that can be modeled by a two-station tandem queues. Products requested by demands have to go through station 1 then station 2 for processing; however, in order to reduce the response time, we may stock some semi-finished products called base stock at station 1 or finished products at station 2. For the underlying queueing system, we may have a queue after each station in additional to the queue in front of it, hence there are four different configurations. In this study, we approximate the distribution of response time for each configuration and, under some operating cost structure we can determine the configuration having the minimal total cost.",
    "keywords": [
      "tandem queue",
      "response time",
      "production system",
      "base stock"
    ]
  },
  {
    "id": "1715",
    "title": "Employing heat maps to mine associations in structured routine care data",
    "abstract": "Mining the electronic medical record (EMR) has the potential to deliver new medical knowledge about causal effects, which are hidden in statistical associations between different patient attributes. It is our goal to detect such causal mechanisms within current research projects which include e.g. the detection of determinants of imminent ICU readmission. An iterative statistical approach to examine each set of considered attribute pairs delivers potential answers but is difficult to interpret. Therefore, we aimed to improve the interpretation of the resulting matrices by the use of heat maps. We propose strategies to adapt heat maps for the search for associations and causal effects within routine EMR data. Heat maps visualize tabulated metric datasets as grid-like choropleth maps, and thus present measures of association between numerous attribute pairs clearly arranged. Basic assumptions about plausible exposures and outcomes are used to allocate distinct attribute sets to both matrix dimensions. The image then avoids certain redundant graphical elements and provides a clearer picture of the supposed associations. Specific color schemes have been chosen to incorporate preexisting information about similarities between attributes. The use of measures of association as a clustering input has been taken as a trigger to apply transformations which ensure that distance metrics always assume finite values and treat positive and negative associations in the same way. To evaluate the general capability of the approach, we conducted analyses of simulated datasets and assessed diagnostic and procedural codes in a large routine care dataset. Simulation results demonstrate that the proposed clustering procedure rearranges attributes similar to simulated statistical associations. Thus, heat maps are an excellent tool to indicate whether associations concern the same attributes or different ones, and whether affected attribute sets conform to any preexisting relationship between attributes. The dendrograms help in deciding if contiguous sequences of attributes effectively correspond to homogeneous attribute associations. The exemplary analysis of a routine care dataset revealed patterns of associations that follow plausible medical constellations for several diseases and the associated medical procedures and activities. Cases with breast cancer (ICD C50), for example, appeared to be associated with radiation therapy (852). In cross check, approximately 60 percent of the attribute pairs in this dataset showed a strong negative association, which can be explained by diseases treated in a medical specialty which routinely does not perform the respective procedures in these cases. The corresponding diagram clearly reflects these relationships in the shape of coherent subareas. We could demonstrate that heat maps of measures of association are effective for the visualization of patterns in routine care EMRs. The adjustable method for the assignment of attributes to image dimensions permits a balance between the display of ample information and a favorable level of graphical complexity. The scope of the search can be adapted by the use of pre-existing assumptions about plausible effects to select exposure and outcome attributes. Thus, the proposed method promises to simplify the detection of undiscovered causal effects within routine EMR data.",
    "keywords": [
      "hierarchical clustering",
      "visualization techniques",
      "associations in clinical data"
    ]
  },
  {
    "id": "1716",
    "title": "A branch and cut algorithm for the location-routing problem with simultaneous pickup and delivery",
    "abstract": "This paper addresses a location-routing problem with simultaneous pickup and delivery (LRPSPD) which is a general case of the location-routing problem. The LRPSPD is defined as finding locations of the depots and designing vehicle routes in such a way that pickup and delivery demands of each customer must be performed with same vehicle and the overall cost is minimized. We propose an effective branch-and-cut algorithm for solving the LRPSPD. The proposed algorithm implements several valid inequalities adapted from the literature for the problem and a local search based on simulated annealing algorithm to obtain upper bounds. Computational results, for a large number of instances derived from the literature, show that some instances with up to 88 customers and 8 potential depots can be solved in a reasonable computation time.",
    "keywords": [
      "logistics",
      "location-routing problem",
      "simultaneous pickup and delivery",
      "branch-and-cut",
      "simulated annealing"
    ]
  },
  {
    "id": "1717",
    "title": "Adaptive radio resource with borrowing for multi-operators 3G+ wireless networks with heterogeneous traffic",
    "abstract": "Recently, sharing the radio access network (RAN) has become an important issue for 3G and beyond mobile wireless operators. In addition to user and service considerations, there are economic and technical advantages to 3G and beyond shared networks. This is driving optimizations in the deployment and development of national 3G and 4G rollouts. However, shared 3G and 4G networks exhibit many of the resource management opportunities and challenges as more complex heterogeneous traffic and sharing techniques are considered. In this paper, we propose a roaming-based sharing adaptive partitioning with borrowing (APB) scheme where an operator is allowed to borrow from the unutilized resources originally belonging to other operators sharing the same RAN. This algorithm controls the resource sharing between the operators in addition to controlling the admission control (CAC) within each operator's domain. The borrowing margin is specified either as a fixed portion or an adaptive percentage of the unutilized resource. Furthermore, the APB CAC function makes a distinction between newly originating calls and handoff calls by assuming a higher priority level for handoff calls in the form of possibility of queuing. Therefore, the prescribed call admission control scheme provides a higher quality of service for the handoff requests of real-time services expected in 3rd and 4th G wide band code division multiple access (WCDMA) systems. Simulation results indicate that APB provides higher resource utilization under all load conditions leading in turn to increased revenue. In addition, a higher quality of service for traffic is provided especially when an operator is allowed to borrow from the unutilized resources.  ",
    "keywords": [
      "adaptive resource allocation",
      "admission control",
      "multi-operator",
      "mobile wireless",
      "queuing",
      "wcdma",
      "3g"
    ]
  },
  {
    "id": "1718",
    "title": "Lattice effect algebras with (o)-continuous faithful valuations",
    "abstract": "We prove that if there exists an order-continuous, faithful valuation omega on a lattice effect algebra E then E is modular, separable and order-continuous. It is also shown that such an effect algebra E can be supremum and infimum densely embedded into a complete effect algebra (E) over cap which is also modular separable and order-continuous, since the valuation omega can be extended to a unique order-continuous faithful valuation  )over cap> on (E) over cap.  ",
    "keywords": [
      "effect algebra",
      "state",
      "valuation",
      "order continuity",
      "macneille completion"
    ]
  },
  {
    "id": "1719",
    "title": "DCE-MRI Data Analysis for Cancer Area Classification",
    "abstract": "Objectives: The paper aims at improving the support of medical researchers in the context of in-vivo cancer imaging. Morphological and functional parameters obtained by dynamic contrast-enhanced MRI (DCE-MRI) techniques are analyzed, which aim at investigating the development of tumor microvessels. The main contribution consists in proposing a machine learning methodology to segment automatically these MRI data, by isolating tumor areas with different meaning, in a histological sense. Methods: The proposed approach is based on a three-step procedure: i) robust feature extraction from raw time-intensity curves, ii) voxel segmentation, and iii) voxel classification based on a learning-by-example approach. In the first step, few robust features that compactly represent the response of the tissue to the DCE-MRI analysis are computed. The second step provides a segmentation based on the mean shift (MS) paradigm, which has recently shown to be robust and useful for different and heterogeneous clustering tasks. Finally, in the third step, a support vector machine (SVM) is trained to classify voxels according to the labels obtained by the clustering phase (i.e., each class corresponds to a cluster). indeed, the SVM is able to classify new unseen subjects with the same kind of tumor. Results: Experiments on different subjects affected by the same kind of tumor evidence that the extracted regions by both the MS clustering and the SVM classifier exhibit a precise medical meaning, as carefully validated by the medical researchers. Moreover, our approach is more stable and robust than methods based on quantification of DCE-MRI data by means of pharmacokinetic models. Conclusions: The proposed method allows to analyze the DCE-MRI data more precisely and faster than previous automated or manual approaches.",
    "keywords": [
      "dce-mri",
      "cluster analysis",
      "classification",
      "svm"
    ]
  },
  {
    "id": "1720",
    "title": "A new algorithm for linearly constrained c-convex vector optimization with a supply chain network risk application",
    "abstract": "Vector optimization is studied. A proximal point algorithm is proposed for vector optimization. The global and local convergence results for the new algorithm are presented. The efficiency of the new algorithm is shown by an application to a supply chain network risk management problem.",
    "keywords": [
      "multiple objective programming",
      "pareto optimum",
      "c-convex",
      "proximal point algorithm",
      "supply chain network risk management"
    ]
  },
  {
    "id": "1721",
    "title": "Improving activity-based costing heuristics by higher-level cost drivers",
    "abstract": "Activity-based costing (ABC) tries to allocate overhead costs to cost objects more accurately than traditional cost systems. However, since ABC proportionalizes overhead costs it is a heuristic. The paper uses simulations and mixed-integer programming to analyze the extent of the sub-optimality incurred by ABC-heuristics. While previous research has focused on ABC systems with a simple set of cost drivers, thereby restricting the potential of ABC as a heuristic, the paper analyzes the effects of establishing a cost driver corresponding to a higher cost level. Specifically, a portfolio-based cost driver captures the demand heterogeneity triggered by the portfolio. This heterogeneity driver is then used to proportionalize all costs due to inflexible overhead resources. One of the main findings is that such a heterogeneity driver improves the quality of ABC-heuristics significantly.",
    "keywords": [
      "activity-based costing",
      "heuristics",
      "simulation",
      "mixed-integer programming"
    ]
  },
  {
    "id": "1722",
    "title": "A DEA-based approach for fair reduction and reallocation of emission permits",
    "abstract": "How to allocate initial emission permits is vital for the cap-and-trade system in controlling the total emission level and improving the operation efficiency of the trade mechanism. And data envelopment analysis (DEA) has been proved to be more suitable to fairly resolve the problem as a performance-based allocation is involved here. In this paper, we present a novel DEA approach for fair reduction and reallocation of emission permits for areas under administration. The presented DEA models are based on the ideas, where, according to their eco-efficiency and current emission levels, all decision making units (DMUs) compete with each other for the least reduction amount by introducing a bargaining game, on the other hand, the central authority can improve the overall efficiency by reallocating the remaining emission permits. The approach is applied on the dataset of agricultural greenhouse gas emissions from 15 European Union members (EU15). The results of the application show that the reduction and reallocation mechanism is fair, which benefits the countries that work at the optimal scale operation while punishes those that fail to operate on the optimal scale. Furthermore, it is effective in improving the overall efficiency of the system.  ",
    "keywords": [
      "data envelopment analysis ",
      "eco-efficiency",
      "bargaining game",
      "emission permits"
    ]
  },
  {
    "id": "1723",
    "title": "Regularity and controllability robustness of TS fuzzy descriptor systems with structured parametric uncertainties",
    "abstract": "The problem of robust global regularity and controllability is considered in TakagiSugeno (TS) fuzzy descriptor control systems with structured parametric uncertainties. Sufficient conditions are proposed to ensure both global regularity and controllability in these uncertain systems. The conditions also provide the explicit relationships of bounds on parametric uncertainties to achieve regularity and controllability. One numerical and two engineering examples are given to illustrate the applications of the proposed sufficient conditions.",
    "keywords": [
      "robust controllability",
      "takagisugeno  fuzzy descriptor system",
      "structured parametric uncertainties"
    ]
  },
  {
    "id": "1724",
    "title": "On perfect p-ary codes of length p+1",
    "abstract": "Let p be a prime number and assume p >= 5. We will use a result of L. Redei to prove, that every perfect 1-error correcting code C of length p + 1 over an alphabet of cardinality p, such that C has a rank equal to p and a kernel of dimension p - 2, will be equivalent to some Hamming code H. Further, C can be obtained from H, by the permutation of the symbols, in just one coordinate position.",
    "keywords": [
      "perfect codes",
      "redei theorem"
    ]
  },
  {
    "id": "1725",
    "title": "Viscoelastic behaviour of non-homogeneous variable-section beams with post-poned restraints",
    "abstract": "The aim of this paper is to develop a procedure able to calculate the long-term stress and strain patterns in modem prestressed composite structures which are largely influenced by creep and shrinkage and whose final static configuration is the result of many phases of loading and restraints conditions. The introduction of equivalent moduli, depending on the viscous and elastic features of materials, can guarantee a significant simplification of the problem presented above. The proposed calculation model has been used to design the \"Quattroquercie Viaduct\" located on the highway \"A3\" Salerno-Reggio Calabria, Italy.",
    "keywords": [
      "prestressed composite structures",
      "creep",
      "shrinkage",
      "variation of static schemes",
      "long term effects",
      "equivalent moduli"
    ]
  },
  {
    "id": "1726",
    "title": "Multipath QoS routing with interference provision in ad hoc wireless network",
    "abstract": "Mobile nodes are interconnected via multihop routing paths that consist of unstable radio links in ad hoc wireless network. Providing QoS routing for such networks is complex owing to imprecise network information, insufficient bandwidth and dynamic topology. Many multipath routing protocols have been proposed to improve network stability and throughput. The sender node discovers multiple disjoined routing paths and spreads traffic among them according to their delay or available bandwidth. For real-time multimedia application, insufficient bandwidth or unstable throughput invite unexpected delays or jitters. Some multipath routing protocols pre-evaluate available path bandwidth and select sufficient bandwidth from them. To minimize the path cost, paths with smaller hopcounts are selected in advance. These selected paths are generally too closed and the total network throughput cannot simple be summed due to \"path interference\". Discovering and selecting multiple high-interference paths is ineffectual and the total available bandwidth is not precise. In this paper, we proposed an interference-aware QoS multipath routing protocol for QoS-constraint multimedia or real-time applications in ad hoc wireless network. Specifically, this paper applied a scheme for evaluating available bandwidth according to the network capacities with different Media Access Control (MAC) protocols. In this paper, we show the \"Interference ratio\" of multipath and we also evaluate the stability and throughput improvement is assessed through simulations.",
    "keywords": [
      "ad hoc networks",
      "multipath routing",
      "interference",
      "qos",
      "multimedia"
    ]
  },
  {
    "id": "1727",
    "title": "out-of-core compression for gigantic polygon meshes",
    "abstract": "Polygonal models acquired with emerging 3D scanning technology or from large scale CAD applications easily reach sizes of several gigabytes and do not fit in the address space of common 32-bit desktop PCs. In this paper we propose an out-of-core mesh compression technique that converts such gigantic meshes into a streamable, highly compressed representation. During decompression only a small portion of the mesh needs to be kept in memory at any time. As full connectivity information is available along the decompression boundaries, this provides seamless mesh access for incremental in-core processing on gigantic meshes. Decompression speeds are CPU-limited and exceed one million vertices and two million triangles per second on a 1.8 GHz Athlon processor.A novel external memory data structure provides our compression engine with transparent access to arbitrary large meshes. This out-of-core mesh was designed to accommodate the access pattern of our region-growing based compressor, which - in return - performs mesh queries as seldom and as local as possible by remembering previous queries as long as needed and by adapting its traversal slightly. The achieved compression rates are state-of-the-art.",
    "keywords": [
      "fit",
      "mesh",
      "applications",
      "engine",
      "large-scale",
      "data structures",
      "addressing",
      "art",
      "timing",
      "polygon",
      "access",
      "model",
      "paper",
      "representation",
      "external memory data structures",
      "scan",
      "processing sequences",
      "informal",
      "out-of-core algorithms",
      "locality",
      "streaming meshes",
      "mesh compression",
      "pattern",
      "incremental",
      "processor",
      "adapt",
      "technologies",
      "transparency",
      "region",
      "space",
      "process",
      "memorialized",
      "queries",
      "3d",
      "core",
      "connection",
      "compression"
    ]
  },
  {
    "id": "1728",
    "title": "Deterministic fuzzy time series model for forecasting enrollments",
    "abstract": "The fuzzy time series has recently received increasing attention because of its capability of dealing with vague and incomplete data. There have been a variety of models developed to either improve forecasting accuracy or reduce computation overhead. However, the issues of controlling uncertainty in forecasting, effectively partitioning intervals, and consistently achieving forecasting accuracy with different interval lengths have been rarely investigated. This paper proposes a novel deterministic forecasting model to manage these crucial issues. In addition, an important parameter, the maximum length of subsequence in a fuzzy time series resulting in a certain state, is deterministically quantified. Experimental results using the University of Alabamas enrollment data demonstrate that the proposed forecasting model outperforms the existing models in terms of accuracy, robustness, and reliability. Moreover, the forecasting model adheres to the consistency principle that a shorter interval length leads to more accurate results.",
    "keywords": [
      "fuzzy time series",
      "forecasting",
      "fuzzy logical relationship",
      "state transition",
      "interval partitioning"
    ]
  },
  {
    "id": "1729",
    "title": "Epidemiology tools program and selected medical applications",
    "abstract": "In epidemiological research the calculation of appropriate measures of disease frequency is the basis for a comparison of populations and, therefore, the identification of disease determinants. Two frequencies being compared can be combined into a single summary parameter that estimates the association between an exposure and a disease. This can be accomplished by calculating either the ratio of the measures of disease frequency for two populations which indicates how much more likely one population is to develop a disease than another, or the difference between the frequencies which indicates how much greater the frequency of a disease is in one population compared with the other. In epidemiology we often need to compute measures of disease frequency mentioned above. We can do that by means of complex statistical software, but sometimes it is not easy or possible to use this software for an analysis of epidemiological data. At the EuroMISE Center of Charles University and Academy of Sciences in the framework of the research supported from grant COPERNICUS JRP-10053 the E.T. program (Epidemiology Tools) was developed. This program can be used for practical analysis of data from retrospective (case-control) studies or prospective (cohort) studies and for standardization. However, the program can be also a useful teaching tool in the epidemiology courses. ",
    "keywords": [
      "epidemiology",
      "standardization",
      "cohort study",
      "case-control study"
    ]
  },
  {
    "id": "1730",
    "title": "A graphical test for the interval stability of fractional-delay systems",
    "abstract": "This paper investigates the BIBO (bounded input and bounded output) interval stability testing of fractional-delay systems, a problem of justifying the BIBO stability of a polytopic family of functions involving fractional-order powers as well as exponential powers. It proves that the BIBO stability of the polytope is governed by the BIBO stability of the edges of the polytope, and the latter can be tested graphically via frequency response plots. The main results generalize some results in the literature.",
    "keywords": [
      "robust stability",
      "edge theorem",
      "graphical test",
      "fractional-delay systems"
    ]
  },
  {
    "id": "1731",
    "title": "Global optimization in R-n with box constraints and applications: A MAPLE code",
    "abstract": "A variant of the cubic algorithm [l] is presented for global optimization of continuous functions with box constraints. On this basis, a MAPLE code is developed for full global optimization of functions of n variables with application to the finding of all roots of a polynomial, to the eigenvalue problems, and to the solution of nonlinear systems of equations, including underdetermined and overdetermined systems. The code does not create ill-conditioned situations. Graphics are included, and the solution set can be visualized in projections on coordinate planes. The code is ready for engineering applications. Results of numerical experiments are presented, with graphs, to illustrate the use of the code.  ",
    "keywords": [
      "global optimization",
      "cubic algorithm",
      "numerical methods"
    ]
  },
  {
    "id": "1732",
    "title": "Preimage and pseudo-collision attacks on step-reduced SM3 hash function",
    "abstract": "SM3 [12] is the Chinese cryptographic hash standard which was announced in 2010 and designed by Wang et al. It is based on the Merkle-Damgard design and its compression function can be seen as a block cipher used in Davies-Meyer mode. It uses message block of length 512 bits and outputs hash value of length 256 bits. This letter studies the security of SM3 hash function against preimage attack and pseudo-collision attack by using the weakness of diffusion process and linear message expansion. We propose preimage attacks on 29-step and 30-step SM3, and pseudo-preimage attacks on 31-step and 32-step SM3 out of 64 steps. The complexities of these attacks are 2245 29-step operations, 2(251.1) 30-step operations, 2(245) 31-step operations and 2(251.1) 32-step operations, respectively. These (pseudo-)preimage attacks are all from the 1-st step of the reduced SM3. Furthermore, these (pseudo-)preimage attacks can be converted into pseudo-collision attacks on SM3 reduced to 29 steps, 30 steps, 31 steps and 32 steps with complexities of 2(122), 2(125.1), 2(122) and 2(125.1) respectively. As far as we know, the previously best known preimage attacks on SM3 cover 28 steps (from the 1-st step) and 30 steps (from the 7-th step).  ",
    "keywords": [
      "cryptography",
      "preimage attack",
      "collision attack",
      "differential meet-in-the-middle",
      "sm3",
      "hash function"
    ]
  },
  {
    "id": "1733",
    "title": "Unified parallel encoding and decoding algorithms for Dandelion-like codes",
    "abstract": "The Dandelion-like codes are eight bijections between labeled trees and strings of node labels. The literature contains optimal sequential algorithms for these bijections, but no parallel algorithms have been reported. In this paper the first parallel encoding and decoding algorithms for Dandelion-like codes are presented. Namely, a unique encoding algorithm and a unique decoding algorithm, which when properly parameterized, can be used for all Dandelion-like codes, are designed. These algorithms are optimal in the sequential setting. The encoding algorithm implementation on an EREW PRAM is optimal, while the efficient implementation of the decoding algorithm requires concurrent reading.",
    "keywords": [
      "bijective tree encoding",
      "prfer code",
      "dandelion-like codes",
      "pram algorithms"
    ]
  },
  {
    "id": "1734",
    "title": "Latent class model based diagnostic system utilizing traditional Chinese medicine for patients with systemic lupus erythematosus",
    "abstract": "Systemic lupus erythematosus (SLE) can affect nearly any organ system, and is frequently an evolving disease with varied manifestations. Traditional Chinese medicine (TCM) physicians have identified different SLE patterns that they have difficulty summarizing, but the latent class model helps solve this problem. This study applies the latent class model and disease pattern coding system (B-code) to design a TCM diagnostic expert system. This study gathered 2047 valid records and classified three clusters of main disease patterns. Compared with the experience of the TCM expert, the accuracy rate of the expert system reached 77.47%. The results show that this diagnostic system performed well in identifying the disease patterns of SLE and may be clinically useful for TCM physicians.",
    "keywords": [
      "latent class model",
      "disease pattern",
      "systemic lupus erythematosus "
    ]
  },
  {
    "id": "1735",
    "title": "Genome data classification based on fuzzy matching",
    "abstract": "Genomic data mining and knowledge extraction is an important problem in bioinformatics. Some research work has been done on unknown genome identification and is based on exact pattern matching of n-grams. In most of the real world biological problems exact matching may not give desired results and the problem in using n-grams is exponential explosion. In this paper we propose a method for genome data classification based on approximate matching. The algorithm works by selecting random samples from the genome database. Tolerance is allowed by generating candidates of varied length to query from these sample sequences. The Levenshtein distance is then checked for each candidate and whether they are k-fuzzily equal. The total number of fuzzy matches for each sequence is then calculated. This is then classified using the data mining techniques namely, naive Bayes, support vector machine, back propagation and also by nearest neighbor. Experiment results are provided for different tolerance levels and they show that accuracy increases as tolerance does. We also show the effect of sampling size on the classification accuracy and it was observed that classification accuracy increases with sampling size. Genome data of two species namely Yeast and E. coli are used to verify proposed method.",
    "keywords": [
      "bioinformatics",
      "soft computing",
      "genome data",
      "data mining",
      "approximate pattern matching",
      "exact matching"
    ]
  },
  {
    "id": "1736",
    "title": "Test data compression scheme based on variable-to-fixed-plus-variable-length coding",
    "abstract": "A test data compression scheme based on Variable-to-Fixed-Plus-Variable-Length (VTFPVL) coding is presented, by using which the test data can be compressed efficiently. In this scheme, code words are divided into fixed-length head section and variable-length tail section. In order to attain further compression, the highest bit of the tail is omitted from the code words, because all of the highest bits in the tail section of the code words are the same as 1. A special shift counter is also used, which further eases the control circuit. Experimental results of the MinTest fault sets which are part of ISCAS-89 benchmark circuits show that the proposed scheme is obviously better than traditional coding methods in the compression ratio and the implementation of decompression, such as Golomb, FDR, VIHC, v9C coding.",
    "keywords": [
      "test data compression",
      "coding",
      "fixed-length coding",
      "variable-length coding"
    ]
  },
  {
    "id": "1737",
    "title": "pingin' in the rain",
    "abstract": "Residential Internet connections are susceptible to weather-caused outages: Lightning and wind cause local power failures, direct lightning strikes destroy equipment, and water in the atmosphere degrades satellite links. Outages caused by severe events such as fires and undersea cable cuts are often reported upon by operators and studied by researchers. In contrast, outages cause by ordinary weather are typically limited in scope, and because of their small scale, there has not been comparable effort to understand how weather affects everyday last-mile Internet connectivity. We design and deploy a measurement tool called ThunderPing that measures the connectivity of residential Inter- net hosts before, during, and after forecast periods of severe weather. ThunderPing uses weather alerts from the US National Weather Service to choose a set of residential host addresses to ping from several vantage points on the Internet. We then process this ping data to determine when hosts lose connectivity, completely or partially, and categorize whether these failures occur during periods of severe weather or when the skies are clear. In our preliminary results, we find that compared to clear weather, failures are four times as likely during thunderstorms and two times as likely during rain. We also find that the duration of weather induced outages is relatively small for a satellite provider we focused on.",
    "keywords": [
      "ping",
      "weather",
      "thunderping",
      "outage"
    ]
  },
  {
    "id": "1738",
    "title": "Strategy space exploration of a multi-agent model for the labor market",
    "abstract": "We present a multi-agent system where typical labor market mechanisms emerge. Based on a few simple rules, our model allows for different interpretative paradigms to be represented and for different scenarios to be tried out. We thoroughly explore the space of possible strategies both for those unemployed and for companies and analyze the trade-off between these strategies regarding global social and economical indicators.",
    "keywords": [
      "multiagent model",
      "labor market"
    ]
  },
  {
    "id": "1739",
    "title": "Can international environmental cooperation be bought: Comment",
    "abstract": "Fuentes-Albero and Rubio (2010) analytically examine the effects of the countries heterogeneity on the international environmental cooperation. They consider two types of countries having different abatement costs in one case and different environmental damages in another case. Furthermore it is analyzed whether a self-financed transfer system can diminish these heterogeneity effects. The paper shows for both scenarios of asymmetry and no transfers that the maximum level of cooperation consists of three countries of the same type. For the case of heterogeneity in environmental damages, Fuentes-Albero and Rubio conclude that an agreement between one type 1 and one type 2 country is also self-enforcing given that the differences in the damages are not very large. In this comment, the derivation of the last mentioned result is shown to be incorrect by proving that this coalition is not self-enforcing.",
    "keywords": [
      "game theory",
      "self-enforcing international environmental agreements",
      "environment",
      "group decision and negotiation"
    ]
  },
  {
    "id": "1740",
    "title": "A New Perturbed Iterative Algorithm with Mixed Errors for Solving System of Generalized Nonlinear Variational Inclusions",
    "abstract": "A new system of generalized nonlinear variational inclusions with A-maximal m-relaxed eta-accretive (so called (A,eta)-accretive [22]) mappings in q-uniformly smooth Banach spaces is introduced and studied. By using the resolvent operator technique associated with A-maximal m-relaxed eta-accretive mappings due to Lan et al., the existence and uniqueness, of solution for this system of generalized nonlinear variational inclusions is verified and a new perturbed iterative algorithm with mixed errors for solving the aforementioned system is constructed. Also the convergence of the sequences generated by the our algorithm in q-uniformly smooth Banach spaces is proved. The results presented in this paper extend and improve some known results in the literature.",
    "keywords": [
      "a-maximal m-relaxed eta-accretive",
      "perturbed iterative algorithm with mixed errors",
      "a system of generalized nonlinear variational inclusions",
      "variational convergence",
      "resolvent operator technique"
    ]
  },
  {
    "id": "1741",
    "title": "Two effective measures of intuitionistic fuzzy entropy",
    "abstract": "Based on the concept of fuzzy entropy, two effective measures of intuitionistic fuzzy entropy are proposed in intuitionistic fuzzy information, and then the essential properties of these measures are introduced. These measures are a generalized version of the fuzzy entropy and a complementarity of existing entropy for intuitionistic fuzzy sets. Based on this generalization, a connection between the concepts of the fuzzy entropy and the intuitionistic fuzzy entropy is established. Finally, a numeral example is given to show that the information measures of the proposed intuitionistic fuzzy entropy are reasonable and effective by the comparison of the proposed entropy and existing entropy.",
    "keywords": [
      "fuzzy set",
      "fuzzy entropy",
      "intuitionistic fuzzy set",
      "information measure",
      "intuitionistic fuzzy entropy"
    ]
  },
  {
    "id": "1742",
    "title": "Parallel approach to NNMF on multicore architecture",
    "abstract": "We tackle the parallelization of Non-Negative Matrix Factorization (NNMF), using the Alternating Least Squares and Lee and Seung algorithms, motivated by its use in audio source separation. For the first algorithm, a very suitable technique is the use of active set algorithms for solving several non-negative inequality constraints least squares problems. We have addressed the NNMF for dense matrix on multicore architectures, by organizing these optimization problems for independent columns. Although in the sequential case, the method is not as efficient as the block pivoting variant used by other authors, they are very effective in the parallel case, producing satisfactory results for the type of applications where is to be used. For the Lee and Seung method, we propose a reorganization of the algorithm steps that increases the convergence speed and a parallelization of the solution. The article also includes a theoretical and experimental study of the performance obtained with similar matrices to that which arise in applications that have motivated this work.",
    "keywords": [
      "nnmf",
      "parallel computing",
      "multicore architectures",
      "alternating least squares method",
      "lee and seung method"
    ]
  },
  {
    "id": "1743",
    "title": "Top-down mining of frequent closed patterns from very high dimensional data",
    "abstract": "Frequent pattern mining is an essential theme in data mining. Existing algorithms usually use a bottom-up search strategy. However, for very high dimensional data, this strategy cannot fully utilize the minimum Support constraint to prune the rowset search space. In this paper, we propose a new method called top-down mining together with a novel row enumeration tree to make full use of the pruning power of the minimum support constraint. Furthermore, to efficiently check if a rowset is closed, we develop a method called the trace-based method. Based on these methods, an algorithm called TD-Close is designed for mining a complete set of frequent closed patterns. To enhance its performance further, we improve it by using new pruning strategies and new data structures that lead to a new algorithm TTD-Close. Our performance study shows that the top-down strategy is effective in cutting down search space and saving memory space, while the trace-based method facilitates the closeness-checking. As a result, the algorithm TTD-Close outperforms the bottom-up search algorithms such as Carpenter and FPclose in most cases. It also runs faster than TD-Close.  ",
    "keywords": [
      "data mining",
      "association rules",
      "frequent patterns",
      "high dimensional data"
    ]
  },
  {
    "id": "1744",
    "title": "Wall effects on density fluctuations in the GBL thermal lattice gas automatons",
    "abstract": "We measure density fluctuations in a 19-bits thermal lattice gas automaton, in the presence of solid walls. The walls have a prominent effect on the dynamic structure factor. Fluctuating hydrodynamics predicts extra peaks in the spectrum. These extra features are indeed observed in the simulated dynamic structure factor.  ",
    "keywords": [
      "lattice gas automata",
      "gbl-model",
      "fluctuating hydrodynamics"
    ]
  },
  {
    "id": "1745",
    "title": "A theorical model design for ERP software selection process under the constraints of cost and quality: A fuzzy approach",
    "abstract": "Enterprise Resource Planning (ERP) software selection is one of the most important decision making issues covering both qualitative and quantitative factors for organizations. Multiple criteria decision making (MCDM) has been found to be a useful approach to analyze these conflicting factors. Qualitative criteria are often accompanied by ambiguities and vagueness. This makes fuzzy logic a more natural approach to this kind of problems. This study presents a beneficial structure to the managers for use in ERP software vendor selection process. In order to evaluate ERP vendors methodologically, a hierarchical framework is also proposed. As a MCDM tool, we used analytic hierarchy process (AHP) and its fuzzy extension to obtain more decisive judgments by prioritizing criteria and assigning weights to the alternatives. The objective of this paper is to select the most appropriate alternative that meets the customer's requirements with respect to cost and quality constraints. In the end of this study, a real-world case study from Turkey is also presented to illustrate efficiency of the methodology and its applicability in practice.",
    "keywords": [
      "ahp",
      "fuzzy ahp",
      "multiple criteria decision making",
      "software selection"
    ]
  },
  {
    "id": "1746",
    "title": "using a technique from graphic designers to develop innovative system designs",
    "abstract": "Rapid technological change requires that system designers explore potential design spaces widely before committing to a local design space in which to evolve a problem solution. We discuss an approach for doing this, which we base on an analogy with an approach used by graphic designers. We have observed that our colleagues in the graphic design community begin exploring a problem space by generating multiple, divergent design ideas. They then proceed to elaborate them -- extending, combining and discarding -- as the problem space dictates. We illustrate our adaptation of this approach with a case study of our initial design work on a system for supporting self-service sales of information technology (IT).",
    "keywords": [
      "problem space",
      "design space",
      "internet communities"
    ]
  },
  {
    "id": "1747",
    "title": "Building quantitative stereology data files with scion image, a public domain image processing and analysis software",
    "abstract": "Two-dimensional data obtained from a histological cross-section of a tissue can be utilized to obtain three-dimensional information by the methods of quantitative stereology. The resulting quantitative information is useful in both experimental studies and whole-animal investigations for regulatory and safety purposes. Quantitative stereologic analysis requires considerable data collection and calculation and is thus practical only through the use of computer hardware and software. We have previously reported the development of a program, STEREO, which compiles data from carcinogenesis experiments, recording information from tissue sections for the estimation of the number of altered hepatic foci (AHF) per liver and the volume fraction of AHF in liver on a three-dimensional basis. The data file itself was built by measuring tissue and focal transections through a slide-reading process that involved the manual use of a digitizer. In order to increase the speed and efficiency of the analytical process, we have integrated the STEREO program with a public domain software, Scion Image. This software integration involves two portions: the building macros and the interface. Macros for quantitative stereology used in Scion Image were written to customize and simplify the measurement and to generate data needed for building each of the data files. An interface program, BuildFi.exe, was developed to receive data generated from Scion Image and to align sequential tissue plots from up to four serial sections stained with different markers. As a result, the user can store data on a disk in the format of the STEREO data files. By combining STEREO with Scion Image, the slide-reading process is simplified and can be performed automatically. It has proven to be more objective, time saving, and efficient than all earlier versions.",
    "keywords": [
      "quantitative stereology",
      "multistage chemical carcinogenesis",
      "rat",
      "liver",
      "altered hepatic foci ",
      "stereo",
      "scion image"
    ]
  },
  {
    "id": "1748",
    "title": "Numerical study of sloshing liquid in tanks with baffles by time-independent finite difference and fictitious cell method",
    "abstract": "The numerical analysis of liquid sloshing in tanks is a big challenge when the fully nonlinear and viscous effects are all included in the analysis. The analysis will become more complicate as the tank is attached with internal structures, such as baffles. The width of the baffle is very thin compared with the breadth length and the numerical technique used to capture the detailed flow phenomenon (vortex generation and shedding) around the baffle is very rare in the literatures. In this paper, a time-independent finite difference scheme with fictitious cell technique is used to study viscous fluid sloshing in 2D tanks with baffles. The Navier-Stokes equations in a moving coordinate system are derived and they are mapped onto a time-independent and stretched domain. The developed numerical model is rigorously validated by extensive comparisons with reported results. An experiment setup was also made to validate the present numerical sloshing results in a tank with baffles. The method is applied to a number of problems including impulsive flow past a flat plate, sloshing fluid in a 2D tank with a surface-piercing baffle, sloshing fluid in 2D tanks with bottom-mounted baffles. The effects of baffles on the resonant frequency are discussed. The present developed numerical model can successfully analyze the sloshing phenomenon in 2D tanks with internal structures and can be easily extended to 3D model.  ",
    "keywords": [
      "tank sloshing",
      "baffle",
      "time-independent finite difference",
      "fictitious cell"
    ]
  },
  {
    "id": "1749",
    "title": "Real-time optimization of an off-gas distribution system of an iron and titanium plant",
    "abstract": "In this paper, the distribution of off-gas used as fuel by many different plant units in an iron and titanium plant, is studied. An optimization algorithm is applied to find the operating strategy leading to the best use of available off-gas by the different consumers. This is realized by adjusting the flow of the material through different kilns and the stock piles. In addition, the choice of whether or not to operate a certain kiln is treated as a decision variable. The contribution of this paper lies in formulating a linear model describing the system and solving the above mentioned problem as a mixed integer optimization problem in real time. A simulation result is shown and explained.  ",
    "keywords": [
      "real-time optimization",
      "off-gas distribution system",
      "iron and titanium process",
      "mixed-integer programming",
      "kilns"
    ]
  },
  {
    "id": "1750",
    "title": "textual and behavioral views of function changes",
    "abstract": "In this paper, we describe an approach that automatically computes function change information between consecutive revisions along the revision history of C language projects. Function changes are computed at two abstract levels. First, we compute the textual changes between two function revisions. Computed results include function additions and deletions, and the quantity and the ratio of textual change in changed functions across two revisions. Second, we compute the behavioral changes of functions using program slicing techniques. We use an XML-formatted document to represent computed function change information. The function change information, together with the SCM change log, helps maintainers understand code changes between two revisions. The structured format of the function change information also helps create traceability links between the changes and other artifacts. We describe our prototype implementation for computing function changes, and we evaluate our approach through a case study on the Sed project.",
    "keywords": [
      "version control",
      "program slice encoding",
      "program slicing"
    ]
  },
  {
    "id": "1751",
    "title": "A practical and efficient approach to the constrained via minimization problem",
    "abstract": "This paper presents an efficient and practical approach to the Constrained Via Minimization (CVM) problem, which assigns wire segments to the layers, using the minimum number of vias, given a feasible partial routing. The feasible partial routing is first represented by a directed bipartite graph to reflect the mutual constraints. An energy function is then proposed to turn the problem into a cost optimization problem. An efficient heuristic algorithm, combining hill-climbing and simulated annealing, is developed for the cost optimization. The algorithm has the capability to escape from the local minimums and eventually reaches a near-optimal or optimal solution. The proposed method is practical as it can handle many practical constraints such as a multi-way wire split from a single via, and pre-allocation of power nets and terminals. Experimental results show that our proposed method is efficient in handling complex grid-based routing.",
    "keywords": [
      "constrained via minimization",
      "topological via minimization",
      "layer assignment problem",
      "heuristic algorithm",
      "cost optimization"
    ]
  },
  {
    "id": "1752",
    "title": "Image segmentation algorithm by piecewise smooth approximation",
    "abstract": "We propose a novel image segmentation algorithm using piecewise smooth (PS) approximation to image. The proposed algorithm is inspired by four well-known active contour models, i.e., Chan and Vese piecewise constant (PC)/smooth models, the region-scalable fitting model, and the local image fitting model. The four models share the same algorithm structure to find a PC/smooth approximation to the original image; the main difference is how to define the energy functional to be minimized and the PC/smooth function. In this article, pursuing the same idea we introduce different energy functional and PS function to search for the optimal PS approximation of the original image. The initial function with our model can be chosen as a constant function, which implies that the proposed algorithm is robust to initialization or even free of manual initialization. Experiments show that the proposed algorithm is very appropriate for a wider range of images, including images with intensity inhomogeneity and infrared ship images with low contrast and complex background.",
    "keywords": [
      "image segmentation",
      "active contour model",
      "piecewise smooth approximation",
      "level set method",
      "partial differential equation"
    ]
  },
  {
    "id": "1753",
    "title": "Diameter distribution estimation with laser scanning based multisource single tree inventory",
    "abstract": "Tree detection and tree species recognition are bottlenecks of the airborne remote sensing-based single tree inventories. The effect of these factors in forest attribute estimation can be reduced if airborne measurements are aided with tree mapping information that is collected from the ground. The main objective here was to demonstrate the use of terrestrial laser scanning-derived (TLS) tree maps in aiding airborne laser scanning-based (ALS) single tree inventory (multisource single tree inventory, MS-STI) and its capability in predicting diameter distribution in various forest conditions. Automatic measurement of TLS point clouds provided the tree maps and the required reference information from the tree attributes. The study area was located in Evo, Finland, and the reference data was acquired from 27 different sample plots with varying forest conditions. The workflow of MS-STI included: (1) creation of automatic tree map from TLS point clouds, (2) automatic diameter at breast height (DBH) measurement from TLS point clouds, (3) individual tree detection (ITD) based on ALS, (4) matching the ITD segments to the field-measured reference, (5) ALS point cloud metric extraction from the single tree segments and (6) DBH estimation based on the derived metrics. MS-STI proved to be accurate and efficient method for DBH estimation and predicting diameter distribution. The overall accuracy (root mean squared error, RMSE) of the DBH was 36.9mm. Results showed that the DBH accuracy decreased if the tree density (trees/ha) increased. The highest accuracies were found in old-growth forests (tree densities less than 500 stems/ha). MS-STI resulted in the best accuracies regarding Norway spruce (Picea abies (L.) H. Karst.)-dominated forests (RMSE of 29.9mm). Diameter distributions were predicted with low error indices, thereby resulting in a good fit compared to the reference. Based on the results, diameter distribution estimation with MS-STI is highly dependent on the forest structure and the accuracy of the tree maps that are used. The most important development step in the future for the MS-STI and automatic measurements of the TLS point cloud is to develop tree species recognition methods and further develop tree detection techniques. The possibility of using MLS or harvester data as a basis for the required tree maps should also be assessed in the future.",
    "keywords": [
      "remote sensing",
      "multisource",
      "tls",
      "als",
      "diameter distribution"
    ]
  },
  {
    "id": "1754",
    "title": "Lattice structure on some fuzzy algebraic systems",
    "abstract": "In this paper, we study the lattice structure of some fuzzy algebraic systems such as (G-)fuzzy groups, some fuzzy ordered algebras and fuzzy hyperstructures. We prove that under suitable conditions, these structures form a distributive or modular lattice.",
    "keywords": [
      "distributive  lattice",
      " bck-algebra",
      "fuzzy  subgroup",
      "g-fuzzy subgroup",
      "fuzzy hyperideal"
    ]
  },
  {
    "id": "1755",
    "title": "Individual creativity in teams: The importance of communication media mix",
    "abstract": "We use compensatory adaptation and dual coding theories to explore the effects of communication media use on creativity. Our field study results show that high levels of self-esteem and information-based demographic differences positively influence creativity. Social category differences negatively influence creativity. Communication media mix is an important moderator, improving the relationship of self-esteem and social category demographic differences with creativity when individuals have proportionally more mediated communication. The relationship between information-based demographic differences and creativity is attenuated when individuals use proportionally more mediated communication. The results have implications for managers encouraging creativity among a diverse workforce using multiple communication media.",
    "keywords": [
      "communication media",
      "creativity",
      "demographic differences"
    ]
  },
  {
    "id": "1756",
    "title": "The sociological turn in information science",
    "abstract": "This paper explores the history of ` the social' in information science. It traces the influence of social scientific thinking on the development of the field's intellectual base. The continuing appropriation of both theoretical and methodological insights from domains such as social studies of science, science and technology studies, and socio- technical systems is discussed.",
    "keywords": [
      "history",
      "information science",
      "social science",
      "sociology"
    ]
  },
  {
    "id": "1757",
    "title": "Proportion-based robust optimization and team orienteering problem with interval data",
    "abstract": "In this paper, a proportion-based robust optimization approach is developed to deal with uncertain combinatorial optimization problems. This approach assumes that a certain proportion of uncertain coefficients in each solution are allowed to change and optimizes a deterministic model so as to achieve a trade-off between optimality and feasibility when the coefficients change. We apply this approach on team orienteering problem with interval data (TOPID), a variant of vehicle routing problem, which has not yet been studied before. A branch and price algorithm is proposed to solve the robust counterpart by using two novel dominance relations. Finally, numerical study is performed. The results show the usefulness of the proposed robust optimization approach and the effectiveness of our algorithm.",
    "keywords": [
      "uncertainty modeling",
      "robust optimization",
      "combinatorial optimization",
      "team orienteering problem",
      "vehicle routing problem"
    ]
  },
  {
    "id": "1758",
    "title": "Investigation of the performance of trackpoint and touchpads with varied right and left buttons function locations",
    "abstract": "This study investigates the relationships of the following 5 factors with commonly-used task patterns: 4 (2 existing and 2 newly-designed) built-in cursor input devices of notebook PCs, usage experiences, genders, sensitivity of cursor movements, and 5 tasks of input applications (including click, drag-drop, click-select, select-drag-drop, and type-select-click). This experiment reveals that there are significant differences among these factors in the operating times and/or error rates of particular tasks. Although somewhat influenced by the task patterns, the results show that the touchpad with the cursor-tracking pad located on the bottom-center and the right and left buttons on the bottom-left beneath the keyboard, which avoids ulnar and radial deviation and hindrance of text-entry-pointer-manipulation switching, leads to higher performance and preference, while the trackpoint leads to lower performance and preference. In addition, the touchpads with sensitivity values of 10 and 12 for cursor movement are preferred over those with the value of 8.",
    "keywords": [
      "cursor input device",
      "touchpad",
      "input application"
    ]
  },
  {
    "id": "1759",
    "title": "Vertical profiling: Understanding the behavior of object-oriented applications",
    "abstract": "Object-oriented programming languages provide a rich set of features that provide significant software engineering benefits. The increased productivity provided by these features comes at a justifiable cost in a more sophisticated runtime system whose responsibility is to implement these features efficiently. However, the virtualization introduced by this sophistication provides a significant challenge to understanding complete system performance, not found in traditionally compiled languages, such as C or C++. Thus, understanding system performance of such a system requires profiling that spans all levels of the execution stack, such as the hardware, operating system, virtual machine, and application. In this work, we suggest an approach, called vertical profiling, that enables this level of understanding. We illustrate the efficacy of this approach by providing deep understandings of performance problems of Java applications run on a VM with vertical profiling support. By incorporating vertical profiling into a programming environment, the programmer will be able to understand how their program interacts with the underlying abstraction levels, such as application server, VM, operating system, and hardware.",
    "keywords": [
      "measurement",
      "performance",
      "experimentation",
      "vertical profiling",
      "whole-system analysis",
      "perturbation",
      "hardware performance monitors",
      "software performance monitors"
    ]
  },
  {
    "id": "1760",
    "title": "Efficient Parallel Computing-Based Implementation Methods of DCT-Kernel-Based Real-Valued Discrete Gabor Transform and Expansion",
    "abstract": "The existing researches of fast parallel algorithms for DCT-based real-valued discrete Gabor transform and expansion are limited to theoretical analysis. In this paper, parallel computing based implementations of parallel lattice structure algorithm are presented. The communication issues are not considered in original algorithms which enable the efficiency lower than serial fast algorithm in implementation. In view of this, improved implementation methods are proposed both for transform and expansion. The recursive part of the original algorithms is expanded into an iterative form. This makes the interprocess communication converted into serial calculation. Thus, each parallel channel (i.e., process in parallel computing) in the improved method is independent, thereby reducing the interprocess communication greatly. Finally, algorithms are tested on a parallel computer. The experimental results are compared and analyzed, which indicate that the proposed implementation methods are attractive for real-time signal processing as compared to the existing parallel lattice structure algorithm and the fastest serial algorithm.",
    "keywords": [
      "dct kernel",
      "parallel computing",
      "parallel lattice structure",
      "real-valued discrete gabor transform"
    ]
  },
  {
    "id": "1761",
    "title": "Performance of adaptive modulation with optimal switching thresholds for distributed antenna system in composite channels",
    "abstract": "In this paper, the performance of distributed antenna system (DAS) with adaptive modulation (AM) over a composite fading channel which takes path loss, Rayleigh fading, and log-normal shadowing into account is studied. Based on target bit error rate (BER), the AM scheme for DAS with average BER constraints is presented. The optimum switching thresholds (STs) for attaining maximum spectrum efficiency (SE) are derived by using Lagrange optimization method. An effective iterative algorithm based on Newton method for finding the optimal STs is proposed. With these thresholds, the closed-form expression of SE and average BER are derived for performance evaluation. Simulation results for SE and BER are in good agreement with the theoretical analysis. The results show that DAS-AM with optimal STs has higher SE than that with conventional fixed thresholds. Moreover, the proposed AM can fulfill the target BER for different signal-to-noise ratios (SNRs).",
    "keywords": [
      "distributed antenna systems",
      "adaptive modulation",
      "thresholds optimization",
      "spectrum efficiency",
      "composite fading channel"
    ]
  },
  {
    "id": "1762",
    "title": "The choice of the best among the shortest routes in transparent optical networks",
    "abstract": "This work introduces the problem of the best choice among M combinations of the shortest paths for dynamic provisioning of lightpaths in all-optical networks. To solve this problem in an optimized way (shortest path and load balance), a new fixed routing algorithm, named Best among the Shortest Routes (BSR), is proposed. The BSRs performance is compared in terms of blocking probability and network utilization with Dijkstras shortest path algorithm and others algorithms proposed in the literature. The evaluated scenarios include several representative topologies for all-optical networking and different wavelength conversion architectures. For all studied scenarios, BSR achieved superior performance.",
    "keywords": [
      "transparent optical networks",
      "routing",
      "shortest path",
      "load balance"
    ]
  },
  {
    "id": "1763",
    "title": "Dynamic behavior of DCT and DDT formulations for the Sanger neural network",
    "abstract": "In this paper, the behavior of the Sanger hebbian artificial neural networks is analyzed. Hebbian networks are employed to implement principal component analysis (PCA), and several improvements over the original model due to Oja have been developed in the last two decades. Among them, Sanger model is designed to directly provide the eigenvectors of the correlation matrix. The behavior of these models has been traditionally considered on a deterministic continuous-time (DCT) formulation whose validity is justified under some hypotheses on the specific asymptotic behavior of the learning gain. In practical applications, these assumptions cannot be guaranteed. This paper addresses a comparative study with a deterministic discrete-time (DDT) formulation that characterizes the average evolution of the net, preserving the discrete-time form of the original network and gathering a more realistic behavior of the learning gain. The results thoroughly characterize the relationship between the learning gain and the eigenvalue structure of the correlation matrix.",
    "keywords": [
      "sanger network",
      "hebbian models",
      "dynamic behavior",
      "dct and ddt formulations"
    ]
  },
  {
    "id": "1764",
    "title": "Nicotinamide Adenine Dinucleotide Phosphate Oxidase (NADPH Oxidase) P22 Phox C242T Gene Polymorphism in Type 1 Diabetes",
    "abstract": "Type 1 diabetes is caused by the immune-mediated destruction of insulin-secreting pancreatic ? cells and is thought to be an autoimmune disease resulting from a complex interaction of genetic and environmental factors. In animal models of type 1 diabetes, macrophages and their products, superoxides, have central roles in the ? cell destruction, but in humans their roles remain unclear. Nicotinamide adenine dinucleotide phosphate (NADPH) oxidase produces superoxide in macrophages, and its essential component, p22 phox, is a critical enzyme for superoxide production. The C242T polymorphism in the p22 phox coding gene has been reported to be associated with reduced oxidase activity. We therefore investigated whether the p22 phox gene polymorphism affected the susceptibility to and clinical course of type 1 diabetes. We examined 287 Japanese type 1 diabetic patients and 425 unrelated nondiabetic subjects. In addition, we allocated the diabetic patients to the following three groups",
    "keywords": [
      "nadph oxidase",
      "type 1 diabetes",
      "polymorphism",
      "oxidative stress"
    ]
  },
  {
    "id": "1765",
    "title": "Why the whole is less than the sum of its parts: Examining knowledge management in acquisitions",
    "abstract": "This paper seeks to explain why an acquiring organization was unable to leverage the expertise of the acquired organization even though both organizations were highly successful in their own right prior to the acquisition. It offers a knowledge-based perspective by teasing out the essential knowledge attributes pertinent to acquisitions. The research was carried out using an interpretative case study methodology at a Singapore-based multinational organization in the semi-conductor industry. The main data collection method used was face-to-face interviews with some 28 staff from October 2006 to December 2006, almost 2 years after the acquisition exercise. The findings suggest that the lackluster outcome could be traced to three knowledge attributes, namely, accessibility, applicability and cumulativeness. In conclusion, a number of practical and research implications are highlighted.",
    "keywords": [
      "knowledge management",
      "merger and acquisition",
      "semi-conductor",
      "case study"
    ]
  },
  {
    "id": "1766",
    "title": "Study of the impact of winding form and film thickness on thin-film inductors",
    "abstract": "Using a DC-magnetron sputtering system, we fabricated several forms of double-sided coupling thin-film inductors on a PCB (Printed Circuit Board). Due to the effects of winding forms and thickness of the film, the inductance, Q factor and resonance frequency of the inductors are different. Of the three types of inductors covering the same area, the meander inductor has the highest resonance frequency (up to 400 MHz), the circular-spiral inductor has the largest inductance (up to 1 ?H/cm2) and Q factor (up to 25) at low frequency, and the hexagon-spiral inductor has moderate inductance and Q factor at an intermediate frequency. So the proper choice of winding form will depend on the specific application.",
    "keywords": [
      "thin film inductor",
      "inductance",
      "magnetic film",
      "thickness of the film"
    ]
  },
  {
    "id": "1767",
    "title": "Automating data-model workflows at a level 12 HUC scale: Watershed modeling in a distributed computing environment",
    "abstract": "Data-model workflows for reproducibility and provenance of distributed hydrological models. Essential Terrestrial Variable (ETV) datasets used to compute hydrological models. HydroTerre data workflows to create rapid data inputs for HUC-12 catchment scales. Infrastructure to support models and big data at high resolution from multiple federal sources.",
    "keywords": [
      "distributed hydrological model",
      "data workflows",
      "data-model workflows",
      "model workflows",
      "provenance",
      "essential terrestrial variables",
      "hydroterre",
      "pihm",
      "geographic information science",
      "data as a service",
      "model as a service"
    ]
  },
  {
    "id": "1768",
    "title": "Reinforcement Learning for Multiple Access Control in Wireless Sensor Networks: Review, Model, and Open Issues",
    "abstract": "Wireless sensor networking is a viable communication technology among low-cost and energy-limited sensor nodes deployed in an environment. Due to high operational features, the application area of this technology is extended significantly but with some energy related challenges. One main cause of the nodes energy wasting in these networks is idle listening characterized with no communication activity. This drawback can be mitigated by the means of energy-efficient multiple access control schemes so as to minimize idle listening. In this paper, we discuss the applicability of distributed learning algorithms namely reinforcement learning towards multiple access control (MAC) in wireless sensor networks. We perform a comparative review of relevant work in the literature and then present a cooperative multi agent reinforcement learning framework for MAC design in wireless sensor networks. Accordingly, the paper concludes with some major challenges and open issues of distributed MAC design using reinforcement learning.",
    "keywords": [
      "multiple access control",
      "reinforcement learning",
      "scheduling",
      "wireless sensor networks",
      "optimization"
    ]
  },
  {
    "id": "1769",
    "title": "Generation and Evaluation of Business Continuity Processes using Algebraic Graph Transformation and the mCRL2 Process Algebra",
    "abstract": "Critical business processes can fail. Therefore, continuity processes are needed as back-up solutions. Today, those continuity processes are set up and maintained manually. They are mostly based on best practices that focus on specific continuity scenarios, Nevertheless, failures can occur in new and unforeseen combinations. As a consequence, a given business continuity plan needs to handle such situations as well. For this purpose, we present a technique for the generation and validation of the universe of continuity processes given a critical business process at Credit Suisse. The presented approach uses a combination of formal methods in the area of algebraic graph transformation and process algebra encompassing modal logic. The overall approach prepares for a sound evaluation of the effectiveness and efficiency of such plans. It uses formal tools, not standard software engineering solutions, to benefit from formal guarantees that facilitate the implementation of local and global security requirements. ACM Classification: C.3 Special-Purpose and Application-based Systems (Process control Systems), G.2.3 Applications, 1.6.4 Model Validation and Analysis, 1.6.5 Model Development",
    "keywords": [
      "business continuity",
      "business process",
      "algebraic graph transformation",
      "process algebra",
      "generation",
      "evaluation",
      "enterprise modeling"
    ]
  },
  {
    "id": "1770",
    "title": "Ranking multicriteria alternatives: The method ZAPROS III",
    "abstract": "The new version of the method for the construction of partial order on the set of multicriteria alternatives is presented. This method belongs to the family of verbal decision analysis (VDA) methods and gives a more efficient means of problem solution. The method is based on psychologically valid operations for information elicitation from a decision maker: comparisons of two distances between the evaluations on the ordinal scales of two criteria. The information received from a decision maker is used for the construction of a binary relation between a pair of alternatives which yields preference, indifference and incomparability relations. The method allows construction of a partial order on the set of given alternatives as well as on the set of all possible alternatives. The illustrative example is given.",
    "keywords": [
      "decision theory",
      "multiple criteria",
      "behavior",
      "project management"
    ]
  },
  {
    "id": "1771",
    "title": "Proving sequential function chart programs using timed automata",
    "abstract": "Applications described by sequential function chart (SFC) often being critical, we have investigated the possibilities of program checking. In particular, physical time can be handled by SFC programs using temporizations, which is why we are interested in the quantitative temporal properties. We have proposed a modeling of SFC in timed automata, a formalism which takes time into account. In this modeling, we use the physical constraints of the environment. Verification of properties can be carried out using the model-checker Kronos. We apply this method to SFC programs of average size like that of the control part of the production cell Korso. The size of the programs remains however a limit and we are studying the means of solving this problem.  ",
    "keywords": [
      "formal methods",
      "checking",
      "timed automata",
      "tctl logic",
      "sequential function chart "
    ]
  },
  {
    "id": "1772",
    "title": "A Fast Ray-Tracing Using Bounding Spheres and Frustum Rays for Dynamic Scene Rendering",
    "abstract": "Ray tracing is one of the most popular techniques for generating photo-realistic images Extensive research and development work has made interactive static scene rendering realistic This paper deals with in dynamic scene rendering in which not only the eye point but also the objects in the scene change then 3D locations every frame In order to realize interactive dynamic scene rendering R-nos (Ray Tracing based on Ray Plane and Bounding Sphere) which utilizes the coherency in rays. objects, and grouped-rays. is Introduced RTRPS uses bounding spheres as the spatial data structure which utilizes the coherency in objects By using bounding spheres, RTRPS can ignore the rotation of moving objects within a sphere, and shorten the update time between frames RTRPS utilizes the coherency in rays by merging rays into a ray-plane, assuming that the secondary rays and shadow rays are shot through an aligned grid Since a pair of ray-planes shares an original ray. the intersection for the ray can be completed using the coherency m the ray-planes Because of die three kinds of coherency, RTRPS can significantly reduce the number of intersection tests for ray tracing Further acceleration techniques for I ay-plane-sphere and ray-triangle intersection are also presented A parallel projection technique converts a 3D vector inner product operation into a 2D operation and reduces the number of floating point operations Techniques based on frustum culling and binary-tree structured ray-planes optimize the order of intersection tests between ray-planes and a sphere. resulting in 50% to 90% reduction of intersection tests Two ray-triangle intersection techniques are also introduced which are effective when a lame number of rays are packed into a ray-plane Our performance evaluations indicate that RTRPS gives 13 to 392 times speed up in comparison with a ray tracing algorithm without organized rays and spheres We found out that RTRPS also provides competitive performance even if only primary rays are used.",
    "keywords": [
      "computer graphics",
      "ray tracing",
      "intersection test",
      "bounding volume",
      "bounding sphere"
    ]
  },
  {
    "id": "1773",
    "title": "Sobolev Duals for Random Frames andQuantization of Compressed Sensing Measurements",
    "abstract": "Quantization of compressed sensing measurements is typically justified by the robust recovery results of Cands, Romberg and Tao, and of Donoho. These results guarantee that if a uniform quantizer of step size ? is used to quantize m measurements y=?x of a k-sparse signal xN , where ? satisfies the restricted isometry property, then the approximate recovery x # via ? 1-minimization is within O(?) of x. The simplest and commonly assumed approach is to quantize each measurement independently. In this paper, we show that if instead an rth-order(SigmaDelta) quantization scheme with the same output alphabet is used to quantize y, then there is an alternative recovery method via Sobolev dual frames which guarantees a reduced approximation error that is of the order ?(k/m)(r?1/2)? for any 0<?<1, if m? r,? k(logN)1/(1). The result holds with high probability on the initial draw of the measurement matrix ? from the Gaussian distribution, and uniformly for all k-sparse signals x whose magnitudes are suitably bounded away from zero on their support.",
    "keywords": [
      "quantization",
      "finite frames",
      "random frames",
      "alternative duals",
      "compressed sensing",
      ""
    ]
  },
  {
    "id": "1774",
    "title": "Scheduling with fuzzy delays and fuzzy precedences",
    "abstract": "A problem of scheduling jobs nonpreemptively on a single machine subject to time delay constraints and precedence constraints is considered. Time delay constraints and precedence constraints are fuzzified. Schedules are evaluated not only by their makespan but also by degrees of satisfaction with time delays and degrees of satisfaction with fuzzy precedence.  ",
    "keywords": [
      "fuzzy system models",
      "fuzzy control",
      "multiple criteria evaluation"
    ]
  },
  {
    "id": "1775",
    "title": "BI-FEATURE VERIFICATION FOR PALMPRINT IMAGES CAPTURED IN PEGLESS SCENARIOS",
    "abstract": "This paper presents a reliable and robust palmprint verification approach that involves using a bi-feature, biometric, palmprint feature-point number (FPN) and a histogram of oriented gradient (HOG). The bi-feature was fused and verified using a support vector machine (SVM) at the feature level. The approach has the advantages of capturing palm images in pegless scenarios with a low cost and low-resolution (100 dpi) digital scanner, and one sensor can capture palmprint bi-feature information. The low-resolution images result in a smaller database. Nine thousand palmprint images were collected from 300 people to verify the validity of the proposed approach. The results showed an accurate classification rate of 99.04%. The experimental results demonstrated that the proposed approach is feasible and effective in palmprint verification. Our findings will help extend palmprint verification technology to security access control systems.",
    "keywords": [
      "palmprint verification",
      "support vector machine",
      "bifeature",
      "multiresolution representation",
      "histogram of oriented gradient"
    ]
  },
  {
    "id": "1776",
    "title": "A total least squares proximal support vector classifier for credit risk evaluation",
    "abstract": "In this paper, a total least squares (TLS) version of proximal support vector machines (PSVM) is proposed for credit risk evaluation. The formulation of this new model is different from the original PSVM model, so a novel iterative algorithm is proposed to solve this model. A simulation test is first implemented on a classic two-spiral dataset, and then an empirical experiment is conducted on two publicly available credit datasets. The experimental results show that the proposed total least squares PSVM (TLS-PSVM) is at least comparable with PSVM and better than other models including standard SVM model.",
    "keywords": [
      "total least squares method",
      "proximal support vector machine",
      "credit risk evaluation"
    ]
  },
  {
    "id": "1777",
    "title": "Improving the scalability of hyperspectral imaging applications on heterogeneous platforms using adaptive run-time data compression",
    "abstract": "Latest generation remote sensing instruments (called hyperspectral imagers) are now able to generate hundreds of images, corresponding to different wavelength channels, for the same area on the surface of the Earth. In previous work, we have reported that the scalability of parallel processing algorithms dealing with these high-dimensional data volumes is affected by the amount of data to be exchanged through the communication network of the system. However, large messages are common in hyperspectral imaging applications since processing algorithms are pixel-based, and each pixel vector to be exchanged through the communication network is made up of hundreds of spectral values. Thus, decreasing the amount of data to be exchanged could improve the scalability and parallel performance. In this paper, we propose a new framework based on intelligent utilization of wavelet-based data compression techniques for improving the scalability of a standard hyperspectral image processing chain on heterogeneous networks of workstations. This type of parallel platform is quickly becoming a standard in hyperspectral image processing due to the distributed nature of collected hyperspectral data as well as its flexibility and low cost. Our experimental results indicate that adaptive lossy compression can lead to improvements in the scalability of the hyperspectral processing chain without sacrificing analysis accuracy, even at sub-pixel precision levels.",
    "keywords": [
      "heterogeneous parallel computing",
      "adaptive run-time data compression",
      "wavelet transform",
      "hyperspectral imaging",
      "remote sensing"
    ]
  },
  {
    "id": "1778",
    "title": "Williamson matrices up to order 59",
    "abstract": "A recent result of Schmidt has brought Williamson matrices back into the spotlight. In this article, a new algorithm is introduced to search for hard to find Williamson matrices. We find all nonequivalent Williamson matrices of odd order n up to n = 59. It turns out that there are none for n = 35, 47, 53, 59 and it seems that the Turyn class may be the only infinite class of these matrices.",
    "keywords": [
      "symmetric circulant matrices",
      "williamson matrices",
      "hadamard matrices"
    ]
  },
  {
    "id": "1779",
    "title": "A calculational approach to path-based properties of the EisensteinStern and SternBrocot trees via matrix algebra",
    "abstract": "This paper proposes a calculational approach to prove properties of the EisensteinStern and the SternBrocot trees. The calculational style of reasoning is enabled by a matrix formulation well-suited to formulate path-based properties. We show that nodes with palindromic paths contain the same rational in both the EisensteinStern and SternBrocot trees. We show how certain numerators and denominators in these trees can be written as the sum of two squares. We show how we can construct Sierpinski's triangle from these trees of rationals.",
    "keywords": [
      "sternbrocot tree",
      "eisensteinstern tree ",
      "calculational method",
      "euclid's algorithm",
      "sierpi?ski's triangle",
      "rational number"
    ]
  },
  {
    "id": "1780",
    "title": "Migrating autonomous objects in a WAN environment",
    "abstract": "Along with the fast progress the interconnection of computer systems makes the need for enterprise-wide distributed solutions grows. These systems have to support hundreds or even thousands of sites located all over the world. The distances between the sites will impose high communication costs on distributed activities thus significantly increasing their response times. This problem can be alleviated through migrating objects to the sites where they are needed. However, migration in such systems cannot be managed globally. Therefore, we present a migration protocol for (autonomous) objects which enables them autonomously to decide whether or not and which site to migrate to, thus making the system self-tuning. We also present two migration strategies objects can use to derive their migration decisions. The first one finds the optimal placement for an object while the second strategy in some cases returns a sub-optimal location but induces lower computation costs. The two strategies are evaluated through benchmarks in a distributed system of autonomous objects. The experiments show a significant decrease in communication costs when migration is employed.",
    "keywords": [
      "distributed systems",
      "object migration",
      "autonomous objects",
      "wide-area networks"
    ]
  },
  {
    "id": "1781",
    "title": "Recent advances in fuzzy arithmetics",
    "abstract": "In the present paper we propose a comparative overview of some recent results in fuzzy arithmetics. This study is motivated by the fact that the multiplication of two trapezoidal fuzzy numbers by using Zadeh's extension principle is not of trapezoidal shape. Since in several applications these are the fuzzy numbers which are only admitted, the result of the multiplication is either approximated by a trapezoidal fuzzy number, either the multiplication is given in such a way. that the result is of trapezoidal shape. We study several approaches to solve this problem both from the theoretical and practical point of view.",
    "keywords": [
      "fuzzy numbers",
      "fuzzy arithmetics"
    ]
  },
  {
    "id": "1782",
    "title": "The initial development of the WebMedQual scale: Domain assessment of the construct of quality of health web sites",
    "abstract": "To develop a comprehensive instrument assessing quality of health-related web sites. Phase I consisted of a literature review to identify constructs thought to indicate web site quality and to identify items. During content analysis, duplicate items were eliminated and items that were not clear, meaningful, or measurable were reworded or removed. Some items were generated by the authors. Phase II: a panel consisting of six healthcare and MIS reviewers was convened to assess each item for its relevance and importance to the construct and to assess item clarity and measurement feasibility. Three hundred and eighty-four items were generated from 26 sources. The initial content analysis reduced the scale to 104 items. Four of the six expert reviewers responded; high concordance on the relevance, importance and measurement feasibility of each item was observed: 3 out of 4, or all raters agreed on 7685% of items. Based on the panel ratings, 9 items were removed, 3 added, and 10 revised. The WebMedQual consists of 8 categories, 8 sub-categories, 95 items and 3 supplemental items to assess web site quality. The constructs are: content (19 items), authority of source (18 items), design (19 items), accessibility and availability (6 items), links (4 items), user support (9 items), confidentiality and privacy (17 items), e-commerce (6 items). The WebMedQual represents a first step toward a comprehensive and standard quality assessment of health web sites. This scale will allow relatively easy assessment of quality with possible numeric scoring.",
    "keywords": [
      "internet",
      "quality",
      "measurement",
      "rating information",
      "scale",
      "psychometrics/methods"
    ]
  },
  {
    "id": "1783",
    "title": "An information-theoretic approach to spectral variability, similarity, and discrimination for hyperspectral image analysis",
    "abstract": "A hyperspectral image can be considered as an image cube where the third dimension is the spectral domain represented by hundreds of spectral wavelengths. As a result, a hyperspectral image pixel is actually a column vector with dimension equal to the number of spectral bands and contains valuable spectral information that can be used to account for pixel variability, similarity and discrimination. In this correspondence, we present a new hyperspectral measure, Spectral Information Measure (SIM), to describe spectral variability and two criteria, spectral information divergence and spectral discriminatory probability, for spectral similarity and discrimination, respectively. The spectral information measure is an information-theoretic measure which treats each pixel as a random variable using its spectral signature histogram as the desired probability distribution. Spectral Information Divergence (SID) compares the similarity between two pixels by measuring the probabilistic discrepancy between two corresponding spectral signatures. The spectral discriminatory probability calculates spectral probabilities of a spectral database (library) relative to a pixel to be identified so as to achieve material identification. In order to compare the discriminately power of one spectral measure relative to another, a criterion is also introduced for performance evaluation, which is based on the power of discriminating one pixel from another relative to a reference pixel. The experimental results demonstrate that the new hyperspectral measure can characterize spectral variability more effectively than the commonly used Spectral Angle Mapper (SAM).",
    "keywords": [
      "hyperspectral image",
      "spectral angle mapper",
      "spectral discriminatory entropy",
      "spectral discriminatory power",
      "spectral discriminatory probability",
      "spectral information divergence",
      "spectral information measure"
    ]
  },
  {
    "id": "1784",
    "title": "Sparse marginbased discriminant analysis for feature extraction",
    "abstract": "The existing margin-based discriminant analysis methods such as nonparametric discriminant analysis use K-nearest neighbor (K-NN) technique to characterize the margin. The manifold learningbased methods use K-NN technique to characterize the local structure. These methods encounter a common problem, that is, the nearest neighbor parameter K should be chosen in advance. How to choose an optimal K is a theoretically difficult problem. In this paper, we present a new margin characterization method named sparse marginbased discriminant analysis (SMDA) using the sparse representation. SMDA can successfully avoid the difficulty of parameter selection. Sparse representation can be considered as a generalization of K-NN technique. For a test sample, it can adaptively select the training samples that give the most compact representation. We characterize the margin by sparse representation. The proposed method is evaluated by using AR, Extended Yale B database, and the CENPARMI handwritten numeral database. Experimental results show the effectiveness of the proposed method; its performance is better than some other state-of-the-art feature extraction methods.",
    "keywords": [
      "sparse margin",
      "dimensional reduction",
      "feature extraction"
    ]
  },
  {
    "id": "1785",
    "title": "norm-oriented programming of electronic institutions",
    "abstract": "Norms constitute a powerful coordination mechanism among heterogeneous agents. We propose means to specify and explicitly manage the normative positions of agents (permissions, prohibitions and obligations), with which distinct deontic notions and their relationships can be captured. Our rule-based formalism includes constraints for more expressiveness and precision and allows the norm-oriented programming of electronic institutions: normative aspects are given a precise computational interpretation. Our formalism has been conceived as a machine language to which other higher-level normative languages can be mapped, allowing their execution.",
    "keywords": [
      "multi-agent system programming",
      "norms",
      "electronic institutions"
    ]
  },
  {
    "id": "1786",
    "title": "parallel performance tuning for haskell",
    "abstract": "Parallel Haskell programming has entered the mainstream with support now included in GHC for multiple parallel programming models, along with multicore execution support in the runtime. However, tuning programs for parallelism is still something of a black art. Without much in the way of feedback provided by the runtime system, it is a matter of trial and error combined with experience to achieve good parallel speedups. This paper describes an early prototype of a parallel profiling system for multicore programming with GHC. The system comprises three parts: fast event tracing in the runtime, a Haskell library for reading the resulting trace files, and a number of tools built on this library for presenting the information to the programmer. We focus on one tool in particular, a graphical timeline browser called ThreadScope. The paper illustrates the use of ThreadScope through a number of case studies, and describes some useful methodologies for parallelizing Haskell programs.",
    "keywords": [
      "parallel profiling",
      "functional programming"
    ]
  },
  {
    "id": "1787",
    "title": "Input variable selection in time-critical knowledge integration applications: A review, analysis, and recommendation paper",
    "abstract": "Input Variable Selection (IVS) helps when processing system inputs is computationally heavy. A framework to accommodate high level perspective of different approaches to IVS is provided. Sensitivity analysis (SA) helps IVS in heterogeneous input variables with time constraint. Event-based SA proves fit by its application in an industrial drilling disaster prevention problem.",
    "keywords": [
      "input variable selection",
      "time-critical control",
      "dimensionality reduction",
      "sensitivity analysis",
      "supervisory control and data acquisition"
    ]
  },
  {
    "id": "1788",
    "title": "Applying Web usage mining for personalizing hyperlinks in Web-based adaptive educational systems",
    "abstract": "Nowadays, the application of Web mining techniques in e-learning and Web-based adaptive educational systems is increasing exponentially. In this paper, we propose an advanced architecture for a personalization system to facilitate Web mining. A specific Web mining tool is developed and a recommender engine is integrated into the AHAI system in order to help the instructor to carry out the whole Web mining process. Our objective is to be able to recommend to a student the most appropriate links/Web pages within the AHAI system to visit next. Several experiments are carried out with real data provided by Eindhoven University of Technology students in order to test both the architecture proposed and the algorithms used. Finally, we have also described the meaning of several recommendations, starting from the rules discovered by the Web mining algorithms.  ",
    "keywords": [
      "education and e-learning",
      "adaptive hypermedia",
      "recommender system",
      "data mining",
      "web mining"
    ]
  },
  {
    "id": "1789",
    "title": "A numerical study of multiple imputation methods using nonparametric multivariate outlier identifiers and depth-based performance criteria with clinical laboratory data",
    "abstract": "It is well known that if a multivariate outlier has one or more missing component values, then multiple imputation (MI) methods tend to impute nonextreme values and make the outlier become less extreme and less likely to be detected. In this paper, nonparametric depth-based multivariate outlier identifiers are used as criteria in a numerical study comparing several established methods of MI as well as a new proposed one, nine in all, in a setting of several actual clinical laboratory data sets of different dimensions. Two criteria, an 'outlier recovery probability' and a 'relative accuracy measure', are developed, based on depth functions. Three outlier identifiers, based on Mahalanobis distance, robust Mahalanobis distance, and generalized principle component analysis are also included in the study. Consequently, not only the comparison of imputation methods but also the comparison of outlier detection methods is accomplished in this study. Our findings show that the performance of an MI method depends on the choice of depth-based outlier detection criterion, as well as the size and dimension of the data and the fraction of missing components. By taking these features into account, an MI method for a given data set can be selected more optimally.",
    "keywords": [
      "multiple imputation",
      "multivariate",
      "nonparametric",
      "outlier detection",
      "depth functions",
      "missing values"
    ]
  },
  {
    "id": "1790",
    "title": "Self-Configurable FPGA-Based Computer Systems",
    "abstract": "Method of information processing in reconfigurable computer systems is formulated and its improvements that allow an information processing efficiency to increase are proposed. New type of high-performance computer systems, which are named self-configurable FPGA-based computer systems and perform information processing according to this improved method, is proposed. The structure of self-configurable FPGA-based computer systems, rules of application of computer software and hardware means, which are necessary for these systems implementation, are described and their execution time characteristics are estimated. The directions for further works are discussed.",
    "keywords": [
      "field programmable gate arrays",
      "high performance computing",
      "reconfigurable architectures",
      "reconfigurable logic",
      "self-configurable computer systems"
    ]
  },
  {
    "id": "1791",
    "title": "An in-silico model of the biosynthesis of neurotransmitter glutamate, elucidates the complex regulatory role of glucocorticoids in neurotransmitter glutamate release",
    "abstract": "An in-silico model, of the glucocorticoid regulated glutamate release, in rat hippocampal tissue, is constructed. The model permits the pseudo-steady state estimation of various fluxes, experimentally impossible to measure, from a set of measured rates. Estimates of the astrocytic pyruvate carboxylase reaction and the neuronal TCA cycle rates are correlated with different dexamethasone concentrations, in order to extrapolate explicit kinetic equations. The model suggests that the observed effects of glucocorticoids can be attributed to the inhibitory actions of dexamethasone on two competing pathways, that of the neuronal TCA cycle and the biosynthetic pathway of neurotransmitter glutamate.",
    "keywords": [
      "neurotransmitter glutamate",
      "glucocorticoids",
      "dexamethasone",
      "metabolic flux analysis of neurotransmitter glutamate",
      "in-silico model of biosynthesis of neurotransmitter glutamate"
    ]
  },
  {
    "id": "1792",
    "title": "A Bayesian approach for object classification based on clusters of SIFT local features",
    "abstract": "Several methods have been presented in the literature that successfully used SIFT features for object identification, as they are reasonably invariant to translation, rotation, scale, illumination and partial occlusion. However, they have poor performance for classification tasks. In this work, SIFT features are used to solve object class recognition problems in images using a two-step process. In its first step, the proposed method performs clustering on the extracted features in order to characterize the appearance of the different classes. Then, in the classification step, it uses a three layer Bayesian network for object class recognition. Experiments show quantitatively that clusters of SIFT features are suitable to represent classes of objects. The main contributions of this paper are the introduction of a Bayesian network approach in the classification step to improve performance in an object class recognition task, and a detailed experimentation that shows robustness to changes in illumination, scale, rotation and partial occlusion.",
    "keywords": [
      "object class recognition",
      "local features",
      "sift",
      "clustering",
      "bayesian networks"
    ]
  },
  {
    "id": "1793",
    "title": "LabeledIn: Cataloging labeled indications for human drugs",
    "abstract": "A semi-automated framework to annotate drug labels for drug indications is proposed. 500 drug labels corresponding to 250 frequently accessed drugs are double-annotated. Annotation achieves 88% agreement and establishes 7805 drugdisease relationships. The results are computable, precise, and linked to the source drug labels. The resultant resource complements existing resources and is made publicly available.",
    "keywords": [
      "corpus annotation",
      "drug labels",
      "drug indications",
      "natural language processing"
    ]
  },
  {
    "id": "1794",
    "title": "Spatial ordered weighted averaging: incorporating spatially variable attitude towards risk in spatial multi-criteria decision-making",
    "abstract": "The paper discusses a decompositionanalysisaggregation approach to multi-criteria spatial decision-making and proposes a novel aggregation method applicable to problems of the object-location or suitability for application type, concentrating on methodological rather than software development aspects. The approach allows the decision maker to: (a) break the problem down into a series of elementary (easier to understand) problems, (b) analyse them (in the broad sense of the word), and then (c) produce an answer for the complex problem by aggregating the answers derived for the elementary problems. The choice of methodology used for this aggregation is very important as different aggregating techniques may yield different results to the (same) original problem. The method presented here, which is in effect an extension of the ordered weighted averaging (OWA) method into a spatial decision-making technique, is termed spatial ordered weighted averaging (SOWA). The main advantage of the method proposed is the incorporation of spatially variable attitude to risk into the decision-making process. The mathematical background of the method and an example of its application in urban water management are presented and discussed. The authors suggest that the method could be useful as an analytical and decision-making tool for the incorporation of spatially variable risk perception in GIS-based decision support systems.",
    "keywords": [
      "multi-criteria spatial decision support",
      "ordered weighted averaging",
      "spatially variable risk perception",
      "suitability evaluation"
    ]
  },
  {
    "id": "1795",
    "title": "A family of multi-point flux approximation schemes for general element types in two and three dimensions with convergence performance",
    "abstract": "A family of flux-continuous, locally conservative, control-volume-distributed multi-point flux approximation (CVD-MPFA) schemes has been developed for solving the general geometry-permeability tensor pressure equation on structured and unstructured grids. These schemes are applicable to the full-tensor pressure equation with generally discontinuous coefficients and remove the O(1) errors introduced by standard reservoir simulation schemes when applied to full-tensor flow approximation. The family of flux-continuous schemes is characterized by a quadrature parameterization. Improved numerical convergence for the family of CVD-MPFA schemes using the quadrature parameterization has been observed for structured and unstructured grids in two dimensions. The CVD-MPFA family cell-vertex formulation is extended to classical general element types in 3-D including prisms, pyramids, hexahedra and tetrahedra. A numerical convergence study of the CVD-MPFA schemes on general unstructured grids comprising of triangular elements in 2-D and prismatic, pyramidal, hexahedral and tetrahedral shape elements in 3-D is presented. ",
    "keywords": [
      "cvd",
      "mpfa",
      "reservoir",
      "pressure",
      "discretization",
      "unstructured",
      "convergence"
    ]
  },
  {
    "id": "1796",
    "title": "A fictitious domain approach for the simulation of dense suspensions",
    "abstract": "Low Reynolds number concentrated suspensions do exhibit an intricate physics which can be partly unraveled by the use of numerical simulation. To this end, a Lagrange multiplier-free fictitious domain approach is described in this work. Unlike some methods recently proposed, the present approach is fully Eulerian and therefore does not need any transfer between the Eulerian background grid and some Lagrangian nodes attached to particles. Lubrication forces between particles play an important role in the suspension rheology and have been properly accounted for in the model. A robust and effective lubrication scheme is outlined which consists in transposing the classical approach used in Stokesian Dynamics to our present direct numerical simulation. This lubrication model has also been adapted to account for solid boundaries such as walls. Contact forces between particles are modeled using a classical Discrete Element Method (DEM), a widely used method in granular matter physics. Comprehensive validations are presented on various one-particle, two-particle or three-particle configurations in a linear shear flow as well as some O(103) O ( 10 3 ) and O(104) O ( 10 4 ) particle simulations.",
    "keywords": [
      "suspensions",
      "fictitious domain",
      "lubrication",
      "discrete element method"
    ]
  },
  {
    "id": "1797",
    "title": "Optimized admission control scheme for coexisting femtocell, wireless and wireline networks",
    "abstract": "The most important challenge for the implementation of the Future Internet is to make the heterogeneity of access technologies transparent to the end user. Compared to the general case where the interworking networks are independent, the case of femtocells interworking with pre-existing wireless networks poses more challenges due to the sharing of the same backhaul capacity. Therefore, while a user is practically able to initiate the same service through multiple network interfaces, he is allocated capacity from the same capacity pool. However, while the femtocell inherits the QoS mechanisms of cellular networks and is able to provide a reliable CAC, this does not apply to the IP-based networks and that may drastically affect the performance of the femtocell. Hence, we propose an integrated Dynamic Service Admission Control (DSAC) framework for coexisting femtocell, wireless and wireline network environments. In particular, DSAC is able to provide QoS guarantees as a conventional capacity partitioning scheme while at the same time offers better performance in terms of acceptance probability and capacity utilization especially when short term variations of traffic load composition occur.",
    "keywords": [
      "femtocell",
      "admission control",
      "future internet",
      "qos"
    ]
  },
  {
    "id": "1798",
    "title": "Chemosensitizing acridones: In vitro calmodulin dependent cAMP phosphodiesterase inhibition, docking, pharmacophore modeling and 3D QSAR studies",
    "abstract": "Calmodulin inhibitors have proved to play a significant role in sensitizing MDR cancer cells by interfering with cellular drug accumulation. The present investigation focuses on the evaluation of in vitro inhibitory efficacy of chloro acridones against calmodulin dependent cAMP phosphodiesterase (PDE1c). Moreover, molecular docking of acridones was performed with PDE1c in order to identify the possible protein ligand interactions and results thus obtained were compared with in vitro data. In addition an efficient pharmacophore model was developed from a set of 38 chemosensitizing acridones effective against doxorubicin resistant (HL-60/DX) cancer cell lines. Pharmacophoric features such as one hydrogen bond acceptor, one hydrophobic region, a positive ion group and three aromatic rings i.e., AHPRRR have been identified. Ligand based 3D-QSAR was also performed by employing partial least square regression analysis.",
    "keywords": [
      "acridone",
      "ahprrr",
      "3d-qsar",
      "pharmacophore",
      "docking",
      "calmodulin"
    ]
  },
  {
    "id": "1799",
    "title": "Basis for the implementation of an EEG-based single-trial binary brain computer interface through the disgust produced by remembering unpleasant odors",
    "abstract": "In order to implement an EEG-based brain computer interface (BCI), a very large number of strategies (ranging from sensorymotor, p300, auditory based, visually based) can be used. However, no technique exists which is based on the olfactory stimulation or, better, based on the imagination of olfactory stimuli. The present paper describes an innovative paradigm, that is the voluntary brain activation with the disgust produced by remembering unpleasant odors, and a simple and robust classification method on which a single trial binary BCI can be implemented. In order to classify the signal, mainly the channels P4, C4, T8 and P8 have been used, by spanning the frequency band between 32 and 42Hz, that is a subset of the gamma band external to the bands usually occupied by other tasks (the interval between 1 and 30Hz), and the alpha band between 8 and 12Hz. Right hemisphere of the brain and gamma band of frequencies are particularly sensitive when experiencing negative emotions, such as the disgust produced by smelling or remembering unpleasant odors, while the alpha band is usually modified with concentration. This constitutes an advantage for the proposed classification technique because it is made intrinsically easy by the localization into particular positions and frequencies: different features are mostly based on different frequency bands. The choice of disgust produced by remembering unpleasant odors is twofold: smelling is an ancestral sensation which is so strong that its EEG signal is produced also in persons affected by hyposmia when they imagine an olfactory situation; it can be used without external stimulation, that is the user can decide freely when and if activate it. The proposed method and the experimental setup are described and a series of experimental measurements are presented and discussed. The accuracy of the proposed method is also evaluated and the reached levels are about 90%. The proposed system can be a useful communication alternative for disabled people that cannot use other BCI paradigms.",
    "keywords": [
      "bci",
      "eeg",
      "humancomputer interface",
      "disgust",
      "unpleasant odor",
      "emotion"
    ]
  },
  {
    "id": "1800",
    "title": "On expander codes",
    "abstract": "Sipser and Spielman have introduced a constructive family of asymptotically good linear error-correcting codes-expander codes-together with a simple parallel algorithm that will always remove a constant fraction of errors. We introduce a variation on their decoding algorithm that, with no extra cost in complexity, provably corrects up to 12 times more errors.",
    "keywords": [
      "decoding",
      "expander code",
      "ramanujan graph"
    ]
  },
  {
    "id": "1801",
    "title": "location cache for web queries",
    "abstract": "This paper proposes a strategy to reduce the amount of hardware involved in the solution of search engine queries. It proposes using a secondary compact cache that keeps minimal information stored in the query receptionist machine to register the processors that must get involved in the solution of queries which are evicted from the standard result cache or are not admitted in it. This cache strategy produces exact answers by using very few processors.",
    "keywords": [
      "inverted files",
      "parallel and distributed computing"
    ]
  },
  {
    "id": "1802",
    "title": "Structural vibration control",
    "abstract": "Undesirable time-variable motions of dynamical structures (e.g. scales, balances, vibratory platforms, bridges and buildings) are mainly caused by unknown or uncertain excitations. In a variety of applications it is desirable or even necessary to attenuate these disturbances in an effective way and with moderate effort. Hence, several passive as well as active methods and techniques have been developed in order to treat these problems. However, employment of active techniques often fails because of their considerable financial costs. We propose an affordable control scheme which accounts for the above-mentioned deficiencies. In addition, we allow constraints on control actions. Furthermore, the number of control inputs (actuators) may be arbitrary, i.e., the system may be mismatched. The scheme is based on Lyapunov stability theory and, provided that the bounds of the uncertainties are a priori known, a stable attractor (ball of ultimate boundedness) of the structure can be computed. In case measurement errors or uncertainties, respectively, are significant, it is shown how the Lyapunov-based control scheme may be combined with a fuzzy control concept. The effectiveness and behavior of the control scheme is demonstrated on two simplified models of elastic structures such as a two story building and a bridge subjected to a moving truck.",
    "keywords": [
      "vibrations",
      "structures",
      "robust control",
      "lyapunov stability",
      "fuzzy theory",
      "identification"
    ]
  },
  {
    "id": "1803",
    "title": "Cataract halos: A driving hazard in aging populations. Implication of the Halometer DG test for assessment of intraocular light scatter",
    "abstract": "Cataract, regardless of etiology, results in light scatter and subjective glare. Senile cataract is emerging as a crucial factor in driving safely, particularly in night driving and adverse weather conditions. The authors examined this visual impairment using a new Halometer DG test in the eyes of older adult drivers with and without cataract. Examined subjects consisted of n=65 older adults with cataract in one or both eyes and n=72 adult drivers who did not have a cataract in either eye. Subjects were examined for distance high contrast visual acuity (VA) and red/green disability glare (DG) with a new halo generating instrument. Subjects also completed a subjective Driving Habits Questionnaire (DHQ), designed to obtain information about driving during the past year. DG increased with age of the driver. VA and Halometer DG testing of better and worse eyes prognosticated impairments which significantly affect driving performance. Cataract subjects demonstrated increased Halometer DG scores and were two to four times more likely to report difficulty with driving at night and with challenging driving situations than were cataract-free drivers. DG is a specific cataract-induced functional age-related risk factor of driving difficulty, easily measured by a technician with a new Halometer DG device. Optometrists and ophthalmologists should incorporate Halometer DG testing in their pre-examination vision testing rooms for patients over age 55, and also perform this test on others who complain about glare. Traffic safety engineers should incorporate automotive optical-microprocessor-aided tests for DG into cars, to alert drivers of mild functional impairments and progressive degrees of DG sensitization.",
    "keywords": [
      "cataract",
      "disability-glare",
      "halos",
      "halometer",
      "visual-acuity",
      "driver safety"
    ]
  },
  {
    "id": "1804",
    "title": "Fitting polynomial surfaces to triangular meshes with Voronoi squared distance minimization",
    "abstract": "This paper introduces Voronoi squared distance minimization (VSDM), an algorithm that fits a surface to an input mesh. VSDM minimizes an objective function that corresponds to a Voronoi-based approximation of the overall squared distance function between the surface and the input mesh (SDM). This objective function is a generalization of the one minimized by centroidal Voronoi tessellation, and can be minimized by a quasi-Newton solver. VSDM naturally adapts the orientation of the mesh elements to best approximate the input, without estimating any differential quantities. Therefore, it can be applied to triangle soups or surfaces with degenerate triangles, topological noise and sharp features. Applications of fitting quad meshes and polynomial surfaces to input triangular meshes are demonstrated.",
    "keywords": [
      "squared distance minimization",
      "centroidal voronoi tessellation",
      "subdivision surface fitting"
    ]
  },
  {
    "id": "1805",
    "title": "Progressive ranking of range aggregates",
    "abstract": "Ranking-aware queries have been gaining much attention recently in many applications such as multimedia databases, search engines and data streams. They are, however, not only restricted to such applications but are also very useful in On-Line Analytical Processing (OLAP) applications. In this paper, we introduce aggregation ranking queries in OLAP data cubes motivated by an online advertisement tracking data warehouse application. These queries aggregate information over a specified range and then return the ranked order of the aggregated values. For instance, an advertiser might be interested in the top-k publishers over the last three months in terms of sales obtained through the online advertisements placed on the publishers. They differ from range aggregate queries in that range aggregate queries are mainly concerned with an aggregate operator such as SUM and MIN/MAX over the selected ranges of all dimensions in the data cubes. Existing techniques for range aggregate queries are not able to process aggregation ranking queries efficiently. Hence, in this paper we propose new algorithms to handle this problem. The essence of the proposed algorithms is based on both ranking and cumulative information to progressively rank aggregation results. Furthermore we empirically evaluate our techniques and the experimental results show that the query cost is improved significantly.",
    "keywords": [
      "data warehousing",
      "on-line analytical processing",
      "aggregation",
      "data cube"
    ]
  },
  {
    "id": "1806",
    "title": "Low computational complexity enhanced zerotree coding for wavelet-based image compression",
    "abstract": "The embedded zerotree wavelet (EZW) algorithm, introduced by J.M. Shapiro and extented by A. Said and W.A. Pearlman, has proven to be a computationally simple and efficient method for image compression. In the current study, we propose a novel algorithm to improve the performance of EZW coding. The proposed method, called enhanced zerotree coding (EZC), is based on two new techniques: adaptive multi-subband decomposition (AMSD) and band flag scheme (BFS). The purpose of AMSD is to change the statistics of transformed coefficients so that the coding performance in peak signal-to-noise ratio (PSNR) can be elevated at a lower bit rate. In addition, BFS is used to reduce execution time in finding zerotrees. In BFS the tree depths are controlled, therefore, many unnecessary comparison operations can be skipped. Experimental results show that the proposed algorithm improves the performance of EZW coding and requires low computational complexity. In addition, the property of embedded coding is preserved, which enables a progressive transmission.",
    "keywords": [
      "image coding",
      "zerotree coding",
      "adaptive multi-subband decomposition",
      "band flag scheme"
    ]
  },
  {
    "id": "1807",
    "title": "A New Method for Modeling the Behavior of Finite Population Evolutionary Algorithms",
    "abstract": "As practitioners we are interested in the likelihood of the population containing a copy of the optimum. The dynamic systems approach, however, does not help us to calculate that quantity. Markov chain analysis can be used in principle to calculate the quantity. However, since the associated transition matrices are enormous even for modest problems, it follows that in practice these calculations are usually computationally infeasible. Therefore, some improvements on this situation are desirable. In this paper, we present a method for modeling the behavior of finite population evolutionary algorithms (EAs), and show that if the population size is greater than 1 and much less than the cardinality of the search space, the resulting exact model requires considerably less memory space for theoretically running the stochastic search process of the original EA than the Nix and Vose-style Markov chain model. We also present some approximate models that use still less memory space than the exact model. Furthermore, based on our models, we examine the selection pressure by fitness-proportionate selection, and observe that on average over all population trajectories, there is. no such strong bias toward selecting the higher fitness individuals as the fitness landscape suggests.",
    "keywords": [
      "finite population evolutionary algorithms",
      "markov chain analysis",
      "exact model",
      "approximate model",
      "fitness-proportionate selection",
      "selection pressure",
      "success probability"
    ]
  },
  {
    "id": "1808",
    "title": "A predictorcorrector method for structural nonlinear analysis",
    "abstract": "A predictorcorrector method is presented for the efficient and reliable analysis of structural nonlinear behaviors. The key idea lies on modifying the starting point of iterations of the Newton iterative method. The conventional Newton method starts iterations at the previously converged solution point. However, in the present predictorcorrector method, a point close to the converged solution of the current step is predicted first, and then the Newton method starts iterative procedure at the predicted point. The predictor, the neural network in the present study, recognizes the pattern of the previously converged solutions to predict the starting point of the current step. Then the corrector, the standard Newton method in the present study, is used to obtain the converged solution by iterative computation starting at the predicted point. Numerical tests are conducted to demonstrate the effectiveness and reliability of the present predictorcorrector method. The performance of the present method is compared with the conventional Newton method and Riks' continuation method. The present predictorcorrector method saves computational cost significantly and yields stable results without diverging, for the nonlinear analysis with monotonous deformation path as well as complicated deformation path including buckling and post-buckling behaviors.",
    "keywords": [
      "nonlinear analysis",
      "iterative algorithm",
      "predictorcorrector",
      "newton method ",
      "neural network",
      "buckling analysis",
      "post-buckling analysis"
    ]
  },
  {
    "id": "1809",
    "title": "Massively parallel forward modeling of scalar and tensor gravimetry data",
    "abstract": "We present an approach to calculate scalar and tensor gravity utilizing the massively parallel architecture of consumer graphics cards. Our parametrization is based on rectilinear blocks with constant density within each blocks. This type of parametrization is well suited for inversion of gravity data or joint inversion with other datasets, but requires the calculation of a large number of model blocks for complex geometries. For models exceeding 10,000 cells we achieve an acceleration of a factor of 40 for scalar data and 30 for tensor data compared to a single thread on the CPU. This significant acceleration allows fast computation of large models exceeding 10(6) model parameters and thousands of measurement sites.  ",
    "keywords": [
      "gravity modeling",
      "cuda",
      "parallel computing"
    ]
  },
  {
    "id": "1810",
    "title": "On the profile of temperature dependent series resistance in Al/Si3N4/p-Si (MIS) Schottky diodes",
    "abstract": "The temperature dependence of capacitancevoltage (CV) and conductancevoltage (G/wV) characteristics of metalinsulatorsemiconductor (Al/Si3N4/p-Si) Schottky barrier diodes (SBDs) was investigated by considering series resistance effect in the temperature range of 80300K. It is found that in the presence of series resistance, the forward bias CV plots exhibit a peak, and experimentally show that the peak positions with a maximum at 260K shift toward lower voltages with increasing temperature. The CV and (G/wV) characteristics confirm that the interface state density (Nss) and series resistance (Rs) of the diode are important parameters that strongly influence the electric parameters of MIS structures. The crossing of the G/wV curves appears as an abnormality compared to the conventional behavior of ideal Schottky diode. It is thought that the presence of series resistance keeps this intersection hidden and unobservable in homogeneous Schottky diodes, but it appears in the case of inhomogeneous Schottky diode. In addition, the high frequency (Cm) and conductance (Gm/w) values measured under both reverse and forward bias were corrected for the effect of series resistance to obtain the real diode capacitance.",
    "keywords": [
      "conductance method",
      "mis structure",
      "temperature dependence",
      "series resistance",
      "nitride passivation"
    ]
  },
  {
    "id": "1811",
    "title": "An efficient protocol for authenticated key agreement",
    "abstract": "This paper proposes an efficient two-pass protocol for authenticated key agreement in the asymmetric (public-key) setting. The protocol is based on Diffie-Hellman key agreement and can be modified to work in an arbitrary finite group and, in particular, elliptic curve groups. Two modifications of this protocol are also presented: a one-pass authenticated key agreement protocol suitable for environments where only one entity is on-line, and a three-pass protocol in which key confirmation is additionally provided. Variants of these protocols have been standardized in IEEE P1363 [17], ANSI X9.42 [2], ANSI X9.63 [4] and ISO 15496-3 [18], and are currently under consideration for standardization and by the U. S. government's National Institute for Standards and Technology [30].",
    "keywords": [
      "diffie-hellman",
      "authenticated key agreement",
      "key confirmation",
      "elliptic curves"
    ]
  },
  {
    "id": "1812",
    "title": "The possible cardinalities of global secure sets in cographs",
    "abstract": "Let G = (V, E) be a graph. A global secure set SD subset of V is a dominating set which also satisfies a condition that vertical bar N [X] boolean AND SD vertical bar >= vertical bar N [X] - SD vertical bar for every subset X subset of SD. The minimum cardinality of the global secure set in the graph G is denoted by gamma(s)(G). In this paper, we introduce the notion of gamma(s)-monotone graphs. The graph G is gamma(s)-monotone if, for every k is an element of {gamma(s)(G), gamma(s)(G) + 1, ... , n}, it has a global secure set of cardinality k. We will also present the results concerning the minimum cardinality of the global secure sets in the class of cographs.  ",
    "keywords": [
      "graph",
      "alliance",
      "secure set",
      "dominating set",
      "cograph",
      "cotree"
    ]
  },
  {
    "id": "1813",
    "title": "Predicting customer churn from valuable B2B customers in the logistics industry: a case study",
    "abstract": "This study uncovers the effect of the length, recency, frequency, monetary, and profit (LRFMP) customer value model in a logistics company to predict customer churn. This unique context has useful business implications compared to the main stream customer churn studies where individual customers (rather than business customers) are the main focus. Our results show the five LRFMP variables had a varying effect on customer churn. Specifically length, recency and monetary variables had a significant effect on churn, while the frequency variable only became a top predictor when the variability of the first three variables was limited. The profit variable had never become a significant predictor. Certain other behavioral variables (such as time between transactions) also had an effect on churn. The resulting set of predictors of churn expands the original LRFMP and RFM models with additional insights. Managerial implications were provided.",
    "keywords": [
      "customer churn",
      "logistics industry",
      "customer value analysis",
      "prediction model"
    ]
  },
  {
    "id": "1814",
    "title": "Supine and Prone Colon Registration Using Quasi-Conformal Mapping",
    "abstract": "In virtual colonoscopy, CT scans are typically acquired with the patient in both supine (facing up) and prone (facing down) positions. The registration of these two scans is desirable so that the user can clarify situations or confirm polyp findings at a location in one scan with the same location in the other, thereby improving polyp detection rates and reducing false positives. However, this supine-prone registration is challenging because of the substantial distortions in the colon shape due to the patient's change in position. We present an efficient algorithm and framework for performing this registration through the use of conformal geometry to guarantee that the registration is a diffeomorphism (a one-to-one and onto mapping). The taeniae coli and colon flexures are automatically extracted for each supine and prone surface, employing the colon geometry. The two colon surfaces are then divided into several segments using the flexures, and each segment is cut along a taenia coli and conformally flattened to the rectangular. domain using holomorphic differentials. The mean curvature is color encoded as texture images, from which feature points are automatically detected using graph cut segmentation, mathematic morphological operations, and principal component analysis. Corresponding feature points are found between supine and prone and are used to adjust the conformal flattening to be quasi-conformal, such that the features become aligned. We present multiple methods of visualizing our results, including 2D flattened rendering, corresponding 3D endoluminal views, and rendering of distortion measurements. We demonstrate the efficiency and efficacy of our registration method by illustrating matched views on both the 2D flattened colon images and in the 3D volume rendered colon endoluminal view. We analytically evaluate the correctness of the results by measuring the distance between features on the registered colons.",
    "keywords": [
      "data registration",
      "geometry-based techniques",
      "medical visualization",
      "mathematical foundations for visualization"
    ]
  },
  {
    "id": "1815",
    "title": "Design of a Training Support System in the Information Technology Area",
    "abstract": "The descent of productivity in some areas where the computer has been the first protagonist and the results encounter in diverse studies have manifested the importance of training in the information technology area. This paper proposes the Design of a Training Support System in the Information Technology Area (SISEPATI). This system tries to bring support to: (1) the training given to a user in the use of a system/software/apphcation (s/s/a); (2) the user to make easier his/her interaction and complete his/her knowledge about the s/s/a; (3) the integrator, in the adjust or changes that he/she has to make on the methods or tools used in the training delivered, according to the comments or results encounter with the user interactions; and to (4) the supra-user so that he/she can evaluated the results of the training and of both systems: the one that supports the training and the s/s/a. The proposed system has to pass by a revision process by trainers and trainees to improve it and change it so it can bring nearest the goals set forth. The improves and changes required in SISEPATI design can be collect with the design of other system, a Support System for the Participative Design, this system will bring the collective results that can be obtain by the opinions, suggestions and ideas given by trainees, trainers, designers and supra-users applying Collective Decision Making Theory. That will increase the consensus level about the aspects to consider in a design of a support training system as the proposed. In other way of ideas there has to be indicated that the development of a system like the expound will be justified in those cases in where the complexity of the s/s/a will merit due to the effort involved to detail the objectives, contents, restrictions, etc. of the s/s/a: or due to the complexity of the procedures or organizational aspects requiring to repeat the training with certain frequency.",
    "keywords": [
      "training",
      "support system",
      "collective decision"
    ]
  },
  {
    "id": "1816",
    "title": "Reversal ?CP using hard stamps",
    "abstract": "In this work, we present a new method of Micro-Contact Printing (?CP) which we call reversal ?CP using hard stamps which can be used for the fabrication of different structures like negative index materials, e.g., split ring resonators (SRRs), dots and squares made of gold. Typically soft stamps made of PDMS (polydimethylsiloxane) inked with thiols are used for ?CP. The softness of the stamp material entails a lot of problems like deformation of the structures, sagging and pairing of the protruding features and reliability of the process. We use hard stamps which are spin coated with a thiol solution so that the thioles stay in the recessed areas of the stamp. In the following ?CP process an EVG620 is used to bring the stamp and substrate into contact so that the thiols on the stamp diffuse and bind to the gold surface and serve as an etch mask for succeeding wet chemical etching. Using this method overcomes the disadvantage of a soft stamp material. Smallest feature sizes down to 100nm are shown.",
    "keywords": [
      "reversal ?cp",
      "hard stamps",
      "ormoceres",
      "gold structures",
      "thiols"
    ]
  },
  {
    "id": "1817",
    "title": "linear ordering and application to placement",
    "abstract": "Given a set of interconnected elements, linear ordering generates a linear sequence of elements of the set, which is the basis for most constructive initial-placement methods. This paper presents a new strategy for linear ordering. The important difference of the new technique from the previous ones is that it starts the ordering process from the most lightly connected seed. It was applied to various placement problems including standard cell and gate array and produced very good results.",
    "keywords": [
      "arrays",
      "process",
      "placement",
      "applications",
      "order",
      "standard cell",
      "strategies",
      "method",
      "paper",
      "sequence"
    ]
  },
  {
    "id": "1818",
    "title": "Robust registration for computer-integrated orthopedic surgery: Laboratory validation and clinical experience",
    "abstract": "In order to provide navigational guidance during computer-integrated orthopedic surgery, the anatomy of the patient must first be registered to a medical image or model. A common registration approach is to digitize points from the surface of a bone and then find the rigid transformation that best matches the points to the model by constrained optimization. Many optimization criteria, including a least-squares objective function, perform poorly if the data include spurious data points (outliers). This paper describes a statistically robust, surface-based registration algorithm that we have developed for orthopedic surgery. To find an initial estimate, the user digitizes points from predefined regions of bone that are large enough to reliably locate even in the absence of anatomic landmarks. Outliers are automatically detected and managed by integrating a statistically robust M-estimator with the iterative-closest-point algorithm. Our in vitro validation method simulated the registration process by drawing registration data points from several sets of densely digitized surface points. The method has been used clinically in computer-integrated surgery for high tibial osteotomy, distal radius osteotomy, and excision of osteoid osteoma.",
    "keywords": [
      "computer-integrated orthopedic surgery",
      "validation",
      "clinical experience"
    ]
  },
  {
    "id": "1819",
    "title": "Computational and theoretical analysis of null space and orthogonal linear discriminant analysis",
    "abstract": "Dimensionality reduction is an important pre-processing step in many applications. Linear discriminant analysis (LDA) is a classical statistical approach for supervised dimensionality reduction. It aims to maximize the ratio of the between-class distance to the within-class distance, thus maximizing the class discrimination. It has been used widely in many applications. However, the classical LDA formulation requires the nonsingularity of the scatter matrices involved. For undersampled problems, where the data dimensionality is much larger than the sample size, all scatter matrices are singular and classical LDA fails. Many extensions, including null space LDA (NLDA) and orthogonal LDA (OLDA), have been proposed in the past to overcome this problem. NLDA aims to maximize the between-class distance in the null space of the within-class scatter matrix, while OLDA computes a set of orthogonal discriminant vectors via the simultaneous diagonalization of the scatter matrices. They have been applied successfully in various applications. In this paper, we present a computational and theoretical analysis of NLDA and OLDA. Our main result shows that under a mild condition which holds in many applications involving high-dimensional data, NLDA is equivalent to OLDA. We have performed extensive experiments on various types of data and results are consistent with our theoretical analysis. We further apply the regularization to OLDA. The algorithm is called regularized OLDA (or ROLDA for short). An efficient algorithm is presented to estimate the regularization value in ROLDA. A comparative study on classification shows that ROLDA is very competitive with OLDA. This confirms the effectiveness of the regularization in ROLDA.",
    "keywords": [
      "linear discriminant analysis",
      "dimensionality reduction",
      "null space",
      "orthogonal matrix",
      "regularization"
    ]
  },
  {
    "id": "1820",
    "title": "Optimal configuration for the self-identification of a two-dimensional variable geometry truss",
    "abstract": "This study addresses the optimal changes in geometry of a two-dimensional variable geometry truss (VGT) to identify the stiffness matrix of the truss using the concept of self-identification. The optimization of the geometry changing of the VGT is a problem of selecting the optimal combination of multiple design variables from a large number of candidate sets. This study proposes a simple optimization method for determining a set of optimal geometric parameters; in this method, the approximated mode shape matrix obtained using spline interpolation techniques is used to calculate the objective function for self-identification. The objective function used in this paper is a function of the condition number of the coefficient matrix of a linear matrix equation and a criterion for self-identification. The proposed algorithm can be used to reduce the number of actual vibration tests required for measuring the mode shapes and modal frequency while it maximizes the objective function. Numerical experiments are conducted to investigate the relationship between the convergence characteristics of the optimization and the target vibration modes. The effectiveness of the optimized geometry changing is verified by comparing the identification error for the uniform geometry changing, the optimized one for the three lower modes of the VGT, and the one found by a classical QR decomposition. Furthermore, the numerical results show that the identification sensitivity with respect to noisy data is reduced by the optimization.",
    "keywords": [
      "stiffness identification",
      "testing configuration optimization",
      "adaptive structures"
    ]
  },
  {
    "id": "1821",
    "title": "The combined effect of physical, psychosocial/organisational and/or environmental risk factors on the presence of work-related musculoskeletal symptoms and its consequences",
    "abstract": "Combined exposure to poor physical and poor psychosocial factors increased the odds of MSS. This combination also increased the odds of reduced activities and absenteeism due to MSS. Favourable psychosocial conditions reduced the odds of MSS due to poor physical conditions. To reduce MSS and its consequences, employers need to adopt a multifaceted approach.",
    "keywords": [
      "back pain",
      "interaction",
      "work stress"
    ]
  },
  {
    "id": "1822",
    "title": "Why GSA: a gravitational search algorithm is not genuinely based on the law of gravity",
    "abstract": "This letter highlights a fundamental inconsistency in the formulation of the Gravitational search algorithm (GSA) (Rashedi et al., Inf Sci 223248, 2009). GSA is said to be based on the law of gravity, that is, candidate solutions attract each other in the search space based on their relative distances and masses (qualities). We show that, contrary to what is claimed, GSA does not take the distances between solutions into account, and therefore cannot be considered to be based on the law of gravity.",
    "keywords": [
      "gravitational search algorithm",
      "heuristic search algorithms",
      "law of gravity",
      "natural computing",
      "optimization"
    ]
  },
  {
    "id": "1823",
    "title": "Sensitivity analysis applied to the construction of radial basis function networks",
    "abstract": "Conventionally, a radial basis function (RBF) network is constructed by obtaining cluster centers of basis function by maximum likelihood learning. This paper proposes a novel learning algorithm for the construction of radial basis function using sensitivity analysis. In training, the number of hidden neurons and the centers of their radial basis functions are determined by the maximization of the output's sensitivity to the training data. In classification, the minimal number of such hidden neurons with the maximal sensitivity will be the most generalizable to unknown data. Our experimental results show that our proposed sensitivity-based RBF classifier outperforms the conventional RBFs and is as accurate as support vector machine (SVM). Hence, sensitivity analysis is expected to be a new alternative way to the construction of RBF networks.",
    "keywords": [
      "sensitivity analysis",
      "radial basis function neural network",
      "orthogonal least square learning",
      "network pruning"
    ]
  },
  {
    "id": "1824",
    "title": "Dominance rules in combinatorial optimization problems",
    "abstract": "The aim of this paper is to study the concept of a dominance rule in the context of combinatorial optimization. A dominance rule is established in order to reduce the solution space of a problem by adding new constraints to it, either in a procedure that aims to reduce the domains of variables, or directly in building interesting solutions. Dominance rules have been extensively used over the last 50 years. Surprisingly, to our knowledge, no detailed description of them can be found in the literature other than a few short formal descriptions in the context of enumerative methods. We are therefore proposing an investigation into what dominance rules are. We first provide a definition of a dominance rule with its different nuances. Next, we analyze how dominance rules are generally formulated and what are the consequences of such formulations. Finally, we enumerate the common characteristics of dominance rules encountered in the literature and in the usual process of solving combinatorial optimization problems.",
    "keywords": [
      "combinatorial optimization",
      "dominance rules",
      "constraints",
      "modeling"
    ]
  },
  {
    "id": "1825",
    "title": "In silico and experimental validation of proteinprotein interactions between PknI and Rv2159c from Mycobacterium tuberculosis",
    "abstract": "PknI interacts with two novel proteins, Rv2159c and Rv0148. The three dimensional structure of Rv2159c was predicted. The in silico docking between PknI and Rv2159c was established well. The possible interacting residues were mutated and their structural stability were analyzed. PknI attracts the hydrophobic residues (AlaGlyTrp) for its interaction with Rv2159c.",
    "keywords": [
      "proteinprotein interaction",
      "pkni",
      "homology modeling",
      "protein docking",
      "molecular dynamics",
      "binding free energy"
    ]
  },
  {
    "id": "1826",
    "title": "An Analytical Study of People Mobility in Opportunistic Networks",
    "abstract": "An opportunistic network is a type of Delay Tolerant Network (DTN) tit which communication opportunities are intermittent Moreover, tit end-to-end path between the source and the destination may never have existed, disconnection and reconnection are common occurrences, and link performance is highly variable or extreme With numerous emerging opportunistic networking applications. strategies that can facilitate effective data communication in such challenging environments have become increasingly desirable In particular, knowing the Fundamental properties of opportunistic networks will soon be the key to the proper design of opportunistic routing schemes and applications In this study, we Investigate opportunistic network scenarios based on two public network traces, namely, the UCSD and Dartmouth traces Our contribution is twofold First, we Identify the censorship Issue in network traces that usually leads to a strongly skewed distribution of the measurements Based on this knowledge, we then apply the Kaplan-Meier Estimator to calculate the survivorship of network measurements The survivorship Feature IS used to design our proposed censorship removal algorithm (CRA) for recovering censored data Second, we perforin an in-depth analysis of the UCSD and Dartmouth network traces We show, that they exhibit strong self-similarity, and call be modeled as such We believe these newly revealed characteristics will be Important tit the Future development and evaluation of opportunistic networks",
    "keywords": [
      "delay tolerant networks",
      "survival analysis",
      "kaplan-meier estimator",
      "self-similarity",
      "censorship removal algorithm"
    ]
  },
  {
    "id": "1827",
    "title": "Multidimensional upwinding for incompressible flows based on characteristics",
    "abstract": "In this paper, the multidimensional characteristic based upwind scheme (MCB) which has been recently introduced by the authors is applied to two another benchmark problems namely flow in a channel with a backward facing step and two-dimensional steady and unsteady flows past a circular cylinder. Extension of MCB scheme for calculating convective fluxes on non-Cartesian grids is presented here. For the flow over backward facing step, obtained results were compared against well-known experimental data and the results show high accuracy of MCB scheme and faster convergence rate with respect to conventional CB scheme. In the case of flow over circular cylinder, the flow at steady and transient regimes is investigated by MCB scheme. Again, the results obtained by MCB are compared to the other results in the literature and show good agreement with them. Also, rapid convergence rate of MCB was observed in this case too. It is concluded that, the genuinely multidimensional characteristic based (MCB) scheme, has been introduced earlier by the authors, is a robust and powerful scheme for modeling incompressible viscous flows for achieving the high accuracy and remarkable advantage in convergence rate with respect to conventional characteristic based schemes.",
    "keywords": [
      "incompressible flow",
      "artificial compressibility",
      "navierstokes equations",
      "mcb scheme",
      "wave angle"
    ]
  },
  {
    "id": "1828",
    "title": "Interactive fuzzy programming for random fuzzy two-level programming problems through possibility-based fractile model",
    "abstract": "This paper focuses on interactive decision making methods for random fuzzy two-level linear programming problems. Considering the probabilities that the decision makers objective function values are smaller than or equal to target variables, fuzzy goals of the decision makers are introduced. Using the fractile model to optimize the target variables under the condition that the degrees of possibility with respect to the attained probabilities are greater than or equal to certain permissible levels, the original random fuzzy two-level programming problems are reduced to deterministic ones. Interactive fuzzy nonlinear programming to obtain a satisfactory solution for the decision maker at the upper level in consideration of the cooperative relation between decision makers is presented. An illustrative numerical example demonstrates the feasibility and efficiency of the proposed method.",
    "keywords": [
      "two-level programming",
      "random fuzzy programming",
      "possibility",
      "fractile criteria optimization",
      "interactive programming"
    ]
  },
  {
    "id": "1829",
    "title": "On the Equivalence Between Stein and De Bruijn Identities",
    "abstract": "This paper focuses on illustrating 1) the equivalence between Stein's identity and De Bruijn's identity, and 2) two extensions of De Bruijn's identity. First, it is shown that Stein's identity is equivalent to De Bruijn's identity under additive noise channels with specific conditions. Second, for arbitrary but fixed input and noise distributions under additive noise channels, the first derivative of the differential entropy is expressed by a function of the posterior mean, and the second derivative of the differential entropy is expressed in terms of a function of Fisher information. Several applications over a number of fields, such as signal processing and information theory, are presented to support the usefulness of the developed results in this paper.",
    "keywords": [
      "bayesian cramer-rao lower bound ",
      "costa's epi",
      "cramer-rao lower bound ",
      "de bruijn's identity",
      "entropy power inequality ",
      "fisher information inequality ",
      "stein's identity"
    ]
  },
  {
    "id": "1830",
    "title": "Access control protocols with two-layer architecture for wireless networks",
    "abstract": "In this paper we study two access control protocols which have similar two-layer access control architectures for wireless networks in public places. The first protocol, called the Lancaster protocol, employs user password for authentication and enforces access control at the IP layer; while the second protocol, referred to as the Stanford protocol, uses public key cryptosystems (PKC) for authentication and performs access control at the link layer. Although both protocols are intended to restrict access to wireless networks only to authorized users, our analysis shows that both protocols have serious security flaws which make them vulnerable to attacks. Then we propose a password-based protocol and a PKC-based protocol for the Lancaster architecture and the Stanford architecture, respectively. Both of our protocols provide mutual authentication, perfect forward secrecy and access control for wireless networks. Moreover, they also provide DoS resistance and identity confidentiality for the client. We present detailed security and performance analysis for our protocols, and show that both of our protocols are secure and efficient for access control in wireless networks.",
    "keywords": [
      "wireless networks",
      "access control",
      "security protocol"
    ]
  },
  {
    "id": "1831",
    "title": "On Effect of Compromised Nodes on Security of Wireless Sensor Network",
    "abstract": "For secure communications in wireless sensor network in adversarial conditions, one way is providing keys to sensor nodes before deployment. Two sensor nodes can communicate provided they have at least one key in common or there is a path between the two nodes where each pair of neighboring nodes on this path has at least one key in common. However node compromise poses severe security threats in sensor networks. Here we have considered dual BIBD design for the distribution of the keys such that any pair of sensors has exactly one key in common. In this scenario we study the probability of disconnection when a number of nodes, not more than the replication number of the keys are compromised and the resulting status of the network in terms of its resiliency properties.",
    "keywords": [
      "combinatorial design",
      "connectivity",
      "resiliency",
      "key pre-distribution",
      "bibd",
      "network security"
    ]
  },
  {
    "id": "1832",
    "title": "Fairness in systems based on multiparty interactions",
    "abstract": "In the context of the Multiparty Interaction Model, fairness is used to insure that an interaction that is enabled sufficiently often in a concurrent program will eventually be selected for execution. Unfortunately, this notion does not take conspiracies into account, i.e. situations in which an interaction never becomes enabled because of an unfortunate interleaving of independent actions; furthermore, eventual execution is usually too weak for practical purposes since this concept can only be used in the context of infinite executions. In this article, we present a new fairness notion, k-conspiracy-free fairness, that improves on others because it takes finite executions into account, alleviates conspiracies that are not inherent to a program, and k may be set a priori to control its goodness to address the above-mentioned problems. ",
    "keywords": [
      "concurrent programs",
      "multiparty interactions",
      "fairness",
      "fair finiteness",
      "conspiracies"
    ]
  },
  {
    "id": "1833",
    "title": "an analysis of representations for hyper-heuristics for the uncapacitated examination timetabling problem in a genetic programming system",
    "abstract": "Earlier research into the examination timetabling problem focused on applying different methodologies to generate solutions to the problem. More recently research has been directed at developing hyper-heuristic systems for timetable construction. Hyper-heuristic systems are used to decide which examination to schedule next during the timetable construction process and aim at allocating those examinations that are most difficult to schedule first. This study investigates using a genetic programming based hyper-heuristic system to evolve heuristic combinations for the uncapacitated examination timetabling problem. More specifically it presents and evaluates three different representations for heuristic combinations in a genetic programming system. The performance of the genetic programming based system using the different representations is applied to three examination timetabling problems with different characteristics and the performance on these problems is compared. The results obtained are also compared to that of other hyper-heuristic systems applied to the same problems.",
    "keywords": [
      "examination timetabling",
      "hyper-heuristics",
      "genetic programming"
    ]
  },
  {
    "id": "1834",
    "title": "CORRELATION ENERGY AND ENTANGLEMENT GAP IN CONTINUOUS MODELS",
    "abstract": "Our goal is to clarify the relation between entanglement and correlation energy in a bipartite system with infinite dimensional Hilbert space. To this aim, we consider the completely solvable Moshinsky's model of two linearly coupled harmonic oscillators. Also, for small values of the couplings, the entanglement of the ground state is nonlinearly related to the correlation energy, involving logarithmic or algebraic corrections. Then, looking for witness observables of the entanglement, we show how to give a physical interpretation of the correlation energy. In particular, we have proven that there exists a set of separable states, continuously connected with the Hartree-Fock state, which may have a larger overlap with the exact ground state, but also a larger energy expectation value. In this sense, the correlation energy provides an entanglement gap, i.e. an energy scale, under which measurements performed on the 1-particle harmonic sub-system can discriminate the ground state from any other separated state of the system. However, in order to verify the generality of the procedure, we have compared the energy distribution cumulants for the 1-particle harmonic sub-system of the Moshinsky's model with the case of a coupling with a damping Ohmic bath at 0 temperature.",
    "keywords": [
      "moshinsky's model",
      "harmonic oscillator",
      "hartree-fock state"
    ]
  },
  {
    "id": "1835",
    "title": "new approach to evaluate the effectiveness of the audio information protection for determining the identity of virtual speech images",
    "abstract": "In this paper, we describe new approach to evaluate the effectiveness of the audio information protection for determining the identity of virtual speech images",
    "keywords": [
      "legibility",
      "information security",
      "virtual adaptive scrambling",
      "virtual scrambling",
      "scrambling",
      "redundancy"
    ]
  },
  {
    "id": "1836",
    "title": "Fourier analysis of the generalized CMAC neural network",
    "abstract": "The Cerebellar Model Articulation Controller (CMAC) is a simple and fast neural network: these characteristics have extended its successful applications, while the analysis of its representation capabilities, as for many other neural networks, did not follow a similar development. In this article we discover the close parallelism between the representation of a function by a Generalized CMAC (GCMAC) and Nyquist sampling theory: discussing the role of different parameters and components of the network according to this similarity. The consideration of a representative example shows how the parallelism can be used to design a GCMAC adapted to its particular application.",
    "keywords": [
      "cmac",
      "associative memory",
      "neural networks",
      "function approximation",
      "modeling capabilities"
    ]
  },
  {
    "id": "1837",
    "title": "the complexity of computing a nash equilibrium",
    "abstract": "We resolve the question of the complexity of Nash equilibrium by showing that the problem of computing a Nash equilibrium in a game with 4 or more players is complete for the complexity class PPAD. Our proof uses ideas from the recently-established equivalence between polynomial time solvability of normal form games and graphical games, establishing that these kinds of games can simulate a PPAD-complete class of Brouwer functions.",
    "keywords": [
      "game theory",
      "nash equilibrium",
      "complexity",
      "ppad-completeness"
    ]
  },
  {
    "id": "1838",
    "title": "TCP Libra: Derivation, analysis, and comparison with other RTT-fair TCPs",
    "abstract": "The Transmission Control Protocol (TCP), the most widely used transport protocol over the Internet, has been advertised to implement fairness between flows competing for the same narrow link. However, when session round-trip-times (RTTs) radically differ, the share may be anything but fair. This RTT-unfairness represents a problem that severely affects the performance of long-RTT flows and whose solution requires a revision of TCPs congestion control scheme. To this aim, we discuss TCP Libra, a new transport protocol able to ensure fairness and scalability regardless of the RTT, while remaining friendly towards legacy TCP. As main contributions of this paper: (i) we focus on the model derivation and show how it leads to the design of TCP Libra; (ii) we analyze the role of its parameters and suggest how they may be adjusted to lead to asymptotic stability and fast convergence; (iii) we perform model-based, simulative, and real testbed comparisons with other TCP versions that have been reported as RTT-fair in the literature. Results demonstrate the ability of TCP Libra in ensuring RTT-fairness while remaining throughput efficient and friendly towards legacy TCP.",
    "keywords": [
      "fairness",
      "rtt",
      "tcp",
      "transport protocol"
    ]
  },
  {
    "id": "1839",
    "title": "Automated seismic event location for hydrocarbon reservoirs",
    "abstract": "An automatic monitoring system has been developed to process continuously recorded microseismic data and locate the associated events. To this end we use P- and S-wave travel times and the direction of the incoming wave field. The processing is organised in four modules: (i) a multi-channel detection algorithm based on signal-to-noise ratios, (ii) a P-wave onset determination based on error prediction filtering with an auto-regressive model, (iii) a P-wave polarisation analysis providing the direction of the incoming wave field, and (iv) a rotation of seismic traces into the ray coordinate system with subsequent S-wave onset determination. For a homogeneous velocity model the event hypocenter is determined by a linearised inversion technique, and for a three-dimensional (3D) velocity model a directed grid search method or the neighbourhood algorithm is applied. We applied the monitoring system to a microseismic data set from the Ekofisk oil field in the North Sea and located about 2000 microseismic events close to the borehole receiver string. Most of the microseismic events occurred in clusters and in a depth range from 2800 to about 3050m depth. Location errors were estimated by bootstrapping, and a comparison between results from directed grid search and neighbourhood algorithm revealed a high level of consistency.",
    "keywords": [
      "microseismicity",
      "event detection",
      "event location",
      "inversion",
      "ray tracing"
    ]
  },
  {
    "id": "1840",
    "title": "Sensorimotor mechanisms in music performance: actions that go partially wrong",
    "abstract": "Even expert musicians make errors occasionally, and overt responses that are correct may be accompanied by partial-error behavior that can be indicative of online error detection processes. We compare pianists production of correct pitches, pitch errors, and partial errors (correct pitches with incorrect force or duration) by examining events prior to errors. Errors tended to be produced with slower durations and softer intensities (associated with force reduction) than correct events. In addition, pre-error events tended to have durations and intensities that fell between those of errors and correct responses, presumably due to response competition with upcoming errors that resulted in partial-error outcomes. These findings support the inference that partial information about upcoming (planned) sequence events is used to guide current responses, consistent with cascade models of activation during sequence production.",
    "keywords": [
      "conflict monitoring",
      "mismatch detection",
      "executive control",
      "partial errors"
    ]
  },
  {
    "id": "1841",
    "title": "Achievable Rate Regions and Performance Comparison of Half Duplex Bi-Directional Relaying Protocols",
    "abstract": "In a bi-directional relay channel, two nodes wish to exchange independent messages over a shared wireless half-duplex channel with the help of a relay. In this paper, we derive achievable rate regions for four new half-duplex protocols and compare these to four existing half-duplex protocols and outer bounds. In time, our protocols consist of either two or three phases. In the two phase protocols, both users simultaneously transmit during the first phase and the relay alone transmits during the second phase, while in the three phase protocol the two users sequentially transmit followed by a transmission from the relay. The relay may forward information in one of four manners; we outline existing amplify and forward (AF), decode and forward (DF), lattice based, and compress and forward (CF) relaying schemes and introduce the novel mixed forward scheme. The latter is a combination of CF in one direction and DF in the other. We derive achievable rate regions for the CF and Mixed relaying schemes for the two and three phase protocols. We provide a comprehensive treatment of eight possible half-duplex bi-directional relaying protocols in Gaussian noise, obtaining their relative performance under different SNR and relay geometries.",
    "keywords": [
      "achievable rate regions",
      "bi-directional communication",
      "compress and forward",
      "relaying"
    ]
  },
  {
    "id": "1842",
    "title": "Fuzzy subgroups as products",
    "abstract": "We find necessary and sufficient conditions for a fuzzy subgroup of a Cartesian product of groups to be a Cartesian product of fuzzy subgroups.  ",
    "keywords": [
      "t-fuzzy subgroup"
    ]
  },
  {
    "id": "1843",
    "title": "Experiments in multirobot coordination",
    "abstract": "Consequent to previously published theoretical work by Marshall, Broucke, and Francis, this paper summarizes the apparatus and results of multirobot coordination experiments conducted at the University of Toronto Institute for Aerospace Studies. These experiments successfully demonstrate the practicality of cyclic pursuit as a distributed control strategy for multiple wheeled-robot systems. The pursuit-based coordination algorithm was found to be surprisingly robust in the presence of unmodelled dynamics and delays due to sensing and information processing. Moreover, the findings of this research not only bode well for continuing research on pursuit-based coordination strategies, but also for other cooperative multirobot control techniques employing similar local interactions.",
    "keywords": [
      "autonomous robots",
      "multiagent coordination",
      "cooperative control"
    ]
  },
  {
    "id": "1844",
    "title": "Microwave Dual-Band Bandstop Filter with Improved Spurious Resonance Behavior",
    "abstract": "A design technique to improve the spurious resonance behavior of dual-band bandstop filters is presented. A compact dual-band bandstop filter with two stop bands that can be controlled independently with improved passband frequency response is designed. The operational bandwidth of the proposed compact dual-band bandstop filter is increased by pushing the first spurious resonance from being about twice the resonance frequency to more than three times the resonance frequency. Stepped impedance open loop resonators with substantially increased outer-edge width are used to improve the spurious resonance response. Both simulation and measured results are presented and good agreement is obtained between the results. The fabricated filter exhibits dual operating frequencies at 1460 MHz and 2640 MHz with 5.5% and 5% stopband fractional bandwidths, respectively. .",
    "keywords": [
      "bandstop filters",
      "dual-band filters",
      "microstrip filters",
      "open-loop resonators",
      "spurious response"
    ]
  },
  {
    "id": "1845",
    "title": "Balanced random interval arithmetic in market model estimation",
    "abstract": "The possibility of estimating bounds for the econometric likelihood function using balanced random interval arithmetic is experimentally investigated. The experiments on the likelihood function with data from housing starts have proved the assumption that distributions of centres and radii of evaluated balanced random intervals are normal. Balanced random interval arithmetic can therefore be used to estimate bounds for this function and global optimization algorithms based on this arithmetic are applicable to optimize it. The interval branch and bound algorithms with bounds calculated using standard and balanced random interval arithmetic were used to optimize the likelihood function. Results of the experiments show that when reliability is essential the algorithm with standard interval arithmetic should be used, but when speed of optimization is more important, the algorithm with balanced random interval arithmetic should be used which in this case finishes faster and provides good, although not always optimal, values.",
    "keywords": [
      "global optimization",
      "interval arithmetic",
      "balanced random interval arithmetic"
    ]
  },
  {
    "id": "1846",
    "title": "Unbordered partial words",
    "abstract": "An unbordered  word is a string over a finite alphabet such that none of its proper prefixes is one of its suffixes. In this paper, we extend the results on unbordered words to unbordered partial words. Partial words are strings that may have a number of do not know symbols. We extend a result of Ehrenfeucht and Silberger which states that if a word u can be written as a concatenation of nonempty prefixes of a word v , then u can be written as a unique concatenation of nonempty unbordered prefixes of v . We study the properties of the longest unbordered prefix of a partial word, investigate the relationship between the minimal weak period of a partial word and the maximal length of its unbordered factors, and also investigate some of the properties of an unbordered partial word and how they relate to its critical factorizations (if any).",
    "keywords": [
      "words",
      "partial words",
      "unbordered words",
      "unbordered partial words"
    ]
  },
  {
    "id": "1847",
    "title": "Using the Rhythm of Nonverbal Human-Robot Interaction as a Signal for Learning",
    "abstract": "Human-robot interaction is a key issue in order to build robots for everyone. The difficulty for people to understand how robots work and how they must be controlled will be one of the mains limit for broad robotics. In this paper, we study a new way of interacting with robots without needing to understand how robots work or to give them explicit instructions. This work is based on psychological data showing that synchronization and rhythm are very important features for pleasant interaction. We propose a biologically inspired architecture using rhythm detection to build an internal reward for learning. After showing the results of keyboard interactions, we present and discuss the results of real human-robots (Aibo and Nao) interactions. We show that our minimalist control architecture allows the discovery and learning of arbitrary sensorimotor associations games with expert users. With nonexpert users, we show that using only the rhythm information is not sufficient for learning all the associations due to the different strategies used by the human. Nevertheless, this last experiment shows that the rhythm is still allowing the discovery of subsets of associations, being one of the promising signal of tomorrow social applications.",
    "keywords": [
      "artificial neural networks",
      "autonomous robotics",
      "human-robot interaction",
      "rhythm detection and prediction",
      "self-supervised learning"
    ]
  },
  {
    "id": "1848",
    "title": "Monochromatic Clique Decompositions of Graphs",
    "abstract": "Let G be a graph whose edges are colored with k colors, and be a k-tuple of graphs. A monochromatic -decomposition of G is a partition of the edge set of G such that each part is either a single edge or forms a monochromatic copy of in color i, for some . Let be the smallest number ?, such that, for every order-n graph and every k-edge-coloring, there is a monochromatic -decomposition with at most ? elements. Extending the previous results of Liu and Sousa [Monochromatic -decompositions of graphs, J Graph Theory 76 (2014), 89100], we solve this problem when each graph in is a clique and is sufficiently large.",
    "keywords": [
      "monochromatic graph decomposition",
      "turn number",
      "ramsey number"
    ]
  },
  {
    "id": "1849",
    "title": "Wave overtopping over a sea dike",
    "abstract": "This paper describes a solver on the simulation of overtopping of water waves over sloping and vertical structures in a numerical wave tank (NWT). It involves a time-implicit cell-staggered approximately factored VOF finite volume (FV) approach for solution of unsteady incompressible NavierStokes (NS) equations with a free surface on nonuniform Cartesian cut-cell grids. The Godunov-type high-order upwind schemes are introduced for discretization of the convective fluxes, while the coupling of the pressure with the velocity is realized by a projection method. The effects of turbulence are incorporated with a subgrid-scale (SGS) model. A novel VOF solver is proposed for the capture of a free surface undergoing severe topological deformation related with breaking waves. Only an approximation for the free-surface boundary conditions neglects the viscous stress but surface tension is modelled as a body force. A blend of second- and fourth-order artificial damping terms is designed for enhancement of the numerical stability. Additionally, the cut-cell techniques are utilized for handling an arbitrary geometry, and an absorbing-generating boundary condition for a wave generator is applied. The calculated results are represented in terms of the surface elevation versus time at certain locations and the velocity fields created by regular and irregular waves. Furthermore, the convergence behavior, the grid refinement effects, the study of different SGS models, the surface tension and Reynolds number effects and the role of a turbulence model under breaking waves are discussed, including a comparison with measurements available.",
    "keywords": [
      "the ns equations with a free-surface",
      "overtopping of breaking waves",
      "a vof-based finite volume solver",
      "nonuniform cartesian cut-cell meshes"
    ]
  },
  {
    "id": "1850",
    "title": "Integrals of motion of the reduced three-wave interaction system",
    "abstract": "By using the Darboux method of integrability and solving linear partial differential equations, the whole classification of the integrals of motion of the reduced three-wave interaction system is obtained. Rigorous proof is given.  ",
    "keywords": [
      "integral of motion",
      "darboux method",
      "polynomial first integral",
      "the method of characteristic curves",
      "reduced three-wave interaction system"
    ]
  },
  {
    "id": "1851",
    "title": "Adaptive frame synchronization for surveillance system across a heterogeneous network",
    "abstract": "As mobile techniques are booming, the surveillance function is extended from a stationary mode to a mobile mode. In a heterogeneous network environment, cameras and viewers are located in different networks so that frame synchronization may span across diverse network domains with different transmission capabilities. The mismatch of transmission capabilities may affect the viewing continuity and playback liveness between cameras and viewers. In the article, we propose an adaptive frame synchronization mechanism for frame capturing at cameras based on the network condition to improve the frame synchronization between two sides across a heterogeneous network. Based on a brief theoretical analysis of the asynchronization effect for video communication in a heterogeneous network environment, the proposed adaptive pause time mechanism can be an effective solution to relieve the asynchronization effect in the unmatched transmission rate situation. The evaluation results show that the proposed scheme can achieve a shorter time delay between the captured frames at the camera site and the viewer site.",
    "keywords": [
      "surveillance system",
      "heterogeneous network",
      "frame synchronization",
      "playback liveness"
    ]
  },
  {
    "id": "1852",
    "title": "Quasi-disjoint pentadiagonal matrix systems for the parallelization of compact finite-difference schemes and filters",
    "abstract": "This paper proposes a novel systematic approach for the parallelization of pentadiagonal compact finite-difference schemes and filters based on domain decomposition. The proposed approach allows a pentadiagonal banded matrix system to be split into quasi-disjoint subsystems by using a linearalgebraic transformation technique. As a result the inversion of pentadiagonal matrices can be implemented within each subdomain in an independent manner subject to a conventional halo-exchange process. The proposed matrix transformation leads to new subdomain boundary (SB) compact schemes and filters that require three halo terms to exchange with neighboring subdomains. The internode communication overhead in the present approach is equivalent to that of standard explicit schemes and filters based on seven-point discretization stencils. The new SB compact schemes and filters demand additional arithmetic operations compared to the original serial ones. However, it is shown that the additional cost becomes sufficiently low by choosing optimal sizes of their discretization stencils. Compared to earlier published results, the proposed SB compact schemes and filters successfully reduce parallelization artifacts arising from subdomain boundaries to a level sufficiently negligible for sophisticated aeroacoustic simulations without degrading parallel efficiency. The overall performance and parallel efficiency of the proposed approach are demonstrated by stringent benchmark tests.",
    "keywords": [
      "parallel computing",
      "compact schemes",
      "compact filters",
      "computational aeroacoustics"
    ]
  },
  {
    "id": "1853",
    "title": "A data cloning algorithm for computing maximum likelihood estimates in spatial generalized linear mixed models",
    "abstract": "Non-Gaussian spatial data are common in many sciences such as environmental sciences, biology and epidemiology. Spatial generalized linear mixed models (SGLMMs) are flexible models for modeling these types of data. Maximum likelihood estimation in SGLMMs is usually made cumbersome due to the high-dimensional intractable integrals involved in the likelihood function and therefore the most commonly used approach for estimating SGLMMs is based on the Bayesian approach. This paper proposes a computationally efficient strategy to fit SGLMMs based on the data cloning (DC) method suggested by Lele etal. (2007). This method uses Markov chain Monte Carlo simulations from an artificially constructed distribution to calculate the maximum likelihood estimates and their standard errors. In this paper, the DC method is adapted and generalized to estimate SGLMMs and some of its asymptotic properties are explored. Performance of the method is illustrated by a set of simulated binary and Poisson count data and also data about car accidents in Mashhad, Iran. The focus is inference in SGLMMs for small and medium data sets.",
    "keywords": [
      "data cloning",
      "generalized linear mixed models",
      "mcmc algorithms",
      "spatial generalized linear mixed models"
    ]
  },
  {
    "id": "1854",
    "title": "Variational approach for a general financial equilibrium problem: The Deficit Formula, the Balance Law and the Liability Formula. A path to the economy recovery",
    "abstract": "General equilibrium financial model. Variational inequality formulation. Propositions for the recovery of the economy.",
    "keywords": [
      "variational inequality formulation",
      "lagrange multipliers",
      "deficit formula",
      "balance law",
      "liability formula",
      "evaluation index"
    ]
  },
  {
    "id": "1855",
    "title": "Transient analysis of an M/G/1 retrial queue subject to disasters and server failures",
    "abstract": "An M/G/1 retrial queueing system with disasters and unreliable server is investigated in this paper. Primary customers arrive in the system according to a Poisson process, and they receive service immediately if the server is available upon their arrivals. Otherwise, they will enter a retrial orbit and try their luck after a random time interval. We assume the catastrophes occur following a Poisson stream, and if a catastrophe occurs, all customers in the system are deleted immediately and it also causes the servers breakdown. Besides, the server has an exponential lifetime in addition to the catastrophe process. Whenever the server breaks down, it is sent for repair immediately. It is assumed that the service time and two kinds of repair time of the server are all arbitrarily distributed. By applying the supplementary variables method, we obtain the Laplace transforms of the transient solutions and also the steady-state solutions for both queueing measures and reliability quantities of interest. Finally, numerical inversion of Laplace transforms is carried out for the blocking probability of the system, and the effects of several system parameters on the blocking probability are illustrated by numerical inversion results.",
    "keywords": [
      "retrial queues",
      "disasters",
      "reliability",
      "transient analysis",
      "laplace transforms",
      "numerical inversion"
    ]
  },
  {
    "id": "1856",
    "title": "Linear programming in disassembly/clustering sequence generation",
    "abstract": "Based on earlier research on optimal disassembly sequence generation, notably graphically supported search methods, a method for solving general optimal disassembly sequence generation problems by linear programming has been developed and described. This method puts no serious restriction to the size of the model,lt is adaptable to changes in model structures and constraints, and it is even not restricted to divergent operations, such as disassembly. This is demonstrated by solving the combined disassembly/clustering problem. Clustering is the combination of different disassembly products to categories that can be considered - and sold - as a single product, e.g. steel parts. The theoretical model is validated and clarified by its application to some standard problems from literature.  ",
    "keywords": [
      "disassembly sequence",
      "recycling",
      "mathematical programming",
      "design for disassembly",
      "design for environment"
    ]
  },
  {
    "id": "1857",
    "title": "Real-time massive convolution for audio applications on GPU Massive convolution on GPU",
    "abstract": "Massive convolution is the basic operation in multichannel acoustic signal processing. This field has experienced a major development in recent years. One reason for this has been the increase in the number of sound sources used in playback applications available to users. Another reason is the growing need to incorporate new effects and to improve the hearing experience. Massive convolution requires high computing capacity. GPUs offer the possibility of parallelizing these operations. This allows us to obtain the processing result in much shorter time and to free up CPU resources. One important aspect lies in the possibility of overlapping the transfer of data from CPU to GPU and vice versa with the computation, in order to carry out real-time applications. Thus, a synthesis of 3D sound scenes could be achieved with only a peer-to-peer music streaming environment using a simple GPU in your computer, while the CPU in the computer is being used for other tasks. Nowadays, these effects are obtained in theaters or funfairs at a very high cost, requiring a large quantity of resources. Thus, our work focuses on two mains points: to describe an efficient massive convolution implementation and to incorporate this task to real-time multichannel-sound applications.",
    "keywords": [
      "massive convolution",
      "multichannel audio processing",
      "fft",
      "gpu"
    ]
  },
  {
    "id": "1858",
    "title": "Security notes on generalization of threshold signature and authenticated encryption",
    "abstract": "In 2000, Wang et al. proposed a (t, n) threshold signature scheme with (k, l) threshold shared verification, and a (t, n) threshold authenticated encryption scheme with (k, l) threshold shared verification. Later, Tseng et al. mounted some attacks against Wang et al.'s schemes. At the same, they also presented the improvements. In this paper, we first point out that Tseng et al.'s attacks are actually invalid due to their misunderstanding of Wang et al.'s Schemes. Then, we show that both Wang et al.'s schemes and Tseng et al.'s improvements are indeed insecure by demonstrating several effective attacks.",
    "keywords": [
      "digital signature",
      "threshold signature",
      "authenticated encryption",
      "security",
      "cryptography"
    ]
  },
  {
    "id": "1859",
    "title": "Brief view on control of grid-interfacing AC-DC-AC converter and active filter under unbalanced and distorted voltage conditions",
    "abstract": "Purpose - The purpose of this paper is to consider both sides of a back-to-back AC-DC-AC interface. Design/methodology/approach - The paper presents a mathematical 'analysis, simulation, laboratory test in scaled model. Findings - The two main findings comprised concept of control methods for grid AC-DC-AC converter applied in renewable energy sources with variable speed operation under distorted grid. Active filtering functionality in case of non-linear current of a parallel load. Second, a control algorithm dedicated for two-level AC-DC converter applied in industrial networks with high-order harmonics compensation working under hard conditions balanced and unbalanced voltage dips. Research limitations/implications - The paper shows preliminary results for AC-DC-AC converter and active filter (AF) during voltage dips and for harmonics compensation. Control methods and/or topology should be improved and tested in scale and after at high-power system. Practical implications - Power quality supplied/received to/from the grid can be increased. In case of low-cost system only AF can be applied to existing non-linear receivers. Moreover, in case of full AC-DC-AC converter energy saving and production is possible. Originality/value - Presented control methods give satisfactory results. Paper presents laboratory results for grid and machine side two different power circuits during steady states and transients. Moreover, active filtering operation during voltage dips is presented.",
    "keywords": [
      "wind power",
      "electric power generation",
      "electric converters"
    ]
  },
  {
    "id": "1860",
    "title": "A methodology for evaluating test coverage criteria of high level Petri nets",
    "abstract": "High level Petri nets have been extensively used for modeling concurrent systems: however, their strong expressive power reduces their ability to be easily analyzed. Currently there are few effective formal analysis techniques to support the validation of high level Petri nets. The executable nature of high level Petri nets means that during validation they can be analyzed using test criteria defined on the net model. Recently, theoretical test adequacy coverage criteria for concurrent systems using high level Petri nets have been proposed. However, determining the applicability of these test adequacy criteria has not yet been undertaken. In this paper, we present an approach for evaluating the proposed test adequacy criteria for high level Petri nets through experimentation. In our experiments we use the simulation functionality of the model checker SPIN to analyze various test coverage criteria on high level Petri nets.  ",
    "keywords": [
      "predicate/transition nets",
      "software testing",
      "test adequacy criteria",
      "model checker spin"
    ]
  },
  {
    "id": "1861",
    "title": "Listing all potential maximal cliques of a graph",
    "abstract": "A potential maximal clique of a graph is a vertex set that induces a maximal clique in some minimal triangulation of that graph. It is known that if these objects can be listed in polynomial time for a class of graphs, the treewidth and the minimum fill-in are polynomially tractable for these graphs. We show here that the potential maximal cliques of a graph can be generated in polynomial time in the number of minimal separators of the graph. Thus, the treewidth and the minimum fill-in are polynomially tractable for all classes of graphs with a polynomial number of minimal separators.  ",
    "keywords": [
      "graph algorithms",
      "treewidth",
      "minimal separators",
      "potential maximal cliques"
    ]
  },
  {
    "id": "1862",
    "title": "Acyclic chromatic indices of planar graphs with large girth",
    "abstract": "An acyclic edge coloring of a graph G is a proper edge coloring such that no bichromatic cycles are produced. The acyclic chromatic index a?(G) a ? ( G ) of G is the smallest k such that G has an acyclic edge coloring using k colors. In this paper, we prove that every planar graph G with girth g(G) g ( G ) and maximum degreehas a?(G)=? a ? ( G ) = ? if there exists a pair (k,m)?{(3,11),(4,8),(5,7),(8,6)} ( k , m ) ? { ( 3 , 11 ) , ( 4 , 8 ) , ( 5 , 7 ) , ( 8 , 6 ) } such that G satisfieskk and g(G)?m g ( G ) ? m .",
    "keywords": [
      "acyclic edge coloring",
      "acyclic chromatic index",
      "planar graph",
      "girth"
    ]
  },
  {
    "id": "1863",
    "title": "cheat-prevention and -analysis in online virtual worlds",
    "abstract": "Virtual environments and online games are becoming a major market force. At the same time, the virtual property contained in these environments is being traded for real money and thus attains a real value. Although the legal issues involved with this virtual property have not yet been decided, they will have to be soon. To protect virtual property, virtual environment systems will have to conform to certain requirements. We analyze what these requirements are in order to either prevent cheating or at least prove a digital offense has transpired. Along with greater security, this will also reduce the cost of support, which is one of the major cost factors for online games.",
    "keywords": [
      "virtual environments",
      "fraud",
      "virtual economy",
      "games",
      "virtual property",
      "cheats"
    ]
  },
  {
    "id": "1864",
    "title": "multi-optimization power management for chip multiprocessors",
    "abstract": "The emergence of power as a first-class design constraint has fueled the proposal of a growing number of run-time power optimizations. Many of these optimizations trade-off power saving opportunity for a variable performance loss which depends on application characteristics and program phase. Furthermore, the potential benefits of these optimizations are sometimes non-additive, and it can be difficult to identify which combinations of these optimizations to apply. Trial-and-error approaches have been proposed to adaptively tune a processor. However, in a chip multiprocessor, the cost of individually configuring each core under a wide range of optimizations might be prohibitive under simple trial-and-error approaches. In this work, we introduce an adaptive, multi-optimization power saving strategy for multi-core power management. Specifically, we solve the problem of meeting a global chip wide power budget through run-time adaptation of highly configurable processor cores. Our approach applies analytic modeling to reduce exploration time and decrease the reliance on trial-and-error methods. We also introduce risk evaluation to balance the benefit of various power saving optimizations versus the potential performance loss. Overall, we find that our approach can significantly reduce processor power consumption compared to alternative optimization strategies.",
    "keywords": [
      "cache resizing",
      "voltage/frequency scaling",
      "applications",
      "analytic models",
      "risk",
      "design",
      "performance",
      "timing",
      "constraint",
      "errors",
      "evaluation",
      "exploration",
      "multi core",
      "processor",
      "chip multiprocessors",
      "variability",
      "method",
      "adapt",
      "strategies",
      "power consumption",
      "meeting",
      "optimality",
      "chip multi-processor",
      "configurability",
      "budget",
      "power",
      "power-save",
      "core",
      "power-management",
      "dynamic power management",
      "cost",
      "global",
      "class",
      "power optimization",
      "emergence"
    ]
  },
  {
    "id": "1865",
    "title": "precise and efficient parametric path analysis",
    "abstract": "Hard real-time systems require tasks to finish in time. To guarantee the timeliness of such a system, static timing analyses derive upper bounds on the worst-case execution time (WCET) of tasks. There are two types of timing analyses: numeric and parametric. A numeric analysis derives a numeric timing bound and, to this end, assumes all information such as loop bounds to be given a priori. If these bounds are unknown during analysis time, a parametric analysis can compute a timing formula parametric in these variables. A performance bottleneck of timing analyses, numeric and especially parametric, is the so-called path analysis, which determines the path in the analyzed task with the longest execution time bound. In this paper, we present a new approach to path analysis. This approach exploits the often rather regular structure of software for hard real-time and safety-critical systems. As we show in the evaluation of this paper, we strongly improve upon former techniques in terms of precision and runtime in the parametric case. Even in the numeric case, the approach competes with state-of-the-art techniques and may be an alternative to commercial tools employed for path analysis.",
    "keywords": [
      "graph theory",
      "parametric timing analysis"
    ]
  },
  {
    "id": "1866",
    "title": "Tolerance design of manipulator parameters using design of experiment approach",
    "abstract": "A robotic arm must manipulate objects with high accuracy and repeatability to perform precise tasks. There are many factors that cause variations in performance and they referred as noise factors. A probabilistic approach has been used to model the effects of noise factors and an experimental design technique has been adopted to select optimal tolerance of kinematic and dynamic parameters for minimal performance variations. The control and noise factor arrays are employed to identify statistically significant parameters and their interactions. The performance measures like signal to noise ratio and reliability have been utilized and results are validated by Monte Carlo simulations. The proposed design of experiment methodology requires minimal computations. The tolerance design methodology of manipulator is illustrated by 2-DOF revolute-revolute planar manipulator following cubic and quintic trajectory to perform a task. The statistical analysis of simulated performances is carried out using analysis of variance technique, which showed that statistically significant parameters are independent of trajectory. The individual parameter tolerance sensitivity has also been carried out.",
    "keywords": [
      "tolerance design",
      "control factors",
      "noise factors",
      "positional error-s/n ratio",
      "reliability",
      "parameter tolerance sensitivity"
    ]
  },
  {
    "id": "1867",
    "title": "body-bias compensation technique for subthreshold cmos static logic gates",
    "abstract": "This paper analyzes the performance of the conventional CMOS inverter, NAND-2 and NOR-2 static logic gates operating in the subthreshold region. The dependence of the drain currents on the process parameters can give rise to drive currents of NMOS and PMOS transistors that differ by an order of magnitude or even more. To compensate for this difference in currents, we propose three bias circuits in single-well processes that adjust the body voltage. Computer simulations using the AMS 0.8um technology and the BSIM3v3 model were carried out to assess the compensation technique. A test chip was fabricated in both AMIS 1.5um and TSMC0.35um to further validate the proposal.",
    "keywords": [
      "subthreshold",
      "logic circuits",
      "cmos",
      "body-bias compensation",
      "static logic",
      "low-power"
    ]
  },
  {
    "id": "1868",
    "title": "temperature-aware microarchitecture",
    "abstract": "With power density and hence cooling costs rising exponentially, processor packaging can no longer be designed for the worst case, and there is an urgent need for runtime processor-level techniques that can regulate operating temperature when the package's capacity is exceeded. Evaluating such techniques, however, requires a thermal model that is practical for architectural studies.This paper describes HotSpot, an accurate yet fast model based on an equivalent circuit of thermal resistances and capacitances that correspond to microarchitecture blocks and essential aspects of the thermal package. Validation was performed using finite-element simulation. The paper also introduces several effective methods for dynamic thermal management (DTM): \"temperature-tracking\" frequency scaling, localized toggling, and migrating computation to spare hardware units. Modeling temperature at the microarchitecture level also shows that power metrics are poor predictors of temperature, and that sensor imprecision has a substantial impact on the performance of DTM.",
    "keywords": [
      "sensor",
      "simulation",
      "scale",
      "performance",
      "computation",
      "metrication",
      "temperature",
      "case",
      "model",
      "paper",
      "microarchitecture",
      "practical",
      "runtime",
      "processor",
      "aspect",
      "method",
      "architecture",
      "packaging",
      "dynamic",
      "capacities",
      "operability",
      "power",
      "tracking",
      "hardware",
      "thermal management",
      "effect",
      "circuits",
      "finite element"
    ]
  },
  {
    "id": "1869",
    "title": "Flux reconstruction and solution post-processing in mimetic finite difference methods",
    "abstract": "We present a post-processing technique for the mimetic finite difference solution of diffusion problems in mixed form. Our post-processing method yields a piecewise linear approximation of the scalar variable that is second-order accurate in the L2-norm on quite general polyhedral meshes, including non-convex and non-matching elements. The post-processing is based on the reconstruction of vector fields projected onto the mimetic space of vector variables. This technique is exact on constant vector fields and is shown to be independent of the mimetic scalar product choice if a local consistency condition is satisfied. The post-processing method is computationally inexpensive. Optimal performance is confirmed by numerical experiments.",
    "keywords": [
      "polyhedral mesh",
      "mimetic finite difference method",
      "compatible discretization",
      "solution post-processing",
      "gradient reconstruction",
      "super-convergence"
    ]
  },
  {
    "id": "1870",
    "title": "A partition of unity method for a class of fourth order elliptic variational inequalities",
    "abstract": "We consider a partition of unity method (PUM) for a class of fourth order elliptic variational inequalities on convex polygonal domains that include obstacle problems of simply supported Kirchhoff plates and elliptic distributed optimal control problems with pointwise state constraints as special cases. By including singular functions in the local approximation spaces we are able to show that the partition of unity method converges optimally. Numerical results that corroborate the theoretical estimates are also presented.",
    "keywords": [
      "Partition of unity method",
      "Fourth order variational inequalities",
      "Optimal control"
    ]
  },
  {
    "id": "1871",
    "title": "approximate computation of multidimensional aggregates of sparse data using wavelets",
    "abstract": "Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries. In this paper, we present a novel method that provides approximate answers to high-dimensional OLAP aggregation queries in massive sparse data sets in a time-efficient and space-efficient manner. We construct a compact data cube , which is an approximate and space-efficient representation of the underlying multidimensional array, based upon a multiresolution wavelet decomposition. In the on-line phase, each aggregation query can generally be answered using the compact data cube in one I/O or a smalll number of I/Os, depending upon the desired accuracy. We present two I/O-efficient algorithms to construct the compact data cube for the important case of sparse high-dimensional arrays , which often arise in practice. The traditional histogram methods are infeasible for the massive high-dimensional data sets in OLAP applications. Previously developed wavelet techniques are efficient only for dense data. Our on-line query processing algorithm is very fast and capable of refining answers as the user demands more accuracy. Experiments on real data show that our method provides significantly more accurate results for typical OLAP aggregation queries than other efficient approximation techniques such as random sampling.",
    "keywords": [
      "histogram",
      "applications",
      "data cube",
      "efficiency",
      "approximation",
      "performance",
      "computation",
      "arrays",
      "experience",
      "case",
      "timing",
      "high-dimensional data",
      "paper",
      "representation",
      "practical",
      "user",
      "multiresolution",
      "random sample",
      "decomposition",
      "method",
      "compact",
      "accuracy",
      "environments",
      "space",
      "wavelet",
      "aggregate",
      "storage",
      "data",
      "queries",
      "data warehouse",
      "algorithm",
      "refine",
      "query processing",
      "query"
    ]
  },
  {
    "id": "1872",
    "title": "Prevention slot flow-control mechanism for low latency torus network-on-chip",
    "abstract": "The challenge for on-chip networks is to provide low latency communication in a very low power budget. To reduce the latency and maintain the simplicity of a mesh topology, torus topology is proposed. As torus topology has an inherent circular dependency, additional effort is needed to prevent deadlock, even if deadlock free routing algorithms are used. The authors propose a novel flow-control mechanism to address cost/performance constraints in torus networks and ensure deadlock freedom. They achieve flow-control by using a prevention mechanism and ensure deadlock freedom while requiring only a single packet buffer per input port. They simplify the router design by having a simple switch allocator that prioritises in-flight packets, and a single packet buffer per input port that eliminates the need for virtual channels. They also propose a mechanism to avoid starvation that can arise because of the prioritised arbitration. Experimental validation reveals that the authors design achieves significant improvement in throughput, as compared with the traditional design, while using significantly fewer buffers.",
    "keywords": [
      "mesh generation",
      "network-on-chip",
      "topology",
      "prevention slot flow-control mechanism",
      "low latency torus network-on-chip",
      "mesh topology",
      "torus topology",
      "inherent circular dependency",
      "deadlock free routing algorithms",
      "novel flow-control mechanism",
      "simple switch allocator",
      "single packet buffer",
      "virtual channels"
    ]
  },
  {
    "id": "1873",
    "title": "?-polymers in crowded media under stretching force",
    "abstract": "We study the peculiarities of stretching of globular polymer macromolecules in a disordered (crowded) environment, using the model of self-attracting self-avoiding walks on site-diluted percolative lattices in space dimensions d=3 d = 3 . Applying the prunedenriched Rosenbluth chain-growth method (PERM), we construct the phase diagram of collapsedextended state coexistence when varying temperature and stretching force. The change in shape characteristics of globular polymers under stretching is analyzed as well.",
    "keywords": [
      "polymers",
      "percolation",
      "self-avoiding walks",
      "disorder"
    ]
  },
  {
    "id": "1874",
    "title": "From Complex B-1 Mapping to Local SAR Estimation for Human Brain MR Imaging Using Multi-Channel Transceiver Coil at 7T",
    "abstract": "Elevated specific absorption rate (SAR) associated with increased main magnetic field strength remains a major safety concern in ultra-high-field (UHF) magnetic resonance imaging (MRI) applications. The calculation of local SAR requires the knowledge of the electric field induced by radio-frequency (RF) excitation, and the local electrical properties of tissues. Since electric field distribution cannot be directly mapped in conventional MR measurements, SAR estimation is usually performed using numerical model-based electromagnetic simulations which, however, are highly time consuming and cannot account for the specific anatomy and tissue properties of the subject undergoing a scan. In the present study, starting from the measurable RF magnetic fields (B-1) in MRI, we conducted a series of mathematical deduction to estimate the local, voxel-wise and subject-specific SAR for each single coil element using a multi-channel transceiver array coil. We first evaluated the feasibility of this approach in numerical simulations including two different human head models. We further conducted experimental study in a physical phantom and in two human subjects at 7T using a multi-channel transceiver head coil. Accuracy of the results is discussed in the context of predicting local SAR in the human brain at UHF MRI using multi-channel RF transmission.",
    "keywords": [
      "electrical properties tomography ",
      "magnetic resonance imaging ",
      "parallel transmission",
      "specific absorption rate ",
      "ultra-high-field ",
      "b-1-mapping"
    ]
  },
  {
    "id": "1875",
    "title": "Investigating Participatory Dynamics Through Social Media Using a Multideterminant Frame Approach: The Case of Climategate on YouTube",
    "abstract": "This paper offers a framework for examining the relationship between social, instrumental, and technological determinants of participation through social media (Dahlberg, 2004) using a discursive approach based in the concepts of frames and framing (Goffman, 1974; Snow & Benford, 1992). We apply our multideterminant framework to investigate participatory dynamics on YouTube in the case of climategate. Our interpretive analysis of videos and comments shows how public responses to climategate were scripted around 3 dominant master frames, reinforced by calls to collective action and media form. Our multideterminant framework makes a contribution to the debate over the transformative potential of social media by providing a method to assess the relative value of social media in response to specific social problems.",
    "keywords": [
      "participation",
      "social media",
      "climate change",
      "youtube",
      "frames",
      "social problems"
    ]
  },
  {
    "id": "1876",
    "title": "Simple scheduling algorithms for use with a waveguide grating multiplexer based local optical network",
    "abstract": "As the need for greater bandwidth in local-area networks grows, wavelength-division multiplexing (WDM) is gathering attention as a viable successor to Gigabit Ethernet technologies. In this paper we introduce a new WDM optical LAN architecture based on the waveguide grating multiplexer (WGM) rather than the traditional passive star coupler (PSC). An N-port WGM allows N-2 x N-2 connectivity via only N physical wavelengths, due to its inherent space-division multiplexing property. Wavelength-routed networks based on the WGM promise to be significant components of future LAN and WAN technologies not only because of their efficient use of wavelengths, but also because they have been implemented as integrated devices. We propose simple, low-complexity TDM schedules for interconnecting MN nodes (M nodes per port) with a WGM in a local environment. Each node is equipped with a single tunable transmitter and a single tunable receiver (both of which can access all N wavelengths). Various transmitter and receiver tuning latencies are considered. We show that, for negligible tuning latencies, aggregate network throughput approaching min(MN, N-2) can be achieved, and for tuning latencies on the order of a packet length or more, throughput on the order of N can be achieved. Since these performance metrics are vastly superior to that of an equivalent PSC-based system (whose maximum throughput is limited by the number of wavelengths, N), we propose that the WGM be considered as an alternative to the PSC for enabling WDM LANs and multiprocessor interconnects.",
    "keywords": [
      "optical network",
      "local-area network",
      "waveguide grating multiplexer",
      "wavelength routing",
      "wdm",
      "tdm packet scheduling"
    ]
  },
  {
    "id": "1877",
    "title": "probabilistic model for the interfaces personalized creation",
    "abstract": "User interfaces are designed taking in account different user needs (preferences, abilities, etc.) and various prototypes and usability tests are carried out during their development. However, when they are used by their final users, it can be seen that the interfaces do not meet the users requirements or, more usually, it is evident that the UI can be improved. This paper presents a system that helps to personalize a user interface for an art digital library. The system has different predefined interfaces according to end-user characteristics, but the users can tailor the final appearance of the information to be showed by the system. From these different adaptations, tailored by users, the system tries to find similarities that can result in a modification of the default interface that is presented to new users of the system. The new interface could be distinct from the one constructed by the original interface designer.",
    "keywords": [
      "personalization",
      "presentation",
      "usability",
      "adaptive interfaces"
    ]
  },
  {
    "id": "1878",
    "title": "Intelligent region-based thresholding for color document images with highlighted regions",
    "abstract": "The study applies an intelligent region-based thresholding method for the binarization of color document images with highlighted regions. The results also indicate that the proposed method can threshold simultaneously when the background is gradually changing, reversed, or inseparable from the foreground, with efficient binarization results. Rather than the traditional method of scanning the entire document at least once, this method intelligently divides a document image into several foreground regions and decides the background range for each foreground region, in order to effectively process the detected document regions. Experimental results demonstrate the high effectiveness of the proposed method in providing promising binarization results with low computational cost. Furthermore, the results of the proposed method are more accurate than global, region-based, local, and hybrid methods. Images were analyzed using MODI OCR measurement data such as recall rate and precision rate. In particular, when test images produced under inadequate illumination are processed using the proposed method, the binarization results of this method have better visual quality and better measurable OCR performance than compared global, region-based, local, and hybrid methods. Moreover, the proposed algorithm can be run in an embedded system due to its simplicity and efficiency.",
    "keywords": [
      "region-based thresholding",
      "color document image",
      "highlighted region background",
      "restricted background projection"
    ]
  },
  {
    "id": "1879",
    "title": "Learning Rates of Tikhonov Regularized Regressions Based on Sample Dependent RKHS",
    "abstract": "It is known that the learning algorithm with sample hypothesis spaces is essentially different from the algorithms with hypothesis spaces independent of the sample. In the present paper, we consider the error bounds for norm square regularized regressions associated with Lipschitz loss and sample depending reproducing kernel spaces. By giving the unique solution with subgradients of the loss functions, we estimate the learning rates with the regularization parameters lambda and the sample number m. The sample error rates obtained are O(1/lambda root m) and the approximation error rates are O(1/root m + lambda).",
    "keywords": [
      "sample depending reproducing kernel hilbert spaces",
      "convex analysis",
      "lipschitz loss"
    ]
  },
  {
    "id": "1880",
    "title": "Reasoning about fuzzy temporal information from the web: towards retrieval of historical events",
    "abstract": "When searching for information about historical events, queries are naturally formulated using temporal constraints. However, the structured temporal information needed to support such constraints is usually not available to information retrieval systems. Furthermore, the temporal boundaries of most historical events are inherently ill-defined, calling for suitable extensions of classical temporal reasoning frameworks. In this paper, we propose a framework based on a fuzzification of Allens Interval Algebra to cope with these issues. By using simple heuristic techniques to extract temporal information from web documents, we initially focus more on recall than on precision, relying on the subsequent application of a fuzzy temporal reasoner to improve the reliability of the extracted information, and to deal with conflicts that arise because of the vagueness of events. Experimental results indicate that a consistent and reliable knowledge base of fuzzy temporal relations can thus be obtained, which effectively allows us to target temporally constrained retrieval tasks.",
    "keywords": [
      "temporal reasoning",
      "fuzzy set theory",
      "event-based retrieval"
    ]
  },
  {
    "id": "1881",
    "title": "On the Termination of Integer Loops",
    "abstract": "In this article we study the decidability of termination of several variants of simple integer loops, without branching in the loop body and with affine constraints as the loop guard (and possibly a precondition). We show that termination of such loops is undecidable in some cases, in particular, when the body of the loop is expressed by a set of linear inequalities where the coefficients are from Z boolean OR {r} with r an arbitrary irrational; when the loop is a sequence of instructions, that compute either linear expressions or the step function; and when the loop body is a piecewise linear deterministic update with two pieces. The undecidability result is proven by a reduction from counter programs, whose termination is known to be undecidable. For the common case of integer linear-constraint loops with rational coefficients we have not succeeded in proving either decidability or undecidability of termination, but we show that a Petri net can be simulated with such a loop; this implies some interesting lower bounds. For example, termination for a partially specified input is at least EXPSPACE-hard.",
    "keywords": [
      "verification",
      "theory",
      "integer loops",
      "termination",
      "linear constraints"
    ]
  },
  {
    "id": "1882",
    "title": "Dairy proteins and the response to pneumovax in senior citizens: a randomized, double-blind, placebo-controlled pilot study",
    "abstract": "With the progressive aging of the world's population, immunosenescence is rapidly becoming a clinical concern as it accounts for a higher incidence of severe infections and poor response to vaccines. To identify nutritional approaches that may counteract immunosenescence is of obvious importance in clinical practice. Dairy products in general and whey proteins in particular share the capacity to stimulate the immune system within the digestive tract while the antibody response to Streptococcus pneumoniae vaccine is a good marker of the immune function. We performed a controlled, randomized, double-blind pilot study to determine if an eight-week supplementation with whey protein (or soy protein used as control) could enhance the serum response to pneumococcal vaccine in healthy senior citizens. Out of 127 volunteers, 17 subjects were eligible and completed the study receiving the vaccine after four weeks of supplementation. Antibody levels were measured at baseline and the end of the study against 14 pneumococcal types and a detailed nutritional questionnaire was administered to all subjects. Subjects receiving whey protein manifested a serum response higher compared to the control soy supplementation against 12/14 bacterial types. In particular, whey led to a higher frequency of response to all four more virulent types (4, 9, 14, and 23). Calorie and protein intake data suggest a better nutritional status in the whey group. Whey protein supplementation is a promising supplement to stimulate the immune response to vaccine in senior citizens and possibly to counteract immunosenescence while larger studies are warranted.",
    "keywords": [
      "immunosenescence",
      "dietary supplements",
      "vaccine"
    ]
  },
  {
    "id": "1883",
    "title": "Shape and motion from simultaneous equations with closed-loop solution",
    "abstract": "This paper proposes a method for simultaneously estimating 2D image motion and 3D object shape and motion from only two frames. The problem is formulated in a system of equations, including the differential epipolar constraint, a newly derived optical flow equation and surface normal constraint, under the assumption of perspective projection, rigid motion, Lambertian reflectance and distant lighting. A closed-loop solver is constructed based on the simultaneous equations to export accurate estimate for optical flow as well as dense shape and motion. Experimental results are also provided.",
    "keywords": [
      "structure from motion",
      "optical flow",
      "surface normal constraint",
      "closed-loop solution"
    ]
  },
  {
    "id": "1884",
    "title": "Error analysis in electric power system available transfer capability computation",
    "abstract": "A key concept in the restructuring of the electric power industry is the ability to accurately and rapidly quantify the capabilities of the transmission system. Transmission transfer capability is limited by a number of different mechanisms, including thermal, voltage, and stability constraints. This paper discusses the available transfer capability (ATC) definitions and determination guidelines approved by the North American Electric Reliability Council (NERC) and presents several concepts for dealing with the potential errors and technical challenges of computation.",
    "keywords": [
      "available transfer capability ",
      "power system loadability"
    ]
  },
  {
    "id": "1885",
    "title": "A novel image denoising algorithm in wavelet domain using total variation and grey theory",
    "abstract": "Purpose - The traditional total variation (TV) models in wavelet domain use thresholding directly in coefficients selection and show that Gibbs' phenomenon exists. However, the nonzero coefficient index set selected by hard thresholding techniques may not be the best choice to obtain the least oscillatory reconstructions near edges. This paper aims to propose an image denoising method based on TV and grey theory in the wavelet domain to solve the defect of traditional methods. Design/methodology/approach - In this paper, the authors divide wavelet into two parts: low frequency area and high frequency area; in different areas different methods are used. They apply grey theory in wavelet coefficient selection. The new algorithm gives a new method of wavelet coefficient selection, solves the nonzero coefficients sort, and achieves a good image denoising result while reducing the phenomenon of \"Gibbs.\" Findings - The results show that the method proposed in this paper can distinguish between the information of image and noise accurately and also reduce the Gibbs artifacts. From the comparisons, the model proposed preserves the important information of the image very well and shows very good performance. Originality/value - The proposed image denoising model introducing grey relation analysis in the wavelet coefficients selecting and modifying is original. The proposed model provides a viable tool to engineers for processing the image.",
    "keywords": [
      "image processing",
      "mathematical modelling",
      "programming and algorithm theory"
    ]
  },
  {
    "id": "1886",
    "title": "Computable error estimators for the approximation of nonlinear problems by linearized models",
    "abstract": "In the modeling of nonlinear phenomena, a nonlinear model may often be replaced by a linear one, giving rise to a modeling or linearization error. This is in addition to the discretization error introduced when this linear model is solved, using, e.g., the finite element method. We investigate the a posteriori estimation of these errors for a general class of problems characterized by strongly monotone operators. Our results lead to the construction of computable upper estimators for the total error, with identifiable components from each of the these error sources. Several numerical tests evaluating the efficiency of our estimators are provided.",
    "keywords": [
      "a posteriori",
      "estimator",
      "linearization",
      "nonlinear elasticity"
    ]
  },
  {
    "id": "1887",
    "title": "Improving video-mediated communication with orchestration",
    "abstract": "Video-mediated communication (VMC) has become a popular communication medium. However, research to date suggests that the inherent constraints of VMC impair effective and efficient communication and task performance. We propose that these negative findings could be attributed to how the technology was used and propose the novel concept of communication orchestration aimed at mitigating some of the signaled limitations. Orchestration is a selection process for displaying information that is deemed relevant for accomplishing an effective and efficient task performance and communicative experience. We report an experiment that confirmed this suggestion. The results indicate that orchestration could be an important novel feature to aid humans when communicating via VMC, but also suggest that there is potential for further improvements in orchestration.",
    "keywords": [
      "video-mediated communication",
      "orchestration",
      "task performance",
      "user experience",
      "videoconferencing",
      "communication"
    ]
  },
  {
    "id": "1888",
    "title": "Algorithm 905: SHEPPACK: Modified Shepard Algorithm for Interpolation of Scattered Multivariate Data",
    "abstract": "Scattered data interpolation problems arise in many applications. Shepard's method for constructing a global interpolant by blending local interpolants using local-support weight functions usually creates reasonable approximations. SHEPPACK is a Fortran 95 package containing five versions of the modified Shepard algorithm: quadratic (Fortran 95 translations of Algorithms 660, 661, and 798), cubic (Fortran 95 translation of Algorithm 791), and linear variations of the original Shepard algorithm. An option to the linear Shepard code is a statistically robust fit, intended to be used when the data is known to contain outliers. SHEPPACK also includes a hybrid robust piecewise linear estimation algorithm RIPPLE (residual initiated polynomial-time piecewise linear estimation) intended for data from piecewise linear functions in arbitrary dimension m. The main goal of SHEPPACK is to provide users with a single consistent package containing most existing polynomial variations of Shepard's algorithm. The algorithms target data of different dimensions. The linear Shepard algorithm, robust linear Shepard algorithm, and RIPPLE are the only algorithms in the package that are applicable to arbitrary dimensional data.",
    "keywords": [
      "algorithms",
      "design",
      "documentation",
      "m-estimation",
      "ripple",
      "shepard's algorithm"
    ]
  },
  {
    "id": "1889",
    "title": "machine vision based liquid level inspection system using isef edge detection technique",
    "abstract": "We have developed a machine vision based liquid level inspection system which decides the liquid level of bottle to be under or overfilled using ISEF edge detection technique. The system is having a conveyer belt controlled by Siemens LOGO24RLC PLC. MATLAB image acquisition toolbox along with a normal web camera is used for image acquisition purpose. We apply ISEF Edge detection technique and an average distance algorithm to decide about the level of the liquid in the bottles. A GUI based interfacing software is made to display the over and under filled status of the bottles on the screen.",
    "keywords": [
      "machine vision",
      "edge detection",
      "average distance algorithm",
      "isef edge detection",
      "siemens plc",
      "quality inspection",
      "gui"
    ]
  },
  {
    "id": "1890",
    "title": "Web generation of experimental designs balanced for indirect effects of treatments (WEB-DBIE)",
    "abstract": "Online software to generate designs balanced for indirect effects of treatments. It consists of randomized layout of Neighbour Balanced and Crossover Designs. Online catalogue of designs within a permissible range of parameters is provided. This software (WEB-DBIE) is deployed at www.iasri.res.in/webdbie. It is freely available for the researchers and students working in this area.",
    "keywords": [
      "crossover designs",
      "indirect effects",
      "neighbour balanced designs",
      "randomization",
      "web generation of designs"
    ]
  },
  {
    "id": "1891",
    "title": "Inconsistency-tolerant reasoning with OWL DL",
    "abstract": "The Web Ontology Language (OWL) is a family of description logic based ontology languages for the Semantic Web and gives well defined meaning to web accessible information and services. The study of inconsistency-tolerant reasoning with description logic knowledge bases is especially important for the Semantic Web since knowledge is not always perfect within it. An important challenge is strengthening the inference power of inconsistency-tolerant reasoning because it is normally impossible for paraconsistent logics to obey all important properties of inference together. This paper presents a non-classical DL called quasi-classical description logic (QCDL) to tolerate inconsistency in OWL DL which is a most important sublanguage of OWL supporting those users who want the maximum expressiveness while retaining computational completeness (i.e., all conclusions are guaranteed to be computable) and decidability (i.e., all computations terminate in finite time). Instead of blocking those inference rules, we validate them conditionally and partially, under which more useful information can still be inferred when inconsistency occurs. This new non-classical DL possesses several important properties as well as its paraconsistency in DL, but it does not bring any extra complexity in worst case. Finally, a transformation-based algorithm is proposed to reduce reasoning problems in QCDL to those in DL so that existing OWL DL reasoners can be used to implement inconsistency-tolerant reasoning. Based on this algorithm, a prototype OWL DL paraconsistent reasoner called PROSE is implemented. Preliminary experiments show that PROSE produces more intuitive results for inconsistent knowledge bases than other systems in general.",
    "keywords": [
      "description logics",
      "owl dl",
      "quasi-classical logic",
      "paraconsistent reasoning",
      "inconsistency-tolerance"
    ]
  },
  {
    "id": "1892",
    "title": "Fault Tolerance in Distributed Systems Using Fused Data Structures",
    "abstract": "Replication is the prevalent solution to tolerate faults in large data structures hosted on distributed servers. To tolerate f crash faults (dead/unresponsive data structures) among n distinct data structures, replication requires f + 1 replicas of each data structure, resulting in nf additional backups. We present a solution, referred to as fusion that uses a combination of erasure codes and selective replication to tolerate f crash faults using just f additional fused backups. We show that our solution achieves O(n) savings in space over replication. Further, we present a solution to tolerate f Byzantine faults (malicious data structures), that requires only nf + f backups as compared to the 2nf backups required by replication. We explore the theory of fused backups and provide a library of such backups for all the data structures in the Java Collection Framework. The theoretical and experimental evaluation confirms that the fused backups are space-efficient as compared to replication, while they cause very little overhead for normal operation. To illustrate the practical usefulness of fusion, we use fused backups for reliability in Amazon's highly available key-value store, Dynamo. While the current replication-based solution uses 300 backup structures, we present a solution that only requires 120 backup structures. This results in savings in space as well as other resources such as power.",
    "keywords": [
      "distributed systems",
      "fault tolerance",
      "data structures"
    ]
  },
  {
    "id": "1893",
    "title": "Data multicasting procedure for increasing configuration speed of coarse grain reconfigurable devices",
    "abstract": "A novel configuration method called Row Multicast Configuration (RoMultiC) is proposed for high speed configuration of coarse grain reconfigurable systems. The same configuration data can be transferred in multicast fashion to configure many Processing Elements (PEs) by using a multicast bit-map provided in row and column directions of PE array. Evaluation results using practical applications show that a model reconfigurable system that incorporates this scheme can reduce configuration clock cycles by up to 73.1% compared with traditional configuration delivery scheme. Amount of required memory to store the configuration data at external memory is also reduced by omitting the duplicated configuration data.",
    "keywords": [
      "dynamically reconfigurable system",
      "high speed configuration",
      "configuration network"
    ]
  },
  {
    "id": "1894",
    "title": "Constitute: The worlds constitutions to read, search, and compare",
    "abstract": "Constitutional design and redesign is constant. Over the last 200 years, countries have replaced their constitutions an average of every 19years and some have amended them almost yearly. A basic problem in the drafting of these documents is the search and analysis of model text deployed in other jurisdictions. Traditionally, this process has been ad hoc and the results suboptimal. As a result, drafters generally lack systematic information about the institutional options and choices available to them. In order to address this informational need, the investigators developed a web application, Constitute [online at http://www.constituteproject.org], with the use of semantic technologies. Constitute provides searchable access to the worlds constitutions using the conceptualization, texts, and data developed by the Comparative Constitutions Project. An OWL ontology represents 330 topicse.g.right to healthwith which the investigators have tagged relevant provisions of nearly all constitutions in force as of September of 2013. The tagged texts were then converted to an RDF representation using R2RML mappings and Capsentas Ultrawrap. The portal implements semantic search features to allow constitutional drafters to read, search, and compare the worlds constitutions. The goal of the project is to improve the efficiency and systemization of constitutional design and, thus, to support the independence and self-reliance of constitutional drafters.",
    "keywords": [
      "constitution",
      "search",
      "rdb2rdf",
      "direct mapping",
      "r2rml",
      "ontology"
    ]
  },
  {
    "id": "1895",
    "title": "Energy efficiency in multiaccess fading channels under QoS constraints",
    "abstract": "In this article, transmission over multiaccess fading channels under quality-of-service (QoS) constraints is studied in the low-power and wideband regimes. QoS constraints are imposed as limitations on the buffer violation probability. The effective capacity, which characterizes the maximum constant arrival rates in the presence of such statistical QoS constraints, is employed as the performance metric. A two-user multiaccess channel model is considered, and the minimum bit energy levels and wideband slope regions are characterized for different transmission and reception strategies, namely time-division multiple-access (TDMA), superposition coding with fixed decoding order, and superposition coding with variable decoding order. It is shown that the minimum received bit energies achieved by these different strategies are the same and independent of the QoS constraints in the low-power regime, while they vary with the QoS constraints in the wideband regime. When wideband slope regions are considered, the suboptimality of TDMA with respect to superposition coding is proven in the low-power regime. On the other hand, it is shown that TDMA in the wideband regime can interestingly outperform superposition coding with fixed decoding order. The impact of varying the decoding order at the receiver under certain assumptions is also investigated. Overall, energy efficiency of different transmission strategies under QoS constraints are analyzed and quantified.",
    "keywords": [
      "effective capacity",
      "energy efficiency",
      "energy per information bit",
      "low-power regime",
      "multiple-access fading channels",
      "quality of service",
      "superposition coding",
      "time-division multiple access",
      "wideband regime",
      "wideband slope"
    ]
  },
  {
    "id": "1896",
    "title": "Fair resource allocation with guaranteed statistical QoS for multimedia traffic in wideband CDMA cellular network",
    "abstract": "A dynamic fair resource allocation scheme is proposed to efficiently support real-time and non-real-time multimedia traffic with guaranteed statistical quality of service (QoS) in the uplink of a wideband code-division multiple access (CDMA) cellular network. The scheme uses the generalized processor sharing (GPS) fair service discipline to allocate uplink channel resources, taking into account the characteristics of channel fading and intercell interference. In specific, the resource allocated to each traffic flow is proportional to an assigned weighting factor. For real-time traffic, the assigned weighting factor is a constant in order to guarantee the traffic statistical delay bound requirement; for non-real-time traffic, the assigned weighting factor can be adjusted dynamically according to fading channel states and the traffic statistical fairness bound requirement. Compared with the conventional static-weight scheme, the proposed dynamic-weight scheme achieves capacity gain. A flexible trade-off between the GIPS fairness and efficient resource utilization can also be achieved. Analysis and simulation results demonstrate that the proposed scheme enhances radio resource utilization and guarantees statistical QoS under different fairness bound requirements.",
    "keywords": [
      "resource allocation",
      "generalized processor sharing",
      "w-cdma cellular network",
      "quality of service",
      "multimedia traffic"
    ]
  },
  {
    "id": "1897",
    "title": "Looking beyond bone mineral density",
    "abstract": "Bone fracture is related to bone strength. In current clinical practice, bone mineral density (BMD) is used as the prime indicator of bone strength, not infrequently at the neglect of an even more pertinent measure of reduced bone strength, namely the radiographic presence of an insufficiency fracture. Bone strength depends not just on BMD but also on bone quality, which relates to such factors as bone architecture, turnover, mineralization, and cellularity. The high resolution available from current imaging techniques, particularly computed tomography and magnetic resonance imaging, along with advanced analytical software has greatly enhanced evaluation of bone architecture and strength. This has improved our knowledge of the pathophysiological processes behind osteoporosis and its treatment beyond that provided by BMD measurement alone. Although still in the experimental stage, these techniques will no doubt be incorporated into clinical practice, leading to a more tailored approach to the screening, monitoring, and treatment of osteoporosis.",
    "keywords": [
      "imaging",
      "bone quality",
      "radiography",
      "dxa",
      "computed tomography",
      "magnetic resonance imaging"
    ]
  },
  {
    "id": "1898",
    "title": "misconceived misconceptions",
    "abstract": "Detailed user activity scripts from two previous studies of novice users working at a command language or a direct representation interface were submitted to independent expert judges for the justified ascription of misconceptions . Our initial hypothesis was that behavioral evidence for such misconceptions comes about as a result of well-articulated hypothetical reasoning. Although the evidence we obtained supports this view, it also suggests that for the direct representation case some activity normally attributed to misconceptions is non-reasoned in nature and governed by inherent powers of the representation.",
    "keywords": [
      "direct",
      "interfaces",
      "activation",
      "case",
      "reasoning",
      "script",
      "evidence",
      "representation",
      "novice",
      "users",
      "user",
      "behavior",
      "language"
    ]
  },
  {
    "id": "1899",
    "title": "Influence of visual stimulus on amplitude and phase of alpha wave as measured by multi-channel EEG",
    "abstract": "The objective of this paper is to study the relationship between a visual stimulus and the amplitude and phase of the alpha wave as a first step to investigating a change in the background wave after a sensory stimulus and an evoked potential. We examined the effect of a, single visual stimulus on the amplitude and phase of alpha waves using the complex demodulation method. The visual stimuli were generated by an LED mounted in goggles with the eyes-closed condition. The amplitude of the alpha wave decreased gradually after the stimulus, until it reached a minimum at around 300 ms after the stimulus. The alpha wave continued to increase, showing some rebound, and returning again to the pre-stimulus level. The phase variation after the stimulus tends to be considerably larger than that before the stimulus. Moreover, the average phase returned to the same slope as the pre-stimulus by 2550 ms after the stimulus. The visual stimulus has an effect on the alpha wave until about 2500 ms after the stimulus. The phase variation difference before and after stimulus is significant from 112 ms to 678 ms after the stimulus. This finding suggests there is a partially pararell time course between the change in VEPs plus ERP complex and the alpha wave.",
    "keywords": [
      "complex demodulation method",
      "amplitude",
      "phase",
      "alpha wave",
      "visual stimulus"
    ]
  },
  {
    "id": "1900",
    "title": "Learning classifier systems with memory condition to solve non-Markov problems",
    "abstract": "In the family of learning classifier systems, the classifier system XCS has been successfully used for many applications. However, the standard XCS has no memory mechanism and can only learn optimal policy in Markov environments, but fails in non-Markov ones. In this work, we aim to develop a new classifier system based on XCS to tackle this problem. It adds a memory list with numbered slots to XCS to record input sensation history, and extends only a small number of classifiers with memory conditions. The classifiers memory condition, as a foothold to disambiguate non-Markov states, is used to sense a specified element in the memory list, which makes our system can jump over irrelevant or confusing states to get decisive prior information that may be far back in time. Besides, a detection method is employed to recognize non-Markov states in environments, to avoid these states controlling over classifiers memory conditions. Furthermore, four sets of different complex maze environments have been tested by the proposed method. Experimental results show that our system can overcome the overhead problem often encountered in history-window approaches, and is an effective technique to solve non-Markov environments.",
    "keywords": [
      "learning classifier system",
      "xcs",
      "memory condition",
      "aliasing state detection",
      "partially observable environments",
      "non-markov problems"
    ]
  },
  {
    "id": "1901",
    "title": "Interspliced transcription chimeras: Neglected pathological mechanism infiltrating gene accession queries",
    "abstract": "Over half of the DNA of mammalian genomes is transcribed, and one of the emerging enigmas in the field of RNA research is intergenic splicing or transcription induced chimerism. We argue that fused low-copy-number transcripts constitute neglected pathological mechanism akin to copy number variation, due to loss of stoichiometric subunit ratios in protein complexes. An obstacle for transcriptomics meta-analysis of published microarrays is the traditional nomenclature of merged transcript neighbors under same accession codes. Tandem transcripts cover 420% of genomes but are only loosely overlapping in population. They were most enriched in systems medicine annotations concerning neurology, thalassemia and genital disorders in the GeneGo Inc. MetaCore-MetaDrugTM knowledgebase, evaluated with external randomizations here. Clinical transcriptomics is good news since new disease etiologies offer new remedies. We identified homeotic HOX-transfactors centered around BMI-1, the Grb2 adaptor network, the kallikrein system, and thalassemia RNA surveillance as vulnerable hotspot chimeras. As a cure, RNA interference would require verification of chimerism from symptomatic tissue contra healthy control tissue from the same patient.",
    "keywords": [
      "clinical transcriptomics",
      "systems medicine",
      "intergenic splicing",
      "chimerism",
      "transcriptome meta-analysis"
    ]
  },
  {
    "id": "1902",
    "title": "Multi-GPU simulations of Vlasovs equation using Vlasiator",
    "abstract": "We use GPUs to accelerate the propagation of six-dimensional Vlasov equation using a finite volume method. The simulation has been parallelized with MPI. CUDA streams were used to pipeline GPUCPU and CPUCPU data transfers. Presented pipeline strategy simplifies the coding of parallel GPU simulations.",
    "keywords": [
      "plasma simulations",
      "vlasovs equation",
      "conservative scheme",
      "finite volume methods",
      "magnetosphere",
      "graphics processing units"
    ]
  },
  {
    "id": "1903",
    "title": "Industrial semiosis: founding the deployment of the ubiquitous information infrastructure",
    "abstract": "Increasingly, models and data pertaining to products, resources and industrial processes are being stored on computers. Moreover, corporate and personal computers and information systems are rapidly being connected into intranets, extranets, and a world wide web making ubiquitous information services for industry feasible, both from the technical and economical points of view. But whereas information and communication technology (ICT) developments are breathtaking, our methods to deploy these technologies in industry do not keep pace. They are developed slowly, heavily influenced by mature technologies, often at odds with newer technologies. Following an effort to bridge the gap between methods and technology this paper presents the result of a fundamental investigation into the relationship between industrial networks on the one hand, and the possible services of ICT networks on the other hand. The result is a framework of industrial semiosis (FIS) which applies and further elaborates the concepts of semiotics in the context of industry. These concepts are technology-independent, but at the same time they support an easy characterization, evaluation, and inclusion of the services of new technologies. The framework is proposed to influence more concrete, directly applicable modelling research and information system development and implementation methods, as well as curriculum components covering information and automation systems and e-commerce.  ",
    "keywords": [
      "ubiquitous computing",
      "e-commerce",
      "semiosis",
      "enterprise modelling",
      "product life cycle modelling"
    ]
  },
  {
    "id": "1904",
    "title": "Identification by Proteomic Tool of Atypical Anti-Liver/Kidney Microsome Autoantibodies Targets in de Novo Autoimmune Hepatitis after Liver Transplantation",
    "abstract": "De novo autoimmune hepatitis (AIH) occurs after liver transplantation for nonautoimmune disorders. Autoantibodies so-called atypical anti-liver/kidney microsome antibodies (LKMA) with an unusual liver/kidney cytoplasmic staining as judged by indirect immunofluorescence, can be detected in some patients' sera. Few studies investigated their molecular targets, and the aim of this work was to identify the atypical anti-LKMA targets by proteomic tool. This proteomic approach consisted of (a) two-dimensional gel electrophoresis of cytosolic and microsomal proteins obtained by differential centrifugations of rat liver and rat kidney, followed by (b) two-dimensional immunoblotting with sera of patients with de novo AIH (n= 8, including 2 with anti-LKMA antibodies) and then (c) identifications of interest spots performed by ion trap mass spectrometry. By this way several proteins at 25 kDa were unambiguously identifie",
    "keywords": [
      "de novo autoimmune hepatitis",
      "atypical anti-liver/kidney microsome autoantibodies",
      "mass spectrometry",
      "proteomic"
    ]
  },
  {
    "id": "1905",
    "title": "Orientation uncertainty goes bananas: An algorithm to visualise the uncertainty sample space on stereonets for oriented objects measured in boreholes",
    "abstract": "Measurements of structure orientations are afflicted with uncertainties which arise from many sources. Commonly, such uncertainties involve instrument imprecision, external disturbances and human factors. The aggregated uncertainty depends on the uncertainty of each of the sources. The orientation of an object measured in a borehole (e.g. a fracture) is calculated using four parameters: the bearing and inclination of the borehole and two relative angles of the measured object to the borehole. Each parameter may be a result of one or several measurements. The aim of this paper is to develop a method to both calculate and visualize the aggregated uncertainty resulting from the uncertainty in each of the four geometrical constituents. Numerical methods were used to develop a VBA-application in Microsoft Excel to calculate the aggregated uncertainty. The code calculates two different representations of the aggregated uncertainty: a 1-parameter uncertainty, the minimum dihedral angle, denoted by ?; and, a non-parametric visual representation of the uncertainty, denoted by ?. The simple 1-parameter uncertainty algorithm calculates the minimum dihedral angle accurately, but overestimates the probability space that plots as an ellipsoid on a lower hemisphere stereonet. The non-parametric representation plots the uncertainty probability space accurately, usually as a sector of an annulus for steeply inclined boreholes, but is difficult to express numerically. The 1-parameter uncertainty can be used for evaluating statistics of large datasets whilst the non-parametric representation is useful when scrutinizing single or a few objects.",
    "keywords": [
      "drillcore",
      "borehole",
      "orientation uncertainty",
      "sample space",
      "stereographic projection",
      "algorithm"
    ]
  },
  {
    "id": "1906",
    "title": "Investigation of fuzzy control based LCL resonant converter in RTOS environment",
    "abstract": "This paper presents a comparative evaluation of Fuzzy Logic (FLC) Controller and Open loop Controller for a modified LCL Resonant Converter has been simulated and the performance is analyzed. A three element LCL Resonant converter working under load independent operation is presented in this paper. In this work, the applicability of the ARM (Advanced RISC Machine) processor LPC 2148 is to be investigated as the controller for resonant converter. The simulation study indicates the superiority of Fuzzy Logic control over the conventional control method. The evaluation version of MATLAB was used to model the LCL topology for varied loads and LCL configurations. A LCL Resonant Inverter is proposed for applications in high frequency distributed AC power systems and Resonant Converter is proposed for applications in many space and radar power supplies. The advantages of the LCL topology are low total harmonic distortion (THD) high efficiency and the ability to handle varying loads.",
    "keywords": [
      "arm processor",
      "dynamic response",
      "fuzzy controller",
      "resonant converter"
    ]
  },
  {
    "id": "1907",
    "title": "A hierarchical algorithm for fuzzy template matching in emotional facial images",
    "abstract": "The paper aims at developing a hierarchical algorithm for matching a given template of m x non an image of M x N pixels partitioned into equal sized blocks of m x n pixels. The algorithm employs a fuzzy metric to measure the dispersion of individual feature of a block with respect to that of the template. A fuzzy threshold, preset by the user, is employed to restrict less likely blocks from participation in the matching. A decision tree is used to test the feasibility of a block for matching with the template. The tree at each link examines the condition for fuzzy thresholding for one feature of the image. If the block satisfies the condition, it is passed on to the next level in the tree for testing its feasibility of matching with respect to the next feature. If it fails, the block is discarded from the search space, and the next block from the partitioned image is passed on for examination. The process goes on until all the blocks pass through the decision tree. If a suitable block satisfies all the test conditions in the decision tree, the block is declared as the solution for the matching problem. The ordering of features to be examined by the tree is performed here by an entropy measure as used in classical decision tree. The time-complexity of the algorithm is of the order of MN/mn, and the elegance of the algorithm lies in its power of approximate matching using fuzzy conditions. The algorithm has successfully been implemented for template matching of human eyes in facial images carrying different emotions, and the classification accuracy is as high as 96%.",
    "keywords": [
      "template matching",
      "decision tree",
      "hierarchical search",
      "entropy measure",
      "fuzzy threshold"
    ]
  },
  {
    "id": "1908",
    "title": "Development of a skin care formulation using experimental designs",
    "abstract": "New cosmetic emulsion with biomimetic molecules have been investigated using experimental designs. For this purpose, six variables have been studied: the nature of surfactant (three mixture variables), the percentages of surfactant, co-surfactant and squalene, and five properties have been measured: stability, centrifugation, viscosity, pH, microscope analysis. Because of the complexity of the study, the optimization of these five responses has been carried out in two steps. First, a simple additive model has been postulated, in order to define a smaller domain of interest. A Doehlert, a grid and a simplex centriod designs have been built in order to generate candidate points. Exchange algorithm has allowed to extract a design with only 28 experiments. Results have shown that the percentage of squalene exhibits the greatest influence on the stability and on the viscosity of the emulsions. Secondly, a multiplicative model has been postulated using composite and simplex centroid design to generate candidate points. A set of 36 experiments has been selected. Conditions for the percentage of surfactant and co-surfactant, and for mixture variables have been determined in order to obtain a good compromise between the five responses. Finally, these studies have allowed to determine in the whole domain of interest an optimal formula composed only with biomimetic products and presenting good physico-chemical characteristics and stability.  ",
    "keywords": [
      "biomimetic emulsion",
      "response surfaces",
      "mixture",
      "combined experimental design",
      "optimization"
    ]
  },
  {
    "id": "1909",
    "title": "An improved algorithm for tree edit distance with applications for RNA secondary structure comparison",
    "abstract": "An ordered labeled tree is a tree in which the nodes are labeled and the left-to-right order among siblings is relevant. The edit distance between two ordered labeled trees is the minimum cost of transforming one tree into the other through a sequence of edit operations. We present techniques for speeding up the tree edit distance computation which are applicable to a family of algorithms based on closely related recursion strategies. These techniques aim to reduce repetitious steps in the original algorithms by exploring certain structural features in the tree. When these features exist in a large portion of the tree, the speedup due to our techniques would be significant. Viable examples for application include RNA secondary structure comparison and structured text comparison.",
    "keywords": [
      "tree edit distance",
      "rna secondary structure comparison",
      "structured text comparison"
    ]
  },
  {
    "id": "1910",
    "title": "comparison of cmos pla and polycell representations of control logic",
    "abstract": "A comparison of the area and speed characteristics of CMOS PLA and Polycell representations of various control logic blocks was carried out. Both types of layout were generated automatically, and were derived from the same high level logic description. The major objectives were to quantify the differences between the two types of circuit and to predict Polycell speed and circuit area from the PLA truth table. Both objectives were achieved. Polycell layout was shown to be a viable alternative to PLAs under certain circumstances.",
    "keywords": [
      "layout",
      "object",
      "logic",
      "comparisons",
      "control",
      "predict",
      "circuits",
      "types"
    ]
  },
  {
    "id": "1911",
    "title": "Volume analysis of disk spanning logical volumes",
    "abstract": "As storage requirements increase, it is becoming more common for computers to use disk spanning techniques to make multiple disks look like one large disk, called a logical volume. This impacts an investigator because she must be able to recreate the logical volume so that the computer can be analyzed. In this paper, we look at the basic concepts of disk spanning and we look at three Windows and Linux implementations.",
    "keywords": [
      "volume analysis",
      "digital forensics",
      "logical volumes"
    ]
  },
  {
    "id": "1912",
    "title": "Numerical modeling and characterization of selected electromagnetic cloaking structures (Invited paper)",
    "abstract": "Electromagnetic cloaking has been widely studied in recent years. In this review article we explain the basic principles of numerical modeling and characterization of selected broadband cloaking structures that are designed to operate within the microwave frequency range. Because the cloaks are periodical structures, the numerical modeling is simplified by employing symmetry boundary conditions. The modeling and characterization are done both with frequency-domain and time-domain simulations. Various experimental results are also compared to numerical results.  ",
    "keywords": [
      "scattering",
      "scattering cross section"
    ]
  },
  {
    "id": "1913",
    "title": "Real-time medical video processing, enabled by hardware accelerated correlations",
    "abstract": "Image processing involving correlation based filter algorithms have proved extremely useful for image enhancement, feature extraction and recognition, in a wide range of medical applications, but is almost exclusively used with still images due to the amount of computations required by the correlations. In this paper, we present two different practical methods for applying correlation-based algorithms to real-time video images, using hardware accelerated correlation, as well as our results in applying the method to optical venography. The first method employs a GPU accelerated personal computer, while the second method employs an embedded FPGA. We will discuss major difference between the two approaches, and their suitability for clinical use. The system presented detects blood vessels in human forearms in images from NIR camera setup for the use in a clinical environment.",
    "keywords": [
      "gpu",
      "fpga",
      "convolution",
      "image processing",
      "blood sampling",
      "matched filtering",
      "vessel detection",
      "optical venography"
    ]
  },
  {
    "id": "1914",
    "title": "Business  IT-Alignment  Ergebnisse einer Befragung von IT-Fhrungskrften in Deutschland",
    "abstract": "Nachdem Nicholas Carr (2003) noch behauptet hatte IT doesnt matter, hat sich inzwischen die Erkenntnis durchgesetzt, dass viele Geschftsmodelle, insbesondere im Dienstleistungssektor, ohne eine profunde IT-Unterlegung unmglich wren. Durch diese strategische Bedeutung fr das Geschft wird die IT in vielen Branchen selbst zum Wettbewerbsfaktor und Grundlage fr die Anpassungsfhigkeit von Unternehmen. Zunehmend wird dabei von der IT verlangt, nicht mehr nur enabler der Unternehmensprozesse zu sein, sondern auch zu Innovationen auf der fachlichen Seite des Geschfts beizutragen. In einer umfangreichen Online-Befragung von IT-Fhrungskrften wurde nun untersucht, wie diese vernderten Aufgaben in den IT-Bereichen groer deutscher Unternehmen umgesetzt werden und welche Strukturen dafr geschaffen wurden. Der nachfolgende Beitrag stellt hieraus ausgewhlte Ergebnisse vor. Dabei zeigt sich unter anderem, dass die IT heute berwiegend zentralisiert und als Cost Center gefhrt wird. Bei der Besetzung der IT-Fhrung sind Kandidaten mit Hochschulstudium bevorzugt und IT-Kenntnisse werden hher gewichtet als Fhrungserfahrung. Der IT-Auftrag wird zwar differenziert, jedoch noch immer stark kostenorientiert gesehen. IT-Fhrungskrfte berichten meistens an den CEO oder CFO. Aus Sicht des IT Top Managements erfolgt die Abstimmung der IT-Strategie mit der Unternehmensstrategie insgesamt in einem groen bis sehr groem Umfang. Gleichzeitig wird das IT-Verstndnis des Top-Managements von den IT-Fhrungskrften auf mittlerem Niveau eingestuft, wobei es starke Branchenunterschiede gibt. Dies und die Tatsache, dass die IT Top Manager nur im mittleren Umfang an der Entwicklung der Unternehmensstrategie beteiligt sind, erschweren jedoch in der Praxis das Business-IT-Alignment.",
    "keywords": [
      "business-it-alignment",
      "it-strategie",
      "it-auftrag",
      "it-organisation",
      "cio"
    ]
  },
  {
    "id": "1915",
    "title": "Enhanced iterative projection for subclass discriminant analysis under EM-alike framework",
    "abstract": "We focus on the classification of the large sample size problems. We propose a new EM-alike iterative projection approach (EMIPA) for subclass division. Our proposed approach outperforms the traditional MDA and SDA. Our approach operates subclass division and eigenvector seeking class by class. Our proposed approach takes only a bit more time than MDA and SDA.",
    "keywords": [
      "linear discriminant analysis",
      "larger sample size problems",
      "mixture discriminant analysis",
      "subclass discriminant analysis",
      "em-alike framework",
      "iterative steps",
      "generalized eigenvalue problem",
      "k-means clustering"
    ]
  },
  {
    "id": "1916",
    "title": "Two Phase Admission Control for QoS Mobile Ad Hoc Networks",
    "abstract": "In this paper a novel and effective two phase admission control (TPAC) for QoS mobile ad hoc networks is proposed that satisfies the real-time traffic requirements in mobile ad hoc networks. With a limited amount of extra overhead, TPAC can avoid network congestions by a simple and precise admission control which blocks most of the overloading flow-requests in the route discovery process. When compared with previous QoS routing schemes such as QoS-aware routing protocol and CACP protocols, it is shown from system simulations that the proposed scheme can increase the system throughput and reduce both the dropping rate and the end-to-end delay. Therefore, TPAC is surely an effective QoS-guarantee protocol to provide for real-time traffic.",
    "keywords": [
      "quality of service ",
      "ad hoc on-demand distance vector ",
      "contention-aware admission control protocol "
    ]
  },
  {
    "id": "1917",
    "title": "A new morphology method for enhancing power quality monitoring system",
    "abstract": "By means of mathematical morphology (MM), power quality monitoring systems can detect disturbances very soon. However, the signal under investigation is often corrupted by noises, and the performance of the MM would be greatly degraded. This paper proposed a new fast approach to detection of transient disturbances in a noisy environment. In the proposed approach, the appropriate morphologic structure element, new proper combination of the erosion and the dilation morphologic operators can enhance the MMs capability. Furthermore, the soft-threshold de-noising method based on the wavelet transform (WT) was used for reference. In this way, the abilities of the MM can hence be restored. This method possesses the following advantages: high calculation speed, easy implementation of hardware and better use value. Finally, the validity of the proposed method is verified by the results of the simulation and the actual field tests.",
    "keywords": [
      "power quality",
      "mathematical morphology",
      "wavelet transform",
      "signal processing"
    ]
  },
  {
    "id": "1918",
    "title": "Structural analysis of the functional influence of the surface peptide Gtf-P1 on Streptococcus mutans glucosyltransferase C activity",
    "abstract": "Glucosyltransferases (GtfB/C/D) in Streptococcus mutans are responsible for synthesizing water-insoluble and water-soluble glucans from sucrose and play very crucial roles in the formation of dental plaque. A monoclonal antibody against a 19-mer peptide fragment named Gtf-P1 was found in GtfC to reduce the enzyme activity to 50%. However, a similar experiment suggested almost unchanged activity in GtfD, despite of the very high sequence homology between the two enzymes. No further details are yet available to elucidate the biochemical mechanism responsible for such discrimination. For a better understanding of the catalytic behavior of these glucosyltransferases, structural and functional analyses were performed. First, the exact epitope was identified to specify the residue(s) required for monoclonal antibody recognition. The results suggest that the discrimination is determined solely by single residue substitution. Second, based on a combined sequence and secondary structure alignment against known crystal structure of segments from closely related proteins, a three-dimensional homology model for GtfC was built. Structural analysis for the region communicating between Gtf-P1 and the catalytic triad revealed the possibility for an 'en bloc' movement of hydrophobic residues, which may transduce the functional influence on enzyme activity from the surface of molecule into the proximity of the active site.",
    "keywords": [
      "glucosyltransferases",
      "streptococcus mutans",
      "homology modeling",
      "en bloc movement",
      "monoclonal antibody recognition",
      "dental caries",
      "gtf-p1"
    ]
  },
  {
    "id": "1919",
    "title": "A NEW SUPERVISED FEATURE SELECTION METHOD FOR PATTERN CLASSIFICATION",
    "abstract": "With the rapid development of information techniques, the dimensionality of data in many application domains, such as text categorization and bioinformatics, is getting higher and higher. The high-dimensionality data may bring many adverse situations, such as overfitting, poor performance, and low efficiency, to traditional learning algorithms in pattern classification. Feature selection aims at reducing the dimensionality of data and providing discriminative features for pattern learning algorithms. Due to its effectiveness, feature selection is now gaining increasing attentions from a variety of disciplines and currently many efforts have been attempted in this field. In this paper, we propose a new supervised feature selection method to pick important features by using information criteria. Unlike other selection methods, the main characteristic of our method is that it not only takes both maximal relevance to the class labels and minimal redundancy to the selected features into account, but also works like feature clustering in an agglomerative way. To measure the relevance and redundancy of feature exactly, two different information criteria, i.e., mutual information and coefficient of relevance, have been adopted in our method. The performance evaluations on 12 benchmark data sets show that the proposed method can achieve better performance than other popular feature selection methods in most cases.",
    "keywords": [
      "feature selection",
      "dimensionality reduction",
      "information entropy",
      "pattern classification",
      "clustering"
    ]
  },
  {
    "id": "1920",
    "title": "on the design of camelot, an outdoor game for children",
    "abstract": "This paper describes the design of Camelot, a mobile outdoor game for small groups of children aged 7-10. Camelot was designed with the aim to encourage social interaction between the players and to encourage physical activity. The paper extends the research literature on design methodology for children, by recording and reflecting upon the lessons learnt by applying a range of techniques for involving children in the design of interactive systems.",
    "keywords": [
      "children",
      "pervasive gaming",
      "social gaming"
    ]
  },
  {
    "id": "1921",
    "title": "QoS provisioning by EFuNNs-based handoff planning in cellular MPLS networks",
    "abstract": "In recent years, Multi Protocol Label Switching (MPLS) has been considered as the preeminent technology to incur Quality of Service (QoS) for integrated services. However, in wireless networks the remotes mobility endangers resource management procedure and QoS provisioning. In this paper we propose a new location prediction method based on Evolving Fuzzy Neural Networks (EFuNNs), to manage Label Switched Paths (LSPs) in an MPLS domain. The proposed predictor employs geographical characteristics of underlying area and the movement history of a remote, to produce a set of confidence ratios as the output. That set is considered as a criterion for establishing and managing LSPs so that QoS preserved. The simulation results have shown superior performance in terms of prediction accuracy and utilization improvement for the proposed methods.  ",
    "keywords": [
      "cellular mpls networks",
      "lsp management",
      "evolving fuzzy neural networks",
      "quality of service",
      "mobility prediction"
    ]
  },
  {
    "id": "1922",
    "title": "Finding the maximum suffix with fewer comparisons",
    "abstract": "It is shown how to compute the lexicographically maximum suffix of a string of n?2 n ? 2 characters over a totally ordered alphabet using at most (4/3)n?5/3 ( 4 / 3 ) n ? 5 / 3 three-way character comparisons. The best previous bound, which has stood unchallenged for more than 25 years, is (3/2)n?O(1) ( 3 / 2 ) n ? O ( 1 ) comparisons. We also prove an interesting property of an algorithm for computing the maximum suffix both with respect to a total order  .",
    "keywords": [
      "string algorithms",
      "maximum suffix",
      "character comparisons"
    ]
  },
  {
    "id": "1923",
    "title": "Wide-bandwidth piezoelectric energy harvester integrated with parylene-C beam structures",
    "abstract": "A wide-bandwidth MEMS piezoelectric energy harvester by the Duffing effect. Parylene-C was employed as the structure material to reduce the resonance frequency. The loaddeflection relationship for trapezoid doubly-clamped beams was formulated. At 615Hz, with a maximum open-circuit voltage 368mV, the power achieves to 0.28?W. A wide operational frequencies range is obtained (from 200Hz to 600Hz).",
    "keywords": [
      "energy harvester",
      "mems",
      "ambient vibration",
      "piezoelectric",
      "doubly-clamped beam"
    ]
  },
  {
    "id": "1924",
    "title": "Robust Reconstruction of MRSI Data Using a Sparse Spectral Model and High Resolution MRI Priors",
    "abstract": "We introduce a novel algorithm to address the challenges in magnetic resonance (MR) spectroscopic imaging. In contrast to classical sequential data processing schemes, the proposed method combines the reconstruction and postprocessing steps into a unified algorithm. This integrated approach enables us to inject a range of prior information into the data processing scheme, thus constraining the reconstructions. We use high resolution, 3-D estimate of the magnetic field inhomogeneity map to generate an accurate forward model, while a high resolution estimate of the fat/water boundary is used to minimize spectral leakage artifacts. We parameterize the spectrum at each voxel as a sparse linear combination of spikes and polynomials to capture the metabolite and baseline components, respectively. The constrained model makes the problem better conditioned in regions with significant field inhomogeneity, thus enabling the recovery even in regions with high field map variations. To exploit the high resolution MR information, we formulate the problem as an anatomically constrained total variation optimization scheme on a grid with the same spacing as the magnetic resonance imaging data. We analyze the performance of the proposed scheme using phantom and human subjects. Quantitative and qualitative comparisons indicate a significant improvement in spectral quality and lower leakage artifacts.",
    "keywords": [
      "b inhomogeneity compensation",
      "l-minimization",
      "fat leakage",
      "field map",
      "magnetic resonance spectroscopic imaging ",
      "sparsity",
      "total variation"
    ]
  },
  {
    "id": "1925",
    "title": "Compressible memory data structures for event-based trace analysis",
    "abstract": "The paper presents a new compressible memory data structure for trace events. Its primary intention is to aid the analysis of huge traces by reducing the memory requirements significantly. Furthermore, customized evaluation algorithms reduce the computational effort. The data structure as well as algorithms for construction and evaluation are discussed in detail. Experiments with real-life traces demonstrate the theoretically derived capabilities of the new approach.",
    "keywords": [
      "performance analysis",
      "tracing",
      "data structures",
      "compression"
    ]
  },
  {
    "id": "1926",
    "title": "BiGlobal stability analysis of steady flow in constricted channel geometries",
    "abstract": "In this paper we apply a BiGlobal stability analysis technique to measure the stability of two-dimensional constricted channel flows to three-dimensional perturbations. Critical Reynolds numbers and spanwise perturbation wavelengths are presented for three instabilities of steady flow in constricted channels. ",
    "keywords": [
      "biglobal",
      "stability",
      "constricted",
      "channel"
    ]
  },
  {
    "id": "1927",
    "title": "Operator Splittings, Bregman Methods and Frame Shrinkage in Image Processing",
    "abstract": "We examine the underlying structure of popular algorithms for variational methods used in image processing. We focus here on operator splittings and Bregman methods based on a unified approach via fixed point iterations and averaged operators. In particular, the recently proposed alternating split Bregman method can be interpreted from different points of view-as a Bregman, as an augmented Lagrangian and as a Douglas-Rachford splitting algorithm which is a classical operator splitting method. We also study similarities between this method and the forward-backward splitting method when applied to two frequently used models for image denoising which employ a Besov-norm and a total variation regularization term, respectively. In the first setting, we show that for a discretization based on Parseval frames the gradient descent reprojection and the alternating split Bregman algorithm are equivalent and turn out to be a frame shrinkage method. For the total variation regularizer, we also present a numerical comparison with multistep methods.",
    "keywords": [
      "douglas-rachford splitting",
      "forward-backward splitting",
      "bregman methods",
      "augmented lagrangian method",
      "alternating split bregman algorithm",
      "image denoising"
    ]
  },
  {
    "id": "1928",
    "title": "Secure mutual authentication for ad hoc wireless networks",
    "abstract": "As wireless LANs and wireless terminals deployed, Security and authentication, among other things, became important. We propose a mutual authentication scheme for access security among wireless terminals. We use user identification together with hardware identification in creating a certificate for the wireless access. And authentication is performed based on the certificate. It enables reliable user authentication, establishment and check of access rights (when a terminal is managed in connection with a network gateway), Peer-to-Peer access with authentication, and international roaming of terminals.",
    "keywords": [
      "authentication",
      "ad hoc wireless network"
    ]
  },
  {
    "id": "1929",
    "title": "an experience report on using collaboration technologies for distance and on-campus learning",
    "abstract": "This paper summarises our experiences and observations using two online collaboration technologies, Blackboard and Wimba Live Classroom, while teaching Software Engineering courses. Blackboard is used to make announcements and course materials available. The Wiki facility of Blackboard is used to update information about software tools used in the course. In addition to the text chat and record feature in Blackboard, Wimba Live Classroom is used for audio chatting with application sharing and the whiteboard facility, and also to record lectures. Blended learning, which combines traditional face-to-face teaching with online methods, can improve the quality, convenience and flexibility of communication for participants and student satisfaction. This paper reports our experiences using Blackboard and Wimba in a blended-learning environment and presents our suggestions for using collaboration technologies in teaching.",
    "keywords": [
      "software engineering education",
      "collaboration technology",
      "blended learning"
    ]
  },
  {
    "id": "1930",
    "title": "Probability of failure of overloaded lines in cascading failures",
    "abstract": "Blackouts in transmission power systems are due to cascading failures. Outages of overloaded lines are main contributors to cascading failures. The probability of line tripping as a function of the overload must be modeled. Previous models do not rely on empirical evidence or detailed analysis. This paper proposes a simple model backed up by a detailed analysis.",
    "keywords": [
      "cascading failures",
      "overloaded overhead line",
      "thermal model",
      "probabilistic approach"
    ]
  },
  {
    "id": "1931",
    "title": "QSAR design of triazolopyridine mGlu2 receptor positive allosteric modulators",
    "abstract": "Topomer CoMFA identification of new positive allosteric modulators of the mGlu2 receptor. Comparison of 3D and 2D QSAR approaches in prospective application. Design and synthesis performed based on predictions from QSAR modelling. QSAR model performance deterioration during project evolution. R-group substituents accessing new chemical space identified based on QSAR model.",
    "keywords": [
      "qsar",
      "topomer comfa",
      "top-comfa",
      "svm",
      "allosteric modulator"
    ]
  },
  {
    "id": "1932",
    "title": "Vicarious radiometric calibration of a multispectral sensor from an aerial trike applied to precision agriculture",
    "abstract": "Vicarious radiometric calibration of a multispectral sensor. High spatial resolution vegetation indices from a low-cost manned aerial platform. Management of extensive crop areas using low-cost geomatic techniques. Low-cost remote sensing for precision agriculture. Vicarious calibration to acquire quantitative physical parameters.",
    "keywords": [
      "radiometric calibration",
      "vegetation index images",
      "precision agriculture",
      "multispectral sensor",
      "remote sensing",
      "vicarious calibration"
    ]
  },
  {
    "id": "1933",
    "title": "Lower bound on complexity of optimization of continuous functions",
    "abstract": "This paper considers the problem of approximating the minimum of a continuous function using a fixed number of sequentially selected function evaluations. A lower bound on the complexity is established by analyzing the average case for the Brownian bridge.  ",
    "keywords": [
      "global optimization",
      "wiener measure",
      "average case complexity"
    ]
  },
  {
    "id": "1934",
    "title": "a modeling language's evolution driven by tight interaction between academia and industry",
    "abstract": "Domain specific languages play an important role in model-driven engineering of software-intensive industrial systems. A rich body of knowledge exists on the development of languages, modeling environments, and transformation systems. The understanding of architectural choices for combining these parts into a feasible solution, however, is not particularly deep. We report on an endeavor in the realm of a technology transfer process from academia to industry, where we encountered unexpected influences of the architecture on the modeling language. By examining the evolution of our language and its programming interface, we show that these influences mainly stemmed from practical considerations; for identifying these early on, tight interaction between our research lab and the industrial partner was key. In addition, we share insights into the practice of cooperating with industry by presenting essential lessons we learned.",
    "keywords": [
      "model driven engineering architecture",
      "domains specific language",
      "academic-industry cooperation",
      "clabjects"
    ]
  },
  {
    "id": "1935",
    "title": "Sparse learning via Boolean relaxations",
    "abstract": "We introduce novel relaxations for cardinality-constrained learning problems, including least-squares regression as a special but important case. Our approach is based on reformulating a cardinality-constrained problem exactly as a Boolean program, to which standard convex relaxations such as the Lasserre and Sherali-Adams hierarchies can be applied. We analyze the first-order relaxation in detail, deriving necessary and sufficient conditions for exactness in a unified manner. In the special case of least-squares regression, we show that these conditions are satisfied with high probability for random ensembles satisfying suitable incoherence conditions, similar to results on (ell _1)-relaxations. In contrast to known methods, our relaxations yield lower bounds on the objective, and it can be verified whether or not the relaxation is exact. If it is not, we show that randomization based on the relaxed solution offers a principled way to generate provably good feasible solutions. This property enables us to obtain high quality estimates even if incoherence conditions are not met, as might be expected in real datasets. We numerically illustrate the performance of the relaxation-randomization strategy in both synthetic and real high-dimensional datasets, revealing substantial improvements relative to (ell _1)-based methods and greedy selection heuristics.",
    "keywords": [
      "sparsity",
      "regularization",
      "convex relaxation",
      "combinatorial optimization",
      "machine learning",
      " convex programming",
      " large-scale problems",
      " boolean programming",
      " learning and adaptive systems"
    ]
  },
  {
    "id": "1936",
    "title": "Decentralized string-stability analysis for heterogeneous cascades subject to load-matching requirements",
    "abstract": "A heterogeneous cascade of stable linear time-invariant subsystems is studied in terms of the spatial and temporal propagation of boundary conditions. The particular context requires constant spatial boundary conditions to be asymptotically matched by the interconnection signals along the string (e.g., to match supply to demand in steady state). Furthermore, the transient response associated with a step change in the spatial boundary condition must remain bounded across space in a string-length independent fashion. With this in mind, an infinite cascade abstraction is considered. A corresponding decentralized string-stability certificate for the desired behaviour is established in terms of the subsystem (H_infty ) norms, via Lyapunov-type analysis of a two-dimensional model in Roesser form. Verification of the certificate implies uniformly bounded interconnection signals in response to the following system inputs: (i) a square-summable (across space) sequence of initial conditions; and (ii) a uniformly-bounded (across time) finite-energy input applied as the spatial boundary condition (e.g., finite duration on-off pulse). The decentralized nature of the certificate facilitates subsystem-by-subsystem design of local controllers that achieve string-stable behaviour overall. This application of the analysis is explored within the context of a scalable approach to the design of distributed distant-downstream controllers for the sections of an automated irrigation channel.",
    "keywords": [
      "distributed control",
      "2-d roesser models",
      "scalable analysis",
      "string stability",
      "irrigation channel automation"
    ]
  },
  {
    "id": "1937",
    "title": "An evaluation of the compactness of superpixels",
    "abstract": "We propose a metric to measure the compactness of superpixel segmentations. We show that there is a negative correlation between compactness and boundary recall. The proposed algorithm offers a transparent compactness control. The segmentation optionally conforms to a superpixel lattice. An example application demonstrates the relevance of controlling compactness.",
    "keywords": [
      "superpixel segmentation",
      "compactness",
      "superpixel lattices"
    ]
  },
  {
    "id": "1938",
    "title": "Hybrid modelization of intracoronary stents",
    "abstract": "In this paper we suggest a general procedure to model tube-shaped stents by CAD, in particular of intravascular coronary kind. The methodology utilisation is highly independent of modelization softwares and enables us to build up careful stent models. These models can be usefully used to perform structural and/or blood fluid dynamic analysis. The procedure is based on particular considerations of Descriptive Geometry that are inspired by the laser cutting technology of the stents. The study is developed by using a hybrid technique of surfaces and solids modelling. The method suggested is illustrated with reference to a class of intracoronary stents cut by laser beam. Some general conclusions show how it is possible to apply the procedure also for modelling of stents manufactured by using welded stainless steel wire.",
    "keywords": [
      "stent",
      "laser",
      "cad modelization",
      "methodology"
    ]
  },
  {
    "id": "1939",
    "title": "Characteristic of mutually coupled two-layer CNN and its stability",
    "abstract": "This paper presents some interesting image processing applications with the mutually coupled two-layer Cellular Neural Networks (CNNs). We found that the two-layer CNNs are very useful compared to single layer CNNs in some applications such as center point detection, skeletonization, and so on. We also focus our discussions on both their transients and operations. In addition, the stability of the two-layer CNNs with mutually coupled symmetric templates is also discussed based on those of decoupling CNN technique.",
    "keywords": [
      "two-layer cnn",
      "template",
      "center point detection",
      "skeletonization",
      "stability"
    ]
  },
  {
    "id": "1940",
    "title": "Managing capacity flexibility in make-to-order production environments",
    "abstract": "This paper addresses the problem of managing flexible production capacity in a make-to-order (MTO) manufacturing environment. We present a multi-period capacity management model where we distinguish between process flexibility (the ability to produce multiple products on multiple production lines) and operational flexibility (the ability to dynamically change capacity allocations among different product families over time). For operational flexibility, we consider two polices: a fixed allocation policy where the capacity allocations are fixed throughout the planning horizon and a dynamic allocation policy where the capacity allocations change from period to period. The former approach is modeled as a single-stage stochastic program and solved using a cutting-plane method. The latter approach is modeled as a multi-stage stochastic program and a sampling-based decomposition method is presented to identify a feasible policy and assess the quality of that policy. A computational experiment quantifies the benefits of operational flexibility and demonstrates that it is most beneficial when the demand and capacity are well-balanced and the demand variability is high. Additionally, our results reveal that myopic operating policies may lead a firm to adopt more process flexibility and form denser flexibility configuration chains. That is, process flexibility may be over-valued in the literature since it is assumed that a firm will operate optimally after the process flexibility decision. We also show that the value of process flexibility increases with the number of periods in the planning horizon if an optimal operating policy is employed. This result is reversed if a myopic allocation policy is adopted instead.",
    "keywords": [
      "supply chain management",
      "capacity flexibility",
      "stochastic programming",
      "make-to-order",
      "assignment"
    ]
  },
  {
    "id": "1941",
    "title": "integrating bioinformatic data sources over the sfsu er design tools xml databus",
    "abstract": "The SFSU ER Design Tools were developed to support database design and data integration over multiple implementation data models. These tools allow users to enter and view Entity Relationship (ER) schemas and to translate ER schemas into a variety of equivalent implementation schemas, including Relational (ANSI SQL2), Object Oriented (ODMG 3.0), Spreadsheet (Universal Relation with associated functional dependencies) and W3C XML DTD. In addition, for each implementation data model, the Tools generate DDL statements to create a database, as well as simple JDBC/ODBC based code to dump stored data into an XML file and to load data from an XML file into a database. Data can be transferred from one data store to another over an HTTP based XML Databus. In this paper we describe the design and implementation of our XML Databus using Web Services, as well as a new strategy to support integration of bioinformatics data sets. We first manually identify semantically equivalent attributes in both schemas, then automatically join the corresponding data sets into a single integrated collection of XML formatted data. Our software is operational, and preliminary performance measurements over DTD and data downloaded from the NIH-NCBI Web site show that our strategy is feasible for moderately sized data sets.",
    "keywords": [
      "distributed query processing",
      "join processing",
      "schema integration",
      "web services data integration"
    ]
  },
  {
    "id": "1942",
    "title": "PHOTON INDUCED ENTANGLEMENT IN ATOM-CAVITY SYSTEMS",
    "abstract": "We study the evolution of quantum entanglement in double cavity systems. The entanglement of cavity atoms induced by entangled pair of photons is investigated. Both entanglement sudden death and entanglement sudden birth phenomena are shown to exist and are analyzed in detail. We also propose a strategy to enhance the entanglement between the atom in one cavity and the photon in another cavity by using quantum Zeno effect.",
    "keywords": [
      "quantum entanglement",
      "double j-c model",
      "esd and esb",
      "quantum zero effect"
    ]
  },
  {
    "id": "1943",
    "title": "A framework to support qualitative reasoning about COAs in a dynamic spatial environment",
    "abstract": "We propose a framework to reason qualitatively about courses of action (COAs) which need to be executed in a realistic geographic space that may change. Particularly, the framework aims to support human mental 'What-If' analysis by simulating the execution of COAs in a virtual geographic environment, which can change during the simulation, and by allowing the user to explore various scenarios (different COAs) and to analyse their outcomes using causal reasoning techniques. In this article, we first present a framework which is based on a conceptual model of spatio-temporal situations, a multi-agent geosimulation platform and qualitative spatio-temporal causal reasoning techniques. Then, we illustrate the framework using a case study.",
    "keywords": [
      "modelling and analysis of dynamic phenomena in virtual geographic environments",
      "multi-agent geosimulation",
      "qualitative spatio-temporal reasoning",
      "course of action assessment"
    ]
  },
  {
    "id": "1944",
    "title": "Performance analysis of UWB systems in the presence of timing jitter",
    "abstract": "In this paper, performances of different ultra-wideband (UWB) modulation schemes in the presence of timing jitter are evaluated and compared. Static and Rayleigh fading channels are considered. For fading channels, flat and dispersive channels are assumed. First, bit error rate (BER) performances for each case are derived for a fixed value of timing jitter. Later, a uniform distribution of jitter is assumed to evaluate the performance of the system, and the theoretical results are verified by computer simulations. Finger estimation error is treated as timing jitter and an appropriate model is generated. Furthermore, a worst case distribution that provides an upper bound on the system performance is presented and compared with other distributions. Effects of timing jitter on systems employing different pulse shapes are analyzed to show the dependency of UWB performance on pulse shape. Although our analysis assumes uniform timing jitter, our framework can be used to evaluate the BER performance for any given probability distribution function of the jitter.",
    "keywords": [
      "maximal ratio combining",
      "multiband uwb",
      "performance analysis",
      "rake receivers",
      "timing jitter",
      "ultra-wideband"
    ]
  },
  {
    "id": "1945",
    "title": "Improving braincomputer interface classification using adaptive common spatial patterns",
    "abstract": "An adaptive common spatial patterns method is proposed for EEG spatial filtering. The method is evaluated for intra- and inter-subject classifications. The method is compared to existing techniques and shows superior performance. The effects of adding misclassified trials to training data are investigated. The method is potentially applicable to various real-time BCI tasks.",
    "keywords": [
      "braincomputer interface",
      "common spatial patterns",
      "electroencephalography",
      "adaptive",
      "nonstationarity"
    ]
  },
  {
    "id": "1946",
    "title": "The PEWs microarchitecture: reducing complexity through data-dependence based decentralization",
    "abstract": "This paper presents a microarchitecture based on exploiting the locality of data dependences for efficiently executing many instructions per cycle. The instruction window is split into multiple hardware units, and the instruction stream is distributed among them in such a way that data-dependent instructions are generally allocated to the same unit. The fetch bandwidth of the processor is enhanced by doing control predictions at a level higher than that of branches, which permits fetching a large number of instructions every cycle despite the presence of taken branches in the instruction stream. The microarchitecture also off-loads most of the work involved in instruction distribution decisions to hardware that is not in the critical path of execution. It also uses a novel hardware mechanism to resolve memory references. The paper also represents detailed simulation results, which show the capability of the proposed microarchitecture to achieve high issue rates. A sensitivity study of the microarchitecture shows its issue rate to be steadily increasing as the number of hardware units is increased from 1 to 8.",
    "keywords": [
      "decentralization",
      "dynamic scheduling",
      "hardware window",
      "instruction-level parallelism ",
      "speculative execution"
    ]
  },
  {
    "id": "1947",
    "title": "Performance analysis of Nitzberg's clutter map for Weibull distribution",
    "abstract": "The clutter map analyzed by Nitzberg, which averages the detector outputs of the same resolution cell over several past scans to set the detection threshold, plays an important role in radar detection fields. In this work, the average detection threshold (ADT) of Nitzberg's clutter map is derived and its steady-state property is analyzed, moreover, the detection performance of Nitzberg's clutter map for Weibull background is analyzed by the importance sampling technique.",
    "keywords": [
      "radar detection",
      "cfar",
      "clutter map",
      "weibull distribution"
    ]
  },
  {
    "id": "1948",
    "title": "Large-scale Internet benchmarking: Technology and application in warehousing operations",
    "abstract": "Effective benchmarking requires standards for measuring performance across a broad range of organizations. Industry-benchmarking measures the relative performance levels of similar operations, but gathering sufficient data to robustly characterize best performance is the primary hurdle to more widespread implementation. This paper describes the development of an innovative, large-scale benchmarking methodology which employs Internet technology to overcome the data collection problem. We discuss a particular instance of this approach, iDEAs-W-the result of an ongoing collaboration between academia and the warehousing industry. This paper's purpose is to demonstrate the methodology for the warehousing industry and share our insights to allow others to refine the tool for applications in other industries.  ",
    "keywords": [
      "benchmarking",
      "data envelopment analysis",
      "internet tools",
      "warehousing",
      "performance measurement"
    ]
  },
  {
    "id": "1949",
    "title": "An Agent-Based Distributed Coordination Mechanism for Wireless Visual Sensor Nodes Using Dynamic Programming",
    "abstract": "The efficient management of the limited energy resources of a wireless visual sensor network is central to its successful operation. Within this context, this article focuses on the adaptive sampling, forwarding and routing actions of each node in order to maximize the information value of the data collected. These actions are inter-related in a multi-hop routing scenario because each node's energy consumption must be optimally allocated between sampling and transmitting its own data, receiving and forwarding the data of other nodes, and routing any data. Thus, we develop two optimal agent-based decentralized algorithms to solve this distributed constraint optimization problem. The first assumes that the route by which data is forwarded to the base station is fixed, and then calculates the optimal sampling, transmitting and forwarding actions that each node should perform. The second assumes flexible routing, and makes optimal decisions regarding both the integration of actions that each node should choose and also the route by which the data should be forwarded to the base station. The two algorithms represent a trade-off in optimality, communication cost and processing time. In an empirical evaluation on sensor networks (whose underlying communication networks exhibit loops), we show that the algorithm with flexible routing is able to deliver approximately twice the quantity of information to the base station compared with the algorithm using fixed routing (where an arbitrary choice of route is made). However, this gain comes at a considerable communication and computational cost (increasing both by a factor of 100 times). Thus, while the algorithm with flexible routing is suitable for networks with a small number of nodes, it scales poorly, and as the size of the network increases, the algorithm with fixed routing is favoured.",
    "keywords": [
      "distributed algorithms",
      "decentralized mechanism",
      "information metric",
      "inter-related adaptive sampling",
      "transmitting",
      "receiving",
      "forwarding and routing",
      "distributed constraint optimization"
    ]
  },
  {
    "id": "1950",
    "title": "The inapproximability of non-NP-hard optimization problems",
    "abstract": "The inapproximability of non-NP-hard optimization problems is investigated. Techniques are given to show that the problems LOG DOMINATING SET and LOG HYPERGRAPH VERTEX COVER cannot be approximated to a constant ratio in polynomial time unless the corresponding NP-hard versions are also approximable in deterministic subexponential time. A direct connection is established between non-NP-hard problems and a PCP characterization of NP. Reductions from the PCP characterization show that LOG CLIQUE is not approximable in polynomial time and MAX SPARSE SAT does not have a FPTAS under the assumption that NP not subset of or equal to DTIME(2(O(rootn log n))). A number of nontrivial approximation-preserving reductions are also presented, making it possible to extend inapproximability results to more natural non-NP-hard problems such as TOURNAMENT DOMINATING SET and RICH HYPERGRAPH VERTEX COVER.  ",
    "keywords": [
      "computational complexity",
      "approximation hardness",
      "combinatorial optimization",
      "reductions"
    ]
  },
  {
    "id": "1951",
    "title": "Multiple Workflow Scheduling Strategies with User Run Time Estimates on a Grid",
    "abstract": "In this paper, we present an experimental study of deterministic non-preemptive multiple workflow scheduling strategies on a Grid. We distinguish twenty five strategies depending on the type and amount of information they require. We analyze scheduling strategies that consist of two and four stages: labeling, adaptive allocation, prioritization, and parallel machine scheduling. We apply these strategies in the context of executing the Cybershake, Epigenomics, Genome, Inspiral, LIGO, Montage, and SIPHT workflows applications. In order to provide performance comparison, we performed a joint analysis considering three metrics. A case study is given and corresponding results indicate that well known DAG scheduling algorithms designed for single DAG and single machine settings are not well suited for Grid scheduling scenarios, where user run time estimates are available. We show that the proposed new strategies outperform other strategies in terms of approximation factor, mean critical path waiting time, and critical path slowdown. The robustness of these strategies is also discussed.",
    "keywords": [
      "grid computing",
      "workflow scheduling",
      "resource management",
      "user run time estimate"
    ]
  },
  {
    "id": "1952",
    "title": "Dynamic Bayesian networks based performance evaluation of subsea blowout preventers in presence of imperfect repair",
    "abstract": "The reliability and availability of subsea BOP system are evaluated. The fault tree is translated into dynamic Bayesian network directly. The imperfect repair is modeled using multi-state degraded system. The imperfect repair cannot degrade the system performances significantly. Eight basic events contribute much to subsea BOP system failure.",
    "keywords": [
      "dynamic bayesian networks",
      "fault tree",
      "imperfect repair",
      "subsea blowout preventer",
      "reliability",
      "availability"
    ]
  },
  {
    "id": "1953",
    "title": "Time matrix controller design of flexible manufacturing systems",
    "abstract": "A matrix framework that based on the Petri nets theory (PNs) and the standard industrial engineering (IE) techniques were adopted into this paper. In this paper, the operational times are modeled and introduced into the matrix system model to activate the matrix system functioning. Different types of time include parts input, for operations and processing, for resources arriving, and time for finished goods or products output are developed for designing manufacturing systems. Therefore, different time matrices Ts (i.e., T-u, T-v, T-r, T-y) are constructed in this paper. Those time matrices are the key factors for integrating the manufacturing systems to approach the real time manufacturing world. Here, the key procedure for developing those time matrices is to develop and integrate the manufacturing matrix framework with the techniques of max/plus algebra. The result of this paper is to introduce and establish time into the matrix manufacturing system to approach the real world production situation. System simulation is also provided in this paper to demonstrate the integration between matrix formulation and time matrices.  ",
    "keywords": [
      "petri nets theory",
      "timed matrix framework",
      "max/plus technique",
      "flexible manufacturing system"
    ]
  },
  {
    "id": "1954",
    "title": "perceptually consistent example-based human motion retrieval",
    "abstract": "Large amount of human motion capture data have been increasingly recorded and used in animation and gaming applications. Efficient retrieval of logically similar motions from a large data repository thereby serves as a fundamental basis for these motion data based applications. In this paper we present a perceptually consistent, example-based human motion retrieval approach that is capable of efficiently searching for and ranking similar motion sequences given a query motion input. Our method employs a motion pattern discovery and matching scheme that breaks human motions into a part-based, hierarchical motion representation. Building upon this representation, a fast string match algorithm is used for efficient runtime motion query processing. Finally, we conducted comparative user studies to evaluate the accuracy and perceptual-consistency of our approach by comparing it with the state of the art example-based human motion search algorithms.",
    "keywords": [
      "motion pattern extraction",
      "character animation",
      "motion capture",
      "motion retrieval",
      "perceptual consistency"
    ]
  },
  {
    "id": "1955",
    "title": "Story Tracker: Incremental visual text analytics of news story development",
    "abstract": "Online news sources produce thousands of news articles every day, reporting on local and global real-world events. New information quickly replaces the old, making it difficult for readers to put current events in the context of the past. The stories about these events have complex relationships and characteristics that are difficult to model: they can be weakly or strongly related or they can merge or split over time. In this article, we present a visual analytics system for temporal analysis of news stories in dynamic information streams, which combines interactive visualization and text mining techniques to facilitate the analysis of similar topics that split and merge over time. Text clustering algorithms extract stories from online news streams in consecutive time windows and identify similar stories from the past. The stories are displayed in a visualization, which (1) sorts the stories by minimizing clutter and overlap from edge crossings, (2) shows their temporal characteristics in different time frames with different levels of detail, and (3) allows incremental updates of the display without recalculating the past data. Stories can be interactively filtered by their duration and connectivity in order to be explored in full detail. To demonstrate the system's capabilities for detailed dynamic text stream exploration, we present a use case with real news data about the Arabic Uprising in 2011.",
    "keywords": [
      "news stream analysis",
      "topic evolution",
      "dynamic visualization",
      "text analytics"
    ]
  },
  {
    "id": "1956",
    "title": "improving web accessibility using content-aware plug-ins",
    "abstract": "This paper describes a novel approach to improve blind and visually impaired people's access to the Web by using a content-aware Web browser plug-in coupled with audio and haptic tools. The Web plug-in accesses the current mouse position on-screen, and makes the co-ordinates available to the audio and haptic modalities. This allows the user to be informed when they are in the vicinity of an image or hyperlink; previously they would only have been informed when they are physically on the link. Thus, when the user is close to an image or hyperlink, haptics and audio will be used to inform and guide them to the actual spatial position. The Web browser plug-in and the associated audio and haptic feedback tools are described in the paper. Finally, results from a pilot study on the usability of this system are also presented.",
    "keywords": [
      "assistive technology",
      "auditory",
      "accessibility",
      "web plug-in",
      "haptic interfaces",
      "navigation"
    ]
  },
  {
    "id": "1957",
    "title": "Improved induced matchings in sparse graphs",
    "abstract": "An induced matching in a graph G=(V,E) G = ( V , E ) is a matching M such that (V,M) ( V , M ) is an induced subgraph of G . Clearly, among two vertices with the same neighbourhood (called twins) at most one is matched in any induced matching, and if one of them is matched then there is another matching of the same size that matches the other vertex. Motivated by this, Kanj etal. [10] studied induced matchings in twinless graphs. They showed that any twinless planar graph contains an induced matching of size at least n 40 and that there are twinless planar graphs that do not contain an induced matching of size greater than n 27 + O ( 1 ) . We improve both these bounds to n 28 + O ( 1 ) , which is tight up to an additive constant. This implies that the problem of deciding whether a planar graph has an induced matching of size k has a kernel of size at most 28k 28 k . We also show for the first time that this problem is fixed parameter tractable for graphs of bounded arboricity. Kanj et al. also presented an algorithm which decides in O ( 2 159 k + n ) -time whether an n-vertex planar graph contains an induced matching of size k . Our results improve the time complexity analysis of their algorithm. However, we also show a more efficient O ( 2 25.5 k + n ) -time algorithm. Its main ingredient is a new, O?(4l) O ? ( 4 l ) -time algorithm for finding a maximum induced matching in a graph of branch width at most l .",
    "keywords": [
      "induced matching",
      "sparse graphs",
      "planar graphs",
      "fpt algorithm"
    ]
  },
  {
    "id": "1958",
    "title": "Researching the adoption of ICT in Ethiopia: a case study of small hotels in Addis Ababa",
    "abstract": "Purpose - The purpose of this paper is to analyse the difficulties encountered when researching the adoption of information and communications technology (ICT) by small hotels in Addis Ababa, Ethiopia, and to indicate how some of these difficulties were overcome. Design/methodology/approach - The background and theoretical framework of the research project is described, and the research difficulties analysed in the context of the literature and of experience elsewhere in Africa. Issues such as informed consent are considered from different cultural perspectives. Findings - The conclusion is that an understanding of the political, economic and cultural context is essential to the carrying-out of a successful research project in Ethiopia, and that these same factors are likely to have a major influence on the diffusion of ICT within the country. Practical implications - An Ethiopian case study is of particular interest because unlike Tanzania, Kenya and much of the rest of Africa, the country's telecommunications industry remains in government hands, a broadband connection is very expensive, and the percentage of Ethiopians using the internet is very small - no more than 0.75 percent of the population in 2010 according to the figures of the International Telecommunication Union. Originality/value - Little has been written about small hotels and other small and micro businesses in Addis Ababa, and little or nothing about the setbacks that can occur when researching them. This paper fills some of the gaps in the literature.",
    "keywords": [
      "research process",
      "information and communication technology",
      "small and micro-size business enterprises",
      "hotels",
      "ethiopia",
      "small to medium-sized enterprises",
      "hospitality services"
    ]
  },
  {
    "id": "1959",
    "title": "Mathematical conditions for and physical meaning of a maximum of the determinant of (K)over-tilde(T) in the prebuckling regime",
    "abstract": "It is shown that the determinant of the tangent stiffness matrix has a maximum in the prebuckling regime if and only if the determinant of a specific linear combination of the first and the third derivative of this matrix with respect to a dimensionless load factor vanishes. The mathematical tool for this proof is the so-called consistently linearized eigenproblem in the frame of the Finite Element Method. The physical meaning of the mentioned maximum is the one of,a minimum of the percentage bending energy of the total strain energy. The paper provides mathematical and physical background knowledge on numerical results that were obtained 35 years ago.  ",
    "keywords": [
      "buckling",
      "consistently linearized eigenproblem",
      "maximum of determinant",
      "percentage bending energy",
      "pressure vessel",
      "tangent stiffness matrix"
    ]
  },
  {
    "id": "1960",
    "title": "New Facet of Antiphospholipid Antibodies",
    "abstract": "Abstract: Since the aCL test was first described, several reports have described the heterogeneity of aPL, which binds to different anionic phospholipids, proteins, or to a phospholipid-protein complex. It has been recently reported that antiphospholipids (aPLs) from the sera of patients with the antiphospholipid syndrome (APS) are able to bind some newly identified antigens, the lyso(bis)phosphatidic acid (LBPA), lipid restricted to the late endosomes, and the sulfatides, acidic glycosphingolipids involved in the hemostatic process. Of interest, aLBPAs are present in the sera of a large number of patients with APS showing similar sensitivity and specificity compared to anti-?2 glycoprotein I antibodies (a?2-GPIs) and close association with lupus anticoagulant. Moreover, ?2-GPI binds to sulfatides and the majority of the aPL reacting with cardiolipin-?2-GPI complex also react with the sulfatide-?2-GPI complex. Different mechanisms involved in the production of autoantibodies in autoimmune diseases have been proposed and, among them, apoptosis or programmed cell death seems to play a leading role. The relocation of CL and its metabolites during apoptosis may represent an in vivo trigger for the generation of aCL, and the higher reactivity of sera from APS patients to monolysocardiolipin, the immediate degradation product of mitochondrial CL validates this hypothesis. Finally, increasing evidence suggests that oxidative stress could be a pathogenic link between aPL and thrombosis, and antioxidant treatment may have some efficacy in preventing the clinical manifestations of this syndrome.",
    "keywords": [
      "antiphospholipid antibodies",
      "antiphospholipid syndrome",
      "lysophosphatidic acid",
      "monolysocardiolipin",
      "sulfatides",
      "?2-gpi",
      "apoptosis"
    ]
  },
  {
    "id": "1961",
    "title": "GPS accuracy estimation using map matching techniques: Applied to vehicle positioning and odometer calibration",
    "abstract": "A test-bed application, called Map Matched GPS (MMGPS) processes raw GPS output data, from RINEX files, or GPS derived coordinates. This developed method uses absolute GPS positioning, map matched, to locate the vehicle on a road centre-line, when GPS is known to be sufficiently accurate. MMGPS software has now been adapted to incorporate positioning based on odometer derived distances (OMMGPS), when GPS positions are not available. Relative GPS positions are used to calibrate the odometer. If a GPS position is detected to be inaccurate, it is not used for positioning, or for calibrating the odometer correction factor. In OMMGPS, GPS pseudorange observations are combined with DTM height information and odometer positions to provide a vehicle position at 1s epochs. The described experiment used GPS and odometer observations taken on a London bus on a predefined route in central of London. Therefore, map matching techniques are used to test GPS positioning accuracy, and to identify grossly inaccurate GPS positions. In total, over 15,000 vehicle positions were computed and tested using OMMGPS. In general, the position quality provided by GPS alone was extremely poor, due to multipath effects caused by the urban canyons of central London, so that odometer positioning was used much more often to position the vehicle than GPS. Typically, the ratio is 7:3 odometer positions to GPS positions. In the case of one particular trip, OMMGPS provides a mean error of position of 8.8m compared with 53.7m for raw GPS alone.",
    "keywords": [
      "gps",
      "odometer calibration",
      "map matching"
    ]
  },
  {
    "id": "1962",
    "title": "An algorithm for the class of pure implicational formulas",
    "abstract": "Heusch introduced the notion of pure implicational formulas. He showed that the falsifiability problem for pure implicational formulas with k negations is solvable in time . Such falsifiability results are easily transformed to satisfiability results on CNF formulas. We show that the falsifiability problem for pure implicational formulas is solvable in time , which is polynomial for a fixed k. Thus this problem is fixed-parameter tractable.",
    "keywords": [
      "satisfiability",
      "implicational formulas",
      "fixed parameter tractable",
      "boolean functions"
    ]
  },
  {
    "id": "1963",
    "title": "Spatial and spectral morphological template matching",
    "abstract": "Template matching is a very topical issue in a wide range of imaging applications. Mathematical morphology offers the hit-or-miss transform, an operator which has been successfully applied for template matching in binary images. More recently, it has been extended to grayscale images and even to multivariate images. Nevertheless, these extensions, despite being relevant from a theoretical point-of-view, might lack practical interest due to the inherent difficulty to set up correctly the transform and its parameters (e.g. the structuring functions). In this paper, we propose a new and more intuitive operator which allows for morphological template matching in multivariate images from both a spatial and spectral point of view. We illustrate the potential of this operator in the context of remote sensing.  ",
    "keywords": [
      "hit-or-miss transform",
      "remote sensing",
      "multivariate morphology",
      "template matching"
    ]
  },
  {
    "id": "1964",
    "title": "call for bags",
    "abstract": "Inspired by the theme of Everyday Creativity, we asked people across the world to post their old conference bag(s) to us so that they could be recycled into one-of-a-kind reusable conference bags. Each bag sent was then hand crafted and sculpted by up-and-coming British designer Sarah Atkinson. Unlike many consumer products that we buy today, each bag has its previous history attached.",
    "keywords": [
      "recycled",
      "history",
      "bag",
      "everyday creativity"
    ]
  },
  {
    "id": "1965",
    "title": "A method to develop feasible requirements for Java mobile code application",
    "abstract": "We propose a method for analyzing trade-off between an environment where a Java mobile code application is running and requirements for the application. In particular, we focus on the security-related problems that originate in low-level security policy of the code-centric style of the access control in Java runtime. As the result of this method, we get feasible requirements with respect to security issues of mobile codes. This method will help requirements analysts to compromise the differences between customers goals and realizable solutions. Customers will agree to the results of the analysis by this method because they can clearly trace the reasons why some goals are achieved but others are not. We can clarify which functions can be performed under the environment systematically. We also clarify which functions in mobile codes are needed so as to meet the goals of users by goal oriented requirements analysis(GORA). By comparing functions derived from the environment and functions from the coals. we can find conflicts between the environments and the goals, and also find vagueness of the requirements. By resolving the conflicts and by clarifying the vagueness, we can develop bases for the requirements specification.",
    "keywords": [
      "goal oriented requirements analysis",
      "anti-requirements",
      "security policy",
      "access control",
      "java mobile codes"
    ]
  },
  {
    "id": "1966",
    "title": "Foundations of compositional model theory",
    "abstract": "Graphical Markov models, most of all Bayesian networks, have become a very popular way for multidimensional probability distribution representation and processing. What makes representation of a very-high-dimensional probability distribution possible is its independence structure, i.e. a system of conditional independence relations valid for the distribution in question. The fact that some of independence systems can be successfully represented with the help of graphs is reflected in the general title: graphical modelling. However, graphical representation of independence structures is also associated with some disadvantages: only a small part of different independence structures can be faithfully represented by graphs; and still one structure is usually equally well represented by several graphs. These reasons, among others, initiated development of an alternative approach, called here theory of compositional models, which enables us to represent exactly the same class of distributions as Bayesian networks. This paper is a survey of the most important basic concepts and results concerning compositional models necessary for reading advanced papers on computational procedures and other aspects connected with this (relatively new) approach for multidimensional distribution representation.",
    "keywords": [
      "multidimensional probability distribution",
      "conditional independence",
      "graphical markov model",
      "composition of distributions"
    ]
  },
  {
    "id": "1967",
    "title": "Analyzing dependent proportions in cluster randomized trials: Modeling inter-cluster correlation via copula function",
    "abstract": "When two interventions are randomized to multiple sub-clusters within a whole cluster, accounting for the within sub-cluster (intra-cluster) and between sub-clusters (inter-cluster) correlations is needed to produce valid analyses of the effect of interventions. With the growing interest in copulas and their applications in statistical research, we demonstrate, through applications, how copula functions may be used to account for the correlation among responses across sub-clusters. Copulas having asymmetric dependence property may prove useful for modeling the relationship between random functions especially in clinical, health and environmental sciences where response data are in general skewed. These functions can in general be used to study scale-free measures of dependence, and they can be used as a starting point for constructing families of bivariate distributions, with a view to simulations. The core contribution of this paper is to provide an alternative approach for estimating the inter-cluster correlation using copula to accurately estimate the treatment effect when the outcome variable is measured on the dichotomous scale. Two data sets are used to illustrate the proposed methodology.",
    "keywords": [
      "cluster randomization",
      "correlated proportion",
      "beta binomial distribution",
      "copula function"
    ]
  },
  {
    "id": "1968",
    "title": "Two levels of Bayesian model averaging for optimal control of stochastic systems",
    "abstract": "Bayesian model averaging provides the best possible estimate of a model, given the data. This article uses that approach twice: once to get a distribution of plausible models of the world, and again to find a distribution of plausible control functions. The resulting ensemble gives control instructions different from simply taking the single best-fitting model and using it to find a single lowest-error control function for that single model. The only drawback is, of course, the need for more computer time: this article demonstrates that the required computer time is feasible. The test problem here is from flood control and risk management.",
    "keywords": [
      "risk management",
      "rainfall models",
      "bayesian model averaging",
      "neural networks",
      "control"
    ]
  },
  {
    "id": "1969",
    "title": "SASI: a generic texture descriptor for image retrieval",
    "abstract": "In this paper, a generic texture descriptor, namely, Statistical Analysis of Structural Information (SASI) is introduced as a representation of texture. SASI is based on statistics of clique autocorrelation coefficients, calculated over structuring windows. SASI defines a set of clique windows to extract and measure various structural properties of texture by using a spatial multi-resolution method. Experimental results, performed on various image databases, indicate that SASI is more successful then the Gabor Filter descriptors in capturing small granularities and discontinuities such as sharp corners and abrupt changes. Due to the flexibility in designing the clique windows, SASI reaches higher average retrieval rates compared to Gabor Filter descriptors. However, the price of this performance is increased computational complexity.",
    "keywords": [
      "texture similarity",
      "image retrieval",
      "clique",
      "autocorrelation",
      "descriptor"
    ]
  },
  {
    "id": "1970",
    "title": "Correlations between interfacial reactions and bonding strengths of Cu/Sn/Cu pillar bump",
    "abstract": "An investigation of the effects of thermal annealing on the growth of intermetallic compounds (IMCs) and the shear strength of Cu/Sn/Cu pillar bump structures is presented. The Cu6Sn5 phase formed at the Cu/Sn interface right after bonding and grew as with annealing time, while the Cu3Sn phase formed and grew at the Cu/Cu6Sn5 interface with increased annealing time. IMC growth followed a linear relationship with the square root of the annealing time due to a diffusion-controlled mechanism. The shear strength was measured by the lap shear test and was observed to monotonically decrease with annealing until 150h at 150C. It then stayed nearly constant with further annealing, which is correlated with the change in fracture modes from ductile to brittle at around the 150h mark. This is ascribed not only to the increasing thickness of brittle IMCs but also to the decreasing thickness of Sn, as there exists a critical annealing time for a fracture mode transition in our thinly Sn-capped Cu pillar bump structures.",
    "keywords": [
      "cu/sn/cu pillar bump",
      "shear strength",
      "intermetallic compound",
      "fracture mode"
    ]
  },
  {
    "id": "1971",
    "title": "extracting related named entities from blogosphere for event mining",
    "abstract": "We propose a method of extracting named entities that are related to a single input word. Focusing on the syntactic dependency relation in sentences, it is reasonable to extract a case element that syntactically depends on the predicate that the input word depends on. In Japanese, though, a word which has appeared in a previous sentence is often omitted or replaced. Our proposed method, first, extracts \"predicate patterns\" consisting of case elements with case particles and a predicate. Then it combines predicate patterns that have the same predicate to form possible unabridged dependence relations.",
    "keywords": [
      "information extraction",
      "world wide web",
      "text mining"
    ]
  },
  {
    "id": "1972",
    "title": "hierarchical trust management for wireless sensor networks and its application to trust-based routing",
    "abstract": "In this work, we propose a highly scalable cluster-based hierarchical trust management protocol for wireless sensor networks to effectively deal with selfish or malicious nodes. Unlike prior work, we consider multidimensional trust attributes derived from communication and social networks to evaluate the overall trust of a sensor node. Our peer-to-peer trust evaluation method leverages the cluster-based hierarchical structure for efficient communications. We develop a probability model using stochastic Petri net techniques to analyze the performance of the proposed trust management protocol. We validate the protocol design by comparing subjective trust generated as a result of protocol execution against objective trust obtained from actual node status. We apply our hierarchical trust management protocol to trust-based geographical routing as an application. Our results demonstrate that trust-based geographic routing under identified design settings can approach the ideal performance level achievable by flooding-based routing in message delivery ratio and message delay without incurring substantial message overhead. Furthermore, it can significantly outperform traditional geographic routing protocols that do not use trust concept in selecting forwarding nodes in message delivery ratio over a wide range of design parameter settings.",
    "keywords": [
      "trust-based routing",
      "trust management",
      "wireless sensor networks",
      "geographic routing"
    ]
  },
  {
    "id": "1973",
    "title": "Optimal experiment selection for parameter estimation in biological differential equation models",
    "abstract": "Parameter estimation in biological models is a common yet challenging problem. In this work we explore the problem for gene regulatory networks modeled by differential equations with unknown parameters, such as decay rates, reaction rates, Michaelis-Menten constants, and Hill coefficients. We explore the question to what extent parameters can be efficiently estimated by appropriate experimental selection.",
    "keywords": [
      "systems biology",
      "differential equation models",
      "experimental design",
      "parameter estimation",
      "data fitting"
    ]
  },
  {
    "id": "1974",
    "title": "applying adaptation design patterns",
    "abstract": "Dynamic adaptation may be used to prevent software downtime while new requirements and responses to environmental conditions are incorporated into the system. Previously, we studied over thirty adaptation-related projects to develop twelve adaptation-oriented design patterns that can be leveraged from one adaptive system to another. This paper presents a case study in which we apply our adaptation patterns in the design of a dynamically adaptive news web server. This pattern-based design separates the functional logic from the adaptive logic, resulting in a system that supports dynamic adaptation and is easier to maintain and analyze. Furthermore, to address assurance concerns, we applied automated formal verification techniques to ensure instantiated pattern models satisfy invariant properties specified in each adaptation pattern.",
    "keywords": [
      "autonomic systems",
      "design patterns",
      "adaptive systems"
    ]
  },
  {
    "id": "1975",
    "title": "Subset feedback vertex sets in chordal graphs",
    "abstract": "Given a graph G=(V,E) G = ( V , E ) and a set S?V S ? V , a set U?V U ? V is a subset feedback vertex set of (G,S) ( G , S ) if no cycle in G[V?U] G [ V ? U ] contains a vertex of S. The Subset Feedback Vertex Set problem takes as input G, S, and an integer k , and the question is whether (G,S) ( G , S ) has a subset feedback vertex set of cardinality or weight at most k . Both the weighted and the unweighted versions of this problem are NP-complete on chordal graphs, even on their subclass split graphs. We give an algorithm with running time O(1.6708n) O ( 1.6708 n ) that enumerates all minimal subset feedback vertex sets on chordal graphs on n vertices. As a consequence, Subset Feedback Vertex Set can be solved in time O(1.6708n) O ( 1.6708 n ) on chordal graphs, both in the weighted and in the unweighted case. As a comparison, on arbitrary graphs the fastest known algorithm for these problems has O(1.8638n) O ( 1.8638 n ) running time. We also obtain that a chordal graph G  has at most 1.6708n 1.6708 n minimal subset feedback vertex sets, regardless of S . This narrows the gap with respect to the best known lower bound of 1.5848n 1.5848 n on this graph class. For arbitrary graphs, the gap is substantially wider, as the best known upper and lower bounds are 1.8638n 1.8638 n and 1.5927n 1.5927 n , respectively.",
    "keywords": [
      "chordal graph",
      "subset feedback vertex set",
      "maximum number of minimal subset feedback vertex sets",
      "exact exponential-time algorithm"
    ]
  },
  {
    "id": "1976",
    "title": "a dynamic trust network based simulation framework for reputation-based service selection",
    "abstract": "Service-oriented computing is a promising approach to software system construction by selecting and composing autonomous services under the open, dynamic and non-deterministic Internet environment. Appropriate selection of high quality services used in from numerous candidates declaring similar functionalities is crucial to the overall quality of the composed system. As authority centers are not generally available in open environments, reputation-based mechanisms must be adopted to evaluate services. With more and more reputation systems proposed in the literature, there is an increasing need to evaluate and compare them objectively and systematically with a common controlled experiment of trust network environment. In this paper we propose a general simulation framework based on dynamic trust network for this purpose. Especially, the framework is capable to simulate the dynamic evolutions of the trust network, in addition to the static snapshots of the trust relationships. With this framework, some case studies are made to evaluate the effectiveness of several representative reputation mechanisms, and some interesting characters are revealed.",
    "keywords": [
      "service selection",
      "trust network",
      "internetware",
      "reputation"
    ]
  },
  {
    "id": "1977",
    "title": "an iterative method for calculating approximate gcd of univariate polynomials",
    "abstract": "We present an iterative algorithm for calculating approximate greatest common divisor (GCD) of univariate polynomials with the real coefficients. For a given pair of polynomials and a degree, our algorithm finds a pair of polynomials which has a GCD of the given degree and whose coefficients are perturbed from those in the original inputs, making the perturbations as small as possible, along with the GCD. The problem of approximate GCD is transfered to a constrained minimization problem, then solved with a so-called modified Newton method, which is a generalization of the gradient-projection method, by searching the solution iteratively. We demonstrate that our algorithm calculates approximate GCD with perturbations as small as those calculated by a method based on the structured total least norm (STLN) method, while our method runs significantly faster than theirs by approximately up to 30 times, compared with their implementation. We also show that our algorithm properly handles some ill-conditioned problems with GCD containing small or large leading coefficient.",
    "keywords": [
      "ill-conditioned problem",
      "approximate polynomial gcd",
      "optimization",
      "gradient-projection method"
    ]
  },
  {
    "id": "1978",
    "title": "Standardization of islet isolation outcome - A new automatic system to determine pancreatic islet viability",
    "abstract": "Pancreatic islet transplantation is emerging as a therapeutic approach for patients affected by diabetes. This approach consists of a minimally invasive procedure replacing insulin-producing cells (pancreatic islets). The technique has been proven successful, but limitations have been identified. One of the major challenges of the procedure is the counting of the isolated pancreatic islets, which is currently jeopardized by subjectivity and inaccuracy. Determination of the accurate islet number is a crucial factor in determining the correlation between the isolation product and clinical outcome. In the proposed study, we have developed software capable of objectively evaluating islet numbers and other viability variables by image analysis. This software is based on image processing and feature extraction algorithms for recognition of the area of interest. This is the first step toward standardization of the isolation outcome and potential clinical success predictability.  ",
    "keywords": [
      "image analysis",
      "feature extraction",
      "pancreatic islets transplantation"
    ]
  },
  {
    "id": "1979",
    "title": "Cytotoxic effect and molecular docking of 4-ethoxycarbonylmethyl-1-(piperidin-4-ylcarbonyl)-thiosemicarbazide-a novel topoisomerase II inhibitor",
    "abstract": "The preliminary cytotoxic effect of 4-ethoxycarbonylmethyl-1-(piperidin-4-ylcarbonyl)-thiosemicarbazide hydrochloride (1)-a potent topoisomerase II inhibitor-was measured using a MTT assay. It was found that the compound decreased the number of viable cells in both estrogen receptor-positive MCF-7 and estrogen receptor-negative MDA-MB-231breast cancer cells, with IC50 values of 146 +/- 2 and 132 +/- 2 mu M, respectively. To clarify the molecular basis of the inhibitory action of 1, molecular docking studies were carried out. The results suggest that 1 targets the ATP binding pocket.",
    "keywords": [
      "thiosemicarbazide derivative",
      "human topoisomerase ii",
      "cytotoxicity",
      "molecular docking",
      "dft calculation"
    ]
  },
  {
    "id": "1980",
    "title": "Numerical modelling of complex turbulent free-surface flows with the SPH method: an overview",
    "abstract": "The gridless smoothed particle hydrodynamics (SPH) method is now commonly used in computational fluid dynamics (CFD) and appears to be promising in predicting complex free-surface flows. However, increasing flow complexity requires appropriate approaches for taking account of turbulent effects, whereas some authors are still working without any turbulence closure in SPH. A review of recently developed turbulence models adapted to the SPH method is presented herein, from the simplistic point of view of a one-equation model involving mixing length to more sophisticated (and thus realistic) models like explicit algebraic Reynolds stress models (EARSM) or large eddy simulation (LES). Each proposed model is tested and validated on the basis of schematic cases for which laboratory data, theoretical or numerical solutions are available in the general field of turbulent free-surface incompressible flows (e.g. open-channel flow and schematic dam break). They give satisfactory results, even though some progress should be made in the future in terms of free-surface influence and wall conditions. Recommendations are given to SPH users to apply this method to the modelling of complex free-surface turbulent flows. ",
    "keywords": [
      "sph",
      "turbulence",
      "k-epsilon model",
      "earsm",
      "les",
      "gridless methods"
    ]
  },
  {
    "id": "1981",
    "title": "Application of the Viable System Model to analyse communications structures: A case study of disaster response in Japan",
    "abstract": "Recent reviews in EJOR (and JORS) have studied OR's contribution to disaster response. Each review finds little contribution from soft OR or modelling disaster information. Effective communication is vital to manage disaster response resources. The Viable System Model (VSM) can examine rapid disaster communication viability. Four disaster case studies use VSM to identify structural communication faults.",
    "keywords": [
      "viable system model",
      "soft or",
      "systems thinking",
      "communication",
      "disasters"
    ]
  },
  {
    "id": "1982",
    "title": "On the stability analysis of high order Sigma-Delta modulators",
    "abstract": "In this paper we present an approach for stability analysis of high order Sigma-Delta modulators. The approach is based on a parallel decomposition of the modulator. In this representation, the general N-th order modulator is transformed into decomposition of low order modulators, which interact only through the quantizer function. In the simplest case of the loop filter transfer function with real distinct poles, the low order modulators are N first order ones. The decomposition considered helps to extract the sufficient conditions for stability of the N-th order modulator. They are determined by the stability conditions of each of the low order modulators but shifted with respect to the origin of the quantizer function, because of the influence of all other low order modulators. The approach is generalized for the case of repeated poles of the loop filter transfer function.",
    "keywords": [
      "sigma-delta modulator",
      "quantizer function",
      "parallel decomposition",
      "stability analysis"
    ]
  },
  {
    "id": "1983",
    "title": "Cardio-vascular safety beyond hERG: in silico modelling of a guinea pig right atrium assay",
    "abstract": "As chemists can easily produce large numbers of new potential drug candidates, there is growing demand for high capacity models that can help in driving the chemistry towards efficacious and safe candidates before progressing towards more complex models. Traditionally, the cardiovascular (CV) safety domain plays an important role in this process, as many preclinical CV biomarkers seem to have high prognostic value for the clinical outcome. Throughout the industry, traditional ion channel binding data are generated to drive the early selection process. Although this assay can generate data at high capacity, it has the disadvantage of producing high numbers of false negatives. Therefore, our company applies the isolated guinea pig right atrium (GPRA) assay early-on in discovery. This functional multi-channel/multi-receptor model seems much more predictive in identifying potential CV liabilities. Unfortunately however, its capacity is limited, and there is no room for full automation. We assessed the correlation between ion channel binding and the GPRA's Rate of Contraction (RC), Contractile Force (CF), and effective refractory frequency (ERF) measures assay using over six thousand different data points. Furthermore, the existing experimental knowledge base was used to develop a set of in silico classification models attempting to mimic the GPRA inhibitory activity. The Na < ve Bayesian classifier was used to built several models, using the ion channel binding data or in silico computed properties and structural fingerprints as descriptors. The models were validated on an independent and diverse test set of 200 reference compounds. Performances were assessed on the bases of their overall accuracy, sensitivity and specificity in detecting both active and inactive molecules. Our data show that all in silico models are highly predictive of actual GPRA data, at a level equivalent or superior to the ion channel binding assays. Furthermore, the models were interpreted in terms of the descriptors used to highlight the undesirable areas in the explored chemical space, specifically regions of low polarity, high lipophilicity and high molecular weight. In conclusion, we developed a predictive in silico model of a complex physiological assay based on a large and high quality set of experimental data. This model allows high throughput in silico safety screening based on chemical structure within a given chemical space.",
    "keywords": [
      "in silico",
      "in vitro",
      "computational",
      "herg",
      "cardio vascular safety"
    ]
  },
  {
    "id": "1984",
    "title": "Quadratic curve and surface fitting via squared distance minimization",
    "abstract": "Quadratic curve and surface fitting to a set of data points are fundamental problems in reverse engineering and many other application areas. We develop the fitting methods for quadratic curves and surfaces based on the squared distance minimization technology. The basic idea of squared distance minimization for curve and surface fitting is first presented. Then we devise the corresponding squared distance term for each quadratic curve and surface, and minimize it to obtain its parameters. We repeat the squared distance minimization and update the parameters of the quadratic curve and surface by iterations until convergency. Consequently, the final fitting result is achieved. Experimental results demonstrate the effectiveness of the fitting method.",
    "keywords": [
      "quadratic curve fitting",
      "quadratic surface fitting",
      "point cloud approximation",
      "shape reconstruction",
      "squared distance minimization"
    ]
  },
  {
    "id": "1985",
    "title": "Persistent Betti numbers for a noise tolerant shape-based approach to image retrieval",
    "abstract": "In content-based image retrieval a major problem is the presence of noisy shapes. Noise can present itself not only in the form of continuous deformations, but also as topological changes. It is well known that persistent Betti numbers are a shape descriptor that admits dissimilarity distances stable under continuous shape deformations. In this paper we focus on the problem of dealing with noise that alters the topology of the studied objects. We present a general method to turn persistent Betti numbers into stable descriptors also in the presence of topological changes. Retrieval tests on the Kimia-99 database show the effectiveness of the method.",
    "keywords": [
      "multidimensional persistent homology",
      "hausdorff distance",
      "symmetric difference distance"
    ]
  },
  {
    "id": "1986",
    "title": "Skepticism relations for comparing argumentation semantics",
    "abstract": "The issue of formalizing skepticism relations between argumentation semantics has been considered only recently in the literature. In this paper we provide a twofold contribution to this kind of analysis. First, starting from the traditional concepts of skeptical and credulous acceptance, we introduce a comprehensive set of seven skepticism relations, which provide a formal Counterpart to several alternative notions of skepticism at an intuitive level. Then we carry out a systematic comparison of a significant set of literature semantics (namely grounded, complete, preferred, stable, semi-stable, ideal. prudent and CF2 sennantics) on the basis of the proposed skepticism relations.  ",
    "keywords": [
      "argumentation theory",
      "argumentation semantics",
      "skepticism relations"
    ]
  },
  {
    "id": "1987",
    "title": "MARC record conversion: a generalised approach1",
    "abstract": "The problem of different bibliographic catalogue encodings presents a serious obstacle hampering international inter-library communications and worldwide library search. This paper presents a utility for manipulating hierarchically organised data, which was built to perform MARC record conversion. Data representation and conversion language are described in detail. Existing standards for bibliographic record representation are considered and requirements for their conversion are gathered. The impact of MARC processing requirements on the designed generic conversion language is assessed. Character set conversions, accompanying the record translation procedure, are not discussed in this paper.",
    "keywords": [
      "library",
      "marc",
      "conversion",
      "bibliographic record",
      "schema"
    ]
  },
  {
    "id": "1988",
    "title": "The Role of Tuberin in Cellular Differentiation: Are B-Raf and MAPK Involved",
    "abstract": "Abstract: Tuberous sclerosis complex (TSC) is a tumor suppressor gene syndrome whose manifestations can include seizures, mental retardation, autism, and tumors in the brain, retina, kidney, heart, and skin. The products of the TSC1 and TSC2 genes, hamartin and tuberin, respectively, heterodimerize and inhibit the mammalian target of rapamycin (mTOR). This review focuses on the genetic and biochemical basis of the renal and pulmonary manifestations of TSC, angiomyolipomas, and lymphangiomyomatosis, respectively. Genetic analyses of sporadic angiomyolipomas revealed that all three components (smooth muscle, vessels, and fat) derive from a common progenitor cell, indicating the ability of cells lacking tuberin to differentiate into multiple lineages. Other genetic studies showed that the benign smooth muscle cells of pulmonary lymphangiomyomatosis have the ability to migrate to other organs. These findings suggest that tuberin and hamartin play a role in the regulation of cellular migration and differentiation. We have found that tuberin activates B-Raf kinase and p42/44 MAPK and that cells lacking tuberin have low levels of B-Raf activity. We hypothesize that aberrant B-Raf activity in angiomyolipomas leads to abnormal cellular differentiation and migration.",
    "keywords": [
      "tsc",
      "tuberous sclerosis complex",
      "mtor",
      "lam",
      "lymphangiomyomatosis",
      "tsc2",
      "tuberin",
      "tsc1",
      "hamartin",
      "rheb",
      "mapk",
      "b-raf",
      "c-raf"
    ]
  },
  {
    "id": "1989",
    "title": "Data abstractions: Why and how",
    "abstract": "The relational data model has become the standard for mainstream database processing despite its well-known weakness in the area of representing application semantics. The research community's response to this situation has been the development of a collection of semantic data models that allow more of the meaning of information to be represented in a database. The primary tool for accomplishing this has been the use of various data abstractions, most commonly: inclusion, aggregation and association. This paper develops a general model for analyzing data abstractions, and then applies it to these three best-known abstractions.  ",
    "keywords": [
      "data abstractions",
      "inclusion",
      "aggregation",
      "association",
      "relational data model"
    ]
  },
  {
    "id": "1990",
    "title": "Euclidean group invariant computation of stochastic completion fields using shiftable-twistable functions",
    "abstract": "We describe a method for computing the likelihood that a completion joining two contour fragments passes through any given position and orientation in the image plane. Like computations in primary visual cortex ( and unlike all previous models of contour completion), the output of our computation is invariant under rotations and translations of the input pattern. This is achieved by representing the input, output, and intermediate states of the computation in a basis of shiftable-twistable functions.",
    "keywords": [
      "boundary completion",
      "euclidean invariant computation",
      "visual cortex",
      "shiftable basis",
      "fokker-planck equation"
    ]
  },
  {
    "id": "1991",
    "title": "An evaluation of simulation to support contract costing",
    "abstract": "As global markets have become more customer oriented, rapid response rates are now often among the most important metrics in business. To achieve the required agility many companies have turned to outsourcing in order to focus on developing their core activities. By its nature, outsourcing increases the complexity of supply chain networks as more companies are drawn into global logistics networks. The relationships between each of these companies are controlled by contractual agreements. The fast pace of modern industry means that these contracts are entered into relatively quickly often without a full understanding of the true cost implications. This paper presents research conducted as part of a project with the aim of developing contract costing software for outsourcing enterprises. Findings are presented from a study conducted on a number of companies in the electronics sector. A simulation-based study focused on one of these companies, with some associated experimentation, is also presented.",
    "keywords": [
      "outsourcing",
      "contract costing",
      "simulation"
    ]
  },
  {
    "id": "1992",
    "title": "An in-process customer utility prediction system for product conceptualisation",
    "abstract": "Customer involvement in new product development, especially in the early stage of product conceptualisation, plays an important role for a successful product. In this study, a customer utility prediction system (CUPS) is proposed. The system comprises two modules, namely design knowledge acquisition module and customer utility evaluation module. In the design knowledge acquisition module, a knowledge acquisition technique called general sorting is utilised to establish a design knowledge hierarchy (DKH), in which design options can be generated. In the same module, customer voices towards diverse design options called customer-sensitive design criteria are solicited from customer requirements. Subsequently, in the customer utility evaluation module, a measurement for customer desirability, i.e. customer utility index (CUI), is formulated using conjoint analysis (CA) technique. Finally, the rated criteria are also used as inputs to a radial basis function (RBF) neural network for in-process customer utility prediction. A case study on cellular phone design is used to illustrate the proposed approach.",
    "keywords": [
      "product conceptualisation",
      "design knowledge hierarchy",
      "customer utility index",
      "in-process customer utility prediction",
      "conjoint analysis",
      "radial basis function neural network"
    ]
  },
  {
    "id": "1993",
    "title": "The there exists*for all* part of the theory of ground term algebra modulo an AC symbol is undecidable",
    "abstract": "We show that the There Exists*For All* part of the equational theory modulo an AC symbol is undecidable. This solves the open problem 25 from the RTA list. We show that this result holds also for the equational theory modulo an ACI symbol. ",
    "keywords": [
      "unification",
      "equational theories",
      "decidability"
    ]
  },
  {
    "id": "1994",
    "title": "Higher Order Coherent Pairs",
    "abstract": "In this paper, we study necessary and sufficient conditions for the relation $$begin{array}{@{}l}P_n^{{[r]}}(x) + a_{n-1,r} P_{n-1}^{{[r]}}(x)= R_{n-r}(x) + b_{n-1,r} R_{n-r-1}(x),[5pt]quad a_{n-1,r}neq0, ngeq r+1,end{array}$$ where {P n (x)} n?0 and {R n (x)} n?0 are two sequences of monic orthogonal polynomials with respect to the quasi-definite linear functionals (mathcal{U},mathcal{V}), respectively, or associated with two positive Borel measures ? 0,? 1 supported on the real line. We deduce the connection with Sobolev orthogonal polynomials, the relations between these functionals as well as their corresponding formal Stieltjes series. As sake of example, we find the coherent pairs when one of the linear functionals is classical.",
    "keywords": [
      "coherent pairs",
      "sobolev inner product",
      "stieltjes functions",
      "semiclassical linear functionals",
      "orthogonal polynomials",
      ""
    ]
  },
  {
    "id": "1995",
    "title": "Adaptive fuzzy control for chaotic systems with H-infinity tracking performance",
    "abstract": "This paper discusses an adaptive fuzzy controller design method for chaotic systems in the presence of completely unknown nonlinearities and disturbances. All the unknown nonlinear functions are estimated by dynamic fuzzy approximators so that the adaptive fuzzy control approach can be employed in the chaotic systems. Moreover, Hinfinity control method is applied to attenuate the effect of the modeling errors and disturbances in tracking control. An example is given to illustrate the effectiveness of this approach.  ",
    "keywords": [
      "fuzzy control",
      "chaotic system",
      "h-infinity tracking performance"
    ]
  },
  {
    "id": "1996",
    "title": "A characterization of tree-like Resolution size",
    "abstract": "We explain an asymmetric ProverDelayer game which precisely characterizes proof size in tree-like Resolution. Our proof method allows to always achieve the optimal lower bounds. Intuitive information-theoretic interpretation of the game.",
    "keywords": [
      "computational complexity",
      "proof complexity",
      "proverdelayer games",
      "resolution"
    ]
  },
  {
    "id": "1997",
    "title": "Design of an Embedded Arabic Optical Character Recognition",
    "abstract": "This work presents an embedded Arabic OCR system. The proposed system is compact and portable which make it useful for many applications such as blind assistance and language translation. OCR system consists of the sub-systems: image acquisition, pre-processing, segmentation, feature extraction, classification, and post- processing. For each sub-system there are several of algorithms and techniques to be implemented. Working with PCs gives the designer freedom to select the algorithms and techniques according to the required performance, reliability and reusability. However with the embedded systems we are facing many problems and challenges. Such challenges are associated with memory, speed, and computational power. FPGA is selected as the hardware platform for realizing that recognition task. An OCR system is designed and implemented on PC. Then this system is transferred to FPGA after a set of optimization procedures. Utilizing the features of FPGA technology, Hardware / Software co-design is accomplished on an FPGA board. In that design the systems is partitioned into software modules and hardware components to get the advantages of software flexibility and hardware speed. A database of 3000 Arabic characters is used to train and test the performance of the system. The effects of changing the number of features and classification parameters on accuracy, memory and speed are measured. Design points are selected in order to improve the memory required, speed and computation power without affecting the accuracy.",
    "keywords": [
      "arabic character recognition",
      "embedded systems",
      "fpga",
      "discriminant functions"
    ]
  },
  {
    "id": "1998",
    "title": "MUCS: A model for ubiquitous commerce support",
    "abstract": "The evolution of computing technology and the emergence of wireless networks have contributed to the miniaturization of mobile devices and their increase in power, providing services anywhere and anytime. In this context, new opportunities for technology support have arisen in different areas, for example, education, games and entertainment, automobile, and commerce. We propose a model for ubiquitous commerce support (MUCS). This model uses ubiquitous computing concepts to look for deal opportunities for users who act either as buyers or sellers. This paper also describes two everyday scenarios, in which the MUCS model could be applied, and explains the implemented prototype to be used in them. Finally, we present the results obtained from a practical experiment, which was performed with the participation of users who filled out an evaluation questionnaire. ",
    "keywords": [
      "buyer and seller support",
      "design science",
      "experiments",
      "prototype development",
      "ubiquitous computing",
      "ubiquitous commerce"
    ]
  },
  {
    "id": "1999",
    "title": "Reducing the number of ancilla qubits and the gate count required for creating large controlled operations",
    "abstract": "In this paper, we show that it is possible to adapt a qudit scheme for creating a controlled-Toffoli created by Ralph et al. (Phys Rev A 75:022313, 2007) to be applicable to qubits. While this scheme requires more gates than standard schemes for creating large controlled gates, we show that with simple adaptations, it is directly equivalent to the standard scheme in the literature. This scheme is the most gate-efficient way of creating large controlled unitaries currently known; however, it is expensive in terms of the number of ancilla qubits used. We go on to show that using a combination of these standard techniques presented by Barenco et al. (Phys Rev A 52(5):3457, 1995), we can create an n-qubit version of the Toffoli using less gates and the same number of ancilla qubits as recent work using computer optimization. This would be useful in any architecture of quantum computing where gates are cheap but qubit initialization is expensive.",
    "keywords": [
      "quantum computing",
      "gate decompositions",
      "resource reduction",
      "mutli-qubit operations"
    ]
  }
]